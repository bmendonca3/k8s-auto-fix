[
  {
    "id": "201",
    "manifest_path": "data/manifests/artifacthub/bitnami/kafka/011_statefulset_release-name-kafka-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-kafka-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\nspec:\n  podManagementPolicy: Parallel\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: kafka\n      app.kubernetes.io/component: controller-eligible\n      app.kubernetes.io/part-of: kafka\n  serviceName: release-name-kafka-controller-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: kafka\n        app.kubernetes.io/version: 4.0.0\n        helm.sh/chart: kafka-32.4.3\n        app.kubernetes.io/component: controller-eligible\n        app.kubernetes.io/part-of: kafka\n      annotations:\n        checksum/configuration: d96601aff02b0e88a9bc5c8593a6d0446462e05650b1eb84af185e551160d1c8\n        checksum/secret: dba2cdb043e84e63d768aad3292379237050a24915b1b3e70c1f2408e8cd76e8\n    spec:\n      automountServiceAccountToken: false\n      hostNetwork: false\n      hostIPC: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: kafka\n                  app.kubernetes.io/component: controller-eligible\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        seccompProfile:\n          type: RuntimeDefault\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-kafka\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-config\n        image: docker.io/bitnami/kafka:4.0.0-debian-12-r10\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add: []\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \". /opt/bitnami/scripts/libkafka.sh\\nconfigure_kafka_sasl() {\\n    # Replace\\\n          \\ placeholders with passwords\\n    replace_in_file \\\"$KAFKA_CONF_FILE\\\"\\\n          \\ \\\"interbroker-password-placeholder\\\" \\\"$KAFKA_INTER_BROKER_PASSWORD\\\"\\n\\\n          \\    replace_in_file \\\"$KAFKA_CONF_FILE\\\" \\\"controller-password-placeholder\\\"\\\n          \\ \\\"$KAFKA_CONTROLLER_PASSWORD\\\"\\n    read -r -a passwords <<< \\\"$(tr ',;'\\\n          \\ ' ' <<<\\\"${KAFKA_CLIENT_PASSWORDS:-}\\\")\\\"\\n    for ((i = 0; i < ${#passwords[@]};\\\n          \\ i++)); do\\n        replace_in_file \\\"$KAFKA_CONF_FILE\\\" \\\"password-placeholder-${i}\\\\\\\n          \\\"\\\" \\\"${passwords[i]}\\\\\\\"\\\"\\n    done\\n}\\n\\ncp /configmaps/server.properties\\\n          \\ $KAFKA_CONF_FILE\\n\\n# Get pod ID and role, last and second last fields\\\n          \\ in the pod name respectively\\nPOD_ID=\\\"${MY_POD_NAME##*-}\\\"\\nPOD_ROLE=\\\"\\\n          ${MY_POD_NAME%-*}\\\"; POD_ROLE=\\\"${POD_ROLE##*-}\\\"\\n\\n# Configure node.id\\n\\\n          ID=$((POD_ID + KAFKA_MIN_ID))\\n[[ -f \\\"/bitnami/kafka/data/meta.properties\\\"\\\n          \\ ]] && ID=\\\"$(grep \\\"node.id\\\" /bitnami/kafka/data/meta.properties | awk\\\n          \\ -F '=' '{print $2}')\\\"\\nkafka_server_conf_set \\\"node.id\\\" \\\"$ID\\\"\\n# Configure\\\n          \\ initial controllers\\nif [[ \\\"controller\\\" =~ \\\"$POD_ROLE\\\" ]]; then\\n\\\n          \\    INITIAL_CONTROLLERS=()\\n    for ((i = 0; i < 3; i++)); do\\n       \\\n          \\ var=\\\"KAFKA_CONTROLLER_${i}_DIR_ID\\\"; DIR_ID=\\\"${!var}\\\"\\n        [[ $i\\\n          \\ -eq $POD_ID ]] && [[ -f \\\"/bitnami/kafka/data/meta.properties\\\" ]] &&\\\n          \\ DIR_ID=\\\"$(grep \\\"directory.id\\\" /bitnami/kafka/data/meta.properties |\\\n          \\ awk -F '=' '{print $2}')\\\"\\n        INITIAL_CONTROLLERS+=(\\\"${i}@${KAFKA_FULLNAME}-${POD_ROLE}-${i}.${KAFKA_CONTROLLER_SVC_NAME}.${MY_POD_NAMESPACE}.svc.${CLUSTER_DOMAIN}:${KAFKA_CONTROLLER_PORT}:${DIR_ID}\\\"\\\n          )\\n    done\\n    echo \\\"${INITIAL_CONTROLLERS[*]}\\\" | awk -v OFS=',' '{$1=$1}1'\\\n          \\ > /shared/initial-controllers.txt\\nfi\\nreplace_in_file \\\"$KAFKA_CONF_FILE\\\"\\\n          \\ \\\"advertised-address-placeholder\\\" \\\"${MY_POD_NAME}.${KAFKA_FULLNAME}-${POD_ROLE}-headless.${MY_POD_NAMESPACE}.svc.${CLUSTER_DOMAIN}\\\"\\\n          \\nsasl_env_vars=(\\n  KAFKA_CLIENT_PASSWORDS\\n  KAFKA_INTER_BROKER_PASSWORD\\n\\\n          \\  KAFKA_INTER_BROKER_CLIENT_SECRET\\n  KAFKA_CONTROLLER_PASSWORD\\n  KAFKA_CONTROLLER_CLIENT_SECRET\\n\\\n          )\\nfor env_var in \\\"${sasl_env_vars[@]}\\\"; do\\n    file_env_var=\\\"${env_var}_FILE\\\"\\\n          \\n    if [[ -n \\\"${!file_env_var:-}\\\" ]]; then\\n        if [[ -r \\\"${!file_env_var:-}\\\"\\\n          \\ ]]; then\\n            export \\\"${env_var}=$(< \\\"${!file_env_var}\\\")\\\"\\n\\\n          \\            unset \\\"${file_env_var}\\\"\\n        else\\n            warn \\\"\\\n          Skipping export of '${env_var}'. '${!file_env_var:-}' is not readable.\\\"\\\n          \\n        fi\\n    fi\\ndone\\nconfigure_kafka_sasl\\nif [[ -f /secret-config/server-secret.properties\\\n          \\ ]]; then\\n    cat /secret-config/server-secret.properties >> $KAFKA_CONF_FILE\\n\\\n          fi\\n\"\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_FULLNAME\n          value: release-name-kafka\n        - name: CLUSTER_DOMAIN\n          value: cluster.local\n        - name: KAFKA_VOLUME_DIR\n          value: /bitnami/kafka\n        - name: KAFKA_CONF_FILE\n          value: /config/server.properties\n        - name: KAFKA_MIN_ID\n          value: '0'\n        - name: KAFKA_CONTROLLER_SVC_NAME\n          value: release-name-kafka-controller-headless\n        - name: KAFKA_CONTROLLER_PORT\n          value: '9093'\n        - name: KAFKA_CONTROLLER_0_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-0-id\n        - name: KAFKA_CONTROLLER_1_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-1-id\n        - name: KAFKA_CONTROLLER_2_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-2-id\n        - name: KAFKA_CLIENT_USERS\n          value: user1\n        - name: KAFKA_CLIENT_PASSWORDS_FILE\n          value: /opt/bitnami/kafka/config/secrets/client-passwords\n        - name: KAFKA_INTER_BROKER_USER\n          value: inter_broker_user\n        - name: KAFKA_INTER_BROKER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/inter-broker-password\n        - name: KAFKA_CONTROLLER_USER\n          value: controller_user\n        - name: KAFKA_CONTROLLER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/controller-password\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/kafka\n        - name: kafka-config\n          mountPath: /config\n        - name: kafka-configmaps\n          mountPath: /configmaps\n        - name: kafka-secret-config\n          mountPath: /secret-config\n        - name: tmp\n          mountPath: /tmp\n        - name: init-shared\n          mountPath: /shared\n        - name: kafka-sasl\n          mountPath: /opt/bitnami/kafka/config/secrets\n          readOnly: true\n      containers:\n      - name: kafka\n        image: docker.io/bitnami/kafka:4.0.0-debian-12-r10\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n        env:\n        - name: KAFKA_HEAP_OPTS\n          value: -XX:InitialRAMPercentage=75 -XX:MaxRAMPercentage=75\n        - name: KAFKA_CFG_PROCESS_ROLES\n          value: controller,broker\n        - name: KAFKA_INITIAL_CONTROLLERS_FILE\n          value: /shared/initial-controllers.txt\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: KAFKA_KRAFT_CLUSTER_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: cluster-id\n        - name: KAFKA_KRAFT_BOOTSTRAP_SCRAM_USERS\n          value: 'true'\n        - name: KAFKA_CLIENT_USERS\n          value: user1\n        - name: KAFKA_CLIENT_PASSWORDS_FILE\n          value: /opt/bitnami/kafka/config/secrets/client-passwords\n        - name: KAFKA_INTER_BROKER_USER\n          value: inter_broker_user\n        - name: KAFKA_INTER_BROKER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/inter-broker-password\n        - name: KAFKA_CONTROLLER_USER\n          value: controller_user\n        - name: KAFKA_CONTROLLER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/controller-password\n        ports:\n        - name: controller\n          containerPort: 9093\n        - name: client\n          containerPort: 9092\n        - name: interbroker\n          containerPort: 9094\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - pgrep\n            - -f\n            - kafka\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: controller\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/kafka\n        - name: logs\n          mountPath: /opt/bitnami/kafka/logs\n        - name: kafka-config\n          mountPath: /opt/bitnami/kafka/config/server.properties\n          subPath: server.properties\n        - name: tmp\n          mountPath: /tmp\n        - name: init-shared\n          mountPath: /shared\n        - name: kafka-sasl\n          mountPath: /opt/bitnami/kafka/config/secrets\n          readOnly: true\n      volumes:\n      - name: kafka-configmaps\n        configMap:\n          name: release-name-kafka-controller-configuration\n      - name: kafka-secret-config\n        emptyDir: {}\n      - name: kafka-config\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: init-shared\n        emptyDir: {}\n      - name: kafka-sasl\n        projected:\n          sources:\n          - secret:\n              name: release-name-kafka-user-passwords\n      - name: logs\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kafka\" not found"
  },
  {
    "id": "202",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/003_poddisruptionbudget_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "203",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/004_poddisruptionbudget_release-name-keycloak.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: keycloak\n      app.kubernetes.io/component: keycloak\n      app.kubernetes.io/part-of: keycloak\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "204",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/010_service_release-name-postgresql-hl.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "205",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/011_service_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "206",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/012_service_release-name-keycloak-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-keycloak-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: http\n    port: 8080\n    protocol: TCP\n    targetPort: http\n  publishNotReadyAddresses: true\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:keycloak app.kubernetes.io/instance:release-name app.kubernetes.io/name:keycloak app.kubernetes.io/part-of:keycloak])"
  },
  {
    "id": "207",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/013_service_release-name-keycloak.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:keycloak app.kubernetes.io/instance:release-name app.kubernetes.io/name:keycloak app.kubernetes.io/part-of:keycloak])"
  },
  {
    "id": "208",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/014_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 17.6.0\n        helm.sh/chart: postgresql-16.7.26\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnami/postgresql:17.6.0-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_USER\n          value: bn_keycloak\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/password\n        - name: POSTGRES_POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRES_DATABASE\n          value: bitnami_keycloak\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"bn_keycloak\" -d \"dbname=bitnami_keycloak\" -h 127.0.0.1\n              -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"bn_keycloak\" -d \"dbname=bitnami_keycloak\" -h 127.0.0.1\n              -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-postgresql\" not found"
  },
  {
    "id": "209",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/015_statefulset_release-name-keycloak.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  podManagementPolicy: Parallel\n  serviceName: release-name-keycloak-headless\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: keycloak\n      app.kubernetes.io/component: keycloak\n      app.kubernetes.io/part-of: keycloak\n  template:\n    metadata:\n      annotations:\n        checksum/configmap-env-vars: 32b97b2f95a4b4c37d1e7ba71916ca4c1f73d024fd8a3e077f2c86fba821b469\n        checksum/secrets: 04a6ccee11f98c05bad75a87ca3f46ce99e265b4b5d35d83a1a541641c64d855\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: keycloak\n        app.kubernetes.io/version: 26.3.3\n        helm.sh/chart: keycloak-25.2.0\n        app.kubernetes.io/component: keycloak\n        app.kubernetes.io/part-of: keycloak\n    spec:\n      serviceAccountName: release-name-keycloak\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: keycloak\n                  app.kubernetes.io/component: keycloak\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-write-dirs\n        image: docker.io/bitnami/keycloak:26.3.3-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - '. /opt/bitnami/scripts/liblog.sh\n\n\n          info \"Copying writable dirs to empty dir\"\n\n          # In order to not break the application functionality we need to make some\n\n          # directories writable, so we need to copy it to an empty dir volume\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/lib/quarkus /emptydir/app-quarkus-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/data /emptydir/app-data-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/providers /emptydir/app-providers-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/themes /emptydir/app-themes-dir\n\n          info \"Copy operation completed\"\n\n          '\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: keycloak\n        image: docker.io/bitnami/keycloak:26.3.3-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        envFrom:\n        - configMapRef:\n            name: release-name-keycloak-env-vars\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        - name: discovery\n          containerPort: 7800\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 1\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /realms/master\n            port: http\n            scheme: HTTP\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /bitnami/keycloak\n          subPath: app-volume-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/lib/quarkus\n          subPath: app-quarkus-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/data\n          subPath: app-data-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/providers\n          subPath: app-providers-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/themes\n          subPath: app-themes-dir\n        - name: keycloak-secrets\n          mountPath: /opt/bitnami/keycloak/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: keycloak-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-keycloak\n          - secret:\n              name: release-name-postgresql\n              items:\n              - key: password\n                path: db-password\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-keycloak\" not found"
  },
  {
    "id": "210",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/002_poddisruptionbudget_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "211",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/006_service_release-name-mariadb-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\nspec:\n  type: ClusterIP\n  publishNotReadyAddresses: true\n  clusterIP: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/part-of: mariadb\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb app.kubernetes.io/part-of:mariadb])"
  },
  {
    "id": "212",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/007_service_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb])"
  },
  {
    "id": "213",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mariadb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "214",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "215",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-mariadb\" not found"
  },
  {
    "id": "216",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/003_poddisruptionbudget_release-name-minio-console.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: console\n      app.kubernetes.io/part-of: minio\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "217",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/004_poddisruptionbudget_release-name-minio.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: minio\n      app.kubernetes.io/part-of: minio\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "218",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/008_service_release-name-minio-console.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 9090\n    targetPort: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:console app.kubernetes.io/instance:release-name app.kubernetes.io/name:minio app.kubernetes.io/part-of:minio])"
  },
  {
    "id": "219",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/009_service_release-name-minio.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  type: ClusterIP\n  ports:\n  - name: tcp-api\n    port: 9000\n    targetPort: api\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:minio app.kubernetes.io/instance:release-name app.kubernetes.io/name:minio app.kubernetes.io/part-of:minio])"
  },
  {
    "id": "220",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/010_deployment_release-name-minio.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: minio\n      app.kubernetes.io/part-of: minio\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: minio\n        app.kubernetes.io/version: 2025.7.23\n        helm.sh/chart: minio-17.0.21\n        app.kubernetes.io/component: minio\n        app.kubernetes.io/part-of: minio\n      annotations:\n        checksum/credentials-secret: d7f9b363039fe1a5911ad90a3de83dbcfe0eba441592f6815c91be1861016459\n    spec:\n      serviceAccountName: release-name-minio\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: minio\n                  app.kubernetes.io/component: minio\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: OnRootMismatch\n        supplementalGroups: []\n        sysctls: []\n      initContainers: null\n      containers:\n      - name: minio\n        image: docker.io/bitnami/minio:2025.7.23-debian-12-r3\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MINIO_DISTRIBUTED_MODE_ENABLED\n          value: 'no'\n        - name: MINIO_SCHEME\n          value: http\n        - name: MINIO_FORCE_NEW_KEYS\n          value: 'no'\n        - name: MINIO_ROOT_USER_FILE\n          value: /opt/bitnami/minio/secrets/root-user\n        - name: MINIO_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/minio/secrets/root-password\n        - name: MINIO_SKIP_CLIENT\n          value: 'yes'\n        - name: MINIO_API_PORT_NUMBER\n          value: '9000'\n        - name: MINIO_BROWSER\n          value: 'off'\n        - name: MINIO_PROMETHEUS_AUTH_TYPE\n          value: public\n        - name: MINIO_DATA_DIR\n          value: /bitnami/minio/data\n        ports:\n        - name: api\n          containerPort: 9000\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: api\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          tcpSocket:\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/minio/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /.mc\n          subPath: app-mc-dir\n        - name: minio-credentials\n          mountPath: /opt/bitnami/minio/secrets/\n        - name: data\n          mountPath: /bitnami/minio/data\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: minio-credentials\n        secret:\n          secretName: release-name-minio\n      - name: data\n        persistentVolumeClaim:\n          claimName: release-name-minio\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-minio\" not found"
  },
  {
    "id": "221",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/011_deployment_release-name-minio-console.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: console\n      app.kubernetes.io/part-of: minio\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: minio\n        app.kubernetes.io/version: 2025.7.23\n        helm.sh/chart: minio-17.0.21\n        app.kubernetes.io/component: console\n        app.kubernetes.io/part-of: minio\n    spec:\n      serviceAccountName: release-name-minio\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: minio\n                  app.kubernetes.io/component: console\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: console\n        image: docker.io/bitnami/minio-object-browser:2.0.2-debian-12-r3\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - server\n        - --host\n        - 0.0.0.0\n        - --port\n        - '9090'\n        env:\n        - name: CONSOLE_MINIO_SERVER\n          value: http://release-name-minio:9000\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        ports:\n        - name: http\n          containerPort: 9090\n        livenessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n          httpGet:\n            path: /minio\n            port: http\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /.console\n          subPath: app-console-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-minio\" not found"
  },
  {
    "id": "222",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/002_poddisruptionbudget_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "223",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/007_service_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  publishNotReadyAddresses: false\n  ports:\n  - name: mongodb\n    port: 27017\n    targetPort: mongodb\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/component: mongodb\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:mongodb app.kubernetes.io/instance:release-name app.kubernetes.io/name:mongodb])"
  },
  {
    "id": "224",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"log-dir\" is using an invalid container image, \"registry-1.docker.io/bitnami/mongodb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "225",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mongodb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mongodb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "226",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-mongodb\" not found"
  },
  {
    "id": "227",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/002_poddisruptionbudget_release-name-mysql.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mysql\n      app.kubernetes.io/part-of: mysql\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "228",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/006_service_release-name-mysql-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mysql-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: mysql\n    port: 3306\n    targetPort: mysql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mysql])"
  },
  {
    "id": "229",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/007_service_release-name-mysql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mysql app.kubernetes.io/part-of:mysql])"
  },
  {
    "id": "230",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/008_statefulset_release-name-mysql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  podManagementPolicy: ''\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mysql\n      app.kubernetes.io/part-of: mysql\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mysql-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 0aa4c7bb029f4871ca0cdece35adf5a5caaf6f2e016ef25c8cae18901c047e48\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mysql\n        app.kubernetes.io/version: 9.4.0\n        helm.sh/chart: mysql-14.0.3\n        app.kubernetes.io/part-of: mysql\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-mysql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mysql\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: docker.io/bitnami/mysql:9.4.0-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mysql/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mysql/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mysql\n        image: docker.io/bitnami/mysql:9.4.0-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MYSQL_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mysql/secrets/mysql-root-password\n        - name: MYSQL_ENABLE_SSL\n          value: 'no'\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_DATABASE\n          value: my_database\n        envFrom: null\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin ping -uroot -p\\\"${password_aux}\\\" | grep \\\"mysqld is\\\n              \\ alive\\\"\\n\"\n        startupProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin ping -uroot -p\\\"${password_aux}\\\" | grep \\\"mysqld is\\\n              \\ alive\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mysql\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/logs\n          subPath: app-logs-dir\n        - name: config\n          mountPath: /opt/bitnami/mysql/conf/my.cnf\n          subPath: my.cnf\n        - name: mysql-credentials\n          mountPath: /opt/bitnami/mysql/secrets/\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-mysql\n      - name: mysql-credentials\n        secret:\n          secretName: release-name-mysql\n          items:\n          - key: mysql-root-password\n            path: mysql-root-password\n          - key: mysql-password\n            path: mysql-password\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mysql\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-mysql\" not found"
  },
  {
    "id": "231",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/002_poddisruptionbudget_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "232",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/005_service_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\n  annotations: null\nspec:\n  type: LoadBalancer\n  sessionAffinity: None\n  externalTrafficPolicy: Cluster\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n  - name: https\n    port: 443\n    targetPort: https\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: nginx\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:nginx])"
  },
  {
    "id": "233",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"registry-1.docker.io/bitnami/nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "234",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "235",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-nginx\" not found"
  },
  {
    "id": "236",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/002_poddisruptionbudget_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "237",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/005_service_release-name-postgresql-hl.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "238",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/006_service_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "239",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/007_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 18.0.0\n        helm.sh/chart: postgresql-18.0.8\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: registry-1.docker.io/bitnami/postgresql:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"postgresql\" is using an invalid container image, \"registry-1.docker.io/bitnami/postgresql:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "240",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/007_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 18.0.0\n        helm.sh/chart: postgresql-18.0.8\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: registry-1.docker.io/bitnami/postgresql:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-postgresql\" not found"
  },
  {
    "id": "241",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/002_poddisruptionbudget_release-name-rabbitmq.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: rabbitmq\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "242",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/008_service_release-name-rabbitmq-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-rabbitmq-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  clusterIP: None\n  ports:\n  - name: epmd\n    port: 4369\n    targetPort: epmd\n  - name: amqp\n    port: 5672\n    targetPort: amqp\n  - name: dist\n    port: 25672\n    targetPort: dist\n  - name: http-stats\n    port: 15672\n    targetPort: stats\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: rabbitmq\n  publishNotReadyAddresses: true\n  trafficDistribution: PreferClose\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:rabbitmq])"
  },
  {
    "id": "243",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/009_service_release-name-rabbitmq.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: amqp\n    port: 5672\n    targetPort: amqp\n    nodePort: null\n  - name: epmd\n    port: 4369\n    targetPort: epmd\n    nodePort: null\n  - name: dist\n    port: 25672\n    targetPort: dist\n    nodePort: null\n  - name: http-stats\n    port: 15672\n    targetPort: stats\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: rabbitmq\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:rabbitmq])"
  },
  {
    "id": "244",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/010_statefulset_release-name-rabbitmq.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  serviceName: release-name-rabbitmq-headless\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: rabbitmq\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: rabbitmq\n        app.kubernetes.io/version: 4.1.3\n        helm.sh/chart: rabbitmq-16.0.14\n      annotations:\n        checksum/config: 81f24711d28981f706e1ae5b2e3ae075d7014b462120496ce6f50d5053194f5e\n        checksum/secret: 0db9412a28617166460cac5333a5cf8ebbc34cfe87087e0f09b593395c5fa678\n    spec:\n      serviceAccountName: release-name-rabbitmq\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: rabbitmq\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      automountServiceAccountToken: true\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      terminationGracePeriodSeconds: 120\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-plugins-dir\n        image: docker.io/bitnami/rabbitmq:4.1.3-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - '#!/bin/bash\n\n\n          . /opt/bitnami/scripts/liblog.sh\n\n\n          info \"Copying plugins dir to empty dir\"\n\n          # In order to not break the possibility of installing custom plugins, we\n          need\n\n          # to make the plugins directory writable, so we need to copy it to an empty\n          dir volume\n\n          cp -r --preserve=mode /opt/bitnami/rabbitmq/plugins/ /emptydir/app-plugins-dir\n\n          '\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: rabbitmq\n        image: docker.io/bitnami/rabbitmq:4.1.3-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/bash\n              - -ec\n              - \"if [[ -f /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh ]]; then\\n\\\n                \\    /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh -t \\\"120\\\" -d \\\"\\\n                false\\\"\\nelse\\n    rabbitmqctl stop_app\\nfi\\n\"\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: RABBITMQ_FORCE_BOOT\n          value: 'no'\n        - name: RABBITMQ_NODE_NAME\n          value: rabbit@$(MY_POD_NAME).release-name-rabbitmq-headless.$(MY_POD_NAMESPACE).svc.cluster.local\n        - name: RABBITMQ_UPDATE_PASSWORD\n          value: 'no'\n        - name: RABBITMQ_MNESIA_DIR\n          value: /opt/bitnami/rabbitmq/.rabbitmq/mnesia/$(RABBITMQ_NODE_NAME)\n        - name: RABBITMQ_LDAP_ENABLE\n          value: 'no'\n        - name: RABBITMQ_LOGS\n          value: '-'\n        - name: RABBITMQ_ULIMIT_NOFILES\n          value: '65535'\n        - name: RABBITMQ_USE_LONGNAME\n          value: 'true'\n        - name: RABBITMQ_ERL_COOKIE_FILE\n          value: /opt/bitnami/rabbitmq/secrets/rabbitmq-erlang-cookie\n        - name: RABBITMQ_LOAD_DEFINITIONS\n          value: 'no'\n        - name: RABBITMQ_DEFINITIONS_FILE\n          value: /app/load_definition.json\n        - name: RABBITMQ_SECURE_PASSWORD\n          value: 'yes'\n        - name: RABBITMQ_USERNAME\n          value: user\n        - name: RABBITMQ_PASSWORD_FILE\n          value: /opt/bitnami/rabbitmq/secrets/rabbitmq-password\n        - name: RABBITMQ_PLUGINS\n          value: rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_auth_backend_ldap\n        envFrom: null\n        ports:\n        - name: amqp\n          containerPort: 5672\n        - name: dist\n          containerPort: 25672\n        - name: stats\n          containerPort: 15672\n        - name: epmd\n          containerPort: 4369\n        - name: metrics\n          containerPort: 9419\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 20\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - curl -f --user user:$(< $RABBITMQ_PASSWORD_FILE) 127.0.0.1:15672/api/health/checks/virtual-hosts\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 20\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - curl -f --user user:$(< $RABBITMQ_PASSWORD_FILE) 127.0.0.1:15672/api/health/checks/local-alarms\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: configuration\n          mountPath: /bitnami/rabbitmq/conf\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/etc/rabbitmq\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/var/lib/rabbitmq\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/.rabbitmq/\n          subPath: app-erlang-cookie\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/var/log/rabbitmq\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/plugins\n          subPath: app-plugins-dir\n        - name: data\n          mountPath: /opt/bitnami/rabbitmq/.rabbitmq/mnesia\n        - name: rabbitmq-secrets\n          mountPath: /opt/bitnami/rabbitmq/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: configuration\n        projected:\n          sources:\n          - secret:\n              name: release-name-rabbitmq-config\n      - name: rabbitmq-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-rabbitmq\n          - secret:\n              name: release-name-rabbitmq\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: rabbitmq\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-rabbitmq\" not found"
  },
  {
    "id": "245",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/002_poddisruptionbudget_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "246",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/003_poddisruptionbudget_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "247",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/010_service_release-name-redis-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: redis\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: redis\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:redis])"
  },
  {
    "id": "248",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/011_service_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  type: ClusterIP\n  internalTrafficPolicy: Cluster\n  sessionAffinity: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: redis\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/component: master\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:master app.kubernetes.io/instance:release-name app.kubernetes.io/name:redis])"
  },
  {
    "id": "249",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/012_service_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  type: ClusterIP\n  internalTrafficPolicy: Cluster\n  sessionAffinity: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: redis\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/component: replica\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:replica app.kubernetes.io/instance:release-name app.kubernetes.io/name:redis])"
  },
  {
    "id": "250",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/013_statefulset_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: master\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-master\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: master\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-master.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: master\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"registry-1.docker.io/bitnami/redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "251",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/013_statefulset_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: master\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-master\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: master\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-master.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: master\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-redis-master\" not found"
  },
  {
    "id": "252",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/014_statefulset_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: replica\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-replica\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: replica\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-replica.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: replica\n        - name: REDIS_MASTER_HOST\n          value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local\n        - name: REDIS_MASTER_PORT_NUMBER\n          value: '6379'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_MASTER_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        startupProbe:\n          failureThreshold: 22\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: redis\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local_and_master.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local_and_master.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: replica\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"registry-1.docker.io/bitnami/redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "253",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/014_statefulset_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: replica\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-replica\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: replica\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-replica.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: replica\n        - name: REDIS_MASTER_HOST\n          value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local\n        - name: REDIS_MASTER_PORT_NUMBER\n          value: '6379'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_MASTER_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        startupProbe:\n          failureThreshold: 22\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: redis\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local_and_master.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local_and_master.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: replica\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-redis-replica\" not found"
  },
  {
    "id": "254",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/003_poddisruptionbudget_release-name-thanos-query-frontend.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query-frontend\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "255",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/004_poddisruptionbudget_release-name-thanos-query.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "256",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/007_service_release-name-thanos-query-frontend.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9090\n    targetPort: http\n    protocol: TCP\n    name: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/component: query-frontend\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:query-frontend app.kubernetes.io/instance:release-name app.kubernetes.io/name:thanos])"
  },
  {
    "id": "257",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/008_service_release-name-thanos-query-grpc.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query-grpc\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  type: ClusterIP\n  ports:\n  - port: 10901\n    targetPort: grpc\n    protocol: TCP\n    name: grpc\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/component: query\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:query app.kubernetes.io/instance:release-name app.kubernetes.io/name:thanos])"
  },
  {
    "id": "258",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/009_service_release-name-thanos-query.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9090\n    targetPort: http\n    protocol: TCP\n    name: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/component: query\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:query app.kubernetes.io/instance:release-name app.kubernetes.io/name:thanos])"
  },
  {
    "id": "259",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/010_deployment_release-name-thanos-query-frontend.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: thanos\n        app.kubernetes.io/version: 0.39.2\n        helm.sh/chart: thanos-17.3.1\n        app.kubernetes.io/component: query-frontend\n    spec:\n      serviceAccountName: release-name-thanos-query-frontend\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: thanos\n                  app.kubernetes.io/component: query-frontend\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: query-frontend\n        image: docker.io/bitnami/thanos:0.39.2-debian-12-r2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - query-frontend\n        - --log.level=info\n        - --log.format=logfmt\n        - --http-address=0.0.0.0:9090\n        - --query-frontend.downstream-url=http://release-name-thanos-query:9090\n        ports:\n        - name: http\n          containerPort: 9090\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/healthy\n            port: http\n            scheme: HTTP\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/ready\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts: null\n      volumes: null\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-thanos-query-frontend\" not found"
  },
  {
    "id": "260",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/011_deployment_release-name-thanos-query.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: thanos\n        app.kubernetes.io/version: 0.39.2\n        helm.sh/chart: thanos-17.3.1\n        app.kubernetes.io/component: query\n    spec:\n      serviceAccountName: release-name-thanos-query\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: thanos\n                  app.kubernetes.io/component: query\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: query\n        image: docker.io/bitnami/thanos:0.39.2-debian-12-r2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - query\n        - --log.level=info\n        - --log.format=logfmt\n        - --grpc-address=0.0.0.0:10901\n        - --http-address=0.0.0.0:10902\n        - --query.replica-label=replica\n        - --alert.query-url=http://release-name-thanos-query.default.svc.cluster.local:9090\n        ports:\n        - name: http\n          containerPort: 10902\n          protocol: TCP\n        - name: grpc\n          containerPort: 10901\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/healthy\n            port: http\n            scheme: HTTP\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/ready\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts: null\n      volumes: null\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-thanos-query\" not found"
  },
  {
    "id": "261",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/003_poddisruptionbudget_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "262",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/004_poddisruptionbudget_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "263",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/011_service_release-name-mariadb-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\nspec:\n  type: ClusterIP\n  publishNotReadyAddresses: true\n  clusterIP: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/part-of: mariadb\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb app.kubernetes.io/part-of:mariadb])"
  },
  {
    "id": "264",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/012_service_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb])"
  },
  {
    "id": "265",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/013_service_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  type: LoadBalancer\n  externalTrafficPolicy: Cluster\n  sessionAffinity: None\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: wordpress\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:wordpress])"
  },
  {
    "id": "266",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"prepare-base-dir\" is using an invalid container image, \"registry-1.docker.io/bitnami/wordpress:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "267",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wordpress\" is using an invalid container image, \"registry-1.docker.io/bitnami/wordpress:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "268",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-wordpress\" not found"
  },
  {
    "id": "269",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mariadb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "270",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "271",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-mariadb\" not found"
  },
  {
    "id": "272",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/035_service_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ClusterIP\n  ports:\n  - protocol: TCP\n    port: 9402\n    name: http-metrics\n  selector:\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:cainjector app.kubernetes.io/instance:release-name app.kubernetes.io/name:cainjector])"
  },
  {
    "id": "273",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/036_service_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ClusterIP\n  ports:\n  - protocol: TCP\n    port: 9402\n    name: tcp-prometheus-servicemonitor\n    targetPort: http-metrics\n  selector:\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:release-name app.kubernetes.io/name:cert-manager])"
  },
  {
    "id": "274",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/037_service_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ClusterIP\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  - name: metrics\n    port: 9402\n    protocol: TCP\n    targetPort: http-metrics\n  selector:\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:webhook app.kubernetes.io/instance:release-name app.kubernetes.io/name:webhook])"
  },
  {
    "id": "275",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/038_deployment_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-cainjector\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-cert-manager-cainjector\" not found"
  },
  {
    "id": "276",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/038_deployment_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-cainjector\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cert-manager-cainjector\" has cpu request 0"
  },
  {
    "id": "277",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/038_deployment_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-cainjector\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cert-manager-cainjector\" has memory limit 0"
  },
  {
    "id": "278",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/039_deployment_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-cert-manager\" not found"
  },
  {
    "id": "279",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/039_deployment_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cert-manager-controller\" has cpu request 0"
  },
  {
    "id": "280",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/039_deployment_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cert-manager-controller\" has memory limit 0"
  },
  {
    "id": "281",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/040_deployment_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-webhook\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-cert-manager-webhook\" not found"
  },
  {
    "id": "282",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/040_deployment_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-webhook\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cert-manager-webhook\" has cpu request 0"
  },
  {
    "id": "283",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/040_deployment_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-webhook\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cert-manager-webhook\" has memory limit 0"
  },
  {
    "id": "284",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "285",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-cert-manager-startupapicheck\" not found"
  },
  {
    "id": "286",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cert-manager-startupapicheck\" has cpu request 0"
  },
  {
    "id": "287",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cert-manager-startupapicheck\" has memory limit 0"
  },
  {
    "id": "288",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/019_service_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '9964'\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/name: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    io.cilium/app: proxy\nspec:\n  clusterIP: None\n  type: ClusterIP\n  selector:\n    k8s-app: cilium-envoy\n  ports:\n  - name: envoy-metrics\n    port: 9964\n    protocol: TCP\n    targetPort: envoy-metrics\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:cilium-envoy])"
  },
  {
    "id": "289",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/020_service_hubble-peer.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hubble-peer\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: hubble-peer\nspec:\n  selector:\n    k8s-app: cilium\n  ports:\n  - name: peer-service\n    port: 443\n    protocol: TCP\n    targetPort: 4244\n  internalTrafficPolicy: Local\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:cilium])"
  },
  {
    "id": "290",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"cilium-agent\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "291",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "292",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cilium-agent\" does not expose port 9879 for the HTTPGet"
  },
  {
    "id": "293",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"apply-sysctl-overwrites\" does not have a read-only root file system"
  },
  {
    "id": "294",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "295",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "296",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"config\" does not have a read-only root file system"
  },
  {
    "id": "297",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"install-cni-binaries\" does not have a read-only root file system"
  },
  {
    "id": "298",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mount-bpf-fs\" does not have a read-only root file system"
  },
  {
    "id": "299",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mount-cgroup\" does not have a read-only root file system"
  },
  {
    "id": "300",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium\" not found"
  }
]