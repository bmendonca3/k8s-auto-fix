[
  {
    "id": "761",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "762",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "763",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "764",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: default\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "765",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "766",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "767",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "768",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "769",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "errors": []
  },
  {
    "id": "770",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "errors": []
  },
  {
    "id": "771",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "errors": []
  },
  {
    "id": "772",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "errors": []
  },
  {
    "id": "773",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  type: ExternalName\n  externalName: release-name-pgadmin4.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "774",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "errors": []
  },
  {
    "id": "775",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "errors": []
  },
  {
    "id": "776",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "errors": []
  },
  {
    "id": "777",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:stable\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      privileged: false\n      allowPrivilegeEscalation: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "778",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:stable\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      allowPrivilegeEscalation: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "779",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:stable\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      allowPrivilegeEscalation: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "780",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  type: ExternalName\n  externalName: release-name-traefik.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "781",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources: null\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "errors": []
  },
  {
    "id": "782",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "errors": []
  },
  {
    "id": "783",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "errors": []
  },
  {
    "id": "784",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  type: ExternalName\n  externalName: release-name-velero.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "785",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "786",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "787",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "788",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "789",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "790",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "791",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "792",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "793",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "794",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "795",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "796",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "797",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "798",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "799",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "800",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:stable\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n    securityContext:\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      privileged: false\n      allowPrivilegeEscalation: false\n",
    "errors": []
  }
]