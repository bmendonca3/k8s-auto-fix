[
  {
    "id": "561",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"proxy\" has memory limit 0"
  },
  {
    "id": "562",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/020_deployment_release-name-kubernetes-dashboard-api.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-api\n    app.kubernetes.io/version: 1.13.0\n    app.kubernetes.io/component: api\n  annotations: null\n  name: release-name-kubernetes-dashboard-api\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-api\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-api\n        app.kubernetes.io/version: 1.13.0\n        app.kubernetes.io/component: api\n      annotations:\n        checksum/config: 204c1765e58ad5edfbba4d9e2caec1985db487314063c390ec4bb1957d34bc3d\n    spec:\n      containers:\n      - name: kubernetes-dashboard-api\n        image: docker.io/kubernetesui/dashboard-api:1.13.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=default\n        - --metrics-scraper-service-name=release-name-kubernetes-dashboard-metrics-scraper\n        env:\n        - name: CSRF_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kubernetes-dashboard-csrf\n              key: private.key\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          name: api\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: release-name-kubernetes-dashboard-api\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kubernetes-dashboard-api\" not found"
  },
  {
    "id": "563",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/022_deployment_release-name-kubernetes-dashboard-metrics-scraper.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n    app.kubernetes.io/version: 1.2.2\n    app.kubernetes.io/component: metrics-scraper\n  annotations: null\n  name: release-name-kubernetes-dashboard-metrics-scraper\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n        app.kubernetes.io/version: 1.2.2\n        app.kubernetes.io/component: metrics-scraper\n      annotations: null\n    spec:\n      containers:\n      - name: kubernetes-dashboard-metrics-scraper\n        image: docker.io/kubernetesui/dashboard-metrics-scraper:1.2.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8000\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: release-name-kubernetes-dashboard-metrics-scraper\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kubernetes-dashboard-metrics-scraper\" not found"
  },
  {
    "id": "564",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/023_deployment_release-name-kubernetes-dashboard-web.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-web\n    app.kubernetes.io/version: 1.7.0\n    app.kubernetes.io/component: web\n  annotations: null\n  name: release-name-kubernetes-dashboard-web\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-web\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-web\n        app.kubernetes.io/version: 1.7.0\n        app.kubernetes.io/component: web\n      annotations: null\n    spec:\n      containers:\n      - name: kubernetes-dashboard-web\n        image: docker.io/kubernetesui/dashboard-web:1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=default\n        - --settings-config-map-name=release-name-kubernetes-dashboard-web-settings\n        env:\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          name: web\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: release-name-kubernetes-dashboard-web\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kubernetes-dashboard-web\" not found"
  },
  {
    "id": "565",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/055_service_release-name-kyverno-svc.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kyverno-svc\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 443\n    targetPort: https\n    protocol: TCP\n    name: https\n    appProtocol: https\n  selector:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:admission-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "566",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/056_service_release-name-kyverno-svc-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kyverno-svc-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:admission-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "567",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/057_service_kyverno-background-controller-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-background-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: background-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: background-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:background-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "568",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/058_service_kyverno-cleanup-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-cleanup-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 443\n    targetPort: https\n    protocol: TCP\n    name: https\n    appProtocol: https\n  selector:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:cleanup-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "569",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/059_service_kyverno-cleanup-controller-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-cleanup-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:cleanup-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "570",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/060_service_kyverno-reports-controller-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-reports-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: reports-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: reports-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:reports-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "571",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/061_deployment_kyverno-admission-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-admission-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: admission-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: admission-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - admission-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kyverno-pre\n        image: reg.kyverno.io/kyverno/kyvernopre:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --loggingFormat=text\n        - --v=2\n        - --openreportsEnabled=false\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-admission-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:admission-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-admission-controller\n        - name: KYVERNO_SVC\n          value: release-name-kyverno-svc\n      containers:\n      - name: kyverno\n        image: reg.kyverno.io/kyverno/kyverno:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --caSecretName=release-name-kyverno-svc.default.svc.kyverno-tls-ca\n        - --tlsSecretName=release-name-kyverno-svc.default.svc.kyverno-tls-pair\n        - --backgroundServiceAccountName=system:serviceaccount:default:kyverno-background-controller\n        - --reportsServiceAccountName=system:serviceaccount:default:kyverno-reports-controller\n        - --servicePort=443\n        - --webhookServerPort=9443\n        - --resyncPeriod=15m\n        - --crdWatcher=false\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --admissionReports=true\n        - --maxAdmissionReports=1000\n        - --autoUpdateWebhooks=true\n        - --enableConfigMapCaching=true\n        - --controllerRuntimeMetricsAddress=:8080\n        - --enableDeferredLoading=true\n        - --dumpPayload=false\n        - --forceFailurePolicyIgnore=false\n        - --generateValidatingAdmissionPolicy=true\n        - --generateMutatingAdmissionPolicy=false\n        - --dumpPatches=false\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --protectManagedResources=false\n        - --allowInsecureRegistry=false\n        - --registryCredentialHelpers=default,google,amazon,azure,github\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        resources:\n          limits:\n            memory: 384Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics-port\n          protocol: TCP\n        env:\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-admission-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:admission-controller\n        - name: KYVERNO_SVC\n          value: release-name-kyverno-svc\n        - name: TUF_ROOT\n          value: /.sigstore\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-admission-controller\n        startupProbe:\n          failureThreshold: 20\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 2\n          periodSeconds: 6\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 15\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /health/readiness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /.sigstore\n          name: sigstore\n      volumes:\n      - name: sigstore\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "572",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/062_deployment_kyverno-background-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-background-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: background-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: background-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: background-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - background-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-background-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/background-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --resyncPeriod=15m\n        - --enableConfigMapCaching=true\n        - --enableDeferredLoading=true\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-background-controller\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-background-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-background-controller\" not found"
  },
  {
    "id": "573",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/063_deployment_kyverno-cleanup-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-cleanup-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cleanup-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cleanup-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - cleanup-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-cleanup-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/cleanup-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --caSecretName=kyverno-cleanup-controller.default.svc.kyverno-tls-ca\n        - --tlsSecretName=kyverno-cleanup-controller.default.svc.kyverno-tls-pair\n        - --servicePort=443\n        - --cleanupServerPort=9443\n        - --webhookServerPort=9443\n        - --resyncPeriod=15m\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --enableDeferredLoading=true\n        - --dumpPayload=false\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --protectManagedResources=false\n        - --ttlReconciliationInterval=1m\n        env:\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-cleanup-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-cleanup-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:cleanup-controller\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_SVC\n          value: kyverno-cleanup-controller\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        startupProbe:\n          failureThreshold: 20\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 2\n          periodSeconds: 6\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 15\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /health/readiness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-cleanup-controller\" not found"
  },
  {
    "id": "574",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/064_deployment_kyverno-reports-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-reports-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: reports-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: reports-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: reports-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - reports-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-reports-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/reports-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --disableMetrics=false\n        - --openreportsEnabled=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --resyncPeriod=15m\n        - --admissionReports=true\n        - --aggregateReports=true\n        - --policyReports=true\n        - --validatingAdmissionPolicyReports=true\n        - --mutatingAdmissionPolicyReports=false\n        - --backgroundScan=true\n        - --backgroundScanWorkers=2\n        - --backgroundScanInterval=1h\n        - --skipResourceFilters=true\n        - --enableConfigMapCaching=true\n        - --enableDeferredLoading=true\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --allowInsecureRegistry=false\n        - --registryCredentialHelpers=default,google,amazon,azure,github\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-reports-controller\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-reports-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: TUF_ROOT\n          value: /.sigstore\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /.sigstore\n          name: sigstore\n      volumes:\n      - name: sigstore\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-reports-controller\" not found"
  },
  {
    "id": "575",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/073_job_release-name-kyverno-migrate-resources.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-migrate-resources\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '200'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: release-name-kyverno-migrate-resources\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: reg.kyverno.io/kyverno/kyverno-cli:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - migrate\n        - --resource\n        - cleanuppolicies.kyverno.io\n        - --resource\n        - clustercleanuppolicies.kyverno.io\n        - --resource\n        - clusterpolicies.kyverno.io\n        - --resource\n        - globalcontextentries.kyverno.io\n        - --resource\n        - policies.kyverno.io\n        - --resource\n        - policyexceptions.kyverno.io\n        - --resource\n        - updaterequests.kyverno.io\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "576",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/073_job_release-name-kyverno-migrate-resources.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-migrate-resources\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '200'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: release-name-kyverno-migrate-resources\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: reg.kyverno.io/kyverno/kyverno-cli:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - migrate\n        - --resource\n        - cleanuppolicies.kyverno.io\n        - --resource\n        - clustercleanuppolicies.kyverno.io\n        - --resource\n        - clusterpolicies.kyverno.io\n        - --resource\n        - globalcontextentries.kyverno.io\n        - --resource\n        - policies.kyverno.io\n        - --resource\n        - policyexceptions.kyverno.io\n        - --resource\n        - updaterequests.kyverno.io\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kyverno-migrate-resources\" not found"
  },
  {
    "id": "577",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/074_job_release-name-kyverno-rm-mutatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-mutatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - mutatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "578",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/074_job_release-name-kyverno-rm-mutatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-mutatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - mutatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "579",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/075_job_release-name-kyverno-rm-validatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-validatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - validatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "580",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/075_job_release-name-kyverno-rm-validatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-validatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - validatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "581",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/076_job_release-name-kyverno-scale-to-zero.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-scale-to-zero\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '90'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - scale\n        - -n\n        - default\n        - deployment\n        - -l\n        - app.kubernetes.io/part-of=release-name-kyverno\n        - --replicas=0\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "582",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/076_job_release-name-kyverno-scale-to-zero.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-scale-to-zero\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '90'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - scale\n        - -n\n        - default\n        - deployment\n        - -l\n        - app.kubernetes.io/part-of=release-name-kyverno\n        - --replicas=0\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "583",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/035_service_longhorn-backend.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-backend\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    app: longhorn-manager\n  ports:\n  - name: manager\n    port: 9500\n    targetPort: manager\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:longhorn-manager])"
  },
  {
    "id": "584",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/036_service_longhorn-frontend.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-frontend\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    app: longhorn-ui\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n    nodePort: null\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:longhorn-ui])"
  },
  {
    "id": "585",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/037_service_longhorn-admission-webhook.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-admission-webhook\n  name: longhorn-admission-webhook\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    longhorn.io/admission-webhook: longhorn-admission-webhook\n  ports:\n  - name: admission-webhook\n    port: 9502\n    targetPort: admission-wh\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[longhorn.io/admission-webhook:longhorn-admission-webhook])"
  },
  {
    "id": "586",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/038_service_longhorn-recovery-backend.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-recovery-backend\n  name: longhorn-recovery-backend\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    longhorn.io/recovery-backend: longhorn-recovery-backend\n  ports:\n  - name: recovery-backend\n    port: 9503\n    targetPort: recov-backend\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[longhorn.io/recovery-backend:longhorn-recovery-backend])"
  },
  {
    "id": "587",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-manager\" does not have a read-only root file system"
  },
  {
    "id": "588",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pre-pull-share-manager-image\" does not have a read-only root file system"
  },
  {
    "id": "589",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "590",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"longhorn-manager\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "591",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"longhorn-manager\" is privileged"
  },
  {
    "id": "592",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "593",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pre-pull-share-manager-image\" is not set to runAsNonRoot"
  },
  {
    "id": "594",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"longhorn-manager\" has cpu request 0"
  },
  {
    "id": "595",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pre-pull-share-manager-image\" has cpu request 0"
  },
  {
    "id": "596",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"longhorn-manager\" has memory limit 0"
  },
  {
    "id": "597",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pre-pull-share-manager-image\" has memory limit 0"
  },
  {
    "id": "598",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-driver-deployer\" does not have a read-only root file system"
  },
  {
    "id": "599",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-longhorn-manager\" does not have a read-only root file system"
  },
  {
    "id": "600",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  }
]