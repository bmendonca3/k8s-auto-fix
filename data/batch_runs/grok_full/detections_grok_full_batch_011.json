[
  {
    "id": "441",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/009_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-grafana\" not found"
  },
  {
    "id": "442",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/009_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana\" has cpu request 0"
  },
  {
    "id": "443",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/009_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana\" has memory limit 0"
  },
  {
    "id": "444",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-test\" does not have a read-only root file system"
  },
  {
    "id": "445",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-grafana-test\" not found"
  },
  {
    "id": "446",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"release-name-test\" is not set to runAsNonRoot"
  },
  {
    "id": "447",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-test\" has cpu request 0"
  },
  {
    "id": "448",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-test\" has memory limit 0"
  },
  {
    "id": "449",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/011_service_release-name-loki-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki-headless\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n    variant: headless\nspec:\n  clusterIP: None\n  ports:\n  - port: 3100\n    protocol: TCP\n    name: http-metrics\n    targetPort: http-metrics\n  selector:\n    app: loki\n    release: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:loki release:release-name])"
  },
  {
    "id": "450",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/012_service_release-name-loki-memberlist.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki-memberlist\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: http\n    port: 7946\n    targetPort: memberlist-port\n    protocol: TCP\n  selector:\n    app: loki\n    release: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:loki release:release-name])"
  },
  {
    "id": "451",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/013_service_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  type: ClusterIP\n  ports:\n  - port: 3100\n    protocol: TCP\n    name: http-metrics\n    targetPort: http-metrics\n  selector:\n    app: loki\n    release: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:loki release:release-name])"
  },
  {
    "id": "452",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-promtail\" not found"
  },
  {
    "id": "453",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"promtail\" is not set to runAsNonRoot"
  },
  {
    "id": "454",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"promtail\" has cpu request 0"
  },
  {
    "id": "455",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"promtail\" has memory limit 0"
  },
  {
    "id": "456",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/015_statefulset_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: release-name-loki\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-loki\" not found"
  },
  {
    "id": "457",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/015_statefulset_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: release-name-loki\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"loki\" has cpu request 0"
  },
  {
    "id": "458",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/015_statefulset_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: release-name-loki\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"loki\" has memory limit 0"
  },
  {
    "id": "459",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test\" does not have a read-only root file system"
  },
  {
    "id": "460",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test\" is not set to runAsNonRoot"
  },
  {
    "id": "461",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test\" has cpu request 0"
  },
  {
    "id": "462",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test\" has memory limit 0"
  },
  {
    "id": "463",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-promtail\" not found"
  },
  {
    "id": "464",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"promtail\" is not set to runAsNonRoot"
  },
  {
    "id": "465",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"promtail\" has cpu request 0"
  },
  {
    "id": "466",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"promtail\" has memory limit 0"
  },
  {
    "id": "467",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/017_service_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-web\n    port: 80\n    targetPort: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: core\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:core release:release-name])"
  },
  {
    "id": "468",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/018_service_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - port: 5432\n  selector:\n    release: release-name\n    app: harbor\n    component: database\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:database release:release-name])"
  },
  {
    "id": "469",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/019_service_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-jobservice\n    port: 80\n    targetPort: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: jobservice\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:jobservice release:release-name])"
  },
  {
    "id": "470",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/020_service_release-name-harbor-portal.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: portal\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:portal release:release-name])"
  },
  {
    "id": "471",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/021_service_release-name-harbor-redis.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - port: 6379\n  selector:\n    release: release-name\n    app: harbor\n    component: redis\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:redis release:release-name])"
  },
  {
    "id": "472",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/022_service_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-registry\n    port: 5000\n  - name: http-controller\n    port: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: registry\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:registry release:release-name])"
  },
  {
    "id": "473",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/023_service_release-name-harbor-trivy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-trivy\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-trivy\n    protocol: TCP\n    port: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: trivy\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:trivy release:release-name])"
  },
  {
    "id": "474",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/024_deployment_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"core\" does not have a read-only root file system"
  },
  {
    "id": "475",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/024_deployment_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"core\" has cpu request 0"
  },
  {
    "id": "476",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/024_deployment_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"core\" has memory limit 0"
  },
  {
    "id": "477",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/025_deployment_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jobservice\" does not have a read-only root file system"
  },
  {
    "id": "478",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/025_deployment_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jobservice\" has cpu request 0"
  },
  {
    "id": "479",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/025_deployment_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jobservice\" has memory limit 0"
  },
  {
    "id": "480",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/026_deployment_release-name-harbor-portal.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: portal\n    app.kubernetes.io/component: portal\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: portal\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: portal\n        app.kubernetes.io/component: portal\n      annotations:\n        checksum/configmap: c9e04324738b148b4530aa2bd57b606d50600544b949ceef09270ef9c207bf85\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: portal\n        image: goharbor/harbor-portal:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: portal-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n      volumes:\n      - name: portal-config\n        configMap:\n          name: release-name-harbor-portal\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"portal\" does not have a read-only root file system"
  }
]