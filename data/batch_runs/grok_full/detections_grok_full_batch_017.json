[
  {
    "id": "681",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-nfs-subdir-external-provisioner\" not found"
  },
  {
    "id": "682",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nfs-subdir-external-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "683",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nfs-subdir-external-provisioner\" has cpu request 0"
  },
  {
    "id": "684",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nfs-subdir-external-provisioner\" has memory limit 0"
  },
  {
    "id": "685",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/008_service_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n    nodePort: null\n  - port: 443\n    targetPort: 443\n    protocol: TCP\n    name: https\n    nodePort: null\n  selector:\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:nginx-ingress])"
  },
  {
    "id": "686",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/009_deployment_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: release-name-nginx-ingress\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress\" does not have a read-only root file system"
  },
  {
    "id": "687",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/009_deployment_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: release-name-nginx-ingress\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-nginx-ingress\" not found"
  },
  {
    "id": "688",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/009_deployment_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: release-name-nginx-ingress\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress\" has memory limit 0"
  },
  {
    "id": "689",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/004_service_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: http\n    protocol: TCP\n    appProtocol: http\n    name: http\n  - port: 44180\n    protocol: TCP\n    appProtocol: http\n    targetPort: metrics\n    name: metrics\n  selector:\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:oauth2-proxy])"
  },
  {
    "id": "690",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/005_deployment_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: release-name-oauth2-proxy\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-oauth2-proxy\" not found"
  },
  {
    "id": "691",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/005_deployment_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: release-name-oauth2-proxy\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"oauth2-proxy\" has cpu request 0"
  },
  {
    "id": "692",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/005_deployment_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: release-name-oauth2-proxy\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"oauth2-proxy\" has memory limit 0"
  },
  {
    "id": "693",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/050_service_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  type: ClusterIP\n  ports:\n  - name: http-web\n    port: 80\n    protocol: TCP\n    targetPort: grafana\n  selector:\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:grafana])"
  },
  {
    "id": "694",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/051_service_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\n  annotations: null\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    protocol: TCP\n    port: 8080\n    targetPort: http\n  selector:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kube-state-metrics])"
  },
  {
    "id": "695",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/052_service_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\n    jobLabel: node-exporter\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9100\n    targetPort: 9100\n    protocol: TCP\n    name: http-metrics\n  selector:\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:prometheus-node-exporter])"
  },
  {
    "id": "696",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/053_service_release-name-kube-promethe-alertmanager.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-alertmanager\n  namespace: default\n  labels:\n    app: kube-prometheus-stack-alertmanager\n    self-monitor: 'true'\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\nspec:\n  ports:\n  - name: http-web\n    port: 9093\n    targetPort: 9093\n    protocol: TCP\n  - name: reloader-web\n    appProtocol: http\n    port: 8080\n    targetPort: reloader-web\n  selector:\n    app.kubernetes.io/name: alertmanager\n    alertmanager: release-name-kube-promethe-alertmanager\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[alertmanager:release-name-kube-promethe-alertmanager app.kubernetes.io/name:alertmanager])"
  },
  {
    "id": "697",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/054_service_release-name-kube-promethe-coredns.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-coredns\n  labels:\n    app: kube-prometheus-stack-coredns\n    jobLabel: coredns\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 9153\n    protocol: TCP\n    targetPort: 9153\n  selector:\n    k8s-app: kube-dns\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:kube-dns])"
  },
  {
    "id": "698",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/055_service_release-name-kube-promethe-kube-controller-manager.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-controller-manager\n  labels:\n    app: kube-prometheus-stack-kube-controller-manager\n    jobLabel: kube-controller-manager\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 10257\n    protocol: TCP\n    targetPort: 10257\n  selector:\n    component: kube-controller-manager\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:kube-controller-manager])"
  },
  {
    "id": "699",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/056_service_release-name-kube-promethe-kube-etcd.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-etcd\n  labels:\n    app: kube-prometheus-stack-kube-etcd\n    jobLabel: kube-etcd\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 2381\n    protocol: TCP\n    targetPort: 2381\n  selector:\n    component: etcd\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:etcd])"
  },
  {
    "id": "700",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/057_service_release-name-kube-promethe-kube-proxy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-proxy\n  labels:\n    app: kube-prometheus-stack-kube-proxy\n    jobLabel: kube-proxy\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 10249\n    protocol: TCP\n    targetPort: 10249\n  selector:\n    k8s-app: kube-proxy\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:kube-proxy])"
  },
  {
    "id": "701",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/058_service_release-name-kube-promethe-kube-scheduler.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-scheduler\n  labels:\n    app: kube-prometheus-stack-kube-scheduler\n    jobLabel: kube-scheduler\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 10259\n    protocol: TCP\n    targetPort: 10259\n  selector:\n    component: kube-scheduler\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:kube-scheduler])"
  },
  {
    "id": "702",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/059_service_release-name-kube-promethe-operator.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  ports:\n  - name: https\n    port: 443\n    targetPort: https\n  selector:\n    app: kube-prometheus-stack-operator\n    release: release-name\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kube-prometheus-stack-operator release:release-name])"
  },
  {
    "id": "703",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/060_service_release-name-kube-promethe-prometheus.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-prometheus\n  namespace: default\n  labels:\n    app: kube-prometheus-stack-prometheus\n    self-monitor: 'true'\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\nspec:\n  ports:\n  - name: http-web\n    port: 9090\n    targetPort: 9090\n  - name: reloader-web\n    appProtocol: http\n    port: 8080\n    targetPort: reloader-web\n  publishNotReadyAddresses: false\n  selector:\n    app.kubernetes.io/name: prometheus\n    operator.prometheus.io/name: release-name-kube-promethe-prometheus\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:prometheus operator.prometheus.io/name:release-name-kube-promethe-prometheus])"
  },
  {
    "id": "704",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "705",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "706",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-prometheus-node-exporter\" not found"
  },
  {
    "id": "707",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"node-exporter\""
  },
  {
    "id": "708",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"node-exporter\""
  },
  {
    "id": "709",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"node-exporter\""
  },
  {
    "id": "710",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-exporter\" has cpu request 0"
  },
  {
    "id": "711",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-exporter\" has memory limit 0"
  },
  {
    "id": "712",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "713",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana-sc-dashboard\" does not have a read-only root file system"
  },
  {
    "id": "714",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana-sc-datasources\" does not have a read-only root file system"
  },
  {
    "id": "715",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-grafana\" not found"
  },
  {
    "id": "716",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana\" has cpu request 0"
  },
  {
    "id": "717",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana-sc-dashboard\" has cpu request 0"
  },
  {
    "id": "718",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana-sc-datasources\" has cpu request 0"
  },
  {
    "id": "719",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana\" has memory limit 0"
  },
  {
    "id": "720",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana-sc-dashboard\" has memory limit 0"
  }
]