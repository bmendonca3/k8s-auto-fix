[
  {
    "id": "601",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "602",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "603",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "604",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "605",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "606",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "607",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "errors": []
  },
  {
    "id": "608",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: default\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "errors": []
  },
  {
    "id": "609",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "errors": []
  },
  {
    "id": "610",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "errors": []
  },
  {
    "id": "611",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "errors": []
  },
  {
    "id": "612",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "613",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "614",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: default\n",
    "errors": []
  },
  {
    "id": "615",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "616",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "617",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "618",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "619",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "620",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: default\n",
    "errors": []
  },
  {
    "id": "621",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "622",
    "policy_id": "no_privileged",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "623",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "624",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "625",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "626",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "627",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "628",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: default\n",
    "errors": []
  },
  {
    "id": "629",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "630",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "631",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "632",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: metallb-webhook-service\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ExternalName\n  externalName: metallb-webhook-service.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "633",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n            add: []\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "634",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n            add: []\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "635",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n            add: []\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "636",
    "policy_id": "env_var_secret",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metallb-memberlist\n              key: metallb_ml_secret_key_path\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n            add: []\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "637",
    "policy_id": "no_host_network",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n            add: []\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n  hostNetwork: false\n",
    "errors": []
  },
  {
    "id": "638",
    "policy_id": "liveness_port",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n            add: []\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        ports:\n        - containerPort: 7473\n          name: liveness-probe\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "639",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n            add: []\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          readOnlyRootFilesystem: true\n          privileged: false\n          allowPrivilegeEscalation: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "640",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n            add: []\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          readOnlyRootFilesystem: true\n          privileged: false\n          allowPrivilegeEscalation: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  }
]