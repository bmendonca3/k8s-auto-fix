[
  {
    "id": "801",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pulsar-admin\" does not have a read-only root file system"
  },
  {
    "id": "802",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pulsar-admin\" is not set to runAsNonRoot"
  },
  {
    "id": "803",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pulsar-admin\" has cpu request 0"
  },
  {
    "id": "804",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pulsar-admin\" has memory limit 0"
  },
  {
    "id": "805",
    "manifest_path": "data/manifests/the_stack_sample/sample_0001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: open-api-doc\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app: open-api-doc\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:open-api-doc])"
  },
  {
    "id": "806",
    "manifest_path": "data/manifests/the_stack_sample/sample_0002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: argocd-metrics\nspec:\n  ports:\n  - name: http\n    protocol: TCP\n    port: 8082\n    targetPort: 8082\n  selector:\n    app: argocd-server\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:argocd-server])"
  },
  {
    "id": "807",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"datasets-lib-unb-ca\" is using an invalid container image, \"||DEPLOYMENTIMAGE||\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "808",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"datasets-lib-unb-ca\" does not have a read-only root file system"
  },
  {
    "id": "809",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"datasets-lib-unb-ca\" is not set to runAsNonRoot"
  },
  {
    "id": "810",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"datasets-lib-unb-ca\" has cpu request 0"
  },
  {
    "id": "811",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"datasets-lib-unb-ca\" has memory limit 0"
  },
  {
    "id": "812",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"metrics\" does not have a read-only root file system"
  },
  {
    "id": "813",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "814",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"metrics\" has cpu request 0"
  },
  {
    "id": "815",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"metrics\" has memory limit 0"
  },
  {
    "id": "816",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "817",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"statusreconciler\" not found"
  },
  {
    "id": "818",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "819",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "820",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "821",
    "manifest_path": "data/manifests/the_stack_sample/sample_0009.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kubwz0un1-cffc\n  labels:\n    app: kubwz0un1-cffc\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n    targetPort: 8080\n    protocol: TCP\n    name: http\n  selector:\n    app: kubwz0un1-cffc\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kubwz0un1-cffc])"
  },
  {
    "id": "822",
    "manifest_path": "data/manifests/the_stack_sample/sample_0010.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch\n  labels:\n    component: elasticsearch\n    role: data\n  annotations:\n    cloud.google.com/load-balancer-type: Internal\nspec:\n  selector:\n    component: elasticsearch\n    role: data\n  ports:\n  - name: http\n    port: 9200\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:elasticsearch role:data])"
  },
  {
    "id": "823",
    "manifest_path": "data/manifests/the_stack_sample/sample_0012.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    getambassador.io/config: \"---\\napiVersion: ambassador/v0\\nkind:  Mapping\\nname:\\\n      \\ webapp_mapping\\nprefix: /jupyter/\\nservice: jupyter-web-app-service.kubeflow\\n\\\n      add_request_headers:\\n  x-forwarded-prefix: /jupyter\"\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n    run: jupyter-web-app\n  name: jupyter-web-app-service\n  namespace: kubeflow\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 5000\n  selector:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:jupyter-web-app app.kubernetes.io/component:jupyter-web-app app.kubernetes.io/name:jupyter-web-app kustomize.component:jupyter-web-app])"
  },
  {
    "id": "824",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "825",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"healthcheck-ready\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "826",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"holder-vcs-add-profiles\" is using an invalid container image, \"alpine:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "827",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wait\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "828",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"healthcheck-ready\" does not have a read-only root file system"
  },
  {
    "id": "829",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"holder-vcs-add-profiles\" does not have a read-only root file system"
  },
  {
    "id": "830",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait\" does not have a read-only root file system"
  },
  {
    "id": "831",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"healthcheck-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "832",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"holder-vcs-add-profiles\" is not set to runAsNonRoot"
  },
  {
    "id": "833",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait\" is not set to runAsNonRoot"
  },
  {
    "id": "834",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"healthcheck-ready\" has cpu request 0"
  },
  {
    "id": "835",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"holder-vcs-add-profiles\" has cpu request 0"
  },
  {
    "id": "836",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait\" has cpu request 0"
  },
  {
    "id": "837",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"healthcheck-ready\" has memory limit 0"
  },
  {
    "id": "838",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"holder-vcs-add-profiles\" has memory limit 0"
  },
  {
    "id": "839",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait\" has memory limit 0"
  },
  {
    "id": "840",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  }
]