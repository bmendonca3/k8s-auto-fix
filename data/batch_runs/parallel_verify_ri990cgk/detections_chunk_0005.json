[
  {
    "id": "04171",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cirros-vm\" has cpu request 0"
  },
  {
    "id": "04172",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cirros-vm\" has memory limit 0"
  },
  {
    "id": "04173",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"test-rc\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04174",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-rc\" does not have a read-only root file system"
  },
  {
    "id": "04175",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-rc\" is not set to runAsNonRoot"
  },
  {
    "id": "04176",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-rc\" has cpu request 0"
  },
  {
    "id": "04177",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-rc\" has memory limit 0"
  },
  {
    "id": "04178",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"yolo\" is using an invalid container image, \"github.com/josephburnett/kubecon-seattle-2018/yolo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04179",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"yolo\" does not have a read-only root file system"
  },
  {
    "id": "04180",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"yolo\" is not set to runAsNonRoot"
  },
  {
    "id": "04181",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"yolo\" has cpu request 0"
  },
  {
    "id": "04182",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"yolo\" has memory limit 0"
  },
  {
    "id": "04183",
    "manifest_path": "data/manifests/the_stack_sample/sample_1912.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-qualifiers-nrfin00009-pov1\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-qualifiers-nrfin00009-pov1\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py qualifiers NRFIN_00009\n      pov_1 3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cyborg-seeker-qualifiers-nrfin00009-pov1\" does not have a read-only root file system"
  },
  {
    "id": "04184",
    "manifest_path": "data/manifests/the_stack_sample/sample_1912.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-qualifiers-nrfin00009-pov1\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-qualifiers-nrfin00009-pov1\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py qualifiers NRFIN_00009\n      pov_1 3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cyborg-seeker-qualifiers-nrfin00009-pov1\" is not set to runAsNonRoot"
  },
  {
    "id": "04185",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04186",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04187",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04188",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04189",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sleeping-container\" is using an invalid container image, \"sergiofgonzalez/sleeping-container:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04190",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sleeping-container\" does not have a read-only root file system"
  },
  {
    "id": "04191",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sleeping-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04192",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sleeping-container\" has cpu request 0"
  },
  {
    "id": "04193",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sleeping-container\" has memory limit 0"
  },
  {
    "id": "04194",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "04195",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "04196",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "04197",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "04198",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04199",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04200",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04201",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04202",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04203",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-driver-registrar\" is using an invalid container image, \"MUSTPATCHWITHKUSTOMIZE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04204",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"iks-vpc-block-node-driver\" is using an invalid container image, \"MUSTPATCHWITHKUSTOMIZE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04205",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"liveness-probe\" is using an invalid container image, \"MUSTPATCHWITHKUSTOMIZE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04206",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "04207",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"iks-vpc-block-node-driver\" does not have a read-only root file system"
  },
  {
    "id": "04208",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "04209",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"iks-vpc-block-node-driver\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04210",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"iks-vpc-block-node-driver\" is privileged"
  },
  {
    "id": "04211",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "04212",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"iks-vpc-block-node-driver\" is not set to runAsNonRoot"
  },
  {
    "id": "04213",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "04214",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"bitnami/nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04215",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04216",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04217",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04218",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04219",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04220",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04221",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04222",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04223",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable STANDARD_AUTHSERVICE_CERT_SECRET_NAME in container \"address-space-controller\" found"
  },
  {
    "id": "04224",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME in container \"address-space-controller\" found"
  },
  {
    "id": "04225",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"address-space-controller\" does not have a read-only root file system"
  },
  {
    "id": "04226",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"address-space-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "04227",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"address-space-controller\" has cpu request 0"
  },
  {
    "id": "04228",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dc-api\" is using an invalid container image, \"gcr.io/neural-pattern-278618/dc-api\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04229",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dc-api\" does not have a read-only root file system"
  },
  {
    "id": "04230",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dc-api\" is not set to runAsNonRoot"
  },
  {
    "id": "04231",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dc-api\" has memory limit 0"
  },
  {
    "id": "04232",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"simpleapp\" is using an invalid container image, \"dgkanatsios/simpleapp\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04233",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"simpleapp\" does not have a read-only root file system"
  },
  {
    "id": "04234",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"simpleapp\" is not set to runAsNonRoot"
  },
  {
    "id": "04235",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"simpleapp\" has cpu request 0"
  },
  {
    "id": "04236",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"simpleapp\" has memory limit 0"
  },
  {
    "id": "04237",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"init-sysctl\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04238",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"banias-frontend\" does not have a read-only root file system"
  },
  {
    "id": "04239",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-sysctl\" does not have a read-only root file system"
  },
  {
    "id": "04240",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"init-sysctl\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04241",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"init-sysctl\" is privileged"
  },
  {
    "id": "04242",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"banias-frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "04243",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-sysctl\" is not set to runAsNonRoot"
  },
  {
    "id": "04244",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-sysctl\" has cpu request 0"
  },
  {
    "id": "04245",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-sysctl\" has memory limit 0"
  },
  {
    "id": "04246",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04247",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04248",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04249",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04250",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04251",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "04252",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "04253",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "04254",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "04255",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-main\" does not have a read-only root file system"
  },
  {
    "id": "04256",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-self\" does not have a read-only root file system"
  },
  {
    "id": "04257",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-state-metrics\" does not have a read-only root file system"
  },
  {
    "id": "04258",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy-main\" is not set to runAsNonRoot"
  },
  {
    "id": "04259",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy-self\" is not set to runAsNonRoot"
  },
  {
    "id": "04260",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-state-metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "04261",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy-main\" has memory limit 0"
  },
  {
    "id": "04262",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy-self\" has memory limit 0"
  },
  {
    "id": "04263",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "04264",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "04265",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "04266",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "04267",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "04268",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "04269",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "04270",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "04271",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "04272",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "04273",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "04274",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "04275",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"results-app\" is using an invalid container image, \"dockersamples/examplevotingapp_result\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04276",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"results-app\" does not have a read-only root file system"
  },
  {
    "id": "04277",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"results-app\" is not set to runAsNonRoot"
  },
  {
    "id": "04278",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"results-app\" has cpu request 0"
  },
  {
    "id": "04279",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"results-app\" has memory limit 0"
  },
  {
    "id": "04280",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "04281",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "04282",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "04283",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "04284",
    "manifest_path": "data/manifests/the_stack_sample/sample_1963.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: kube-janitor\n  labels:\n    app: kube-janitor\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: kube-janitor\n        containers:\n        - name: kube-janitor\n          image: themagicalkarp/kube-janitor:v0.1.0\n          imagePullPolicy: Always\n          command:\n          - /kube-janitor\n          - -expiration=2880\n          - -annotation=kube.janitor.io\n          - -pendingJobExpiration=60\n          - -verbose\n          resources:\n            limits:\n              cpu: 200m\n              memory: 100Mi\n            requests:\n              cpu: 50m\n              memory: 50Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-janitor\" does not have a read-only root file system"
  },
  {
    "id": "04285",
    "manifest_path": "data/manifests/the_stack_sample/sample_1963.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: kube-janitor\n  labels:\n    app: kube-janitor\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: kube-janitor\n        containers:\n        - name: kube-janitor\n          image: themagicalkarp/kube-janitor:v0.1.0\n          imagePullPolicy: Always\n          command:\n          - /kube-janitor\n          - -expiration=2880\n          - -annotation=kube.janitor.io\n          - -pendingJobExpiration=60\n          - -verbose\n          resources:\n            limits:\n              cpu: 200m\n              memory: 100Mi\n            requests:\n              cpu: 50m\n              memory: 50Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-janitor\" is not set to runAsNonRoot"
  },
  {
    "id": "04286",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"signalfx-agent\" does not have a read-only root file system"
  },
  {
    "id": "04287",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"signalfx-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "04288",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"signalfx-agent\" has cpu request 0"
  },
  {
    "id": "04289",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"signalfx-agent\" has memory limit 0"
  },
  {
    "id": "04290",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"vppagent-client\" is using an invalid container image, \"networkservicemesh/vpp-icmp-vppagent-client:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04291",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vppagent-client\" does not have a read-only root file system"
  },
  {
    "id": "04292",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vppagent-client\" is not set to runAsNonRoot"
  },
  {
    "id": "04293",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vppagent-client\" has cpu request 0"
  },
  {
    "id": "04294",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vppagent-client\" has memory limit 0"
  },
  {
    "id": "04295",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable TLS_SECRET_NAME in container \"proxy\" found"
  },
  {
    "id": "04296",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"proxy\" does not have a read-only root file system"
  },
  {
    "id": "04297",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "04298",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"proxy\" has cpu request 0"
  },
  {
    "id": "04299",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"proxy\" has memory limit 0"
  },
  {
    "id": "04300",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cilium-agent\" is using an invalid container image, \"docker.io/cilium/cilium:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04301",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "04302",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "04303",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04304",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"clean-cilium-state\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04305",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"cilium-agent\" is privileged"
  },
  {
    "id": "04306",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"clean-cilium-state\" is privileged"
  },
  {
    "id": "04307",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "04308",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clean-cilium-state\" is not set to runAsNonRoot"
  },
  {
    "id": "04309",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-agent\" has cpu request 0"
  },
  {
    "id": "04310",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clean-cilium-state\" has cpu request 0"
  },
  {
    "id": "04311",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-agent\" has memory limit 0"
  },
  {
    "id": "04312",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "04313",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "04314",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "04315",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "04316",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "04317",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "04318",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "04319",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "04320",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "04321",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"helloworld\" does not have a read-only root file system"
  },
  {
    "id": "04322",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"helloworld\" is not set to runAsNonRoot"
  },
  {
    "id": "04323",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"helloworld\" has cpu request 0"
  },
  {
    "id": "04324",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"helloworld\" has memory limit 0"
  },
  {
    "id": "04325",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container\" does not have a read-only root file system"
  },
  {
    "id": "04326",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container\" is not set to runAsNonRoot"
  },
  {
    "id": "04327",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container\" has cpu request 0"
  },
  {
    "id": "04328",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container\" has memory limit 0"
  },
  {
    "id": "04329",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "04330",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "04331",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "04332",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "04333",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "04334",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "04335",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "04336",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "04337",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "04338",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "04339",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "04340",
    "manifest_path": "data/manifests/the_stack_sample/sample_1992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: load\n  labels:\n    service: load\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: load\n  template:\n    metadata:\n      labels:\n        service: load\n    spec:\n      containers:\n      - name: load\n        env:\n        - name: HOST\n          value: http://payment.robotshop:8080/health\n        - name: NUM_CLIENTS\n          value: '15'\n        - name: SILENT\n          value: '1'\n        - name: ERROR\n          value: '1'\n        image: robotshop/rs-load:latest\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"load\" is using an invalid container image, \"robotshop/rs-load:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04341",
    "manifest_path": "data/manifests/the_stack_sample/sample_1992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: load\n  labels:\n    service: load\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: load\n  template:\n    metadata:\n      labels:\n        service: load\n    spec:\n      containers:\n      - name: load\n        env:\n        - name: HOST\n          value: http://payment.robotshop:8080/health\n        - name: NUM_CLIENTS\n          value: '15'\n        - name: SILENT\n          value: '1'\n        - name: ERROR\n          value: '1'\n        image: robotshop/rs-load:latest\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"load\" does not have a read-only root file system"
  },
  {
    "id": "04342",
    "manifest_path": "data/manifests/the_stack_sample/sample_1992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: load\n  labels:\n    service: load\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: load\n  template:\n    metadata:\n      labels:\n        service: load\n    spec:\n      containers:\n      - name: load\n        env:\n        - name: HOST\n          value: http://payment.robotshop:8080/health\n        - name: NUM_CLIENTS\n          value: '15'\n        - name: SILENT\n          value: '1'\n        - name: ERROR\n          value: '1'\n        image: robotshop/rs-load:latest\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"load\" is not set to runAsNonRoot"
  },
  {
    "id": "04343",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mlperf-inference-container\" is using an invalid container image, \"aferikoglou/mlperf-inference:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04344",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mlperf-inference-container\" does not have a read-only root file system"
  },
  {
    "id": "04345",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mlperf-inference-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04346",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mlperf-inference-container\" has cpu request 0"
  },
  {
    "id": "04347",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mlperf-inference-container\" has memory limit 0"
  },
  {
    "id": "04348",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ebs-plugin\" is using an invalid container image, \"k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04349",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ebs-plugin\" does not have a read-only root file system"
  },
  {
    "id": "04350",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "04351",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "04352",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"ebs-plugin\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04353",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"ebs-plugin\" is privileged"
  },
  {
    "id": "04354",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ebs-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "04355",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "04356",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "04357",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ebs-plugin\" has cpu request 0"
  },
  {
    "id": "04358",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness-probe\" has cpu request 0"
  },
  {
    "id": "04359",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-driver-registrar\" has cpu request 0"
  },
  {
    "id": "04360",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ebs-plugin\" has memory limit 0"
  },
  {
    "id": "04361",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness-probe\" has memory limit 0"
  },
  {
    "id": "04362",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-driver-registrar\" has memory limit 0"
  },
  {
    "id": "04363",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"smi-adapter-istio\" is using an invalid container image, \"layer5/smi-istio:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04364",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"smi-adapter-istio\" does not have a read-only root file system"
  },
  {
    "id": "04365",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"smi-adapter-istio\" is not set to runAsNonRoot"
  },
  {
    "id": "04366",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"smi-adapter-istio\" has cpu request 0"
  },
  {
    "id": "04367",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"smi-adapter-istio\" has memory limit 0"
  },
  {
    "id": "04368",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"certs\" does not have a read-only root file system"
  },
  {
    "id": "04369",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"certs\" is not set to runAsNonRoot"
  },
  {
    "id": "04370",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"certs\" has cpu request 0"
  },
  {
    "id": "04371",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"certs\" has memory limit 0"
  },
  {
    "id": "04372",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jaeger-operator\" does not have a read-only root file system"
  },
  {
    "id": "04373",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jaeger-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "04374",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jaeger-operator\" has cpu request 0"
  },
  {
    "id": "04375",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jaeger-operator\" has memory limit 0"
  },
  {
    "id": "04376",
    "manifest_path": "data/manifests/the_stack_sample/sample_2005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - uid_entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"argocd-repo-server\" is using an invalid container image, \"quay.io/argoproj/argocd:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04377",
    "manifest_path": "data/manifests/the_stack_sample/sample_2005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - uid_entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"argocd-repo-server\" does not have a read-only root file system"
  },
  {
    "id": "04378",
    "manifest_path": "data/manifests/the_stack_sample/sample_2005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - uid_entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"argocd-repo-server\" is not set to runAsNonRoot"
  },
  {
    "id": "04379",
    "manifest_path": "data/manifests/the_stack_sample/sample_2005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - uid_entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"argocd-repo-server\" has cpu request 0"
  },
  {
    "id": "04380",
    "manifest_path": "data/manifests/the_stack_sample/sample_2005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - uid_entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"argocd-repo-server\" has memory limit 0"
  },
  {
    "id": "04381",
    "manifest_path": "data/manifests/the_stack_sample/sample_2008.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200416-dd6c31b18\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "04382",
    "manifest_path": "data/manifests/the_stack_sample/sample_2008.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200416-dd6c31b18\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "04383",
    "manifest_path": "data/manifests/the_stack_sample/sample_2008.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200416-dd6c31b18\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "04384",
    "manifest_path": "data/manifests/the_stack_sample/sample_2008.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200416-dd6c31b18\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "04385",
    "manifest_path": "data/manifests/the_stack_sample/sample_2009.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod03\n  annotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.1\n    volumeMounts:\n    - mountPath: /var/local/aaa\n      name: mydir\n  volumes:\n  - name: mydir\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04386",
    "manifest_path": "data/manifests/the_stack_sample/sample_2009.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod03\n  annotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.1\n    volumeMounts:\n    - mountPath: /var/local/aaa\n      name: mydir\n  volumes:\n  - name: mydir\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04387",
    "manifest_path": "data/manifests/the_stack_sample/sample_2009.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod03\n  annotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.1\n    volumeMounts:\n    - mountPath: /var/local/aaa\n      name: mydir\n  volumes:\n  - name: mydir\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04388",
    "manifest_path": "data/manifests/the_stack_sample/sample_2009.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod03\n  annotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.1\n    volumeMounts:\n    - mountPath: /var/local/aaa\n      name: mydir\n  volumes:\n  - name: mydir\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04389",
    "manifest_path": "data/manifests/the_stack_sample/sample_2013.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: debian-debug\n  namespace: default\n  annotations:\n    injector.tumblr.com/request: test1\nspec:\n  containers:\n  - image: debian:jessie\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: debian-debug\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"debian-debug\" does not have a read-only root file system"
  },
  {
    "id": "04390",
    "manifest_path": "data/manifests/the_stack_sample/sample_2013.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: debian-debug\n  namespace: default\n  annotations:\n    injector.tumblr.com/request: test1\nspec:\n  containers:\n  - image: debian:jessie\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: debian-debug\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"debian-debug\" is not set to runAsNonRoot"
  },
  {
    "id": "04391",
    "manifest_path": "data/manifests/the_stack_sample/sample_2013.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: debian-debug\n  namespace: default\n  annotations:\n    injector.tumblr.com/request: test1\nspec:\n  containers:\n  - image: debian:jessie\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: debian-debug\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"debian-debug\" has cpu request 0"
  },
  {
    "id": "04392",
    "manifest_path": "data/manifests/the_stack_sample/sample_2013.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: debian-debug\n  namespace: default\n  annotations:\n    injector.tumblr.com/request: test1\nspec:\n  containers:\n  - image: debian:jessie\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: debian-debug\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"debian-debug\" has memory limit 0"
  },
  {
    "id": "04393",
    "manifest_path": "data/manifests/the_stack_sample/sample_2015.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  selector:\n    matchLabels:\n      run: load-balancer-example\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: load-balancer-example\n    spec:\n      containers:\n      - name: hello-world\n        image: gcr.io/google-samples/node-hello:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello-world\" does not have a read-only root file system"
  },
  {
    "id": "04394",
    "manifest_path": "data/manifests/the_stack_sample/sample_2015.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  selector:\n    matchLabels:\n      run: load-balancer-example\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: load-balancer-example\n    spec:\n      containers:\n      - name: hello-world\n        image: gcr.io/google-samples/node-hello:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello-world\" is not set to runAsNonRoot"
  },
  {
    "id": "04395",
    "manifest_path": "data/manifests/the_stack_sample/sample_2015.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  selector:\n    matchLabels:\n      run: load-balancer-example\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: load-balancer-example\n    spec:\n      containers:\n      - name: hello-world\n        image: gcr.io/google-samples/node-hello:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello-world\" has cpu request 0"
  },
  {
    "id": "04396",
    "manifest_path": "data/manifests/the_stack_sample/sample_2015.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  selector:\n    matchLabels:\n      run: load-balancer-example\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: load-balancer-example\n    spec:\n      containers:\n      - name: hello-world\n        image: gcr.io/google-samples/node-hello:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello-world\" has memory limit 0"
  },
  {
    "id": "04397",
    "manifest_path": "data/manifests/the_stack_sample/sample_2016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: kubernetes.io/hostname\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: topology.kubernetes.io/zone\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=kube-system\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        resources:\n          limits:\n            cpu: 200m\n            memory: 64Mi\n          requests:\n            cpu: 200m\n            memory: 64Mi\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pod-identity-webhook\" is using an invalid container image, \"IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04398",
    "manifest_path": "data/manifests/the_stack_sample/sample_2016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: kubernetes.io/hostname\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: topology.kubernetes.io/zone\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=kube-system\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        resources:\n          limits:\n            cpu: 200m\n            memory: 64Mi\n          requests:\n            cpu: 200m\n            memory: 64Mi\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pod-identity-webhook\" does not have a read-only root file system"
  },
  {
    "id": "04399",
    "manifest_path": "data/manifests/the_stack_sample/sample_2016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: kubernetes.io/hostname\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: topology.kubernetes.io/zone\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=kube-system\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        resources:\n          limits:\n            cpu: 200m\n            memory: 64Mi\n          requests:\n            cpu: 200m\n            memory: 64Mi\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pod-identity-webhook\" is not set to runAsNonRoot"
  },
  {
    "id": "04400",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"database\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04401",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"database\" is not set to runAsNonRoot"
  },
  {
    "id": "04402",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"database\" has cpu request 0"
  },
  {
    "id": "04403",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"database\" has memory limit 0"
  },
  {
    "id": "04404",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04405",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy\" does not have a read-only root file system"
  },
  {
    "id": "04406",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "04407",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "04408",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "04409",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-rbac-proxy\" has cpu request 0"
  },
  {
    "id": "04410",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy\" has memory limit 0"
  },
  {
    "id": "04411",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ibmmq-client\" is using an invalid container image, \"mqkeda/sample-app:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04412",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ibmmq-client\" does not have a read-only root file system"
  },
  {
    "id": "04413",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ibmmq-client\" is not set to runAsNonRoot"
  },
  {
    "id": "04414",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ibmmq-client\" has cpu request 0"
  },
  {
    "id": "04415",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ibmmq-client\" has memory limit 0"
  },
  {
    "id": "04416",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "04417",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "04418",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "04419",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "04420",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04421",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04422",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "04423",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "04424",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "04425",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "04426",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "04427",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "04428",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"jaeger-agent\" is using an invalid container image, \"jaegertracing/jaeger-agent\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04429",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jaeger-agent\" does not have a read-only root file system"
  },
  {
    "id": "04430",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress\" does not have a read-only root file system"
  },
  {
    "id": "04431",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"nginx-ingress\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "04432",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jaeger-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "04433",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jaeger-agent\" has cpu request 0"
  },
  {
    "id": "04434",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress\" has cpu request 0"
  },
  {
    "id": "04435",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jaeger-agent\" has memory limit 0"
  },
  {
    "id": "04436",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress\" has memory limit 0"
  },
  {
    "id": "04437",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04438",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04439",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04440",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04441",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04442",
    "manifest_path": "data/manifests/the_stack_sample/sample_2042.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-reflector-controller\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      app: image-reflector-controller\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: image-reflector-controller\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --enable-leader-election\n        image: squaremo/image-reflector-controller\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"squaremo/image-reflector-controller\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04443",
    "manifest_path": "data/manifests/the_stack_sample/sample_2042.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-reflector-controller\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      app: image-reflector-controller\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: image-reflector-controller\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --enable-leader-election\n        image: squaremo/image-reflector-controller\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "04444",
    "manifest_path": "data/manifests/the_stack_sample/sample_2042.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-reflector-controller\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      app: image-reflector-controller\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: image-reflector-controller\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --enable-leader-election\n        image: squaremo/image-reflector-controller\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "04445",
    "manifest_path": "data/manifests/the_stack_sample/sample_2043.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: a042ef704bc21bfd8abcd79f32e9788ecad0bdb70e7653aff9458eda85bafe3b\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: skreet2k\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-keeper\" does not have a read-only root file system"
  },
  {
    "id": "04446",
    "manifest_path": "data/manifests/the_stack_sample/sample_2043.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: a042ef704bc21bfd8abcd79f32e9788ecad0bdb70e7653aff9458eda85bafe3b\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: skreet2k\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-keeper\" is not set to runAsNonRoot"
  },
  {
    "id": "04447",
    "manifest_path": "data/manifests/the_stack_sample/sample_2047.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: privileged\n    image: busybox\n    command:\n    - sleep\n    - '9999'\n    securityContext:\n      allowPrivilegeEscalation: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"privileged\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04448",
    "manifest_path": "data/manifests/the_stack_sample/sample_2047.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: privileged\n    image: busybox\n    command:\n    - sleep\n    - '9999'\n    securityContext:\n      allowPrivilegeEscalation: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"privileged\" does not have a read-only root file system"
  },
  {
    "id": "04449",
    "manifest_path": "data/manifests/the_stack_sample/sample_2047.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: privileged\n    image: busybox\n    command:\n    - sleep\n    - '9999'\n    securityContext:\n      allowPrivilegeEscalation: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"privileged\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "04450",
    "manifest_path": "data/manifests/the_stack_sample/sample_2047.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: privileged\n    image: busybox\n    command:\n    - sleep\n    - '9999'\n    securityContext:\n      allowPrivilegeEscalation: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"privileged\" has cpu request 0"
  },
  {
    "id": "04451",
    "manifest_path": "data/manifests/the_stack_sample/sample_2047.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: privileged\n    image: busybox\n    command:\n    - sleep\n    - '9999'\n    securityContext:\n      allowPrivilegeEscalation: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"privileged\" has memory limit 0"
  },
  {
    "id": "04452",
    "manifest_path": "data/manifests/the_stack_sample/sample_2050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-cloud-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: cloud-controller-manager\n    image: mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0\n    imagePullPolicy: IfNotPresent\n    command:\n    - cloud-controller-manager\n    args:\n    - --cloud-provider=azure\n    - --controllers=cloud-node\n    - --kubeconfig=/etc/kubernetes/secrets/kubeconfig\n    - --cloud-config=/etc/kubernetes/configs/cloud.conf\n    - --leader-elect=false\n    - --port=10267\n    - -v=2\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10267\n      initialDelaySeconds: 20\n      periodSeconds: 10\n      timeoutSeconds: 5\n    volumeMounts:\n    - name: secrets\n      mountPath: /etc/kubernetes/secrets\n    - name: configs\n      mountPath: /etc/kubernetes/configs\n  volumes:\n  - name: secrets\n    hostPath:\n      path: /etc/kubernetes/bootstrap-secrets\n  - name: configs\n    hostPath:\n      path: /etc/kubernetes/bootstrap-configs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cloud-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "04453",
    "manifest_path": "data/manifests/the_stack_sample/sample_2050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-cloud-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: cloud-controller-manager\n    image: mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0\n    imagePullPolicy: IfNotPresent\n    command:\n    - cloud-controller-manager\n    args:\n    - --cloud-provider=azure\n    - --controllers=cloud-node\n    - --kubeconfig=/etc/kubernetes/secrets/kubeconfig\n    - --cloud-config=/etc/kubernetes/configs/cloud.conf\n    - --leader-elect=false\n    - --port=10267\n    - -v=2\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10267\n      initialDelaySeconds: 20\n      periodSeconds: 10\n      timeoutSeconds: 5\n    volumeMounts:\n    - name: secrets\n      mountPath: /etc/kubernetes/secrets\n    - name: configs\n      mountPath: /etc/kubernetes/configs\n  volumes:\n  - name: secrets\n    hostPath:\n      path: /etc/kubernetes/bootstrap-secrets\n  - name: configs\n    hostPath:\n      path: /etc/kubernetes/bootstrap-configs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cloud-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "04454",
    "manifest_path": "data/manifests/the_stack_sample/sample_2050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-cloud-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: cloud-controller-manager\n    image: mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0\n    imagePullPolicy: IfNotPresent\n    command:\n    - cloud-controller-manager\n    args:\n    - --cloud-provider=azure\n    - --controllers=cloud-node\n    - --kubeconfig=/etc/kubernetes/secrets/kubeconfig\n    - --cloud-config=/etc/kubernetes/configs/cloud.conf\n    - --leader-elect=false\n    - --port=10267\n    - -v=2\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10267\n      initialDelaySeconds: 20\n      periodSeconds: 10\n      timeoutSeconds: 5\n    volumeMounts:\n    - name: secrets\n      mountPath: /etc/kubernetes/secrets\n    - name: configs\n      mountPath: /etc/kubernetes/configs\n  volumes:\n  - name: secrets\n    hostPath:\n      path: /etc/kubernetes/bootstrap-secrets\n  - name: configs\n    hostPath:\n      path: /etc/kubernetes/bootstrap-configs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cloud-controller-manager\" has cpu request 0"
  },
  {
    "id": "04455",
    "manifest_path": "data/manifests/the_stack_sample/sample_2050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-cloud-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: cloud-controller-manager\n    image: mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0\n    imagePullPolicy: IfNotPresent\n    command:\n    - cloud-controller-manager\n    args:\n    - --cloud-provider=azure\n    - --controllers=cloud-node\n    - --kubeconfig=/etc/kubernetes/secrets/kubeconfig\n    - --cloud-config=/etc/kubernetes/configs/cloud.conf\n    - --leader-elect=false\n    - --port=10267\n    - -v=2\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10267\n      initialDelaySeconds: 20\n      periodSeconds: 10\n      timeoutSeconds: 5\n    volumeMounts:\n    - name: secrets\n      mountPath: /etc/kubernetes/secrets\n    - name: configs\n      mountPath: /etc/kubernetes/configs\n  volumes:\n  - name: secrets\n    hostPath:\n      path: /etc/kubernetes/bootstrap-secrets\n  - name: configs\n    hostPath:\n      path: /etc/kubernetes/bootstrap-configs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cloud-controller-manager\" has memory limit 0"
  },
  {
    "id": "04456",
    "manifest_path": "data/manifests/the_stack_sample/sample_2052.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220115-3e20513bd7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --supplemental-plugin-config-dir=/etc/plugins\n        - --config-path=/etc/config/config.yaml\n        - --supplemental-prow-config-dir=/etc/config\n        - --github-app-id=$(GITHUB_APP_ID)\n        - --github-app-private-key-path=/etc/github/cert\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        - --projected-token-file=/var/sa-token/token\n        env:\n        - name: GITHUB_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: openshift-prow-github-app\n              key: appid\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: service-account-token\n          mountPath: /var/sa-token\n        - name: github-app-credentials\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: service-account-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              path: token\n      - name: github-app-credentials\n        secret:\n          secretName: openshift-prow-github-app\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        projected:\n          sources:\n          - configMap:\n              name: job-config-misc\n          - configMap:\n              name: job-config-master-periodics\n          - configMap:\n              name: job-config-master-postsubmits\n          - configMap:\n              name: job-config-master-presubmits\n          - configMap:\n              name: job-config-3.x\n          - configMap:\n              name: job-config-4.1\n          - configMap:\n              name: job-config-4.2\n          - configMap:\n              name: job-config-4.3\n          - configMap:\n              name: job-config-4.4\n          - configMap:\n              name: job-config-4.5\n          - configMap:\n              name: job-config-4.6\n          - configMap:\n              name: job-config-4.7\n          - configMap:\n              name: job-config-4.8\n          - configMap:\n              name: job-config-4.9\n          - configMap:\n              name: job-config-4.10\n          - configMap:\n              name: job-config-4.11\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "04457",
    "manifest_path": "data/manifests/the_stack_sample/sample_2052.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220115-3e20513bd7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --supplemental-plugin-config-dir=/etc/plugins\n        - --config-path=/etc/config/config.yaml\n        - --supplemental-prow-config-dir=/etc/config\n        - --github-app-id=$(GITHUB_APP_ID)\n        - --github-app-private-key-path=/etc/github/cert\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        - --projected-token-file=/var/sa-token/token\n        env:\n        - name: GITHUB_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: openshift-prow-github-app\n              key: appid\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: service-account-token\n          mountPath: /var/sa-token\n        - name: github-app-credentials\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: service-account-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              path: token\n      - name: github-app-credentials\n        secret:\n          secretName: openshift-prow-github-app\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        projected:\n          sources:\n          - configMap:\n              name: job-config-misc\n          - configMap:\n              name: job-config-master-periodics\n          - configMap:\n              name: job-config-master-postsubmits\n          - configMap:\n              name: job-config-master-presubmits\n          - configMap:\n              name: job-config-3.x\n          - configMap:\n              name: job-config-4.1\n          - configMap:\n              name: job-config-4.2\n          - configMap:\n              name: job-config-4.3\n          - configMap:\n              name: job-config-4.4\n          - configMap:\n              name: job-config-4.5\n          - configMap:\n              name: job-config-4.6\n          - configMap:\n              name: job-config-4.7\n          - configMap:\n              name: job-config-4.8\n          - configMap:\n              name: job-config-4.9\n          - configMap:\n              name: job-config-4.10\n          - configMap:\n              name: job-config-4.11\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "04458",
    "manifest_path": "data/manifests/the_stack_sample/sample_2052.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220115-3e20513bd7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --supplemental-plugin-config-dir=/etc/plugins\n        - --config-path=/etc/config/config.yaml\n        - --supplemental-prow-config-dir=/etc/config\n        - --github-app-id=$(GITHUB_APP_ID)\n        - --github-app-private-key-path=/etc/github/cert\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        - --projected-token-file=/var/sa-token/token\n        env:\n        - name: GITHUB_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: openshift-prow-github-app\n              key: appid\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: service-account-token\n          mountPath: /var/sa-token\n        - name: github-app-credentials\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: service-account-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              path: token\n      - name: github-app-credentials\n        secret:\n          secretName: openshift-prow-github-app\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        projected:\n          sources:\n          - configMap:\n              name: job-config-misc\n          - configMap:\n              name: job-config-master-periodics\n          - configMap:\n              name: job-config-master-postsubmits\n          - configMap:\n              name: job-config-master-presubmits\n          - configMap:\n              name: job-config-3.x\n          - configMap:\n              name: job-config-4.1\n          - configMap:\n              name: job-config-4.2\n          - configMap:\n              name: job-config-4.3\n          - configMap:\n              name: job-config-4.4\n          - configMap:\n              name: job-config-4.5\n          - configMap:\n              name: job-config-4.6\n          - configMap:\n              name: job-config-4.7\n          - configMap:\n              name: job-config-4.8\n          - configMap:\n              name: job-config-4.9\n          - configMap:\n              name: job-config-4.10\n          - configMap:\n              name: job-config-4.11\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "04459",
    "manifest_path": "data/manifests/the_stack_sample/sample_2054.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-tools\n  namespace: rook-ceph\n  labels:\n    app: rook-ceph-tools\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-ceph-tools\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-tools\n    spec:\n      containers:\n      - name: rook-ceph-tools\n        image: rook/ceph:v1.2.2\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_ADMIN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: admin-secret\n        volumeMounts:\n        - mountPath: /etc/ceph\n          name: ceph-config\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n      - name: ceph-config\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rook-ceph-tools\" does not have a read-only root file system"
  },
  {
    "id": "04460",
    "manifest_path": "data/manifests/the_stack_sample/sample_2054.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-tools\n  namespace: rook-ceph\n  labels:\n    app: rook-ceph-tools\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-ceph-tools\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-tools\n    spec:\n      containers:\n      - name: rook-ceph-tools\n        image: rook/ceph:v1.2.2\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_ADMIN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: admin-secret\n        volumeMounts:\n        - mountPath: /etc/ceph\n          name: ceph-config\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n      - name: ceph-config\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rook-ceph-tools\" is not set to runAsNonRoot"
  },
  {
    "id": "04461",
    "manifest_path": "data/manifests/the_stack_sample/sample_2054.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-tools\n  namespace: rook-ceph\n  labels:\n    app: rook-ceph-tools\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-ceph-tools\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-tools\n    spec:\n      containers:\n      - name: rook-ceph-tools\n        image: rook/ceph:v1.2.2\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_ADMIN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: admin-secret\n        volumeMounts:\n        - mountPath: /etc/ceph\n          name: ceph-config\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n      - name: ceph-config\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rook-ceph-tools\" has cpu request 0"
  },
  {
    "id": "04462",
    "manifest_path": "data/manifests/the_stack_sample/sample_2054.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-tools\n  namespace: rook-ceph\n  labels:\n    app: rook-ceph-tools\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-ceph-tools\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-tools\n    spec:\n      containers:\n      - name: rook-ceph-tools\n        image: rook/ceph:v1.2.2\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_ADMIN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: admin-secret\n        volumeMounts:\n        - mountPath: /etc/ceph\n          name: ceph-config\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n      - name: ceph-config\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rook-ceph-tools\" has memory limit 0"
  },
  {
    "id": "04463",
    "manifest_path": "data/manifests/the_stack_sample/sample_2056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litmus-experiment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: litmus-experiment\n  template:\n    metadata:\n      labels:\n        app: litmus-experiment\n    spec:\n      serviceAccountName: pod-io-error-retval-sa\n      containers:\n      - name: gotest\n        image: busybox\n        command:\n        - sleep\n        - '3600'\n        env:\n        - name: APP_NAMESPACE\n          value: default\n        - name: APP_LABEL\n          value: run=nginx\n        - name: APP_KIND\n          value: deployment\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: MEMORY_CONSUMPTION\n          value: '500'\n        - name: PODS_AFFECTED_PERC\n          value: '100'\n        - name: LIB\n          value: litmus\n        - name: TARGET_POD\n          value: ''\n        - name: TARGET_CONTAINER\n          value: ''\n        - name: SEQUENCE\n          value: parallel\n        - name: CHAOS_NAMESPACE\n          value: default\n        - name: RAMP_TIME\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"gotest\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04464",
    "manifest_path": "data/manifests/the_stack_sample/sample_2056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litmus-experiment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: litmus-experiment\n  template:\n    metadata:\n      labels:\n        app: litmus-experiment\n    spec:\n      serviceAccountName: pod-io-error-retval-sa\n      containers:\n      - name: gotest\n        image: busybox\n        command:\n        - sleep\n        - '3600'\n        env:\n        - name: APP_NAMESPACE\n          value: default\n        - name: APP_LABEL\n          value: run=nginx\n        - name: APP_KIND\n          value: deployment\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: MEMORY_CONSUMPTION\n          value: '500'\n        - name: PODS_AFFECTED_PERC\n          value: '100'\n        - name: LIB\n          value: litmus\n        - name: TARGET_POD\n          value: ''\n        - name: TARGET_CONTAINER\n          value: ''\n        - name: SEQUENCE\n          value: parallel\n        - name: CHAOS_NAMESPACE\n          value: default\n        - name: RAMP_TIME\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gotest\" does not have a read-only root file system"
  },
  {
    "id": "04465",
    "manifest_path": "data/manifests/the_stack_sample/sample_2056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litmus-experiment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: litmus-experiment\n  template:\n    metadata:\n      labels:\n        app: litmus-experiment\n    spec:\n      serviceAccountName: pod-io-error-retval-sa\n      containers:\n      - name: gotest\n        image: busybox\n        command:\n        - sleep\n        - '3600'\n        env:\n        - name: APP_NAMESPACE\n          value: default\n        - name: APP_LABEL\n          value: run=nginx\n        - name: APP_KIND\n          value: deployment\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: MEMORY_CONSUMPTION\n          value: '500'\n        - name: PODS_AFFECTED_PERC\n          value: '100'\n        - name: LIB\n          value: litmus\n        - name: TARGET_POD\n          value: ''\n        - name: TARGET_CONTAINER\n          value: ''\n        - name: SEQUENCE\n          value: parallel\n        - name: CHAOS_NAMESPACE\n          value: default\n        - name: RAMP_TIME\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gotest\" is not set to runAsNonRoot"
  },
  {
    "id": "04466",
    "manifest_path": "data/manifests/the_stack_sample/sample_2056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litmus-experiment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: litmus-experiment\n  template:\n    metadata:\n      labels:\n        app: litmus-experiment\n    spec:\n      serviceAccountName: pod-io-error-retval-sa\n      containers:\n      - name: gotest\n        image: busybox\n        command:\n        - sleep\n        - '3600'\n        env:\n        - name: APP_NAMESPACE\n          value: default\n        - name: APP_LABEL\n          value: run=nginx\n        - name: APP_KIND\n          value: deployment\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: MEMORY_CONSUMPTION\n          value: '500'\n        - name: PODS_AFFECTED_PERC\n          value: '100'\n        - name: LIB\n          value: litmus\n        - name: TARGET_POD\n          value: ''\n        - name: TARGET_CONTAINER\n          value: ''\n        - name: SEQUENCE\n          value: parallel\n        - name: CHAOS_NAMESPACE\n          value: default\n        - name: RAMP_TIME\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gotest\" has cpu request 0"
  },
  {
    "id": "04467",
    "manifest_path": "data/manifests/the_stack_sample/sample_2056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litmus-experiment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: litmus-experiment\n  template:\n    metadata:\n      labels:\n        app: litmus-experiment\n    spec:\n      serviceAccountName: pod-io-error-retval-sa\n      containers:\n      - name: gotest\n        image: busybox\n        command:\n        - sleep\n        - '3600'\n        env:\n        - name: APP_NAMESPACE\n          value: default\n        - name: APP_LABEL\n          value: run=nginx\n        - name: APP_KIND\n          value: deployment\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: MEMORY_CONSUMPTION\n          value: '500'\n        - name: PODS_AFFECTED_PERC\n          value: '100'\n        - name: LIB\n          value: litmus\n        - name: TARGET_POD\n          value: ''\n        - name: TARGET_CONTAINER\n          value: ''\n        - name: SEQUENCE\n          value: parallel\n        - name: CHAOS_NAMESPACE\n          value: default\n        - name: RAMP_TIME\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gotest\" has memory limit 0"
  },
  {
    "id": "04468",
    "manifest_path": "data/manifests/the_stack_sample/sample_2057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"php-redis\" does not have a read-only root file system"
  },
  {
    "id": "04469",
    "manifest_path": "data/manifests/the_stack_sample/sample_2057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"php-redis\" is not set to runAsNonRoot"
  },
  {
    "id": "04470",
    "manifest_path": "data/manifests/the_stack_sample/sample_2057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"php-redis\" has memory limit 0"
  },
  {
    "id": "04471",
    "manifest_path": "data/manifests/the_stack_sample/sample_2060.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: nuodb-app-chaos\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: nuodbns\n        - name: APP_LABEL\n          value: nodetype=sm\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/nuodb/chaos/app_pod_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "04472",
    "manifest_path": "data/manifests/the_stack_sample/sample_2060.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: nuodb-app-chaos\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: nuodbns\n        - name: APP_LABEL\n          value: nodetype=sm\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/nuodb/chaos/app_pod_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "04473",
    "manifest_path": "data/manifests/the_stack_sample/sample_2060.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: nuodb-app-chaos\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: nuodbns\n        - name: APP_LABEL\n          value: nodetype=sm\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/nuodb/chaos/app_pod_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "04474",
    "manifest_path": "data/manifests/the_stack_sample/sample_2060.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: nuodb-app-chaos\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: nuodbns\n        - name: APP_LABEL\n          value: nodetype=sm\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/nuodb/chaos/app_pod_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "04475",
    "manifest_path": "data/manifests/the_stack_sample/sample_2062.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: demo\n  name: demo\nspec:\n  containers:\n  - image: in-cluster\n    name: demo\n    imagePullPolicy: Never\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"demo\" is using an invalid container image, \"in-cluster\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04476",
    "manifest_path": "data/manifests/the_stack_sample/sample_2062.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: demo\n  name: demo\nspec:\n  containers:\n  - image: in-cluster\n    name: demo\n    imagePullPolicy: Never\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"demo\" does not have a read-only root file system"
  },
  {
    "id": "04477",
    "manifest_path": "data/manifests/the_stack_sample/sample_2062.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: demo\n  name: demo\nspec:\n  containers:\n  - image: in-cluster\n    name: demo\n    imagePullPolicy: Never\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"demo\" is not set to runAsNonRoot"
  },
  {
    "id": "04478",
    "manifest_path": "data/manifests/the_stack_sample/sample_2062.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: demo\n  name: demo\nspec:\n  containers:\n  - image: in-cluster\n    name: demo\n    imagePullPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"demo\" has cpu request 0"
  },
  {
    "id": "04479",
    "manifest_path": "data/manifests/the_stack_sample/sample_2062.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: demo\n  name: demo\nspec:\n  containers:\n  - image: in-cluster\n    name: demo\n    imagePullPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"demo\" has memory limit 0"
  },
  {
    "id": "04480",
    "manifest_path": "data/manifests/the_stack_sample/sample_2064.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-background-producer-deployment\nspec:\n  selector:\n    matchLabels:\n      app: kafka-background-producer\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: kafka-background-producer\n    spec:\n      containers:\n      - name: kafka-background-producer\n        image: ${BACKENDV2_IMAGE}\n        command:\n        - npm\n        - run\n        - background-producer\n        imagePullPolicy: Always\n        resources:\n          requests:\n            memory: 300Mi\n            cpu: 100m\n          limits:\n            memory: 500Mi\n            cpu: 500m\n        env:\n        - name: NODE_ENV\n          value: production\n        - name: REDIS_HOST\n          value: quizzes-backend-redis-master.default.svc.cluster.local\n        - name: REDIS_PORT\n          value: '6379'\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: quizzes-backend-redis\n              key: redis-password\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_DATABASE\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_HOST\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_PASSWORD\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_USERNAME\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: KAFKA_HOST\n        - name: MESSAGE_FORMAT_VERSION\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: MESSAGE_FORMAT_VERSION\n        - name: NEW_RELIC_LICENSE_KEY\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_LICENSE_KEY\n        - name: NEW_RELIC_APP_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_APP_NAME\n        - name: NEW_RELIC_NO_CONFIG_FILE\n          value: 'true'\n        - name: SERVICE_ID\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SERVICE_ID\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SENTRY_DSN\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kafka-background-producer\" is using an invalid container image, \"${BACKENDV2_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04481",
    "manifest_path": "data/manifests/the_stack_sample/sample_2064.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-background-producer-deployment\nspec:\n  selector:\n    matchLabels:\n      app: kafka-background-producer\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: kafka-background-producer\n    spec:\n      containers:\n      - name: kafka-background-producer\n        image: ${BACKENDV2_IMAGE}\n        command:\n        - npm\n        - run\n        - background-producer\n        imagePullPolicy: Always\n        resources:\n          requests:\n            memory: 300Mi\n            cpu: 100m\n          limits:\n            memory: 500Mi\n            cpu: 500m\n        env:\n        - name: NODE_ENV\n          value: production\n        - name: REDIS_HOST\n          value: quizzes-backend-redis-master.default.svc.cluster.local\n        - name: REDIS_PORT\n          value: '6379'\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: quizzes-backend-redis\n              key: redis-password\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_DATABASE\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_HOST\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_PASSWORD\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_USERNAME\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: KAFKA_HOST\n        - name: MESSAGE_FORMAT_VERSION\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: MESSAGE_FORMAT_VERSION\n        - name: NEW_RELIC_LICENSE_KEY\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_LICENSE_KEY\n        - name: NEW_RELIC_APP_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_APP_NAME\n        - name: NEW_RELIC_NO_CONFIG_FILE\n          value: 'true'\n        - name: SERVICE_ID\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SERVICE_ID\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SENTRY_DSN\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kafka-background-producer\" does not have a read-only root file system"
  },
  {
    "id": "04482",
    "manifest_path": "data/manifests/the_stack_sample/sample_2064.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-background-producer-deployment\nspec:\n  selector:\n    matchLabels:\n      app: kafka-background-producer\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: kafka-background-producer\n    spec:\n      containers:\n      - name: kafka-background-producer\n        image: ${BACKENDV2_IMAGE}\n        command:\n        - npm\n        - run\n        - background-producer\n        imagePullPolicy: Always\n        resources:\n          requests:\n            memory: 300Mi\n            cpu: 100m\n          limits:\n            memory: 500Mi\n            cpu: 500m\n        env:\n        - name: NODE_ENV\n          value: production\n        - name: REDIS_HOST\n          value: quizzes-backend-redis-master.default.svc.cluster.local\n        - name: REDIS_PORT\n          value: '6379'\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: quizzes-backend-redis\n              key: redis-password\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_DATABASE\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_HOST\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_PASSWORD\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_USERNAME\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: KAFKA_HOST\n        - name: MESSAGE_FORMAT_VERSION\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: MESSAGE_FORMAT_VERSION\n        - name: NEW_RELIC_LICENSE_KEY\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_LICENSE_KEY\n        - name: NEW_RELIC_APP_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_APP_NAME\n        - name: NEW_RELIC_NO_CONFIG_FILE\n          value: 'true'\n        - name: SERVICE_ID\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SERVICE_ID\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SENTRY_DSN\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kafka-background-producer\" is not set to runAsNonRoot"
  },
  {
    "id": "04483",
    "manifest_path": "data/manifests/the_stack_sample/sample_2067.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: vladimir44/microservices-demo-payment-service:0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"paymentservice\" does not have a read-only root file system"
  },
  {
    "id": "04484",
    "manifest_path": "data/manifests/the_stack_sample/sample_2067.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: vladimir44/microservices-demo-payment-service:0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"paymentservice\" is not set to runAsNonRoot"
  },
  {
    "id": "04485",
    "manifest_path": "data/manifests/the_stack_sample/sample_2067.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: vladimir44/microservices-demo-payment-service:0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"paymentservice\" has cpu request 0"
  },
  {
    "id": "04486",
    "manifest_path": "data/manifests/the_stack_sample/sample_2067.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: vladimir44/microservices-demo-payment-service:0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"paymentservice\" has memory limit 0"
  },
  {
    "id": "04487",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ebs-plugin\" is using an invalid container image, \"amazon/aws-ebs-csi-driver:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04488",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-attacher\" does not have a read-only root file system"
  },
  {
    "id": "04489",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "04490",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ebs-plugin\" does not have a read-only root file system"
  },
  {
    "id": "04491",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "04492",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-attacher\" is not set to runAsNonRoot"
  },
  {
    "id": "04493",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "04494",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ebs-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "04495",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "04496",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-attacher\" has cpu request 0"
  },
  {
    "id": "04497",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-provisioner\" has cpu request 0"
  },
  {
    "id": "04498",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ebs-plugin\" has cpu request 0"
  },
  {
    "id": "04499",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness-probe\" has cpu request 0"
  },
  {
    "id": "04500",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-attacher\" has memory limit 0"
  },
  {
    "id": "04501",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-provisioner\" has memory limit 0"
  },
  {
    "id": "04502",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ebs-plugin\" has memory limit 0"
  },
  {
    "id": "04503",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness-probe\" has memory limit 0"
  },
  {
    "id": "04504",
    "manifest_path": "data/manifests/the_stack_sample/sample_2069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v2\n  labels:\n    app: reviews\n    version: v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v2\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v2:1.16.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: LOG_DIR\n          value: /tmp/logs\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: wlp-output\n          mountPath: /opt/ibm/wlp/output\n      volumes:\n      - name: wlp-output\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"reviews\" does not have a read-only root file system"
  },
  {
    "id": "04505",
    "manifest_path": "data/manifests/the_stack_sample/sample_2069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v2\n  labels:\n    app: reviews\n    version: v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v2\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v2:1.16.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: LOG_DIR\n          value: /tmp/logs\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: wlp-output\n          mountPath: /opt/ibm/wlp/output\n      volumes:\n      - name: wlp-output\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"reviews\" is not set to runAsNonRoot"
  },
  {
    "id": "04506",
    "manifest_path": "data/manifests/the_stack_sample/sample_2069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v2\n  labels:\n    app: reviews\n    version: v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v2\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v2:1.16.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: LOG_DIR\n          value: /tmp/logs\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: wlp-output\n          mountPath: /opt/ibm/wlp/output\n      volumes:\n      - name: wlp-output\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"reviews\" has cpu request 0"
  },
  {
    "id": "04507",
    "manifest_path": "data/manifests/the_stack_sample/sample_2069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v2\n  labels:\n    app: reviews\n    version: v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v2\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v2:1.16.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: LOG_DIR\n          value: /tmp/logs\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: wlp-output\n          mountPath: /opt/ibm/wlp/output\n      volumes:\n      - name: wlp-output\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"reviews\" has memory limit 0"
  },
  {
    "id": "04508",
    "manifest_path": "data/manifests/the_stack_sample/sample_2072.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: burpsuite\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: burpsuite\n        app.kubernetes.io/instance: burpsuite\n        app.kubernetes.io/version: 2022.1.1\n    spec:\n      containers:\n      - name: burpsuite\n        image: ghcr.io/k4yt3x/burpsuite:2022.1.1\n        env:\n        - name: DISPLAY\n          value: :0\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: x11-unix\n          mountPath: /tmp/.X11-unix\n      volumes:\n      - name: x11-unix\n        hostPath:\n          path: /tmp/.X11-unix\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"burpsuite\" does not have a read-only root file system"
  },
  {
    "id": "04509",
    "manifest_path": "data/manifests/the_stack_sample/sample_2072.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: burpsuite\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: burpsuite\n        app.kubernetes.io/instance: burpsuite\n        app.kubernetes.io/version: 2022.1.1\n    spec:\n      containers:\n      - name: burpsuite\n        image: ghcr.io/k4yt3x/burpsuite:2022.1.1\n        env:\n        - name: DISPLAY\n          value: :0\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: x11-unix\n          mountPath: /tmp/.X11-unix\n      volumes:\n      - name: x11-unix\n        hostPath:\n          path: /tmp/.X11-unix\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"burpsuite\" is not set to runAsNonRoot"
  },
  {
    "id": "04510",
    "manifest_path": "data/manifests/the_stack_sample/sample_2072.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: burpsuite\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: burpsuite\n        app.kubernetes.io/instance: burpsuite\n        app.kubernetes.io/version: 2022.1.1\n    spec:\n      containers:\n      - name: burpsuite\n        image: ghcr.io/k4yt3x/burpsuite:2022.1.1\n        env:\n        - name: DISPLAY\n          value: :0\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: x11-unix\n          mountPath: /tmp/.X11-unix\n      volumes:\n      - name: x11-unix\n        hostPath:\n          path: /tmp/.X11-unix\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"burpsuite\" has cpu request 0"
  },
  {
    "id": "04511",
    "manifest_path": "data/manifests/the_stack_sample/sample_2072.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: burpsuite\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: burpsuite\n        app.kubernetes.io/instance: burpsuite\n        app.kubernetes.io/version: 2022.1.1\n    spec:\n      containers:\n      - name: burpsuite\n        image: ghcr.io/k4yt3x/burpsuite:2022.1.1\n        env:\n        - name: DISPLAY\n          value: :0\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: x11-unix\n          mountPath: /tmp/.X11-unix\n      volumes:\n      - name: x11-unix\n        hostPath:\n          path: /tmp/.X11-unix\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"burpsuite\" has memory limit 0"
  },
  {
    "id": "04512",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"project-app\" does not have a read-only root file system"
  },
  {
    "id": "04513",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"project-service\" does not have a read-only root file system"
  },
  {
    "id": "04514",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"project-app\" is not set to runAsNonRoot"
  },
  {
    "id": "04515",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"project-service\" is not set to runAsNonRoot"
  },
  {
    "id": "04516",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"project-app\" has cpu request 0"
  },
  {
    "id": "04517",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"project-service\" has cpu request 0"
  },
  {
    "id": "04518",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"project-app\" has memory limit 0"
  },
  {
    "id": "04519",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"project-service\" has memory limit 0"
  },
  {
    "id": "04520",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"buildkitd\" does not have a read-only root file system"
  },
  {
    "id": "04521",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"buildkitd\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04522",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"buildkitd\" is privileged"
  },
  {
    "id": "04523",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"buildkitd\" is not set to runAsNonRoot"
  },
  {
    "id": "04524",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"buildkitd\" has cpu request 0"
  },
  {
    "id": "04525",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"buildkitd\" has memory limit 0"
  },
  {
    "id": "04526",
    "manifest_path": "data/manifests/the_stack_sample/sample_2075.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-test\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysql-test\n  template:\n    metadata:\n      labels:\n        app: mysql-test\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: key\n                operator: In\n                values:\n                - value\n      containers:\n      - name: mysql-test\n        image: statemood/mysql:5.7.21\n        imagePullPolicy: Always\n        env:\n        - name: MYSQL_CONFIG_FILE\n          value: /var/lib/mysql/my.cnf\n        - name: MYSQL_ROOT_PASSWORD\n          value: ''\n        volumeMounts:\n        - mountPath: /var/lib/mysql\n          name: data\n        ports:\n        - containerPort: 3306\n        resources:\n          limits:\n            cpu: 900m\n            memory: 2Gi\n          requests:\n            cpu: 900m\n            memory: 2Gi\n        livenessProbe:\n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          periodSeconds: 20\n        readinessProbe:\n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          periodSeconds: 20\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-mysql-test\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql-test\" does not have a read-only root file system"
  },
  {
    "id": "04527",
    "manifest_path": "data/manifests/the_stack_sample/sample_2075.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-test\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysql-test\n  template:\n    metadata:\n      labels:\n        app: mysql-test\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: key\n                operator: In\n                values:\n                - value\n      containers:\n      - name: mysql-test\n        image: statemood/mysql:5.7.21\n        imagePullPolicy: Always\n        env:\n        - name: MYSQL_CONFIG_FILE\n          value: /var/lib/mysql/my.cnf\n        - name: MYSQL_ROOT_PASSWORD\n          value: ''\n        volumeMounts:\n        - mountPath: /var/lib/mysql\n          name: data\n        ports:\n        - containerPort: 3306\n        resources:\n          limits:\n            cpu: 900m\n            memory: 2Gi\n          requests:\n            cpu: 900m\n            memory: 2Gi\n        livenessProbe:\n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          periodSeconds: 20\n        readinessProbe:\n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          periodSeconds: 20\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-mysql-test\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql-test\" is not set to runAsNonRoot"
  },
  {
    "id": "04528",
    "manifest_path": "data/manifests/the_stack_sample/sample_2076.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-client\n  labels:\n    app: echo-client\n    group: sample\n    version: 0.0.1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echo-client\n      group: sample\n      version: 0.0.1\n  template:\n    metadata:\n      labels:\n        app: echo-client\n        group: sample\n        version: 0.0.1\n    spec:\n      containers:\n      - name: echo-client\n        image: echo-client:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 7001\n        - name: http-metrics\n          containerPort: 7090\n        resources:\n          limits:\n            cpu: 250m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        livenessProbe:\n          httpGet:\n            path: /live\n            port: 7090\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 7090\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"echo-client\" is using an invalid container image, \"echo-client:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04529",
    "manifest_path": "data/manifests/the_stack_sample/sample_2076.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-client\n  labels:\n    app: echo-client\n    group: sample\n    version: 0.0.1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echo-client\n      group: sample\n      version: 0.0.1\n  template:\n    metadata:\n      labels:\n        app: echo-client\n        group: sample\n        version: 0.0.1\n    spec:\n      containers:\n      - name: echo-client\n        image: echo-client:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 7001\n        - name: http-metrics\n          containerPort: 7090\n        resources:\n          limits:\n            cpu: 250m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        livenessProbe:\n          httpGet:\n            path: /live\n            port: 7090\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 7090\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"echo-client\" does not have a read-only root file system"
  },
  {
    "id": "04530",
    "manifest_path": "data/manifests/the_stack_sample/sample_2076.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-client\n  labels:\n    app: echo-client\n    group: sample\n    version: 0.0.1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echo-client\n      group: sample\n      version: 0.0.1\n  template:\n    metadata:\n      labels:\n        app: echo-client\n        group: sample\n        version: 0.0.1\n    spec:\n      containers:\n      - name: echo-client\n        image: echo-client:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 7001\n        - name: http-metrics\n          containerPort: 7090\n        resources:\n          limits:\n            cpu: 250m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        livenessProbe:\n          httpGet:\n            path: /live\n            port: 7090\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 7090\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"echo-client\" is not set to runAsNonRoot"
  },
  {
    "id": "04531",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"read-du\" is using an invalid container image, \"giantswarm/tiny-tools\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04532",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"caddy\" does not have a read-only root file system"
  },
  {
    "id": "04533",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"read-du\" does not have a read-only root file system"
  },
  {
    "id": "04534",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"caddy\" is not set to runAsNonRoot"
  },
  {
    "id": "04535",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"read-du\" is not set to runAsNonRoot"
  },
  {
    "id": "04536",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"caddy\" has cpu request 0"
  },
  {
    "id": "04537",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"read-du\" has cpu request 0"
  },
  {
    "id": "04538",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"caddy\" has memory limit 0"
  },
  {
    "id": "04539",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"read-du\" has memory limit 0"
  },
  {
    "id": "04540",
    "manifest_path": "data/manifests/the_stack_sample/sample_2082.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: localingress-ds\n  namespace: localingress\n  labels:\n    component: localingress\nspec:\n  selector:\n    matchLabels:\n      component: localingress\n  template:\n    metadata:\n      labels:\n        component: localingress\n    spec:\n      volumes:\n      - name: localingress-config\n        configMap:\n          name: localingress-config\n      containers:\n      - name: ingress\n        image: docker.io/library/haproxy:2.2.4\n        volumeMounts:\n        - name: localingress-config\n          mountPath: /usr/local/etc/haproxy/\n        ports:\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 80\n          hostPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ingress\" does not have a read-only root file system"
  },
  {
    "id": "04541",
    "manifest_path": "data/manifests/the_stack_sample/sample_2082.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: localingress-ds\n  namespace: localingress\n  labels:\n    component: localingress\nspec:\n  selector:\n    matchLabels:\n      component: localingress\n  template:\n    metadata:\n      labels:\n        component: localingress\n    spec:\n      volumes:\n      - name: localingress-config\n        configMap:\n          name: localingress-config\n      containers:\n      - name: ingress\n        image: docker.io/library/haproxy:2.2.4\n        volumeMounts:\n        - name: localingress-config\n          mountPath: /usr/local/etc/haproxy/\n        ports:\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 80\n          hostPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ingress\" is not set to runAsNonRoot"
  },
  {
    "id": "04542",
    "manifest_path": "data/manifests/the_stack_sample/sample_2082.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: localingress-ds\n  namespace: localingress\n  labels:\n    component: localingress\nspec:\n  selector:\n    matchLabels:\n      component: localingress\n  template:\n    metadata:\n      labels:\n        component: localingress\n    spec:\n      volumes:\n      - name: localingress-config\n        configMap:\n          name: localingress-config\n      containers:\n      - name: ingress\n        image: docker.io/library/haproxy:2.2.4\n        volumeMounts:\n        - name: localingress-config\n          mountPath: /usr/local/etc/haproxy/\n        ports:\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 80\n          hostPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ingress\" has cpu request 0"
  },
  {
    "id": "04543",
    "manifest_path": "data/manifests/the_stack_sample/sample_2082.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: localingress-ds\n  namespace: localingress\n  labels:\n    component: localingress\nspec:\n  selector:\n    matchLabels:\n      component: localingress\n  template:\n    metadata:\n      labels:\n        component: localingress\n    spec:\n      volumes:\n      - name: localingress-config\n        configMap:\n          name: localingress-config\n      containers:\n      - name: ingress\n        image: docker.io/library/haproxy:2.2.4\n        volumeMounts:\n        - name: localingress-config\n          mountPath: /usr/local/etc/haproxy/\n        ports:\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 80\n          hostPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ingress\" has memory limit 0"
  },
  {
    "id": "04544",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rook-direct-mount\" does not have a read-only root file system"
  },
  {
    "id": "04545",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"rook-direct-mount\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04546",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"rook-direct-mount\" is privileged"
  },
  {
    "id": "04547",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rook-direct-mount\" is not set to runAsNonRoot"
  },
  {
    "id": "04548",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rook-direct-mount\" has cpu request 0"
  },
  {
    "id": "04549",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rook-direct-mount\" has memory limit 0"
  },
  {
    "id": "04550",
    "manifest_path": "data/manifests/the_stack_sample/sample_2088.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-stress-normal\n  labels:\n    app: cassandra-stress\nspec:\n  volumes:\n  - name: cassandra-stress-profile-volume\n    configMap:\n      name: cassandra-stress-normal\n  securityContext:\n    fsGroup: 1\n    runAsNonRoot: true\n    runAsUser: 1006\n    supplementalGroups:\n    - 1\n  containers:\n  - name: cassie1-cassandra-stress\n    image: cassandra\n    imagePullPolicy: IfNotPresent\n    securityContext:\n      capabilities:\n        add:\n        - IPC_LOCK\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - cassandra-stress 'user profile=/opt/cassandra-stress/normal_stress.yaml ops(insert=10,query_by_sub_id=8)\n      duration=120m cl=one -node cassandra-demo -mode native cql3 user=bench password=monbench\n      -rate threads=30 -pop seq=0..100M -tokenrange -graph file=/tmp/stress-normal.html'\n      && echo END && while true ; do sleep 60; done\n    resources:\n      limits:\n        cpu: '3'\n        memory: 8Gi\n      requests:\n        cpu: '3'\n        memory: 8Gi\n    volumeMounts:\n    - name: cassandra-stress-profile-volume\n      mountPath: /opt/cassandra-stress\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cassie1-cassandra-stress\" is using an invalid container image, \"cassandra\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04551",
    "manifest_path": "data/manifests/the_stack_sample/sample_2088.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-stress-normal\n  labels:\n    app: cassandra-stress\nspec:\n  volumes:\n  - name: cassandra-stress-profile-volume\n    configMap:\n      name: cassandra-stress-normal\n  securityContext:\n    fsGroup: 1\n    runAsNonRoot: true\n    runAsUser: 1006\n    supplementalGroups:\n    - 1\n  containers:\n  - name: cassie1-cassandra-stress\n    image: cassandra\n    imagePullPolicy: IfNotPresent\n    securityContext:\n      capabilities:\n        add:\n        - IPC_LOCK\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - cassandra-stress 'user profile=/opt/cassandra-stress/normal_stress.yaml ops(insert=10,query_by_sub_id=8)\n      duration=120m cl=one -node cassandra-demo -mode native cql3 user=bench password=monbench\n      -rate threads=30 -pop seq=0..100M -tokenrange -graph file=/tmp/stress-normal.html'\n      && echo END && while true ; do sleep 60; done\n    resources:\n      limits:\n        cpu: '3'\n        memory: 8Gi\n      requests:\n        cpu: '3'\n        memory: 8Gi\n    volumeMounts:\n    - name: cassandra-stress-profile-volume\n      mountPath: /opt/cassandra-stress\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cassie1-cassandra-stress\" does not have a read-only root file system"
  },
  {
    "id": "04552",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable AGENTS_JWT_SECRET in container \"imicros-users\" found"
  },
  {
    "id": "04553",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable JWT_SECRET in container \"imicros-users\" found"
  },
  {
    "id": "04554",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"imicros-users\" is using an invalid container image, \"al66/imicros-backend:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04555",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"imicros-users\" does not have a read-only root file system"
  },
  {
    "id": "04556",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"imicros-users\" is not set to runAsNonRoot"
  },
  {
    "id": "04557",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"imicros-users\" has cpu request 0"
  },
  {
    "id": "04558",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"imicros-users\" has memory limit 0"
  },
  {
    "id": "04559",
    "manifest_path": "data/manifests/the_stack_sample/sample_2090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: test\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 1\n            memory: 500Mi\n          limits:\n            cpu: 2\n            memory: 1024Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04560",
    "manifest_path": "data/manifests/the_stack_sample/sample_2090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: test\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 1\n            memory: 500Mi\n          limits:\n            cpu: 2\n            memory: 1024Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04561",
    "manifest_path": "data/manifests/the_stack_sample/sample_2090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: test\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 1\n            memory: 500Mi\n          limits:\n            cpu: 2\n            memory: 1024Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04562",
    "manifest_path": "data/manifests/the_stack_sample/sample_2092.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: user\n  template:\n    metadata:\n      labels:\n        app: user\n    spec:\n      containers:\n      - name: user\n        image: p3fightclub.azurecr.io/user\n        ports:\n        - containerPort: 5001\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"user\" is using an invalid container image, \"p3fightclub.azurecr.io/user\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04563",
    "manifest_path": "data/manifests/the_stack_sample/sample_2092.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: user\n  template:\n    metadata:\n      labels:\n        app: user\n    spec:\n      containers:\n      - name: user\n        image: p3fightclub.azurecr.io/user\n        ports:\n        - containerPort: 5001\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"user\" does not have a read-only root file system"
  },
  {
    "id": "04564",
    "manifest_path": "data/manifests/the_stack_sample/sample_2092.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: user\n  template:\n    metadata:\n      labels:\n        app: user\n    spec:\n      containers:\n      - name: user\n        image: p3fightclub.azurecr.io/user\n        ports:\n        - containerPort: 5001\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"user\" is not set to runAsNonRoot"
  },
  {
    "id": "04565",
    "manifest_path": "data/manifests/the_stack_sample/sample_2092.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: user\n  template:\n    metadata:\n      labels:\n        app: user\n    spec:\n      containers:\n      - name: user\n        image: p3fightclub.azurecr.io/user\n        ports:\n        - containerPort: 5001\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"user\" has cpu request 0"
  },
  {
    "id": "04566",
    "manifest_path": "data/manifests/the_stack_sample/sample_2092.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: user\n  template:\n    metadata:\n      labels:\n        app: user\n    spec:\n      containers:\n      - name: user\n        image: p3fightclub.azurecr.io/user\n        ports:\n        - containerPort: 5001\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"user\" has memory limit 0"
  },
  {
    "id": "04567",
    "manifest_path": "data/manifests/the_stack_sample/sample_2094.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04568",
    "manifest_path": "data/manifests/the_stack_sample/sample_2094.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04569",
    "manifest_path": "data/manifests/the_stack_sample/sample_2094.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04570",
    "manifest_path": "data/manifests/the_stack_sample/sample_2094.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04571",
    "manifest_path": "data/manifests/the_stack_sample/sample_2094.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04572",
    "manifest_path": "data/manifests/the_stack_sample/sample_2095.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tickets-celery\n  labels:\n    app: tickets-celery\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tickets-celery\n  template:\n    metadata:\n      labels:\n        name: tickets-celery\n    spec:\n      containers:\n      - name: tickets-celery\n        image: ckreuzberger/tickets:1.0\n        imagePullPolicy: Always\n        command:\n        - celery\n        args:\n        - -A\n        - tickets\n        - worker\n        - --loglevel=info\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_DB\n          value: tickets\n        - name: DATABASE_URL\n          value: postgres://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@postgres:5432/$(POSTGRES_DB)\n        - name: DJANGO_SETTINGS_MODULE\n          value: tickets.settings.prod\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: django-secret-key\n              key: secret_key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tickets-celery\" does not have a read-only root file system"
  },
  {
    "id": "04573",
    "manifest_path": "data/manifests/the_stack_sample/sample_2095.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tickets-celery\n  labels:\n    app: tickets-celery\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tickets-celery\n  template:\n    metadata:\n      labels:\n        name: tickets-celery\n    spec:\n      containers:\n      - name: tickets-celery\n        image: ckreuzberger/tickets:1.0\n        imagePullPolicy: Always\n        command:\n        - celery\n        args:\n        - -A\n        - tickets\n        - worker\n        - --loglevel=info\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_DB\n          value: tickets\n        - name: DATABASE_URL\n          value: postgres://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@postgres:5432/$(POSTGRES_DB)\n        - name: DJANGO_SETTINGS_MODULE\n          value: tickets.settings.prod\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: django-secret-key\n              key: secret_key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tickets-celery\" is not set to runAsNonRoot"
  },
  {
    "id": "04574",
    "manifest_path": "data/manifests/the_stack_sample/sample_2095.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tickets-celery\n  labels:\n    app: tickets-celery\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tickets-celery\n  template:\n    metadata:\n      labels:\n        name: tickets-celery\n    spec:\n      containers:\n      - name: tickets-celery\n        image: ckreuzberger/tickets:1.0\n        imagePullPolicy: Always\n        command:\n        - celery\n        args:\n        - -A\n        - tickets\n        - worker\n        - --loglevel=info\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_DB\n          value: tickets\n        - name: DATABASE_URL\n          value: postgres://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@postgres:5432/$(POSTGRES_DB)\n        - name: DJANGO_SETTINGS_MODULE\n          value: tickets.settings.prod\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: django-secret-key\n              key: secret_key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tickets-celery\" has cpu request 0"
  },
  {
    "id": "04575",
    "manifest_path": "data/manifests/the_stack_sample/sample_2095.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tickets-celery\n  labels:\n    app: tickets-celery\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tickets-celery\n  template:\n    metadata:\n      labels:\n        name: tickets-celery\n    spec:\n      containers:\n      - name: tickets-celery\n        image: ckreuzberger/tickets:1.0\n        imagePullPolicy: Always\n        command:\n        - celery\n        args:\n        - -A\n        - tickets\n        - worker\n        - --loglevel=info\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_DB\n          value: tickets\n        - name: DATABASE_URL\n          value: postgres://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@postgres:5432/$(POSTGRES_DB)\n        - name: DJANGO_SETTINGS_MODULE\n          value: tickets.settings.prod\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: django-secret-key\n              key: secret_key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tickets-celery\" has memory limit 0"
  },
  {
    "id": "04576",
    "manifest_path": "data/manifests/the_stack_sample/sample_2097.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        env:\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - all\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n        - name: tmp\n          mountPath: /tmp\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"argocd-repo-server\" is using an invalid container image, \"quay.io/argoproj/argocd:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04577",
    "manifest_path": "data/manifests/the_stack_sample/sample_2097.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        env:\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - all\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n        - name: tmp\n          mountPath: /tmp\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"argocd-repo-server\" has cpu request 0"
  },
  {
    "id": "04578",
    "manifest_path": "data/manifests/the_stack_sample/sample_2097.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        env:\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - all\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n        - name: tmp\n          mountPath: /tmp\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"argocd-repo-server\" has memory limit 0"
  },
  {
    "id": "04579",
    "manifest_path": "data/manifests/the_stack_sample/sample_2099.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tomcat\n  labels:\n    app: tomcat\nspec:\n  containers:\n  - name: tomcat\n    image: docker635067/test:preethu\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tomcat\" does not have a read-only root file system"
  },
  {
    "id": "04580",
    "manifest_path": "data/manifests/the_stack_sample/sample_2099.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tomcat\n  labels:\n    app: tomcat\nspec:\n  containers:\n  - name: tomcat\n    image: docker635067/test:preethu\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tomcat\" is not set to runAsNonRoot"
  },
  {
    "id": "04581",
    "manifest_path": "data/manifests/the_stack_sample/sample_2099.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tomcat\n  labels:\n    app: tomcat\nspec:\n  containers:\n  - name: tomcat\n    image: docker635067/test:preethu\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tomcat\" has cpu request 0"
  },
  {
    "id": "04582",
    "manifest_path": "data/manifests/the_stack_sample/sample_2099.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tomcat\n  labels:\n    app: tomcat\nspec:\n  containers:\n  - name: tomcat\n    image: docker635067/test:preethu\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tomcat\" has memory limit 0"
  },
  {
    "id": "04583",
    "manifest_path": "data/manifests/the_stack_sample/sample_2100.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20220114-25626d8ef5\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "04584",
    "manifest_path": "data/manifests/the_stack_sample/sample_2100.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20220114-25626d8ef5\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "04585",
    "manifest_path": "data/manifests/the_stack_sample/sample_2100.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20220114-25626d8ef5\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "04586",
    "manifest_path": "data/manifests/the_stack_sample/sample_2100.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20220114-25626d8ef5\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "04587",
    "manifest_path": "data/manifests/the_stack_sample/sample_2101.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: grafana\n    chart: grafana\n    heritage: Tiller\n    release: istio\n  name: istio-grafana-post-install-1.1.6\n  namespace: istio-system\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-grafana\n        chart: grafana\n        heritage: Tiller\n        release: istio\n      name: istio-grafana-post-install\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/grafana/run.sh\n        - /tmp/grafana/custom-resources.yaml\n        image: docker.io/istio/kubectl:1.1.6\n        name: kubectl\n        volumeMounts:\n        - mountPath: /tmp/grafana\n          name: tmp-configmap-grafana\n      serviceAccountName: istio-grafana-post-install-account\n      volumes:\n      - configMap:\n          name: istio-grafana-custom-resources\n        name: tmp-configmap-grafana\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubectl\" does not have a read-only root file system"
  },
  {
    "id": "04588",
    "manifest_path": "data/manifests/the_stack_sample/sample_2101.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: grafana\n    chart: grafana\n    heritage: Tiller\n    release: istio\n  name: istio-grafana-post-install-1.1.6\n  namespace: istio-system\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-grafana\n        chart: grafana\n        heritage: Tiller\n        release: istio\n      name: istio-grafana-post-install\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/grafana/run.sh\n        - /tmp/grafana/custom-resources.yaml\n        image: docker.io/istio/kubectl:1.1.6\n        name: kubectl\n        volumeMounts:\n        - mountPath: /tmp/grafana\n          name: tmp-configmap-grafana\n      serviceAccountName: istio-grafana-post-install-account\n      volumes:\n      - configMap:\n          name: istio-grafana-custom-resources\n        name: tmp-configmap-grafana\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubectl\" is not set to runAsNonRoot"
  },
  {
    "id": "04589",
    "manifest_path": "data/manifests/the_stack_sample/sample_2101.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: grafana\n    chart: grafana\n    heritage: Tiller\n    release: istio\n  name: istio-grafana-post-install-1.1.6\n  namespace: istio-system\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-grafana\n        chart: grafana\n        heritage: Tiller\n        release: istio\n      name: istio-grafana-post-install\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/grafana/run.sh\n        - /tmp/grafana/custom-resources.yaml\n        image: docker.io/istio/kubectl:1.1.6\n        name: kubectl\n        volumeMounts:\n        - mountPath: /tmp/grafana\n          name: tmp-configmap-grafana\n      serviceAccountName: istio-grafana-post-install-account\n      volumes:\n      - configMap:\n          name: istio-grafana-custom-resources\n        name: tmp-configmap-grafana\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubectl\" has cpu request 0"
  },
  {
    "id": "04590",
    "manifest_path": "data/manifests/the_stack_sample/sample_2101.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: grafana\n    chart: grafana\n    heritage: Tiller\n    release: istio\n  name: istio-grafana-post-install-1.1.6\n  namespace: istio-system\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-grafana\n        chart: grafana\n        heritage: Tiller\n        release: istio\n      name: istio-grafana-post-install\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/grafana/run.sh\n        - /tmp/grafana/custom-resources.yaml\n        image: docker.io/istio/kubectl:1.1.6\n        name: kubectl\n        volumeMounts:\n        - mountPath: /tmp/grafana\n          name: tmp-configmap-grafana\n      serviceAccountName: istio-grafana-post-install-account\n      volumes:\n      - configMap:\n          name: istio-grafana-custom-resources\n        name: tmp-configmap-grafana\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubectl\" has memory limit 0"
  },
  {
    "id": "04591",
    "manifest_path": "data/manifests/the_stack_sample/sample_2103.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"task-pv-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04592",
    "manifest_path": "data/manifests/the_stack_sample/sample_2103.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"task-pv-container\" does not have a read-only root file system"
  },
  {
    "id": "04593",
    "manifest_path": "data/manifests/the_stack_sample/sample_2103.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"task-pv-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04594",
    "manifest_path": "data/manifests/the_stack_sample/sample_2103.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"task-pv-container\" has cpu request 0"
  },
  {
    "id": "04595",
    "manifest_path": "data/manifests/the_stack_sample/sample_2103.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"task-pv-container\" has memory limit 0"
  },
  {
    "id": "04596",
    "manifest_path": "data/manifests/the_stack_sample/sample_2107.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sensor-gen\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    app: sensor-gen\n  template:\n    metadata:\n      name: sensor-gen\n      labels:\n        app: sensor-gen\n    spec:\n      containers:\n      - name: sensor-gen\n        image: huanphan/sensor-simulator:0.2\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: sensor-config\n          mountPath: /SimulateSensor/config\n      volumes:\n      - name: sensor-config\n        configMap:\n          name: sensor-config\n          items:\n          - key: config.cfg\n            path: config.cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sensor-gen\" does not have a read-only root file system"
  },
  {
    "id": "04597",
    "manifest_path": "data/manifests/the_stack_sample/sample_2107.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sensor-gen\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    app: sensor-gen\n  template:\n    metadata:\n      name: sensor-gen\n      labels:\n        app: sensor-gen\n    spec:\n      containers:\n      - name: sensor-gen\n        image: huanphan/sensor-simulator:0.2\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: sensor-config\n          mountPath: /SimulateSensor/config\n      volumes:\n      - name: sensor-config\n        configMap:\n          name: sensor-config\n          items:\n          - key: config.cfg\n            path: config.cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sensor-gen\" is not set to runAsNonRoot"
  },
  {
    "id": "04598",
    "manifest_path": "data/manifests/the_stack_sample/sample_2107.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sensor-gen\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    app: sensor-gen\n  template:\n    metadata:\n      name: sensor-gen\n      labels:\n        app: sensor-gen\n    spec:\n      containers:\n      - name: sensor-gen\n        image: huanphan/sensor-simulator:0.2\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: sensor-config\n          mountPath: /SimulateSensor/config\n      volumes:\n      - name: sensor-config\n        configMap:\n          name: sensor-config\n          items:\n          - key: config.cfg\n            path: config.cfg\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sensor-gen\" has cpu request 0"
  },
  {
    "id": "04599",
    "manifest_path": "data/manifests/the_stack_sample/sample_2107.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sensor-gen\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    app: sensor-gen\n  template:\n    metadata:\n      name: sensor-gen\n      labels:\n        app: sensor-gen\n    spec:\n      containers:\n      - name: sensor-gen\n        image: huanphan/sensor-simulator:0.2\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: sensor-config\n          mountPath: /SimulateSensor/config\n      volumes:\n      - name: sensor-config\n        configMap:\n          name: sensor-config\n          items:\n          - key: config.cfg\n            path: config.cfg\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sensor-gen\" has memory limit 0"
  },
  {
    "id": "04600",
    "manifest_path": "data/manifests/the_stack_sample/sample_2112.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04601",
    "manifest_path": "data/manifests/the_stack_sample/sample_2112.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04602",
    "manifest_path": "data/manifests/the_stack_sample/sample_2112.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04603",
    "manifest_path": "data/manifests/the_stack_sample/sample_2112.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04604",
    "manifest_path": "data/manifests/the_stack_sample/sample_2112.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04605",
    "manifest_path": "data/manifests/the_stack_sample/sample_2113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: example\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04606",
    "manifest_path": "data/manifests/the_stack_sample/sample_2113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: example\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04607",
    "manifest_path": "data/manifests/the_stack_sample/sample_2113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: example\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04608",
    "manifest_path": "data/manifests/the_stack_sample/sample_2113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: example\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04609",
    "manifest_path": "data/manifests/the_stack_sample/sample_2115.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: transmission\n  labels:\n    app.kubernetes.io/name: transmission\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: transmission\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: transmission\n    spec:\n      containers:\n      - name: transmission\n        image: ghcr.io/k8s-at-home/transmission:v3.00\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 50m\n            memory: 50Mi\n        ports:\n        - name: gui-port\n          protocol: TCP\n          containerPort: 9091\n        - name: tcp-port\n          protocol: TCP\n          containerPort: 51413\n        - name: udp-port\n          protocol: UDP\n          containerPort: 51413\n        volumeMounts:\n        - name: transmission-configs\n          mountPath: /config/settings.json\n          subPath: settings.json\n        - name: transmission-config\n          mountPath: /config\n        - name: transmission-downloads\n          mountPath: /downloads\n        livenessProbe:\n          tcpSocket:\n            port: tcp-port\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          tcpSocket:\n            port: tcp-port\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        env:\n        - name: TZ\n          value: ${CONFIG_TIMEZONE}\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n      volumes:\n      - name: transmission-configs\n        configMap:\n          name: transmission-configs\n      - name: transmission-config\n        persistentVolumeClaim:\n          claimName: transmission-config\n      - name: transmission-downloads\n        persistentVolumeClaim:\n          claimName: shared-downloads\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"transmission\" does not have a read-only root file system"
  },
  {
    "id": "04610",
    "manifest_path": "data/manifests/the_stack_sample/sample_2115.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: transmission\n  labels:\n    app.kubernetes.io/name: transmission\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: transmission\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: transmission\n    spec:\n      containers:\n      - name: transmission\n        image: ghcr.io/k8s-at-home/transmission:v3.00\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 50m\n            memory: 50Mi\n        ports:\n        - name: gui-port\n          protocol: TCP\n          containerPort: 9091\n        - name: tcp-port\n          protocol: TCP\n          containerPort: 51413\n        - name: udp-port\n          protocol: UDP\n          containerPort: 51413\n        volumeMounts:\n        - name: transmission-configs\n          mountPath: /config/settings.json\n          subPath: settings.json\n        - name: transmission-config\n          mountPath: /config\n        - name: transmission-downloads\n          mountPath: /downloads\n        livenessProbe:\n          tcpSocket:\n            port: tcp-port\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          tcpSocket:\n            port: tcp-port\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        env:\n        - name: TZ\n          value: ${CONFIG_TIMEZONE}\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n      volumes:\n      - name: transmission-configs\n        configMap:\n          name: transmission-configs\n      - name: transmission-config\n        persistentVolumeClaim:\n          claimName: transmission-config\n      - name: transmission-downloads\n        persistentVolumeClaim:\n          claimName: shared-downloads\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"transmission\" has memory limit 0"
  },
  {
    "id": "04611",
    "manifest_path": "data/manifests/the_stack_sample/sample_2116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-dns-metrics-bash\n  namespace: kube-system\n  labels:\n    application: kube-dns-metrics-bash\n    version: v0.0.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: kube-dns-metrics-bash\n  template:\n    metadata:\n      labels:\n        application: kube-dns-metrics-bash\n        version: v0.0.4\n    spec:\n      containers:\n      - image: pierone.stups.zalan.do/teapot/kube-dns-metrics-bash:v0.0.5\n        name: kube-dns-metrics-bash\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 25m\n            memory: 100Mi\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-dns-metrics-bash\" does not have a read-only root file system"
  },
  {
    "id": "04612",
    "manifest_path": "data/manifests/the_stack_sample/sample_2116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-dns-metrics-bash\n  namespace: kube-system\n  labels:\n    application: kube-dns-metrics-bash\n    version: v0.0.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: kube-dns-metrics-bash\n  template:\n    metadata:\n      labels:\n        application: kube-dns-metrics-bash\n        version: v0.0.4\n    spec:\n      containers:\n      - image: pierone.stups.zalan.do/teapot/kube-dns-metrics-bash:v0.0.5\n        name: kube-dns-metrics-bash\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 25m\n            memory: 100Mi\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-dns-metrics-bash\" is not set to runAsNonRoot"
  },
  {
    "id": "04613",
    "manifest_path": "data/manifests/the_stack_sample/sample_2118.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        security.alpha.kubernetes.io/sysctls: net.ipv4.tcp_syncookies=0,net.ipv4.ip_local_port_range=10000\n          65535\n        security.alpha.kubernetes.io/unsafe-sysctls: net.core.somaxconn=65535,net.ipv4.tcp_tw_reuse=1,net.ipv4.tcp_fin_timeout=30,net.ipv4.tcp_keepalive_intvl=4,net.ipv4.tcp_keepalive_probes=3,net.ipv4.tcp_keepalive_time=120,net.ipv4.tcp_max_syn_backlog=65535,net.ipv4.tcp_rfc1337=1,net.ipv4.tcp_slow_start_after_idle=0,net.ipv4.tcp_fack=1,net.ipv4.tcp_fwmark_accept=1,net.ipv4.fwmark_reflect=1\n    spec:\n      containers:\n      - name: redis\n        image: slpcat/redis-3.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /var/lib/redis\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"slpcat/redis-3.2\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04614",
    "manifest_path": "data/manifests/the_stack_sample/sample_2118.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        security.alpha.kubernetes.io/sysctls: net.ipv4.tcp_syncookies=0,net.ipv4.ip_local_port_range=10000\n          65535\n        security.alpha.kubernetes.io/unsafe-sysctls: net.core.somaxconn=65535,net.ipv4.tcp_tw_reuse=1,net.ipv4.tcp_fin_timeout=30,net.ipv4.tcp_keepalive_intvl=4,net.ipv4.tcp_keepalive_probes=3,net.ipv4.tcp_keepalive_time=120,net.ipv4.tcp_max_syn_backlog=65535,net.ipv4.tcp_rfc1337=1,net.ipv4.tcp_slow_start_after_idle=0,net.ipv4.tcp_fack=1,net.ipv4.tcp_fwmark_accept=1,net.ipv4.fwmark_reflect=1\n    spec:\n      containers:\n      - name: redis\n        image: slpcat/redis-3.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /var/lib/redis\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "04615",
    "manifest_path": "data/manifests/the_stack_sample/sample_2118.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        security.alpha.kubernetes.io/sysctls: net.ipv4.tcp_syncookies=0,net.ipv4.ip_local_port_range=10000\n          65535\n        security.alpha.kubernetes.io/unsafe-sysctls: net.core.somaxconn=65535,net.ipv4.tcp_tw_reuse=1,net.ipv4.tcp_fin_timeout=30,net.ipv4.tcp_keepalive_intvl=4,net.ipv4.tcp_keepalive_probes=3,net.ipv4.tcp_keepalive_time=120,net.ipv4.tcp_max_syn_backlog=65535,net.ipv4.tcp_rfc1337=1,net.ipv4.tcp_slow_start_after_idle=0,net.ipv4.tcp_fack=1,net.ipv4.tcp_fwmark_accept=1,net.ipv4.fwmark_reflect=1\n    spec:\n      containers:\n      - name: redis\n        image: slpcat/redis-3.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /var/lib/redis\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "04616",
    "manifest_path": "data/manifests/the_stack_sample/sample_2119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: kuard\n  name: kuard\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      run: kuard\n  template:\n    metadata:\n      labels:\n        run: kuard\n    spec:\n      containers:\n      - image: gcr.io/kuar-demo/kuard-amd64:blue\n        name: kuard\n        resources:\n          limits:\n            cpu: 50m\n            memory: 0.1G\n          requests:\n            cpu: 50m\n            memory: 0.1G\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kuard\" does not have a read-only root file system"
  },
  {
    "id": "04617",
    "manifest_path": "data/manifests/the_stack_sample/sample_2119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: kuard\n  name: kuard\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      run: kuard\n  template:\n    metadata:\n      labels:\n        run: kuard\n    spec:\n      containers:\n      - image: gcr.io/kuar-demo/kuard-amd64:blue\n        name: kuard\n        resources:\n          limits:\n            cpu: 50m\n            memory: 0.1G\n          requests:\n            cpu: 50m\n            memory: 0.1G\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kuard\" is not set to runAsNonRoot"
  },
  {
    "id": "04618",
    "manifest_path": "data/manifests/the_stack_sample/sample_2120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    kustomize.component: iap-ingress\n  name: iap-enabler\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      kustomize.component: iap-ingress\n  template:\n    metadata:\n      labels:\n        kustomize.component: iap-ingress\n        service: iap-enabler\n    spec:\n      containers:\n      - command:\n        - bash\n        - /var/envoy-config/setup_backend.sh\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/run/secrets/sa/admin-gcp-sa.json\n        - name: NAMESPACE\n          value: istio-system\n        - name: SERVICE\n          value: istio-ingressgateway\n        - name: INGRESS_NAME\n          value: envoy-ingress\n        - name: ENVOY_ADMIN\n          value: http://localhost:8001\n        - name: USE_ISTIO\n          value: 'true'\n        image: gcr.io/kubeflow-images-public/ingress-setup:latest\n        name: iap\n        volumeMounts:\n        - mountPath: /var/run/secrets/sa\n          name: sa-key\n          readOnly: true\n        - mountPath: /var/envoy-config/\n          name: config-volume\n      serviceAccountName: kf-admin\n      volumes:\n      - name: sa-key\n        secret:\n          secretName: admin-gcp-sa\n      - configMap:\n          name: envoy-config\n        name: config-volume\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"iap\" is using an invalid container image, \"gcr.io/kubeflow-images-public/ingress-setup:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04619",
    "manifest_path": "data/manifests/the_stack_sample/sample_2120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    kustomize.component: iap-ingress\n  name: iap-enabler\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      kustomize.component: iap-ingress\n  template:\n    metadata:\n      labels:\n        kustomize.component: iap-ingress\n        service: iap-enabler\n    spec:\n      containers:\n      - command:\n        - bash\n        - /var/envoy-config/setup_backend.sh\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/run/secrets/sa/admin-gcp-sa.json\n        - name: NAMESPACE\n          value: istio-system\n        - name: SERVICE\n          value: istio-ingressgateway\n        - name: INGRESS_NAME\n          value: envoy-ingress\n        - name: ENVOY_ADMIN\n          value: http://localhost:8001\n        - name: USE_ISTIO\n          value: 'true'\n        image: gcr.io/kubeflow-images-public/ingress-setup:latest\n        name: iap\n        volumeMounts:\n        - mountPath: /var/run/secrets/sa\n          name: sa-key\n          readOnly: true\n        - mountPath: /var/envoy-config/\n          name: config-volume\n      serviceAccountName: kf-admin\n      volumes:\n      - name: sa-key\n        secret:\n          secretName: admin-gcp-sa\n      - configMap:\n          name: envoy-config\n        name: config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"iap\" does not have a read-only root file system"
  },
  {
    "id": "04620",
    "manifest_path": "data/manifests/the_stack_sample/sample_2120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    kustomize.component: iap-ingress\n  name: iap-enabler\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      kustomize.component: iap-ingress\n  template:\n    metadata:\n      labels:\n        kustomize.component: iap-ingress\n        service: iap-enabler\n    spec:\n      containers:\n      - command:\n        - bash\n        - /var/envoy-config/setup_backend.sh\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/run/secrets/sa/admin-gcp-sa.json\n        - name: NAMESPACE\n          value: istio-system\n        - name: SERVICE\n          value: istio-ingressgateway\n        - name: INGRESS_NAME\n          value: envoy-ingress\n        - name: ENVOY_ADMIN\n          value: http://localhost:8001\n        - name: USE_ISTIO\n          value: 'true'\n        image: gcr.io/kubeflow-images-public/ingress-setup:latest\n        name: iap\n        volumeMounts:\n        - mountPath: /var/run/secrets/sa\n          name: sa-key\n          readOnly: true\n        - mountPath: /var/envoy-config/\n          name: config-volume\n      serviceAccountName: kf-admin\n      volumes:\n      - name: sa-key\n        secret:\n          secretName: admin-gcp-sa\n      - configMap:\n          name: envoy-config\n        name: config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"iap\" is not set to runAsNonRoot"
  },
  {
    "id": "04621",
    "manifest_path": "data/manifests/the_stack_sample/sample_2120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    kustomize.component: iap-ingress\n  name: iap-enabler\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      kustomize.component: iap-ingress\n  template:\n    metadata:\n      labels:\n        kustomize.component: iap-ingress\n        service: iap-enabler\n    spec:\n      containers:\n      - command:\n        - bash\n        - /var/envoy-config/setup_backend.sh\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/run/secrets/sa/admin-gcp-sa.json\n        - name: NAMESPACE\n          value: istio-system\n        - name: SERVICE\n          value: istio-ingressgateway\n        - name: INGRESS_NAME\n          value: envoy-ingress\n        - name: ENVOY_ADMIN\n          value: http://localhost:8001\n        - name: USE_ISTIO\n          value: 'true'\n        image: gcr.io/kubeflow-images-public/ingress-setup:latest\n        name: iap\n        volumeMounts:\n        - mountPath: /var/run/secrets/sa\n          name: sa-key\n          readOnly: true\n        - mountPath: /var/envoy-config/\n          name: config-volume\n      serviceAccountName: kf-admin\n      volumes:\n      - name: sa-key\n        secret:\n          secretName: admin-gcp-sa\n      - configMap:\n          name: envoy-config\n        name: config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"iap\" has cpu request 0"
  },
  {
    "id": "04622",
    "manifest_path": "data/manifests/the_stack_sample/sample_2120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    kustomize.component: iap-ingress\n  name: iap-enabler\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      kustomize.component: iap-ingress\n  template:\n    metadata:\n      labels:\n        kustomize.component: iap-ingress\n        service: iap-enabler\n    spec:\n      containers:\n      - command:\n        - bash\n        - /var/envoy-config/setup_backend.sh\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/run/secrets/sa/admin-gcp-sa.json\n        - name: NAMESPACE\n          value: istio-system\n        - name: SERVICE\n          value: istio-ingressgateway\n        - name: INGRESS_NAME\n          value: envoy-ingress\n        - name: ENVOY_ADMIN\n          value: http://localhost:8001\n        - name: USE_ISTIO\n          value: 'true'\n        image: gcr.io/kubeflow-images-public/ingress-setup:latest\n        name: iap\n        volumeMounts:\n        - mountPath: /var/run/secrets/sa\n          name: sa-key\n          readOnly: true\n        - mountPath: /var/envoy-config/\n          name: config-volume\n      serviceAccountName: kf-admin\n      volumes:\n      - name: sa-key\n        secret:\n          secretName: admin-gcp-sa\n      - configMap:\n          name: envoy-config\n        name: config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"iap\" has memory limit 0"
  },
  {
    "id": "04623",
    "manifest_path": "data/manifests/the_stack_sample/sample_2121.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04624",
    "manifest_path": "data/manifests/the_stack_sample/sample_2121.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04625",
    "manifest_path": "data/manifests/the_stack_sample/sample_2121.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04626",
    "manifest_path": "data/manifests/the_stack_sample/sample_2121.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04627",
    "manifest_path": "data/manifests/the_stack_sample/sample_2121.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04628",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"packageserver\" is using an invalid container image, \"OLM_OPERATOR_IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04629",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"socks5-proxy\" is using an invalid container image, \"SOCKS5_PROXY_IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04630",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"packageserver\" does not have a read-only root file system"
  },
  {
    "id": "04631",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"socks5-proxy\" does not have a read-only root file system"
  },
  {
    "id": "04632",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"packageserver\" is not set to runAsNonRoot"
  },
  {
    "id": "04633",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"socks5-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "04634",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"socks5-proxy\" has cpu request 0"
  },
  {
    "id": "04635",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"packageserver\" has memory limit 0"
  },
  {
    "id": "04636",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"socks5-proxy\" has memory limit 0"
  },
  {
    "id": "04637",
    "manifest_path": "data/manifests/the_stack_sample/sample_2131.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - name: hello\n    image: raelga/hello\n    resources:\n      limits:\n        cpu: 50m\n        memory: 0.1G\n      requests:\n        cpu: 50m\n        memory: 0.1G\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hello\" is using an invalid container image, \"raelga/hello\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04638",
    "manifest_path": "data/manifests/the_stack_sample/sample_2131.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - name: hello\n    image: raelga/hello\n    resources:\n      limits:\n        cpu: 50m\n        memory: 0.1G\n      requests:\n        cpu: 50m\n        memory: 0.1G\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "04639",
    "manifest_path": "data/manifests/the_stack_sample/sample_2131.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - name: hello\n    image: raelga/hello\n    resources:\n      limits:\n        cpu: 50m\n        memory: 0.1G\n      requests:\n        cpu: 50m\n        memory: 0.1G\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "04640",
    "manifest_path": "data/manifests/the_stack_sample/sample_2132.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: workshop-303\nspec:\n  containers:\n  - name: first-container\n    image: fedora:29\n    command:\n    - sleep\n    - '36000'\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: username\n    - name: SECRET_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n    volumeMounts:\n    - name: my-configmap\n      mountPath: /config\n  volumes:\n  - name: my-configmap\n    configMap:\n      name: my-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"first-container\" does not have a read-only root file system"
  },
  {
    "id": "04641",
    "manifest_path": "data/manifests/the_stack_sample/sample_2132.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: workshop-303\nspec:\n  containers:\n  - name: first-container\n    image: fedora:29\n    command:\n    - sleep\n    - '36000'\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: username\n    - name: SECRET_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n    volumeMounts:\n    - name: my-configmap\n      mountPath: /config\n  volumes:\n  - name: my-configmap\n    configMap:\n      name: my-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"first-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04642",
    "manifest_path": "data/manifests/the_stack_sample/sample_2132.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: workshop-303\nspec:\n  containers:\n  - name: first-container\n    image: fedora:29\n    command:\n    - sleep\n    - '36000'\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: username\n    - name: SECRET_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n    volumeMounts:\n    - name: my-configmap\n      mountPath: /config\n  volumes:\n  - name: my-configmap\n    configMap:\n      name: my-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"first-container\" has cpu request 0"
  },
  {
    "id": "04643",
    "manifest_path": "data/manifests/the_stack_sample/sample_2132.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: workshop-303\nspec:\n  containers:\n  - name: first-container\n    image: fedora:29\n    command:\n    - sleep\n    - '36000'\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: username\n    - name: SECRET_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n    volumeMounts:\n    - name: my-configmap\n      mountPath: /config\n  volumes:\n  - name: my-configmap\n    configMap:\n      name: my-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"first-container\" has memory limit 0"
  },
  {
    "id": "04644",
    "manifest_path": "data/manifests/the_stack_sample/sample_2133.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: selenium-node-chrome\n  namespace: selenium\n  labels:\n    app: selenium-node\n    browser: chrome\nspec:\n  selector:\n    matchLabels:\n      app: selenium-node\n      browser: chrome\n  template:\n    metadata:\n      labels:\n        app: selenium-node\n        browser: chrome\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cloud.google.com/gke-preemptible\n                operator: DoesNotExist\n              - key: eks.amazonaws.com/capacityType\n                operator: NotIn\n                values:\n                - SPOT\n              - key: kubernetes.azure.com/scalesetpriority\n                operator: NotIn\n                values:\n                - spot\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - name: selenium-node-chrome\n        image: selenium/node-chrome:90.0\n        ports:\n        - containerPort: 5555\n        - containerPort: 5900\n        - containerPort: 7900\n        env:\n        - name: JAVA_OPTS\n          value: -Xmx512m -Dselenium.LOGGER.level=WARNING\n        - name: SE_OPTS\n          value: ''\n        - name: SE_EVENT_BUS_HOST\n          value: selenium-hub\n        - name: SE_EVENT_BUS_PUBLISH_PORT\n          value: '4442'\n        - name: SE_EVENT_BUS_SUBSCRIBE_PORT\n          value: '4443'\n        - name: VNC_NO_PASSWORD\n          value: '1'\n        readinessProbe:\n          httpGet:\n            path: /status\n            port: 5555\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /status\n            port: 5555\n          initialDelaySeconds: 30\n        resources:\n          limits:\n            cpu: 2\n            memory: 1Gi\n          requests:\n            cpu: 300m\n            memory: 615Mi\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"selenium-node-chrome\" does not have a read-only root file system"
  },
  {
    "id": "04645",
    "manifest_path": "data/manifests/the_stack_sample/sample_2134.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200409-becd20a71\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "04646",
    "manifest_path": "data/manifests/the_stack_sample/sample_2134.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200409-becd20a71\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "04647",
    "manifest_path": "data/manifests/the_stack_sample/sample_2134.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200409-becd20a71\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "04648",
    "manifest_path": "data/manifests/the_stack_sample/sample_2134.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200409-becd20a71\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "04649",
    "manifest_path": "data/manifests/the_stack_sample/sample_2135.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube2iam\n  namespace: kube-system\n  labels:\n    application: kube2iam\n    version: 0.10.7\nspec:\n  selector:\n    matchLabels:\n      application: kube2iam\n  template:\n    metadata:\n      labels:\n        application: kube2iam\n        version: 0.10.7\n    spec:\n      serviceAccountName: kube2iam\n      containers:\n      - image: registry.opensource.zalan.do/teapot/kube2iam:0.10.7\n        name: kube2iam\n        args:\n        - --auto-discover-base-arn\n        - --verbose\n        - --node=$(NODE_NAME)\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - containerPort: 8181\n          hostPort: 8181\n          name: http\n        securityContext:\n          privileged: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8181\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 25m\n            memory: 100Mi\n            ephemeral-storage: 256Mi\n          limits:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube2iam\" does not have a read-only root file system"
  },
  {
    "id": "04650",
    "manifest_path": "data/manifests/the_stack_sample/sample_2135.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube2iam\n  namespace: kube-system\n  labels:\n    application: kube2iam\n    version: 0.10.7\nspec:\n  selector:\n    matchLabels:\n      application: kube2iam\n  template:\n    metadata:\n      labels:\n        application: kube2iam\n        version: 0.10.7\n    spec:\n      serviceAccountName: kube2iam\n      containers:\n      - image: registry.opensource.zalan.do/teapot/kube2iam:0.10.7\n        name: kube2iam\n        args:\n        - --auto-discover-base-arn\n        - --verbose\n        - --node=$(NODE_NAME)\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - containerPort: 8181\n          hostPort: 8181\n          name: http\n        securityContext:\n          privileged: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8181\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 25m\n            memory: 100Mi\n            ephemeral-storage: 256Mi\n          limits:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"kube2iam\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04651",
    "manifest_path": "data/manifests/the_stack_sample/sample_2135.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube2iam\n  namespace: kube-system\n  labels:\n    application: kube2iam\n    version: 0.10.7\nspec:\n  selector:\n    matchLabels:\n      application: kube2iam\n  template:\n    metadata:\n      labels:\n        application: kube2iam\n        version: 0.10.7\n    spec:\n      serviceAccountName: kube2iam\n      containers:\n      - image: registry.opensource.zalan.do/teapot/kube2iam:0.10.7\n        name: kube2iam\n        args:\n        - --auto-discover-base-arn\n        - --verbose\n        - --node=$(NODE_NAME)\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - containerPort: 8181\n          hostPort: 8181\n          name: http\n        securityContext:\n          privileged: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8181\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 25m\n            memory: 100Mi\n            ephemeral-storage: 256Mi\n          limits:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"kube2iam\" is privileged"
  },
  {
    "id": "04652",
    "manifest_path": "data/manifests/the_stack_sample/sample_2135.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube2iam\n  namespace: kube-system\n  labels:\n    application: kube2iam\n    version: 0.10.7\nspec:\n  selector:\n    matchLabels:\n      application: kube2iam\n  template:\n    metadata:\n      labels:\n        application: kube2iam\n        version: 0.10.7\n    spec:\n      serviceAccountName: kube2iam\n      containers:\n      - image: registry.opensource.zalan.do/teapot/kube2iam:0.10.7\n        name: kube2iam\n        args:\n        - --auto-discover-base-arn\n        - --verbose\n        - --node=$(NODE_NAME)\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - containerPort: 8181\n          hostPort: 8181\n          name: http\n        securityContext:\n          privileged: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8181\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 25m\n            memory: 100Mi\n            ephemeral-storage: 256Mi\n          limits:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube2iam\" is not set to runAsNonRoot"
  },
  {
    "id": "04653",
    "manifest_path": "data/manifests/the_stack_sample/sample_2136.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nodejs\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: web-app\n        image: bmuschko/nodejs-hello-world:1.0.0\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-app\" does not have a read-only root file system"
  },
  {
    "id": "04654",
    "manifest_path": "data/manifests/the_stack_sample/sample_2136.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nodejs\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: web-app\n        image: bmuschko/nodejs-hello-world:1.0.0\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-app\" is not set to runAsNonRoot"
  },
  {
    "id": "04655",
    "manifest_path": "data/manifests/the_stack_sample/sample_2136.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nodejs\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: web-app\n        image: bmuschko/nodejs-hello-world:1.0.0\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-app\" has cpu request 0"
  },
  {
    "id": "04656",
    "manifest_path": "data/manifests/the_stack_sample/sample_2136.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nodejs\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: web-app\n        image: bmuschko/nodejs-hello-world:1.0.0\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web-app\" has memory limit 0"
  },
  {
    "id": "04657",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nvidiaheartbeat\" does not have a read-only root file system"
  },
  {
    "id": "04658",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"nvidiaheartbeat\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04659",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"nvidiaheartbeat\" is privileged"
  },
  {
    "id": "04660",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nvidiaheartbeat\" is not set to runAsNonRoot"
  },
  {
    "id": "04661",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nvidiaheartbeat\" has cpu request 0"
  },
  {
    "id": "04662",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nvidiaheartbeat\" has memory limit 0"
  },
  {
    "id": "04663",
    "manifest_path": "data/manifests/the_stack_sample/sample_2140.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8112\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04664",
    "manifest_path": "data/manifests/the_stack_sample/sample_2140.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8112\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04665",
    "manifest_path": "data/manifests/the_stack_sample/sample_2140.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8112\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04666",
    "manifest_path": "data/manifests/the_stack_sample/sample_2140.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8112\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04667",
    "manifest_path": "data/manifests/the_stack_sample/sample_2140.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8112\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04668",
    "manifest_path": "data/manifests/the_stack_sample/sample_2141.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app: kube-state-metrics\n    spec:\n      containers:\n      - name: kube-state-metrics\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-state-metrics\" does not have a read-only root file system"
  },
  {
    "id": "04669",
    "manifest_path": "data/manifests/the_stack_sample/sample_2141.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app: kube-state-metrics\n    spec:\n      containers:\n      - name: kube-state-metrics\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-state-metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "04670",
    "manifest_path": "data/manifests/the_stack_sample/sample_2141.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app: kube-state-metrics\n    spec:\n      containers:\n      - name: kube-state-metrics\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-state-metrics\" has cpu request 0"
  },
  {
    "id": "04671",
    "manifest_path": "data/manifests/the_stack_sample/sample_2141.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app: kube-state-metrics\n    spec:\n      containers:\n      - name: kube-state-metrics\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "04672",
    "manifest_path": "data/manifests/the_stack_sample/sample_2142.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        image: controller:latest\n        name: manager\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: ${K8S_CP_LABEL:=node-role.kubernetes.io/control-plane}\n                operator: Exists\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04673",
    "manifest_path": "data/manifests/the_stack_sample/sample_2142.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        image: controller:latest\n        name: manager\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: ${K8S_CP_LABEL:=node-role.kubernetes.io/control-plane}\n                operator: Exists\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "04674",
    "manifest_path": "data/manifests/the_stack_sample/sample_2142.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        image: controller:latest\n        name: manager\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: ${K8S_CP_LABEL:=node-role.kubernetes.io/control-plane}\n                operator: Exists\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "04675",
    "manifest_path": "data/manifests/the_stack_sample/sample_2142.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        image: controller:latest\n        name: manager\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: ${K8S_CP_LABEL:=node-role.kubernetes.io/control-plane}\n                operator: Exists\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"manager\" has cpu request 0"
  },
  {
    "id": "04676",
    "manifest_path": "data/manifests/the_stack_sample/sample_2142.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        image: controller:latest\n        name: manager\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: ${K8S_CP_LABEL:=node-role.kubernetes.io/control-plane}\n                operator: Exists\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"manager\" has memory limit 0"
  },
  {
    "id": "04677",
    "manifest_path": "data/manifests/the_stack_sample/sample_2143.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: echoheaders-https\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: echoheaders-https\n    spec:\n      containers:\n      - name: echoheaders-https\n        image: gcr.io/google_containers/echoserver:1.10\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"echoheaders-https\" does not have a read-only root file system"
  },
  {
    "id": "04678",
    "manifest_path": "data/manifests/the_stack_sample/sample_2143.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: echoheaders-https\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: echoheaders-https\n    spec:\n      containers:\n      - name: echoheaders-https\n        image: gcr.io/google_containers/echoserver:1.10\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"echoheaders-https\" is not set to runAsNonRoot"
  },
  {
    "id": "04679",
    "manifest_path": "data/manifests/the_stack_sample/sample_2143.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: echoheaders-https\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: echoheaders-https\n    spec:\n      containers:\n      - name: echoheaders-https\n        image: gcr.io/google_containers/echoserver:1.10\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"echoheaders-https\" has cpu request 0"
  },
  {
    "id": "04680",
    "manifest_path": "data/manifests/the_stack_sample/sample_2143.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: echoheaders-https\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: echoheaders-https\n    spec:\n      containers:\n      - name: echoheaders-https\n        image: gcr.io/google_containers/echoserver:1.10\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"echoheaders-https\" has memory limit 0"
  },
  {
    "id": "04681",
    "manifest_path": "data/manifests/the_stack_sample/sample_2145.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9533\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04682",
    "manifest_path": "data/manifests/the_stack_sample/sample_2145.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9533\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04683",
    "manifest_path": "data/manifests/the_stack_sample/sample_2145.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9533\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04684",
    "manifest_path": "data/manifests/the_stack_sample/sample_2145.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9533\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04685",
    "manifest_path": "data/manifests/the_stack_sample/sample_2145.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9533\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04686",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"aqua-gateway\" does not have a read-only root file system"
  },
  {
    "id": "04687",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"aqua-gateway\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04688",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"aqua-gateway\" is privileged"
  },
  {
    "id": "04689",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"aqua-gateway\" is not set to runAsNonRoot"
  },
  {
    "id": "04690",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"aqua-gateway\" has cpu request 0"
  },
  {
    "id": "04691",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"aqua-gateway\" has memory limit 0"
  },
  {
    "id": "04692",
    "manifest_path": "data/manifests/the_stack_sample/sample_2150.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n        version: v1\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      containers:\n      - name: main\n        image: sitaramiyer/bookstore-demo:loadgenerator-v0.1.2\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "04693",
    "manifest_path": "data/manifests/the_stack_sample/sample_2150.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n        version: v1\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      containers:\n      - name: main\n        image: sitaramiyer/bookstore-demo:loadgenerator-v0.1.2\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "04694",
    "manifest_path": "data/manifests/the_stack_sample/sample_2151.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: import-named-polls-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: import-named-polls\n          image: docker.pkg.github.com/demokratie-live/democracy-development/import-named-polls:0.1.2\n          env:\n          - name: DB_URL\n            valueFrom:\n              configMapKeyRef:\n                name: bio-api-config\n                key: DB_URL\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"import-named-polls\" does not have a read-only root file system"
  },
  {
    "id": "04695",
    "manifest_path": "data/manifests/the_stack_sample/sample_2151.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: import-named-polls-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: import-named-polls\n          image: docker.pkg.github.com/demokratie-live/democracy-development/import-named-polls:0.1.2\n          env:\n          - name: DB_URL\n            valueFrom:\n              configMapKeyRef:\n                name: bio-api-config\n                key: DB_URL\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"import-named-polls\" is not set to runAsNonRoot"
  },
  {
    "id": "04696",
    "manifest_path": "data/manifests/the_stack_sample/sample_2151.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: import-named-polls-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: import-named-polls\n          image: docker.pkg.github.com/demokratie-live/democracy-development/import-named-polls:0.1.2\n          env:\n          - name: DB_URL\n            valueFrom:\n              configMapKeyRef:\n                name: bio-api-config\n                key: DB_URL\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"import-named-polls\" has cpu request 0"
  },
  {
    "id": "04697",
    "manifest_path": "data/manifests/the_stack_sample/sample_2151.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: import-named-polls-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: import-named-polls\n          image: docker.pkg.github.com/demokratie-live/democracy-development/import-named-polls:0.1.2\n          env:\n          - name: DB_URL\n            valueFrom:\n              configMapKeyRef:\n                name: bio-api-config\n                key: DB_URL\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"import-named-polls\" has memory limit 0"
  },
  {
    "id": "04698",
    "manifest_path": "data/manifests/the_stack_sample/sample_2152.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia-deploy\n  namespace: chp16-set1612\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia-deploy\n  template:\n    metadata:\n      name: kubia-deploy\n      labels:\n        app: kubia-deploy\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsNonRoot: true\n      serviceAccountName: foo\n      containers:\n      - image: georgebaptista/kubia\n        name: kubia-deploy\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          runAsNonRoot: true\n          runAsUser: 1000\n        resources:\n          limits:\n            cpu: 200m\n          requests:\n            cpu: 100m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kubia-deploy\" is using an invalid container image, \"georgebaptista/kubia\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04699",
    "manifest_path": "data/manifests/the_stack_sample/sample_2152.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia-deploy\n  namespace: chp16-set1612\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia-deploy\n  template:\n    metadata:\n      name: kubia-deploy\n      labels:\n        app: kubia-deploy\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsNonRoot: true\n      serviceAccountName: foo\n      containers:\n      - image: georgebaptista/kubia\n        name: kubia-deploy\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          runAsNonRoot: true\n          runAsUser: 1000\n        resources:\n          limits:\n            cpu: 200m\n          requests:\n            cpu: 100m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubia-deploy\" does not have a read-only root file system"
  },
  {
    "id": "04700",
    "manifest_path": "data/manifests/the_stack_sample/sample_2152.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia-deploy\n  namespace: chp16-set1612\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia-deploy\n  template:\n    metadata:\n      name: kubia-deploy\n      labels:\n        app: kubia-deploy\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsNonRoot: true\n      serviceAccountName: foo\n      containers:\n      - image: georgebaptista/kubia\n        name: kubia-deploy\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          runAsNonRoot: true\n          runAsUser: 1000\n        resources:\n          limits:\n            cpu: 200m\n          requests:\n            cpu: 100m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubia-deploy\" has memory limit 0"
  },
  {
    "id": "04701",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dss\" is using an invalid container image, \"dss-node-image\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04702",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dind\" does not have a read-only root file system"
  },
  {
    "id": "04703",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dood\" does not have a read-only root file system"
  },
  {
    "id": "04704",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dss\" does not have a read-only root file system"
  },
  {
    "id": "04705",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"dind\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04706",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"dss\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04707",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"dind\" is privileged"
  },
  {
    "id": "04708",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"dss\" is privileged"
  },
  {
    "id": "04709",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dind\" is not set to runAsNonRoot"
  },
  {
    "id": "04710",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dood\" is not set to runAsNonRoot"
  },
  {
    "id": "04711",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dss\" is not set to runAsNonRoot"
  },
  {
    "id": "04712",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dss\" has cpu request 0"
  },
  {
    "id": "04713",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dss\" has memory limit 0"
  },
  {
    "id": "04714",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"aci-containers-host\" does not have a read-only root file system"
  },
  {
    "id": "04715",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cnideploy\" does not have a read-only root file system"
  },
  {
    "id": "04716",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"opflex-agent\" does not have a read-only root file system"
  },
  {
    "id": "04717",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"opflex-server\" does not have a read-only root file system"
  },
  {
    "id": "04718",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"aci-containers-host\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "04719",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cnideploy\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "04720",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"aci-containers-host\" is not set to runAsNonRoot"
  },
  {
    "id": "04721",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cnideploy\" is not set to runAsNonRoot"
  },
  {
    "id": "04722",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"opflex-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "04723",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"opflex-server\" is not set to runAsNonRoot"
  },
  {
    "id": "04724",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"aci-containers-host\" has cpu request 0"
  },
  {
    "id": "04725",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cnideploy\" has cpu request 0"
  },
  {
    "id": "04726",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"opflex-agent\" has cpu request 0"
  },
  {
    "id": "04727",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"opflex-server\" has cpu request 0"
  },
  {
    "id": "04728",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"aci-containers-host\" has memory limit 0"
  },
  {
    "id": "04729",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cnideploy\" has memory limit 0"
  },
  {
    "id": "04730",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"opflex-agent\" has memory limit 0"
  },
  {
    "id": "04731",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"opflex-server\" has memory limit 0"
  },
  {
    "id": "04732",
    "manifest_path": "data/manifests/the_stack_sample/sample_2155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5267\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04733",
    "manifest_path": "data/manifests/the_stack_sample/sample_2155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5267\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04734",
    "manifest_path": "data/manifests/the_stack_sample/sample_2155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5267\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04735",
    "manifest_path": "data/manifests/the_stack_sample/sample_2155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5267\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04736",
    "manifest_path": "data/manifests/the_stack_sample/sample_2155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5267\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04737",
    "manifest_path": "data/manifests/the_stack_sample/sample_2159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04738",
    "manifest_path": "data/manifests/the_stack_sample/sample_2159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "04739",
    "manifest_path": "data/manifests/the_stack_sample/sample_2159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04740",
    "manifest_path": "data/manifests/the_stack_sample/sample_2159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "04741",
    "manifest_path": "data/manifests/the_stack_sample/sample_2162.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210204-de2fa22b93\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"needs-rebase\" does not have a read-only root file system"
  },
  {
    "id": "04742",
    "manifest_path": "data/manifests/the_stack_sample/sample_2162.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210204-de2fa22b93\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"needs-rebase\" is not set to runAsNonRoot"
  },
  {
    "id": "04743",
    "manifest_path": "data/manifests/the_stack_sample/sample_2162.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210204-de2fa22b93\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"needs-rebase\" has cpu request 0"
  },
  {
    "id": "04744",
    "manifest_path": "data/manifests/the_stack_sample/sample_2162.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210204-de2fa22b93\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"needs-rebase\" has memory limit 0"
  },
  {
    "id": "04745",
    "manifest_path": "data/manifests/the_stack_sample/sample_2165.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: gcr.io/ml-pipeline/minio:RELEASE.2019-08-14T20-37-41Z-license-compliance\n        args:\n        - gateway\n        - gcs\n        - $(GCP_PROJECT_ID)\n        env:\n        - name: GCP_PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: pipeline-install-config\n              key: gcsProjectId\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable MINIO_SECRET_KEY in container \"minio\" found"
  },
  {
    "id": "04746",
    "manifest_path": "data/manifests/the_stack_sample/sample_2165.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: gcr.io/ml-pipeline/minio:RELEASE.2019-08-14T20-37-41Z-license-compliance\n        args:\n        - gateway\n        - gcs\n        - $(GCP_PROJECT_ID)\n        env:\n        - name: GCP_PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: pipeline-install-config\n              key: gcsProjectId\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minio\" does not have a read-only root file system"
  },
  {
    "id": "04747",
    "manifest_path": "data/manifests/the_stack_sample/sample_2165.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: gcr.io/ml-pipeline/minio:RELEASE.2019-08-14T20-37-41Z-license-compliance\n        args:\n        - gateway\n        - gcs\n        - $(GCP_PROJECT_ID)\n        env:\n        - name: GCP_PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: pipeline-install-config\n              key: gcsProjectId\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minio\" is not set to runAsNonRoot"
  },
  {
    "id": "04748",
    "manifest_path": "data/manifests/the_stack_sample/sample_2165.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: gcr.io/ml-pipeline/minio:RELEASE.2019-08-14T20-37-41Z-license-compliance\n        args:\n        - gateway\n        - gcs\n        - $(GCP_PROJECT_ID)\n        env:\n        - name: GCP_PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: pipeline-install-config\n              key: gcsProjectId\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minio\" has cpu request 0"
  },
  {
    "id": "04749",
    "manifest_path": "data/manifests/the_stack_sample/sample_2165.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: gcr.io/ml-pipeline/minio:RELEASE.2019-08-14T20-37-41Z-license-compliance\n        args:\n        - gateway\n        - gcs\n        - $(GCP_PROJECT_ID)\n        env:\n        - name: GCP_PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: pipeline-install-config\n              key: gcsProjectId\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minio\" has memory limit 0"
  },
  {
    "id": "04750",
    "manifest_path": "data/manifests/the_stack_sample/sample_2168.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: prod-cron-cancel-booking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-cancel-booking\n          image: 414928843086.dkr.ecr.ap-southeast-1.amazonaws.com/core-api-v2:<VERSION>\n          imagePullPolicy: Always\n          args:\n          - run\n          - start-cron:cancel-booking:prod\n          env:\n          - name: NODE_ENV\n            value: production\n          - name: PORT\n            value: '3000'\n          - name: HOST\n            value: 0.0.0.0\n          - name: DB_DIALECT\n            valueFrom:\n              secretKeyRef:\n                key: DB_DIALECT\n                name: core-api\n          - name: DB_PORT\n            valueFrom:\n              secretKeyRef:\n                key: DB_PORT\n                name: core-api\n          - name: DB_HOST\n            valueFrom:\n              secretKeyRef:\n                key: DB_HOST\n                name: core-api\n          - name: DB_USERNAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_USERNAME\n                name: core-api\n          - name: DB_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                key: DB_PASSWORD\n                name: core-api\n          - name: DB_NAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_NAME\n                name: core-api\n          - name: JWT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: JWT_SECRET\n                name: core-api\n          - name: JWT_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: JWT_EXPIRATION\n                name: core-api\n          - name: DEFAULT_COMMISSION\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_COMMISSION\n                name: core-api\n          - name: TERMINATION_NOTICE_DAYS\n            valueFrom:\n              secretKeyRef:\n                key: TERMINATION_NOTICE_DAYS\n                name: core-api\n          - name: FILTER_BY_STOCK_URL\n            valueFrom:\n              secretKeyRef:\n                name: core-api\n                key: FILTER_BY_STOCK_URL\n          - name: TWILIO_ACCOUNT_SID\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_ACCOUNT_SID\n                name: twilio-secret\n          - name: TWILIO_AUTH_TOKEN\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_AUTH_TOKEN\n                name: twilio-secret\n          - name: TWILIO_SENDER_PHONE\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_SENDER_PHONE\n                name: twilio-secret\n          - name: AWS_S3_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_SECRET_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_REGION\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_REGION\n                name: aws-secret\n          - name: AWS_S3_BUCKET_NAME\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_NAME\n                name: aws-secret\n          - name: AWS_S3_BUCKET_URL\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_URL\n                name: aws-secret\n          - name: DEFAULT_RESIZE_WIDTH\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_RESIZE_WIDTH\n                name: aws-secret\n          - name: SUPPORT_MIME_TYPE\n            valueFrom:\n              secretKeyRef:\n                key: SUPPORT_MIME_TYPE\n                name: aws-secret\n          - name: SENDGRID_API_KEY\n            valueFrom:\n              secretKeyRef:\n                key: SENDGRID_API_KEY\n                name: core-api\n          - name: REFRESH_TOKEN_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: REFRESH_TOKEN_EXPIRATION\n                name: core-api\n          - name: GOOGLE_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_ID\n                name: google-secret\n          - name: GOOGLE_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_SECRET\n                name: google-secret\n          - name: GOOGLE_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CALLBACK_URL\n                name: google-secret\n          - name: CLIENT_BASE_URL\n            valueFrom:\n              secretKeyRef:\n                key: CLIENT_BASE_URL\n                name: google-secret\n          - name: STRIPE_SECRET_KEY\n            valueFrom:\n              secretKeyRef:\n                key: STRIPE_SECRET_KEY\n                name: stripe-secret\n          - name: FACEBOOK_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_ID\n                name: facebook-secret\n          - name: FACEBOOK_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_SECRET\n                name: facebook-secret\n          - name: FACEBOOK_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CALLBACK_URL\n                name: facebook-secret\n          - name: SITE_LINK_LOGIN_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_LOGIN_URL\n                name: sitelink-secret\n          - name: SITE_LINK_UNITS_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_UNITS_URL\n                name: sitelink-secret\n          - name: SITE_LINK_USER\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_USER\n                name: sitelink-secret\n          - name: SITE_LINK_PASS\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_PASS\n                name: sitelink-secret\n          - name: SITE_LINK_CODE\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_CODE\n                name: sitelink-secret\n          - name: SITE_LINK_COMMISSION_PERCENT\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_COMMISSION_PERCENT\n                name: sitelink-secret\n          - name: SITE_LINK_SITES_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_SITES_URL\n                name: sitelink-secret\n          - name: BROWSER_URL\n            valueFrom:\n              secretKeyRef:\n                key: BROWSERLESS_URL\n                name: sitelink-secret\n          - name: GOGOX_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_ID\n          - name: GOGOX_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_SECRET\n          - name: GOGOX_API_URL\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_API_URL\n          - name: YOTPO_API_KEY\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_KEY\n          - name: YOTPO_API_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_SECRET\n          - name: SITE_LINK_USER_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_USER_JWD\n          - name: SITE_LINK_PASS_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_PASS_JWD\n          - name: SITE_LINK_CODE_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_CODE_JWD\n          - name: SITE_LINK_LOCATION_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_LOCATION_JWD\n          - name: RABBITMQ_URL\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: RABBITMQ_URL\n          - name: SND_EXCHANGE\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_EXCHANGE\n          - name: SND_UPDATE_ES_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_ES_KEY\n          - name: SND_UPDATE_STOCK_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_STOCK_KEY\n          - name: CLEVERTAP_ACCOUNT_ID\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_ID\n          - name: CLEVERTAP_ACCOUNT_PASSCODE\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_PASSCODE\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cron-cancel-booking\" does not have a read-only root file system"
  },
  {
    "id": "04751",
    "manifest_path": "data/manifests/the_stack_sample/sample_2168.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: prod-cron-cancel-booking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-cancel-booking\n          image: 414928843086.dkr.ecr.ap-southeast-1.amazonaws.com/core-api-v2:<VERSION>\n          imagePullPolicy: Always\n          args:\n          - run\n          - start-cron:cancel-booking:prod\n          env:\n          - name: NODE_ENV\n            value: production\n          - name: PORT\n            value: '3000'\n          - name: HOST\n            value: 0.0.0.0\n          - name: DB_DIALECT\n            valueFrom:\n              secretKeyRef:\n                key: DB_DIALECT\n                name: core-api\n          - name: DB_PORT\n            valueFrom:\n              secretKeyRef:\n                key: DB_PORT\n                name: core-api\n          - name: DB_HOST\n            valueFrom:\n              secretKeyRef:\n                key: DB_HOST\n                name: core-api\n          - name: DB_USERNAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_USERNAME\n                name: core-api\n          - name: DB_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                key: DB_PASSWORD\n                name: core-api\n          - name: DB_NAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_NAME\n                name: core-api\n          - name: JWT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: JWT_SECRET\n                name: core-api\n          - name: JWT_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: JWT_EXPIRATION\n                name: core-api\n          - name: DEFAULT_COMMISSION\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_COMMISSION\n                name: core-api\n          - name: TERMINATION_NOTICE_DAYS\n            valueFrom:\n              secretKeyRef:\n                key: TERMINATION_NOTICE_DAYS\n                name: core-api\n          - name: FILTER_BY_STOCK_URL\n            valueFrom:\n              secretKeyRef:\n                name: core-api\n                key: FILTER_BY_STOCK_URL\n          - name: TWILIO_ACCOUNT_SID\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_ACCOUNT_SID\n                name: twilio-secret\n          - name: TWILIO_AUTH_TOKEN\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_AUTH_TOKEN\n                name: twilio-secret\n          - name: TWILIO_SENDER_PHONE\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_SENDER_PHONE\n                name: twilio-secret\n          - name: AWS_S3_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_SECRET_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_REGION\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_REGION\n                name: aws-secret\n          - name: AWS_S3_BUCKET_NAME\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_NAME\n                name: aws-secret\n          - name: AWS_S3_BUCKET_URL\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_URL\n                name: aws-secret\n          - name: DEFAULT_RESIZE_WIDTH\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_RESIZE_WIDTH\n                name: aws-secret\n          - name: SUPPORT_MIME_TYPE\n            valueFrom:\n              secretKeyRef:\n                key: SUPPORT_MIME_TYPE\n                name: aws-secret\n          - name: SENDGRID_API_KEY\n            valueFrom:\n              secretKeyRef:\n                key: SENDGRID_API_KEY\n                name: core-api\n          - name: REFRESH_TOKEN_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: REFRESH_TOKEN_EXPIRATION\n                name: core-api\n          - name: GOOGLE_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_ID\n                name: google-secret\n          - name: GOOGLE_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_SECRET\n                name: google-secret\n          - name: GOOGLE_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CALLBACK_URL\n                name: google-secret\n          - name: CLIENT_BASE_URL\n            valueFrom:\n              secretKeyRef:\n                key: CLIENT_BASE_URL\n                name: google-secret\n          - name: STRIPE_SECRET_KEY\n            valueFrom:\n              secretKeyRef:\n                key: STRIPE_SECRET_KEY\n                name: stripe-secret\n          - name: FACEBOOK_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_ID\n                name: facebook-secret\n          - name: FACEBOOK_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_SECRET\n                name: facebook-secret\n          - name: FACEBOOK_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CALLBACK_URL\n                name: facebook-secret\n          - name: SITE_LINK_LOGIN_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_LOGIN_URL\n                name: sitelink-secret\n          - name: SITE_LINK_UNITS_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_UNITS_URL\n                name: sitelink-secret\n          - name: SITE_LINK_USER\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_USER\n                name: sitelink-secret\n          - name: SITE_LINK_PASS\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_PASS\n                name: sitelink-secret\n          - name: SITE_LINK_CODE\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_CODE\n                name: sitelink-secret\n          - name: SITE_LINK_COMMISSION_PERCENT\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_COMMISSION_PERCENT\n                name: sitelink-secret\n          - name: SITE_LINK_SITES_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_SITES_URL\n                name: sitelink-secret\n          - name: BROWSER_URL\n            valueFrom:\n              secretKeyRef:\n                key: BROWSERLESS_URL\n                name: sitelink-secret\n          - name: GOGOX_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_ID\n          - name: GOGOX_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_SECRET\n          - name: GOGOX_API_URL\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_API_URL\n          - name: YOTPO_API_KEY\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_KEY\n          - name: YOTPO_API_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_SECRET\n          - name: SITE_LINK_USER_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_USER_JWD\n          - name: SITE_LINK_PASS_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_PASS_JWD\n          - name: SITE_LINK_CODE_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_CODE_JWD\n          - name: SITE_LINK_LOCATION_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_LOCATION_JWD\n          - name: RABBITMQ_URL\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: RABBITMQ_URL\n          - name: SND_EXCHANGE\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_EXCHANGE\n          - name: SND_UPDATE_ES_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_ES_KEY\n          - name: SND_UPDATE_STOCK_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_STOCK_KEY\n          - name: CLEVERTAP_ACCOUNT_ID\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_ID\n          - name: CLEVERTAP_ACCOUNT_PASSCODE\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_PASSCODE\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cron-cancel-booking\" is not set to runAsNonRoot"
  },
  {
    "id": "04752",
    "manifest_path": "data/manifests/the_stack_sample/sample_2168.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: prod-cron-cancel-booking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-cancel-booking\n          image: 414928843086.dkr.ecr.ap-southeast-1.amazonaws.com/core-api-v2:<VERSION>\n          imagePullPolicy: Always\n          args:\n          - run\n          - start-cron:cancel-booking:prod\n          env:\n          - name: NODE_ENV\n            value: production\n          - name: PORT\n            value: '3000'\n          - name: HOST\n            value: 0.0.0.0\n          - name: DB_DIALECT\n            valueFrom:\n              secretKeyRef:\n                key: DB_DIALECT\n                name: core-api\n          - name: DB_PORT\n            valueFrom:\n              secretKeyRef:\n                key: DB_PORT\n                name: core-api\n          - name: DB_HOST\n            valueFrom:\n              secretKeyRef:\n                key: DB_HOST\n                name: core-api\n          - name: DB_USERNAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_USERNAME\n                name: core-api\n          - name: DB_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                key: DB_PASSWORD\n                name: core-api\n          - name: DB_NAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_NAME\n                name: core-api\n          - name: JWT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: JWT_SECRET\n                name: core-api\n          - name: JWT_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: JWT_EXPIRATION\n                name: core-api\n          - name: DEFAULT_COMMISSION\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_COMMISSION\n                name: core-api\n          - name: TERMINATION_NOTICE_DAYS\n            valueFrom:\n              secretKeyRef:\n                key: TERMINATION_NOTICE_DAYS\n                name: core-api\n          - name: FILTER_BY_STOCK_URL\n            valueFrom:\n              secretKeyRef:\n                name: core-api\n                key: FILTER_BY_STOCK_URL\n          - name: TWILIO_ACCOUNT_SID\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_ACCOUNT_SID\n                name: twilio-secret\n          - name: TWILIO_AUTH_TOKEN\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_AUTH_TOKEN\n                name: twilio-secret\n          - name: TWILIO_SENDER_PHONE\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_SENDER_PHONE\n                name: twilio-secret\n          - name: AWS_S3_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_SECRET_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_REGION\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_REGION\n                name: aws-secret\n          - name: AWS_S3_BUCKET_NAME\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_NAME\n                name: aws-secret\n          - name: AWS_S3_BUCKET_URL\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_URL\n                name: aws-secret\n          - name: DEFAULT_RESIZE_WIDTH\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_RESIZE_WIDTH\n                name: aws-secret\n          - name: SUPPORT_MIME_TYPE\n            valueFrom:\n              secretKeyRef:\n                key: SUPPORT_MIME_TYPE\n                name: aws-secret\n          - name: SENDGRID_API_KEY\n            valueFrom:\n              secretKeyRef:\n                key: SENDGRID_API_KEY\n                name: core-api\n          - name: REFRESH_TOKEN_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: REFRESH_TOKEN_EXPIRATION\n                name: core-api\n          - name: GOOGLE_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_ID\n                name: google-secret\n          - name: GOOGLE_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_SECRET\n                name: google-secret\n          - name: GOOGLE_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CALLBACK_URL\n                name: google-secret\n          - name: CLIENT_BASE_URL\n            valueFrom:\n              secretKeyRef:\n                key: CLIENT_BASE_URL\n                name: google-secret\n          - name: STRIPE_SECRET_KEY\n            valueFrom:\n              secretKeyRef:\n                key: STRIPE_SECRET_KEY\n                name: stripe-secret\n          - name: FACEBOOK_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_ID\n                name: facebook-secret\n          - name: FACEBOOK_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_SECRET\n                name: facebook-secret\n          - name: FACEBOOK_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CALLBACK_URL\n                name: facebook-secret\n          - name: SITE_LINK_LOGIN_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_LOGIN_URL\n                name: sitelink-secret\n          - name: SITE_LINK_UNITS_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_UNITS_URL\n                name: sitelink-secret\n          - name: SITE_LINK_USER\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_USER\n                name: sitelink-secret\n          - name: SITE_LINK_PASS\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_PASS\n                name: sitelink-secret\n          - name: SITE_LINK_CODE\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_CODE\n                name: sitelink-secret\n          - name: SITE_LINK_COMMISSION_PERCENT\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_COMMISSION_PERCENT\n                name: sitelink-secret\n          - name: SITE_LINK_SITES_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_SITES_URL\n                name: sitelink-secret\n          - name: BROWSER_URL\n            valueFrom:\n              secretKeyRef:\n                key: BROWSERLESS_URL\n                name: sitelink-secret\n          - name: GOGOX_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_ID\n          - name: GOGOX_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_SECRET\n          - name: GOGOX_API_URL\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_API_URL\n          - name: YOTPO_API_KEY\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_KEY\n          - name: YOTPO_API_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_SECRET\n          - name: SITE_LINK_USER_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_USER_JWD\n          - name: SITE_LINK_PASS_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_PASS_JWD\n          - name: SITE_LINK_CODE_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_CODE_JWD\n          - name: SITE_LINK_LOCATION_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_LOCATION_JWD\n          - name: RABBITMQ_URL\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: RABBITMQ_URL\n          - name: SND_EXCHANGE\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_EXCHANGE\n          - name: SND_UPDATE_ES_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_ES_KEY\n          - name: SND_UPDATE_STOCK_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_STOCK_KEY\n          - name: CLEVERTAP_ACCOUNT_ID\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_ID\n          - name: CLEVERTAP_ACCOUNT_PASSCODE\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_PASSCODE\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cron-cancel-booking\" has cpu request 0"
  },
  {
    "id": "04753",
    "manifest_path": "data/manifests/the_stack_sample/sample_2168.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: prod-cron-cancel-booking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-cancel-booking\n          image: 414928843086.dkr.ecr.ap-southeast-1.amazonaws.com/core-api-v2:<VERSION>\n          imagePullPolicy: Always\n          args:\n          - run\n          - start-cron:cancel-booking:prod\n          env:\n          - name: NODE_ENV\n            value: production\n          - name: PORT\n            value: '3000'\n          - name: HOST\n            value: 0.0.0.0\n          - name: DB_DIALECT\n            valueFrom:\n              secretKeyRef:\n                key: DB_DIALECT\n                name: core-api\n          - name: DB_PORT\n            valueFrom:\n              secretKeyRef:\n                key: DB_PORT\n                name: core-api\n          - name: DB_HOST\n            valueFrom:\n              secretKeyRef:\n                key: DB_HOST\n                name: core-api\n          - name: DB_USERNAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_USERNAME\n                name: core-api\n          - name: DB_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                key: DB_PASSWORD\n                name: core-api\n          - name: DB_NAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_NAME\n                name: core-api\n          - name: JWT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: JWT_SECRET\n                name: core-api\n          - name: JWT_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: JWT_EXPIRATION\n                name: core-api\n          - name: DEFAULT_COMMISSION\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_COMMISSION\n                name: core-api\n          - name: TERMINATION_NOTICE_DAYS\n            valueFrom:\n              secretKeyRef:\n                key: TERMINATION_NOTICE_DAYS\n                name: core-api\n          - name: FILTER_BY_STOCK_URL\n            valueFrom:\n              secretKeyRef:\n                name: core-api\n                key: FILTER_BY_STOCK_URL\n          - name: TWILIO_ACCOUNT_SID\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_ACCOUNT_SID\n                name: twilio-secret\n          - name: TWILIO_AUTH_TOKEN\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_AUTH_TOKEN\n                name: twilio-secret\n          - name: TWILIO_SENDER_PHONE\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_SENDER_PHONE\n                name: twilio-secret\n          - name: AWS_S3_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_SECRET_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_REGION\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_REGION\n                name: aws-secret\n          - name: AWS_S3_BUCKET_NAME\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_NAME\n                name: aws-secret\n          - name: AWS_S3_BUCKET_URL\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_URL\n                name: aws-secret\n          - name: DEFAULT_RESIZE_WIDTH\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_RESIZE_WIDTH\n                name: aws-secret\n          - name: SUPPORT_MIME_TYPE\n            valueFrom:\n              secretKeyRef:\n                key: SUPPORT_MIME_TYPE\n                name: aws-secret\n          - name: SENDGRID_API_KEY\n            valueFrom:\n              secretKeyRef:\n                key: SENDGRID_API_KEY\n                name: core-api\n          - name: REFRESH_TOKEN_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: REFRESH_TOKEN_EXPIRATION\n                name: core-api\n          - name: GOOGLE_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_ID\n                name: google-secret\n          - name: GOOGLE_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_SECRET\n                name: google-secret\n          - name: GOOGLE_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CALLBACK_URL\n                name: google-secret\n          - name: CLIENT_BASE_URL\n            valueFrom:\n              secretKeyRef:\n                key: CLIENT_BASE_URL\n                name: google-secret\n          - name: STRIPE_SECRET_KEY\n            valueFrom:\n              secretKeyRef:\n                key: STRIPE_SECRET_KEY\n                name: stripe-secret\n          - name: FACEBOOK_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_ID\n                name: facebook-secret\n          - name: FACEBOOK_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_SECRET\n                name: facebook-secret\n          - name: FACEBOOK_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CALLBACK_URL\n                name: facebook-secret\n          - name: SITE_LINK_LOGIN_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_LOGIN_URL\n                name: sitelink-secret\n          - name: SITE_LINK_UNITS_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_UNITS_URL\n                name: sitelink-secret\n          - name: SITE_LINK_USER\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_USER\n                name: sitelink-secret\n          - name: SITE_LINK_PASS\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_PASS\n                name: sitelink-secret\n          - name: SITE_LINK_CODE\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_CODE\n                name: sitelink-secret\n          - name: SITE_LINK_COMMISSION_PERCENT\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_COMMISSION_PERCENT\n                name: sitelink-secret\n          - name: SITE_LINK_SITES_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_SITES_URL\n                name: sitelink-secret\n          - name: BROWSER_URL\n            valueFrom:\n              secretKeyRef:\n                key: BROWSERLESS_URL\n                name: sitelink-secret\n          - name: GOGOX_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_ID\n          - name: GOGOX_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_SECRET\n          - name: GOGOX_API_URL\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_API_URL\n          - name: YOTPO_API_KEY\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_KEY\n          - name: YOTPO_API_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_SECRET\n          - name: SITE_LINK_USER_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_USER_JWD\n          - name: SITE_LINK_PASS_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_PASS_JWD\n          - name: SITE_LINK_CODE_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_CODE_JWD\n          - name: SITE_LINK_LOCATION_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_LOCATION_JWD\n          - name: RABBITMQ_URL\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: RABBITMQ_URL\n          - name: SND_EXCHANGE\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_EXCHANGE\n          - name: SND_UPDATE_ES_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_ES_KEY\n          - name: SND_UPDATE_STOCK_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_STOCK_KEY\n          - name: CLEVERTAP_ACCOUNT_ID\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_ID\n          - name: CLEVERTAP_ACCOUNT_PASSCODE\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_PASSCODE\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cron-cancel-booking\" has memory limit 0"
  },
  {
    "id": "04754",
    "manifest_path": "data/manifests/the_stack_sample/sample_2169.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9520\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04755",
    "manifest_path": "data/manifests/the_stack_sample/sample_2169.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9520\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04756",
    "manifest_path": "data/manifests/the_stack_sample/sample_2169.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9520\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04757",
    "manifest_path": "data/manifests/the_stack_sample/sample_2169.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9520\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04758",
    "manifest_path": "data/manifests/the_stack_sample/sample_2169.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9520\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04759",
    "manifest_path": "data/manifests/the_stack_sample/sample_2170.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: accountapi-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: accountapi\n  template:\n    metadata:\n      labels:\n        app: accountapi\n    spec:\n      containers:\n      - name: accountapi-container\n        image: gcr.io/staffjoy-prod/accountapi:VERSION\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        env:\n        - name: DEPLOY\n          value: VERSION\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: dsn\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"accountapi-container\" does not have a read-only root file system"
  },
  {
    "id": "04760",
    "manifest_path": "data/manifests/the_stack_sample/sample_2170.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: accountapi-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: accountapi\n  template:\n    metadata:\n      labels:\n        app: accountapi\n    spec:\n      containers:\n      - name: accountapi-container\n        image: gcr.io/staffjoy-prod/accountapi:VERSION\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        env:\n        - name: DEPLOY\n          value: VERSION\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: dsn\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"accountapi-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04761",
    "manifest_path": "data/manifests/the_stack_sample/sample_2170.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: accountapi-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: accountapi\n  template:\n    metadata:\n      labels:\n        app: accountapi\n    spec:\n      containers:\n      - name: accountapi-container\n        image: gcr.io/staffjoy-prod/accountapi:VERSION\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        env:\n        - name: DEPLOY\n          value: VERSION\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: dsn\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"accountapi-container\" has cpu request 0"
  },
  {
    "id": "04762",
    "manifest_path": "data/manifests/the_stack_sample/sample_2170.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: accountapi-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: accountapi\n  template:\n    metadata:\n      labels:\n        app: accountapi\n    spec:\n      containers:\n      - name: accountapi-container\n        image: gcr.io/staffjoy-prod/accountapi:VERSION\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        env:\n        - name: DEPLOY\n          value: VERSION\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: dsn\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"accountapi-container\" has memory limit 0"
  },
  {
    "id": "04763",
    "manifest_path": "data/manifests/the_stack_sample/sample_2171.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  namespace: kube-system\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: 'true'\n    spec:\n      containers:\n      - name: smarter-device-manager\n        image: registry.gitlab.com/arm-research/smarter/smarter-device-manager:v1.1.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          mountPath: /dev\n        - name: sys-dir\n          mountPath: /sys\n        - name: config\n          mountPath: /root/config\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n      - name: dev-dir\n        hostPath:\n          path: /dev\n      - name: sys-dir\n        hostPath:\n          path: /sys\n      - name: config\n        configMap:\n          name: smarter-device-manager\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"smarter-device-manager\" does not have a read-only root file system"
  },
  {
    "id": "04764",
    "manifest_path": "data/manifests/the_stack_sample/sample_2171.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  namespace: kube-system\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: 'true'\n    spec:\n      containers:\n      - name: smarter-device-manager\n        image: registry.gitlab.com/arm-research/smarter/smarter-device-manager:v1.1.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          mountPath: /dev\n        - name: sys-dir\n          mountPath: /sys\n        - name: config\n          mountPath: /root/config\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n      - name: dev-dir\n        hostPath:\n          path: /dev\n      - name: sys-dir\n        hostPath:\n          path: /sys\n      - name: config\n        configMap:\n          name: smarter-device-manager\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"smarter-device-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "04765",
    "manifest_path": "data/manifests/the_stack_sample/sample_2172.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: siegetest\nspec:\n  replicas: 1\n  selector:\n    name: siegetest\n    version: 1.0.0\n  template:\n    metadata:\n      labels:\n        name: siegetest\n        version: 1.0.0\n    spec:\n      containers:\n      - name: siegetest\n        image: ipedrazas/siegetest:1.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"siegetest\" does not have a read-only root file system"
  },
  {
    "id": "04766",
    "manifest_path": "data/manifests/the_stack_sample/sample_2172.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: siegetest\nspec:\n  replicas: 1\n  selector:\n    name: siegetest\n    version: 1.0.0\n  template:\n    metadata:\n      labels:\n        name: siegetest\n        version: 1.0.0\n    spec:\n      containers:\n      - name: siegetest\n        image: ipedrazas/siegetest:1.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"siegetest\" is not set to runAsNonRoot"
  },
  {
    "id": "04767",
    "manifest_path": "data/manifests/the_stack_sample/sample_2172.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: siegetest\nspec:\n  replicas: 1\n  selector:\n    name: siegetest\n    version: 1.0.0\n  template:\n    metadata:\n      labels:\n        name: siegetest\n        version: 1.0.0\n    spec:\n      containers:\n      - name: siegetest\n        image: ipedrazas/siegetest:1.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"siegetest\" has cpu request 0"
  },
  {
    "id": "04768",
    "manifest_path": "data/manifests/the_stack_sample/sample_2172.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: siegetest\nspec:\n  replicas: 1\n  selector:\n    name: siegetest\n    version: 1.0.0\n  template:\n    metadata:\n      labels:\n        name: siegetest\n        version: 1.0.0\n    spec:\n      containers:\n      - name: siegetest\n        image: ipedrazas/siegetest:1.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"siegetest\" has memory limit 0"
  },
  {
    "id": "04769",
    "manifest_path": "data/manifests/the_stack_sample/sample_2180.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-batch\nspec:\n  containers:\n  - name: test-batch\n    image: '{{ test_batch_image.image }}'\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: HAIL_TOKEN_FILE\n      value: /test-jwt/jwt\n    - name: BATCH_URL\n      value: http://batch.{{ default_ns.name }}\n    volumeMounts:\n    - mountPath: /test-jwt\n      readOnly: true\n      name: test-jwt\n  volumes:\n  - name: test-jwt\n    secret:\n      secretName: test-jwt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"test-batch\" is using an invalid container image, \"{{ test_batch_image.image }}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04770",
    "manifest_path": "data/manifests/the_stack_sample/sample_2180.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-batch\nspec:\n  containers:\n  - name: test-batch\n    image: '{{ test_batch_image.image }}'\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: HAIL_TOKEN_FILE\n      value: /test-jwt/jwt\n    - name: BATCH_URL\n      value: http://batch.{{ default_ns.name }}\n    volumeMounts:\n    - mountPath: /test-jwt\n      readOnly: true\n      name: test-jwt\n  volumes:\n  - name: test-jwt\n    secret:\n      secretName: test-jwt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-batch\" does not have a read-only root file system"
  },
  {
    "id": "04771",
    "manifest_path": "data/manifests/the_stack_sample/sample_2180.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-batch\nspec:\n  containers:\n  - name: test-batch\n    image: '{{ test_batch_image.image }}'\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: HAIL_TOKEN_FILE\n      value: /test-jwt/jwt\n    - name: BATCH_URL\n      value: http://batch.{{ default_ns.name }}\n    volumeMounts:\n    - mountPath: /test-jwt\n      readOnly: true\n      name: test-jwt\n  volumes:\n  - name: test-jwt\n    secret:\n      secretName: test-jwt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-batch\" is not set to runAsNonRoot"
  },
  {
    "id": "04772",
    "manifest_path": "data/manifests/the_stack_sample/sample_2180.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-batch\nspec:\n  containers:\n  - name: test-batch\n    image: '{{ test_batch_image.image }}'\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: HAIL_TOKEN_FILE\n      value: /test-jwt/jwt\n    - name: BATCH_URL\n      value: http://batch.{{ default_ns.name }}\n    volumeMounts:\n    - mountPath: /test-jwt\n      readOnly: true\n      name: test-jwt\n  volumes:\n  - name: test-jwt\n    secret:\n      secretName: test-jwt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-batch\" has cpu request 0"
  },
  {
    "id": "04773",
    "manifest_path": "data/manifests/the_stack_sample/sample_2180.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-batch\nspec:\n  containers:\n  - name: test-batch\n    image: '{{ test_batch_image.image }}'\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: HAIL_TOKEN_FILE\n      value: /test-jwt/jwt\n    - name: BATCH_URL\n      value: http://batch.{{ default_ns.name }}\n    volumeMounts:\n    - mountPath: /test-jwt\n      readOnly: true\n      name: test-jwt\n  volumes:\n  - name: test-jwt\n    secret:\n      secretName: test-jwt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-batch\" has memory limit 0"
  },
  {
    "id": "04774",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "04775",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "04776",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "04777",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "04778",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "04779",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "04780",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "04781",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "04782",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "04783",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "04784",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "04785",
    "manifest_path": "data/manifests/the_stack_sample/sample_2185.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lockvalidation-dpl\n  namespace: kube-lock\n  labels:\n    app: lockvalidation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lockvalidation\n  template:\n    metadata:\n      labels:\n        app: lockvalidation\n    spec:\n      serviceAccountName: lockvalidation-sa\n      containers:\n      - name: lockvalidation\n        image: pkotas/lockvalidation:devel\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /etc/lockvalidation/cert\n          readOnly: true\n      volumes:\n      - name: webhook-certs\n        secret:\n          secretName: lockvalidation-crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lockvalidation\" does not have a read-only root file system"
  },
  {
    "id": "04786",
    "manifest_path": "data/manifests/the_stack_sample/sample_2185.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lockvalidation-dpl\n  namespace: kube-lock\n  labels:\n    app: lockvalidation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lockvalidation\n  template:\n    metadata:\n      labels:\n        app: lockvalidation\n    spec:\n      serviceAccountName: lockvalidation-sa\n      containers:\n      - name: lockvalidation\n        image: pkotas/lockvalidation:devel\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /etc/lockvalidation/cert\n          readOnly: true\n      volumes:\n      - name: webhook-certs\n        secret:\n          secretName: lockvalidation-crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lockvalidation\" is not set to runAsNonRoot"
  },
  {
    "id": "04787",
    "manifest_path": "data/manifests/the_stack_sample/sample_2185.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lockvalidation-dpl\n  namespace: kube-lock\n  labels:\n    app: lockvalidation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lockvalidation\n  template:\n    metadata:\n      labels:\n        app: lockvalidation\n    spec:\n      serviceAccountName: lockvalidation-sa\n      containers:\n      - name: lockvalidation\n        image: pkotas/lockvalidation:devel\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /etc/lockvalidation/cert\n          readOnly: true\n      volumes:\n      - name: webhook-certs\n        secret:\n          secretName: lockvalidation-crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"lockvalidation\" has cpu request 0"
  },
  {
    "id": "04788",
    "manifest_path": "data/manifests/the_stack_sample/sample_2185.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lockvalidation-dpl\n  namespace: kube-lock\n  labels:\n    app: lockvalidation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lockvalidation\n  template:\n    metadata:\n      labels:\n        app: lockvalidation\n    spec:\n      serviceAccountName: lockvalidation-sa\n      containers:\n      - name: lockvalidation\n        image: pkotas/lockvalidation:devel\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /etc/lockvalidation/cert\n          readOnly: true\n      volumes:\n      - name: webhook-certs\n        secret:\n          secretName: lockvalidation-crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"lockvalidation\" has memory limit 0"
  },
  {
    "id": "04789",
    "manifest_path": "data/manifests/the_stack_sample/sample_2188.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.3.2\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.3.2\n        app.kubernetes.io/version: latest\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: gcr.io/jenkinsxio/jx-pipelines-visualizer:1.3.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.18.134.92.246.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /home/jenkins\n        - name: GIT_SECRET_MOUNT_PATH\n          value: /secrets/git\n        - name: AWS_REGION\n          value: eu-west-2\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable GIT_SECRET_MOUNT_PATH in container \"jx-pipelines-visualizer\" found"
  },
  {
    "id": "04790",
    "manifest_path": "data/manifests/the_stack_sample/sample_2188.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.3.2\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.3.2\n        app.kubernetes.io/version: latest\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: gcr.io/jenkinsxio/jx-pipelines-visualizer:1.3.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.18.134.92.246.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /home/jenkins\n        - name: GIT_SECRET_MOUNT_PATH\n          value: /secrets/git\n        - name: AWS_REGION\n          value: eu-west-2\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jx-pipelines-visualizer\" does not have a read-only root file system"
  },
  {
    "id": "04791",
    "manifest_path": "data/manifests/the_stack_sample/sample_2188.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.3.2\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.3.2\n        app.kubernetes.io/version: latest\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: gcr.io/jenkinsxio/jx-pipelines-visualizer:1.3.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.18.134.92.246.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /home/jenkins\n        - name: GIT_SECRET_MOUNT_PATH\n          value: /secrets/git\n        - name: AWS_REGION\n          value: eu-west-2\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jx-pipelines-visualizer\" is not set to runAsNonRoot"
  },
  {
    "id": "04792",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"azurefile\" does not have a read-only root file system"
  },
  {
    "id": "04793",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-attacher\" does not have a read-only root file system"
  },
  {
    "id": "04794",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "04795",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-resizer\" does not have a read-only root file system"
  },
  {
    "id": "04796",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-snapshotter\" does not have a read-only root file system"
  },
  {
    "id": "04797",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "04798",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"azurefile\" is not set to runAsNonRoot"
  },
  {
    "id": "04799",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-attacher\" is not set to runAsNonRoot"
  },
  {
    "id": "04800",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "04801",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-resizer\" is not set to runAsNonRoot"
  },
  {
    "id": "04802",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-snapshotter\" is not set to runAsNonRoot"
  },
  {
    "id": "04803",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "04804",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"installation-complete\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04805",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"api-registry-delete\" does not have a read-only root file system"
  },
  {
    "id": "04806",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"events-broker-delete\" does not have a read-only root file system"
  },
  {
    "id": "04807",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fn-delete\" does not have a read-only root file system"
  },
  {
    "id": "04808",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"installation-complete\" does not have a read-only root file system"
  },
  {
    "id": "04809",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"istio-delete\" does not have a read-only root file system"
  },
  {
    "id": "04810",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kafka-delete\" does not have a read-only root file system"
  },
  {
    "id": "04811",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"observability-tools-delete\" does not have a read-only root file system"
  },
  {
    "id": "04812",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"api-registry-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "04813",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"events-broker-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "04814",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fn-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "04815",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"installation-complete\" is not set to runAsNonRoot"
  },
  {
    "id": "04816",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"istio-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "04817",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kafka-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "04818",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"observability-tools-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "04819",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"api-registry-delete\" has cpu request 0"
  },
  {
    "id": "04820",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"events-broker-delete\" has cpu request 0"
  },
  {
    "id": "04821",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fn-delete\" has cpu request 0"
  },
  {
    "id": "04822",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"installation-complete\" has cpu request 0"
  },
  {
    "id": "04823",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"istio-delete\" has cpu request 0"
  },
  {
    "id": "04824",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kafka-delete\" has cpu request 0"
  },
  {
    "id": "04825",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"observability-tools-delete\" has cpu request 0"
  },
  {
    "id": "04826",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"api-registry-delete\" has memory limit 0"
  },
  {
    "id": "04827",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"events-broker-delete\" has memory limit 0"
  },
  {
    "id": "04828",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fn-delete\" has memory limit 0"
  },
  {
    "id": "04829",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"installation-complete\" has memory limit 0"
  },
  {
    "id": "04830",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"istio-delete\" has memory limit 0"
  },
  {
    "id": "04831",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kafka-delete\" has memory limit 0"
  },
  {
    "id": "04832",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"observability-tools-delete\" has memory limit 0"
  },
  {
    "id": "04833",
    "manifest_path": "data/manifests/the_stack_sample/sample_2196.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://Kubernetes.kubernetes:6443?inClusterConfig=true&insecure=true\n        - --sink=influxdb:http://monitoring-influxdb:8086\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "04834",
    "manifest_path": "data/manifests/the_stack_sample/sample_2196.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://Kubernetes.kubernetes:6443?inClusterConfig=true&insecure=true\n        - --sink=influxdb:http://monitoring-influxdb:8086\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  },
  {
    "id": "04835",
    "manifest_path": "data/manifests/the_stack_sample/sample_2196.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://Kubernetes.kubernetes:6443?inClusterConfig=true&insecure=true\n        - --sink=influxdb:http://monitoring-influxdb:8086\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"heapster\" has cpu request 0"
  },
  {
    "id": "04836",
    "manifest_path": "data/manifests/the_stack_sample/sample_2196.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://Kubernetes.kubernetes:6443?inClusterConfig=true&insecure=true\n        - --sink=influxdb:http://monitoring-influxdb:8086\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"heapster\" has memory limit 0"
  },
  {
    "id": "04837",
    "manifest_path": "data/manifests/the_stack_sample/sample_2198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04838",
    "manifest_path": "data/manifests/the_stack_sample/sample_2198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04839",
    "manifest_path": "data/manifests/the_stack_sample/sample_2198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04840",
    "manifest_path": "data/manifests/the_stack_sample/sample_2198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04841",
    "manifest_path": "data/manifests/the_stack_sample/sample_2198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04842",
    "manifest_path": "data/manifests/the_stack_sample/sample_2199.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: nginx\n  template:\n    metadata:\n      name: nginx\n    spec:\n      containers:\n      - image: nginx:1.15.3\n        name: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04843",
    "manifest_path": "data/manifests/the_stack_sample/sample_2199.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: nginx\n  template:\n    metadata:\n      name: nginx\n    spec:\n      containers:\n      - image: nginx:1.15.3\n        name: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04844",
    "manifest_path": "data/manifests/the_stack_sample/sample_2199.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: nginx\n  template:\n    metadata:\n      name: nginx\n    spec:\n      containers:\n      - image: nginx:1.15.3\n        name: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04845",
    "manifest_path": "data/manifests/the_stack_sample/sample_2199.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: nginx\n  template:\n    metadata:\n      name: nginx\n    spec:\n      containers:\n      - image: nginx:1.15.3\n        name: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04846",
    "manifest_path": "data/manifests/the_stack_sample/sample_2201.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etcd-job\nspec:\n  template:\n    metadata:\n      name: etcd-job\n    spec:\n      containers:\n      - name: etcd-job\n        image: tenstartups/etcdctl\n        env:\n        - name: ETCDCTL_ENDPOINT\n          value: http://example-etcd-cluster-client-service:2379\n        command:\n        - etcdctl\n        - mkdir\n        - pod-list\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"etcd-job\" is using an invalid container image, \"tenstartups/etcdctl\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04847",
    "manifest_path": "data/manifests/the_stack_sample/sample_2201.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etcd-job\nspec:\n  template:\n    metadata:\n      name: etcd-job\n    spec:\n      containers:\n      - name: etcd-job\n        image: tenstartups/etcdctl\n        env:\n        - name: ETCDCTL_ENDPOINT\n          value: http://example-etcd-cluster-client-service:2379\n        command:\n        - etcdctl\n        - mkdir\n        - pod-list\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"etcd-job\" does not have a read-only root file system"
  },
  {
    "id": "04848",
    "manifest_path": "data/manifests/the_stack_sample/sample_2201.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etcd-job\nspec:\n  template:\n    metadata:\n      name: etcd-job\n    spec:\n      containers:\n      - name: etcd-job\n        image: tenstartups/etcdctl\n        env:\n        - name: ETCDCTL_ENDPOINT\n          value: http://example-etcd-cluster-client-service:2379\n        command:\n        - etcdctl\n        - mkdir\n        - pod-list\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"etcd-job\" is not set to runAsNonRoot"
  },
  {
    "id": "04849",
    "manifest_path": "data/manifests/the_stack_sample/sample_2201.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etcd-job\nspec:\n  template:\n    metadata:\n      name: etcd-job\n    spec:\n      containers:\n      - name: etcd-job\n        image: tenstartups/etcdctl\n        env:\n        - name: ETCDCTL_ENDPOINT\n          value: http://example-etcd-cluster-client-service:2379\n        command:\n        - etcdctl\n        - mkdir\n        - pod-list\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"etcd-job\" has cpu request 0"
  },
  {
    "id": "04850",
    "manifest_path": "data/manifests/the_stack_sample/sample_2201.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etcd-job\nspec:\n  template:\n    metadata:\n      name: etcd-job\n    spec:\n      containers:\n      - name: etcd-job\n        image: tenstartups/etcdctl\n        env:\n        - name: ETCDCTL_ENDPOINT\n          value: http://example-etcd-cluster-client-service:2379\n        command:\n        - etcdctl\n        - mkdir\n        - pod-list\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"etcd-job\" has memory limit 0"
  },
  {
    "id": "04851",
    "manifest_path": "data/manifests/the_stack_sample/sample_2203.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: puzyrevyaroslav/hipster_frontend:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"paymentservice\" does not have a read-only root file system"
  },
  {
    "id": "04852",
    "manifest_path": "data/manifests/the_stack_sample/sample_2203.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: puzyrevyaroslav/hipster_frontend:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"paymentservice\" is not set to runAsNonRoot"
  },
  {
    "id": "04853",
    "manifest_path": "data/manifests/the_stack_sample/sample_2203.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: puzyrevyaroslav/hipster_frontend:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"paymentservice\" has cpu request 0"
  },
  {
    "id": "04854",
    "manifest_path": "data/manifests/the_stack_sample/sample_2203.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: puzyrevyaroslav/hipster_frontend:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"paymentservice\" has memory limit 0"
  },
  {
    "id": "04855",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clone-mysql\" does not have a read-only root file system"
  },
  {
    "id": "04856",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-mysql\" does not have a read-only root file system"
  },
  {
    "id": "04857",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql\" does not have a read-only root file system"
  },
  {
    "id": "04858",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"xtrabackup\" does not have a read-only root file system"
  },
  {
    "id": "04859",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clone-mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "04860",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "04861",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "04862",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"xtrabackup\" is not set to runAsNonRoot"
  },
  {
    "id": "04863",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clone-mysql\" has cpu request 0"
  },
  {
    "id": "04864",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-mysql\" has cpu request 0"
  },
  {
    "id": "04865",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clone-mysql\" has memory limit 0"
  },
  {
    "id": "04866",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-mysql\" has memory limit 0"
  },
  {
    "id": "04867",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysql\" has memory limit 0"
  },
  {
    "id": "04868",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"xtrabackup\" has memory limit 0"
  },
  {
    "id": "04869",
    "manifest_path": "data/manifests/the_stack_sample/sample_2209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: discord-voice-log-bot-deployment\n  labels:\n    app: discord-voice-log-bot\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: discord-voice-log-bot\n  template:\n    metadata:\n      labels:\n        app: discord-voice-log-bot\n    spec:\n      containers:\n      - name: discord-voice-log-bot\n        image: quay.io/satackey/discord-voice-log-bot:latest\n        volumeMounts:\n        - name: bot-config\n          mountPath: /app/configs\n          readOnly: true\n      volumes:\n      - name: bot-config\n        secret:\n          secretName: discord-voice-log-bot-secret\n          items:\n          - key: config\n            path: config.ini\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"discord-voice-log-bot\" is using an invalid container image, \"quay.io/satackey/discord-voice-log-bot:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04870",
    "manifest_path": "data/manifests/the_stack_sample/sample_2209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: discord-voice-log-bot-deployment\n  labels:\n    app: discord-voice-log-bot\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: discord-voice-log-bot\n  template:\n    metadata:\n      labels:\n        app: discord-voice-log-bot\n    spec:\n      containers:\n      - name: discord-voice-log-bot\n        image: quay.io/satackey/discord-voice-log-bot:latest\n        volumeMounts:\n        - name: bot-config\n          mountPath: /app/configs\n          readOnly: true\n      volumes:\n      - name: bot-config\n        secret:\n          secretName: discord-voice-log-bot-secret\n          items:\n          - key: config\n            path: config.ini\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"discord-voice-log-bot\" does not have a read-only root file system"
  },
  {
    "id": "04871",
    "manifest_path": "data/manifests/the_stack_sample/sample_2209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: discord-voice-log-bot-deployment\n  labels:\n    app: discord-voice-log-bot\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: discord-voice-log-bot\n  template:\n    metadata:\n      labels:\n        app: discord-voice-log-bot\n    spec:\n      containers:\n      - name: discord-voice-log-bot\n        image: quay.io/satackey/discord-voice-log-bot:latest\n        volumeMounts:\n        - name: bot-config\n          mountPath: /app/configs\n          readOnly: true\n      volumes:\n      - name: bot-config\n        secret:\n          secretName: discord-voice-log-bot-secret\n          items:\n          - key: config\n            path: config.ini\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"discord-voice-log-bot\" is not set to runAsNonRoot"
  },
  {
    "id": "04872",
    "manifest_path": "data/manifests/the_stack_sample/sample_2209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: discord-voice-log-bot-deployment\n  labels:\n    app: discord-voice-log-bot\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: discord-voice-log-bot\n  template:\n    metadata:\n      labels:\n        app: discord-voice-log-bot\n    spec:\n      containers:\n      - name: discord-voice-log-bot\n        image: quay.io/satackey/discord-voice-log-bot:latest\n        volumeMounts:\n        - name: bot-config\n          mountPath: /app/configs\n          readOnly: true\n      volumes:\n      - name: bot-config\n        secret:\n          secretName: discord-voice-log-bot-secret\n          items:\n          - key: config\n            path: config.ini\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"discord-voice-log-bot\" has cpu request 0"
  },
  {
    "id": "04873",
    "manifest_path": "data/manifests/the_stack_sample/sample_2209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: discord-voice-log-bot-deployment\n  labels:\n    app: discord-voice-log-bot\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: discord-voice-log-bot\n  template:\n    metadata:\n      labels:\n        app: discord-voice-log-bot\n    spec:\n      containers:\n      - name: discord-voice-log-bot\n        image: quay.io/satackey/discord-voice-log-bot:latest\n        volumeMounts:\n        - name: bot-config\n          mountPath: /app/configs\n          readOnly: true\n      volumes:\n      - name: bot-config\n        secret:\n          secretName: discord-voice-log-bot-secret\n          items:\n          - key: config\n            path: config.ini\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"discord-voice-log-bot\" has memory limit 0"
  },
  {
    "id": "04874",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04875",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "04876",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"manager\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04877",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"manager\" is privileged"
  },
  {
    "id": "04878",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "04879",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"manager\" has cpu request 0"
  },
  {
    "id": "04880",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"manager\" has memory limit 0"
  },
  {
    "id": "04881",
    "manifest_path": "data/manifests/the_stack_sample/sample_2214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deploy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      name: hello-pod\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello-container\n        image: aswroma3/hello:2021-kube\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello-container\" does not have a read-only root file system"
  },
  {
    "id": "04882",
    "manifest_path": "data/manifests/the_stack_sample/sample_2214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deploy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      name: hello-pod\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello-container\n        image: aswroma3/hello:2021-kube\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04883",
    "manifest_path": "data/manifests/the_stack_sample/sample_2214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deploy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      name: hello-pod\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello-container\n        image: aswroma3/hello:2021-kube\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello-container\" has cpu request 0"
  },
  {
    "id": "04884",
    "manifest_path": "data/manifests/the_stack_sample/sample_2214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deploy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      name: hello-pod\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello-container\n        image: aswroma3/hello:2021-kube\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello-container\" has memory limit 0"
  },
  {
    "id": "04885",
    "manifest_path": "data/manifests/the_stack_sample/sample_2215.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opentsdb-read\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: opentsdb-read\n  template:\n    metadata:\n      labels:\n        app: opentsdb-read\n    spec:\n      containers:\n      - name: opentsdb-read\n        image: gcr.io/cloud-solutions-images/opentsdb-bigtable:v2.1\n        ports:\n        - containerPort: 4242\n          protocol: TCP\n        volumeMounts:\n        - name: opentsdb-config\n          mountPath: /opt/opentsdb\n      volumes:\n      - name: opentsdb-config\n        configMap:\n          name: opentsdb-config\n          items:\n          - key: opentsdb.conf\n            path: opentsdb.conf\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"opentsdb-read\" does not have a read-only root file system"
  },
  {
    "id": "04886",
    "manifest_path": "data/manifests/the_stack_sample/sample_2215.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opentsdb-read\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: opentsdb-read\n  template:\n    metadata:\n      labels:\n        app: opentsdb-read\n    spec:\n      containers:\n      - name: opentsdb-read\n        image: gcr.io/cloud-solutions-images/opentsdb-bigtable:v2.1\n        ports:\n        - containerPort: 4242\n          protocol: TCP\n        volumeMounts:\n        - name: opentsdb-config\n          mountPath: /opt/opentsdb\n      volumes:\n      - name: opentsdb-config\n        configMap:\n          name: opentsdb-config\n          items:\n          - key: opentsdb.conf\n            path: opentsdb.conf\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"opentsdb-read\" is not set to runAsNonRoot"
  },
  {
    "id": "04887",
    "manifest_path": "data/manifests/the_stack_sample/sample_2215.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opentsdb-read\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: opentsdb-read\n  template:\n    metadata:\n      labels:\n        app: opentsdb-read\n    spec:\n      containers:\n      - name: opentsdb-read\n        image: gcr.io/cloud-solutions-images/opentsdb-bigtable:v2.1\n        ports:\n        - containerPort: 4242\n          protocol: TCP\n        volumeMounts:\n        - name: opentsdb-config\n          mountPath: /opt/opentsdb\n      volumes:\n      - name: opentsdb-config\n        configMap:\n          name: opentsdb-config\n          items:\n          - key: opentsdb.conf\n            path: opentsdb.conf\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"opentsdb-read\" has cpu request 0"
  },
  {
    "id": "04888",
    "manifest_path": "data/manifests/the_stack_sample/sample_2215.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opentsdb-read\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: opentsdb-read\n  template:\n    metadata:\n      labels:\n        app: opentsdb-read\n    spec:\n      containers:\n      - name: opentsdb-read\n        image: gcr.io/cloud-solutions-images/opentsdb-bigtable:v2.1\n        ports:\n        - containerPort: 4242\n          protocol: TCP\n        volumeMounts:\n        - name: opentsdb-config\n          mountPath: /opt/opentsdb\n      volumes:\n      - name: opentsdb-config\n        configMap:\n          name: opentsdb-config\n          items:\n          - key: opentsdb.conf\n            path: opentsdb.conf\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"opentsdb-read\" has memory limit 0"
  },
  {
    "id": "04889",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"machine-controller\" does not have a read-only root file system"
  },
  {
    "id": "04890",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"machine-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "04891",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"machine-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "04892",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"machine-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "04893",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"machine-controller-manager\" has cpu request 0"
  },
  {
    "id": "04894",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"machine-controller-manager\" has memory limit 0"
  },
  {
    "id": "04895",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"menu-backend\" is using an invalid container image, \"scilifelabdatacentre/menu-backend\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04896",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"menu-frontend\" is using an invalid container image, \"scilifelabdatacentre/menu-frontend\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04897",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"menu-backend\" does not have a read-only root file system"
  },
  {
    "id": "04898",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"menu-frontend\" does not have a read-only root file system"
  },
  {
    "id": "04899",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"menu-backend\" is not set to runAsNonRoot"
  },
  {
    "id": "04900",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"menu-frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "04901",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"menu-backend\" has cpu request 0"
  },
  {
    "id": "04902",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"menu-frontend\" has cpu request 0"
  },
  {
    "id": "04903",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"menu-backend\" has memory limit 0"
  },
  {
    "id": "04904",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"menu-frontend\" has memory limit 0"
  },
  {
    "id": "04905",
    "manifest_path": "data/manifests/the_stack_sample/sample_2225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: gateway\n        name: gateway\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"gateway\" is using an invalid container image, \"gateway\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04906",
    "manifest_path": "data/manifests/the_stack_sample/sample_2225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: gateway\n        name: gateway\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gateway\" does not have a read-only root file system"
  },
  {
    "id": "04907",
    "manifest_path": "data/manifests/the_stack_sample/sample_2225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: gateway\n        name: gateway\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gateway\" is not set to runAsNonRoot"
  },
  {
    "id": "04908",
    "manifest_path": "data/manifests/the_stack_sample/sample_2225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: gateway\n        name: gateway\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gateway\" has cpu request 0"
  },
  {
    "id": "04909",
    "manifest_path": "data/manifests/the_stack_sample/sample_2225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: gateway\n        name: gateway\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gateway\" has memory limit 0"
  },
  {
    "id": "04910",
    "manifest_path": "data/manifests/the_stack_sample/sample_2228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1551\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04911",
    "manifest_path": "data/manifests/the_stack_sample/sample_2228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1551\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04912",
    "manifest_path": "data/manifests/the_stack_sample/sample_2228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1551\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04913",
    "manifest_path": "data/manifests/the_stack_sample/sample_2228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1551\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04914",
    "manifest_path": "data/manifests/the_stack_sample/sample_2228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1551\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04915",
    "manifest_path": "data/manifests/the_stack_sample/sample_2230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sloth\n  namespace: default\n  labels:\n    helm.sh/chart: sloth-0.1.0\n    app.kubernetes.io/managed-by: Helm\n    app: sloth\n    app.kubernetes.io/name: sloth\n    app.kubernetes.io/instance: sloth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sloth\n      app.kubernetes.io/name: sloth\n      app.kubernetes.io/instance: sloth\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: sloth-0.1.0\n        app.kubernetes.io/managed-by: Helm\n        app: sloth\n        app.kubernetes.io/name: sloth\n        app.kubernetes.io/instance: sloth\n      annotations:\n        kubectl.kubernetes.io/default-container: sloth\n    spec:\n      serviceAccountName: sloth\n      containers:\n      - name: sloth\n        image: slok/sloth:v0.6.0\n        args:\n        - kubernetes-controller\n        - --resync-interval=3m\n        - --sli-plugins-path=/plugins\n        ports:\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /plugins/sloth-common-sli-plugins\n        resources:\n          limits:\n            memory: 150Mi\n          requests:\n            cpu: 5m\n            memory: 75Mi\n      - name: git-sync-plugins\n        image: k8s.gcr.io/git-sync/git-sync:v3.3.4\n        args:\n        - --repo=https://github.com/slok/sloth-common-sli-plugins\n        - --branch=main\n        - --wait=30\n        - --webhook-url=http://localhost:8082/-/reload\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /tmp/git\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 5m\n            memory: 50Mi\n      volumes:\n      - name: sloth-common-sli-plugins\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"git-sync-plugins\" does not have a read-only root file system"
  },
  {
    "id": "04916",
    "manifest_path": "data/manifests/the_stack_sample/sample_2230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sloth\n  namespace: default\n  labels:\n    helm.sh/chart: sloth-0.1.0\n    app.kubernetes.io/managed-by: Helm\n    app: sloth\n    app.kubernetes.io/name: sloth\n    app.kubernetes.io/instance: sloth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sloth\n      app.kubernetes.io/name: sloth\n      app.kubernetes.io/instance: sloth\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: sloth-0.1.0\n        app.kubernetes.io/managed-by: Helm\n        app: sloth\n        app.kubernetes.io/name: sloth\n        app.kubernetes.io/instance: sloth\n      annotations:\n        kubectl.kubernetes.io/default-container: sloth\n    spec:\n      serviceAccountName: sloth\n      containers:\n      - name: sloth\n        image: slok/sloth:v0.6.0\n        args:\n        - kubernetes-controller\n        - --resync-interval=3m\n        - --sli-plugins-path=/plugins\n        ports:\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /plugins/sloth-common-sli-plugins\n        resources:\n          limits:\n            memory: 150Mi\n          requests:\n            cpu: 5m\n            memory: 75Mi\n      - name: git-sync-plugins\n        image: k8s.gcr.io/git-sync/git-sync:v3.3.4\n        args:\n        - --repo=https://github.com/slok/sloth-common-sli-plugins\n        - --branch=main\n        - --wait=30\n        - --webhook-url=http://localhost:8082/-/reload\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /tmp/git\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 5m\n            memory: 50Mi\n      volumes:\n      - name: sloth-common-sli-plugins\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sloth\" does not have a read-only root file system"
  },
  {
    "id": "04917",
    "manifest_path": "data/manifests/the_stack_sample/sample_2230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sloth\n  namespace: default\n  labels:\n    helm.sh/chart: sloth-0.1.0\n    app.kubernetes.io/managed-by: Helm\n    app: sloth\n    app.kubernetes.io/name: sloth\n    app.kubernetes.io/instance: sloth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sloth\n      app.kubernetes.io/name: sloth\n      app.kubernetes.io/instance: sloth\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: sloth-0.1.0\n        app.kubernetes.io/managed-by: Helm\n        app: sloth\n        app.kubernetes.io/name: sloth\n        app.kubernetes.io/instance: sloth\n      annotations:\n        kubectl.kubernetes.io/default-container: sloth\n    spec:\n      serviceAccountName: sloth\n      containers:\n      - name: sloth\n        image: slok/sloth:v0.6.0\n        args:\n        - kubernetes-controller\n        - --resync-interval=3m\n        - --sli-plugins-path=/plugins\n        ports:\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /plugins/sloth-common-sli-plugins\n        resources:\n          limits:\n            memory: 150Mi\n          requests:\n            cpu: 5m\n            memory: 75Mi\n      - name: git-sync-plugins\n        image: k8s.gcr.io/git-sync/git-sync:v3.3.4\n        args:\n        - --repo=https://github.com/slok/sloth-common-sli-plugins\n        - --branch=main\n        - --wait=30\n        - --webhook-url=http://localhost:8082/-/reload\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /tmp/git\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 5m\n            memory: 50Mi\n      volumes:\n      - name: sloth-common-sli-plugins\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"git-sync-plugins\" is not set to runAsNonRoot"
  },
  {
    "id": "04918",
    "manifest_path": "data/manifests/the_stack_sample/sample_2230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sloth\n  namespace: default\n  labels:\n    helm.sh/chart: sloth-0.1.0\n    app.kubernetes.io/managed-by: Helm\n    app: sloth\n    app.kubernetes.io/name: sloth\n    app.kubernetes.io/instance: sloth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sloth\n      app.kubernetes.io/name: sloth\n      app.kubernetes.io/instance: sloth\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: sloth-0.1.0\n        app.kubernetes.io/managed-by: Helm\n        app: sloth\n        app.kubernetes.io/name: sloth\n        app.kubernetes.io/instance: sloth\n      annotations:\n        kubectl.kubernetes.io/default-container: sloth\n    spec:\n      serviceAccountName: sloth\n      containers:\n      - name: sloth\n        image: slok/sloth:v0.6.0\n        args:\n        - kubernetes-controller\n        - --resync-interval=3m\n        - --sli-plugins-path=/plugins\n        ports:\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /plugins/sloth-common-sli-plugins\n        resources:\n          limits:\n            memory: 150Mi\n          requests:\n            cpu: 5m\n            memory: 75Mi\n      - name: git-sync-plugins\n        image: k8s.gcr.io/git-sync/git-sync:v3.3.4\n        args:\n        - --repo=https://github.com/slok/sloth-common-sli-plugins\n        - --branch=main\n        - --wait=30\n        - --webhook-url=http://localhost:8082/-/reload\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /tmp/git\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 5m\n            memory: 50Mi\n      volumes:\n      - name: sloth-common-sli-plugins\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sloth\" is not set to runAsNonRoot"
  },
  {
    "id": "04919",
    "manifest_path": "data/manifests/the_stack_sample/sample_2232.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: selenium-sessions\n  namespace: selenium\n  labels:\n    app: selenium-sessions\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: selenium-sessions\n  template:\n    metadata:\n      labels:\n        app: selenium-sessions\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cloud.google.com/gke-preemptible\n                operator: DoesNotExist\n              - key: eks.amazonaws.com/capacityType\n                operator: NotIn\n                values:\n                - SPOT\n              - key: kubernetes.azure.com/scalesetpriority\n                operator: NotIn\n                values:\n                - spot\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - name: selenium-sessions\n        image: selenium/sessions:4.0.0\n        ports:\n        - containerPort: 5556\n        env:\n        - name: JAVA_OPTS\n          value: -Xmx512m\n        - name: SE_EVENT_BUS_HOST\n          value: selenium-event-bus\n        - name: SE_EVENT_BUS_PUBLISH_PORT\n          value: '4442'\n        - name: SE_EVENT_BUS_SUBSCRIBE_PORT\n          value: '4443'\n        readinessProbe:\n          tcpSocket:\n            port: 5556\n          initialDelaySeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: 5556\n          initialDelaySeconds: 30\n        resources:\n          limits:\n            cpu: 2\n            memory: 1Gi\n          requests:\n            cpu: 100m\n            memory: 600Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"selenium-sessions\" does not have a read-only root file system"
  },
  {
    "id": "04920",
    "manifest_path": "data/manifests/the_stack_sample/sample_2236.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 64d2b982dc80b5c65271f2b9ff8b94777e4a148eca3fed68a4eebb84f6342f24\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: darrendignam\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 0b85b745d799aa9f82651a8851b657ee809ef8f3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "04921",
    "manifest_path": "data/manifests/the_stack_sample/sample_2236.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 64d2b982dc80b5c65271f2b9ff8b94777e4a148eca3fed68a4eebb84f6342f24\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: darrendignam\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 0b85b745d799aa9f82651a8851b657ee809ef8f3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "04922",
    "manifest_path": "data/manifests/the_stack_sample/sample_2238.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tiger\n  namespace: tiger\nspec:\n  containers:\n  - image: alpine\n    name: main\n    args:\n    - /bin/sh\n    - -c\n    - sleep 60; touch /tmp/healthy; sleep 86400\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 65\n      periodSeconds: 5\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"main\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04923",
    "manifest_path": "data/manifests/the_stack_sample/sample_2238.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tiger\n  namespace: tiger\nspec:\n  containers:\n  - image: alpine\n    name: main\n    args:\n    - /bin/sh\n    - -c\n    - sleep 60; touch /tmp/healthy; sleep 86400\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 65\n      periodSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "04924",
    "manifest_path": "data/manifests/the_stack_sample/sample_2238.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tiger\n  namespace: tiger\nspec:\n  containers:\n  - image: alpine\n    name: main\n    args:\n    - /bin/sh\n    - -c\n    - sleep 60; touch /tmp/healthy; sleep 86400\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 65\n      periodSeconds: 5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "04925",
    "manifest_path": "data/manifests/the_stack_sample/sample_2238.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tiger\n  namespace: tiger\nspec:\n  containers:\n  - image: alpine\n    name: main\n    args:\n    - /bin/sh\n    - -c\n    - sleep 60; touch /tmp/healthy; sleep 86400\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 65\n      periodSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "04926",
    "manifest_path": "data/manifests/the_stack_sample/sample_2238.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tiger\n  namespace: tiger\nspec:\n  containers:\n  - image: alpine\n    name: main\n    args:\n    - /bin/sh\n    - -c\n    - sleep 60; touch /tmp/healthy; sleep 86400\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 65\n      periodSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "04927",
    "manifest_path": "data/manifests/the_stack_sample/sample_2239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tileserver-import\nspec:\n  template:\n    spec:\n      containers:\n      - image: greenhalos/tile-server-import:latest\n        name: tileserver-import\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tileserver-import\" is using an invalid container image, \"greenhalos/tile-server-import:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04928",
    "manifest_path": "data/manifests/the_stack_sample/sample_2239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tileserver-import\nspec:\n  template:\n    spec:\n      containers:\n      - image: greenhalos/tile-server-import:latest\n        name: tileserver-import\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tileserver-import\" does not have a read-only root file system"
  },
  {
    "id": "04929",
    "manifest_path": "data/manifests/the_stack_sample/sample_2239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tileserver-import\nspec:\n  template:\n    spec:\n      containers:\n      - image: greenhalos/tile-server-import:latest\n        name: tileserver-import\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tileserver-import\" is not set to runAsNonRoot"
  },
  {
    "id": "04930",
    "manifest_path": "data/manifests/the_stack_sample/sample_2239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tileserver-import\nspec:\n  template:\n    spec:\n      containers:\n      - image: greenhalos/tile-server-import:latest\n        name: tileserver-import\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tileserver-import\" has cpu request 0"
  },
  {
    "id": "04931",
    "manifest_path": "data/manifests/the_stack_sample/sample_2239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tileserver-import\nspec:\n  template:\n    spec:\n      containers:\n      - image: greenhalos/tile-server-import:latest\n        name: tileserver-import\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tileserver-import\" has memory limit 0"
  },
  {
    "id": "04932",
    "manifest_path": "data/manifests/the_stack_sample/sample_2241.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: esc\n  annotations:\n    version: 1.19.10\n  labels:\n    io.esc.service: web-server\n  name: web-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.esc.service: web-server\n  template:\n    metadata:\n      labels:\n        io.esc.service: web-server\n    spec:\n      containers:\n      - image: eu.gcr.io/zendphp-313619/esc-nginx\n        name: web-server\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n        resources: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web-server\" is using an invalid container image, \"eu.gcr.io/zendphp-313619/esc-nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04933",
    "manifest_path": "data/manifests/the_stack_sample/sample_2241.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: esc\n  annotations:\n    version: 1.19.10\n  labels:\n    io.esc.service: web-server\n  name: web-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.esc.service: web-server\n  template:\n    metadata:\n      labels:\n        io.esc.service: web-server\n    spec:\n      containers:\n      - image: eu.gcr.io/zendphp-313619/esc-nginx\n        name: web-server\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n        resources: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-server\" does not have a read-only root file system"
  },
  {
    "id": "04934",
    "manifest_path": "data/manifests/the_stack_sample/sample_2241.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: esc\n  annotations:\n    version: 1.19.10\n  labels:\n    io.esc.service: web-server\n  name: web-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.esc.service: web-server\n  template:\n    metadata:\n      labels:\n        io.esc.service: web-server\n    spec:\n      containers:\n      - image: eu.gcr.io/zendphp-313619/esc-nginx\n        name: web-server\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n        resources: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-server\" is not set to runAsNonRoot"
  },
  {
    "id": "04935",
    "manifest_path": "data/manifests/the_stack_sample/sample_2241.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: esc\n  annotations:\n    version: 1.19.10\n  labels:\n    io.esc.service: web-server\n  name: web-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.esc.service: web-server\n  template:\n    metadata:\n      labels:\n        io.esc.service: web-server\n    spec:\n      containers:\n      - image: eu.gcr.io/zendphp-313619/esc-nginx\n        name: web-server\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n        resources: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-server\" has cpu request 0"
  },
  {
    "id": "04936",
    "manifest_path": "data/manifests/the_stack_sample/sample_2241.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: esc\n  annotations:\n    version: 1.19.10\n  labels:\n    io.esc.service: web-server\n  name: web-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.esc.service: web-server\n  template:\n    metadata:\n      labels:\n        io.esc.service: web-server\n    spec:\n      containers:\n      - image: eu.gcr.io/zendphp-313619/esc-nginx\n        name: web-server\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n        resources: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web-server\" has memory limit 0"
  },
  {
    "id": "04937",
    "manifest_path": "data/manifests/the_stack_sample/sample_2242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-nuevo\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9.1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04938",
    "manifest_path": "data/manifests/the_stack_sample/sample_2242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-nuevo\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9.1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04939",
    "manifest_path": "data/manifests/the_stack_sample/sample_2242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-nuevo\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9.1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04940",
    "manifest_path": "data/manifests/the_stack_sample/sample_2242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-nuevo\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9.1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04941",
    "manifest_path": "data/manifests/the_stack_sample/sample_2244.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mysql-backup\n  namespace: mariadb-galera\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - env:\n          - name: MYSQL_ENV_DB_HOST\n            value: mariadb-galera\n          - name: MYSQL_ENV_DB_USER\n            value: root\n          - name: MYSQL_ENV_DB_PASS\n            valueFrom:\n              secretKeyRef:\n                key: mariadb-root-password\n                name: mariadb-galera\n                optional: false\n          image: cube8021/mysql-backups:v0.0.4\n          imagePullPolicy: IfNotPresent\n          name: mysqldump\n          volumeMounts:\n          - name: mysql-backup\n            mountPath: /mysqldump\n        volumes:\n        - name: mysql-backup\n          persistentVolumeClaim:\n            claimName: mysql-backup-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysqldump\" does not have a read-only root file system"
  },
  {
    "id": "04942",
    "manifest_path": "data/manifests/the_stack_sample/sample_2244.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mysql-backup\n  namespace: mariadb-galera\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - env:\n          - name: MYSQL_ENV_DB_HOST\n            value: mariadb-galera\n          - name: MYSQL_ENV_DB_USER\n            value: root\n          - name: MYSQL_ENV_DB_PASS\n            valueFrom:\n              secretKeyRef:\n                key: mariadb-root-password\n                name: mariadb-galera\n                optional: false\n          image: cube8021/mysql-backups:v0.0.4\n          imagePullPolicy: IfNotPresent\n          name: mysqldump\n          volumeMounts:\n          - name: mysql-backup\n            mountPath: /mysqldump\n        volumes:\n        - name: mysql-backup\n          persistentVolumeClaim:\n            claimName: mysql-backup-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysqldump\" is not set to runAsNonRoot"
  },
  {
    "id": "04943",
    "manifest_path": "data/manifests/the_stack_sample/sample_2244.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mysql-backup\n  namespace: mariadb-galera\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - env:\n          - name: MYSQL_ENV_DB_HOST\n            value: mariadb-galera\n          - name: MYSQL_ENV_DB_USER\n            value: root\n          - name: MYSQL_ENV_DB_PASS\n            valueFrom:\n              secretKeyRef:\n                key: mariadb-root-password\n                name: mariadb-galera\n                optional: false\n          image: cube8021/mysql-backups:v0.0.4\n          imagePullPolicy: IfNotPresent\n          name: mysqldump\n          volumeMounts:\n          - name: mysql-backup\n            mountPath: /mysqldump\n        volumes:\n        - name: mysql-backup\n          persistentVolumeClaim:\n            claimName: mysql-backup-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysqldump\" has cpu request 0"
  },
  {
    "id": "04944",
    "manifest_path": "data/manifests/the_stack_sample/sample_2244.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mysql-backup\n  namespace: mariadb-galera\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - env:\n          - name: MYSQL_ENV_DB_HOST\n            value: mariadb-galera\n          - name: MYSQL_ENV_DB_USER\n            value: root\n          - name: MYSQL_ENV_DB_PASS\n            valueFrom:\n              secretKeyRef:\n                key: mariadb-root-password\n                name: mariadb-galera\n                optional: false\n          image: cube8021/mysql-backups:v0.0.4\n          imagePullPolicy: IfNotPresent\n          name: mysqldump\n          volumeMounts:\n          - name: mysql-backup\n            mountPath: /mysqldump\n        volumes:\n        - name: mysql-backup\n          persistentVolumeClaim:\n            claimName: mysql-backup-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysqldump\" has memory limit 0"
  },
  {
    "id": "04945",
    "manifest_path": "data/manifests/the_stack_sample/sample_2249.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cra-boilerplate-front\n  labels:\n    app: cra-boilerplate\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: front\n  template:\n    metadata:\n      labels:\n        app: front\n    spec:\n      containers:\n      - name: cra-front\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cra-front\" does not have a read-only root file system"
  },
  {
    "id": "04946",
    "manifest_path": "data/manifests/the_stack_sample/sample_2249.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cra-boilerplate-front\n  labels:\n    app: cra-boilerplate\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: front\n  template:\n    metadata:\n      labels:\n        app: front\n    spec:\n      containers:\n      - name: cra-front\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cra-front\" is not set to runAsNonRoot"
  },
  {
    "id": "04947",
    "manifest_path": "data/manifests/the_stack_sample/sample_2249.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cra-boilerplate-front\n  labels:\n    app: cra-boilerplate\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: front\n  template:\n    metadata:\n      labels:\n        app: front\n    spec:\n      containers:\n      - name: cra-front\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cra-front\" has cpu request 0"
  },
  {
    "id": "04948",
    "manifest_path": "data/manifests/the_stack_sample/sample_2249.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cra-boilerplate-front\n  labels:\n    app: cra-boilerplate\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: front\n  template:\n    metadata:\n      labels:\n        app: front\n    spec:\n      containers:\n      - name: cra-front\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cra-front\" has memory limit 0"
  },
  {
    "id": "04949",
    "manifest_path": "data/manifests/the_stack_sample/sample_2251.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gruppenname-frontend\n  namespace: sachs\n  labels:\n    app: gruppenname-frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: gruppenname-frontend\n    spec:\n      containers:\n      - name: gruppenname-frontend\n        image: registry.datexis.com/ksachs/gruppenname-frontend\n        ports:\n        - name: client-port\n          containerPort: 8080\n  selector:\n    matchLabels:\n      app: gruppenname-frontend\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"gruppenname-frontend\" is using an invalid container image, \"registry.datexis.com/ksachs/gruppenname-frontend\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04950",
    "manifest_path": "data/manifests/the_stack_sample/sample_2251.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gruppenname-frontend\n  namespace: sachs\n  labels:\n    app: gruppenname-frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: gruppenname-frontend\n    spec:\n      containers:\n      - name: gruppenname-frontend\n        image: registry.datexis.com/ksachs/gruppenname-frontend\n        ports:\n        - name: client-port\n          containerPort: 8080\n  selector:\n    matchLabels:\n      app: gruppenname-frontend\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gruppenname-frontend\" does not have a read-only root file system"
  },
  {
    "id": "04951",
    "manifest_path": "data/manifests/the_stack_sample/sample_2251.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gruppenname-frontend\n  namespace: sachs\n  labels:\n    app: gruppenname-frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: gruppenname-frontend\n    spec:\n      containers:\n      - name: gruppenname-frontend\n        image: registry.datexis.com/ksachs/gruppenname-frontend\n        ports:\n        - name: client-port\n          containerPort: 8080\n  selector:\n    matchLabels:\n      app: gruppenname-frontend\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gruppenname-frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "04952",
    "manifest_path": "data/manifests/the_stack_sample/sample_2251.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gruppenname-frontend\n  namespace: sachs\n  labels:\n    app: gruppenname-frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: gruppenname-frontend\n    spec:\n      containers:\n      - name: gruppenname-frontend\n        image: registry.datexis.com/ksachs/gruppenname-frontend\n        ports:\n        - name: client-port\n          containerPort: 8080\n  selector:\n    matchLabels:\n      app: gruppenname-frontend\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gruppenname-frontend\" has cpu request 0"
  },
  {
    "id": "04953",
    "manifest_path": "data/manifests/the_stack_sample/sample_2251.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gruppenname-frontend\n  namespace: sachs\n  labels:\n    app: gruppenname-frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: gruppenname-frontend\n    spec:\n      containers:\n      - name: gruppenname-frontend\n        image: registry.datexis.com/ksachs/gruppenname-frontend\n        ports:\n        - name: client-port\n          containerPort: 8080\n  selector:\n    matchLabels:\n      app: gruppenname-frontend\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gruppenname-frontend\" has memory limit 0"
  },
  {
    "id": "04954",
    "manifest_path": "data/manifests/the_stack_sample/sample_2252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.14.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jaeger-operator\" does not have a read-only root file system"
  },
  {
    "id": "04955",
    "manifest_path": "data/manifests/the_stack_sample/sample_2252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.14.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jaeger-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "04956",
    "manifest_path": "data/manifests/the_stack_sample/sample_2252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.14.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jaeger-operator\" has cpu request 0"
  },
  {
    "id": "04957",
    "manifest_path": "data/manifests/the_stack_sample/sample_2252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.14.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jaeger-operator\" has memory limit 0"
  },
  {
    "id": "04958",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"airbyte-scheduler-container\" is using an invalid container image, \"airbyte/scheduler\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04959",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"airbyte-seed\" is using an invalid container image, \"airbyte/seed\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04960",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"airbyte-scheduler-container\" does not have a read-only root file system"
  },
  {
    "id": "04961",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"airbyte-seed\" does not have a read-only root file system"
  },
  {
    "id": "04962",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"airbyte-scheduler-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04963",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"airbyte-seed\" is not set to runAsNonRoot"
  },
  {
    "id": "04964",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"airbyte-scheduler-container\" has cpu request 0"
  },
  {
    "id": "04965",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"airbyte-seed\" has cpu request 0"
  },
  {
    "id": "04966",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"airbyte-scheduler-container\" has memory limit 0"
  },
  {
    "id": "04967",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"airbyte-seed\" has memory limit 0"
  },
  {
    "id": "04968",
    "manifest_path": "data/manifests/the_stack_sample/sample_2258.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slack-post-message\n  labels:\n    app: slack-post-message\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: slack-post-message\n  template:\n    metadata:\n      labels:\n        app: slack-post-message\n    spec:\n      containers:\n      - name: slack-post-message\n        image: gcr.io/k8s-staging-slack-infra/slack-post-message:v20200901-117c06f\n        args:\n        - --config-path=/etc/slack-post-message/config.json\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: PATH_PREFIX\n          value: /infra/post-message\n        volumeMounts:\n        - mountPath: /etc/slack-post-message\n          name: config\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTP\n            port: 8080\n      volumes:\n      - name: config\n        secret:\n          secretName: slack-post-message-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"slack-post-message\" does not have a read-only root file system"
  },
  {
    "id": "04969",
    "manifest_path": "data/manifests/the_stack_sample/sample_2258.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slack-post-message\n  labels:\n    app: slack-post-message\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: slack-post-message\n  template:\n    metadata:\n      labels:\n        app: slack-post-message\n    spec:\n      containers:\n      - name: slack-post-message\n        image: gcr.io/k8s-staging-slack-infra/slack-post-message:v20200901-117c06f\n        args:\n        - --config-path=/etc/slack-post-message/config.json\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: PATH_PREFIX\n          value: /infra/post-message\n        volumeMounts:\n        - mountPath: /etc/slack-post-message\n          name: config\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTP\n            port: 8080\n      volumes:\n      - name: config\n        secret:\n          secretName: slack-post-message-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"slack-post-message\" is not set to runAsNonRoot"
  },
  {
    "id": "04970",
    "manifest_path": "data/manifests/the_stack_sample/sample_2258.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slack-post-message\n  labels:\n    app: slack-post-message\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: slack-post-message\n  template:\n    metadata:\n      labels:\n        app: slack-post-message\n    spec:\n      containers:\n      - name: slack-post-message\n        image: gcr.io/k8s-staging-slack-infra/slack-post-message:v20200901-117c06f\n        args:\n        - --config-path=/etc/slack-post-message/config.json\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: PATH_PREFIX\n          value: /infra/post-message\n        volumeMounts:\n        - mountPath: /etc/slack-post-message\n          name: config\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTP\n            port: 8080\n      volumes:\n      - name: config\n        secret:\n          secretName: slack-post-message-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"slack-post-message\" has cpu request 0"
  },
  {
    "id": "04971",
    "manifest_path": "data/manifests/the_stack_sample/sample_2258.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slack-post-message\n  labels:\n    app: slack-post-message\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: slack-post-message\n  template:\n    metadata:\n      labels:\n        app: slack-post-message\n    spec:\n      containers:\n      - name: slack-post-message\n        image: gcr.io/k8s-staging-slack-infra/slack-post-message:v20200901-117c06f\n        args:\n        - --config-path=/etc/slack-post-message/config.json\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: PATH_PREFIX\n          value: /infra/post-message\n        volumeMounts:\n        - mountPath: /etc/slack-post-message\n          name: config\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTP\n            port: 8080\n      volumes:\n      - name: config\n        secret:\n          secretName: slack-post-message-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"slack-post-message\" has memory limit 0"
  },
  {
    "id": "04972",
    "manifest_path": "data/manifests/the_stack_sample/sample_2259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: scc-broker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: scc-broker\n  template:\n    metadata:\n      labels:\n        component: scc-broker\n    spec:\n      containers:\n      - name: scc-broker\n        image: socketcluster/scc-broker:v6.0.1\n        ports:\n        - containerPort: 8888\n        env:\n        - name: SCC_STATE_SERVER_HOST\n          value: scc-state\n        - name: SOCKETCLUSTER_WORKERS\n          value: '1'\n        - name: SOCKETCLUSTER_BROKERS\n          value: '1'\n        - name: SCC_INSTANCE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SCC_BROKER_SERVER_LOG_LEVEL\n          value: '2'\n        livenessProbe:\n          httpGet:\n            path: /health-check\n            port: 8888\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"scc-broker\" does not have a read-only root file system"
  },
  {
    "id": "04973",
    "manifest_path": "data/manifests/the_stack_sample/sample_2259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: scc-broker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: scc-broker\n  template:\n    metadata:\n      labels:\n        component: scc-broker\n    spec:\n      containers:\n      - name: scc-broker\n        image: socketcluster/scc-broker:v6.0.1\n        ports:\n        - containerPort: 8888\n        env:\n        - name: SCC_STATE_SERVER_HOST\n          value: scc-state\n        - name: SOCKETCLUSTER_WORKERS\n          value: '1'\n        - name: SOCKETCLUSTER_BROKERS\n          value: '1'\n        - name: SCC_INSTANCE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SCC_BROKER_SERVER_LOG_LEVEL\n          value: '2'\n        livenessProbe:\n          httpGet:\n            path: /health-check\n            port: 8888\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"scc-broker\" is not set to runAsNonRoot"
  },
  {
    "id": "04974",
    "manifest_path": "data/manifests/the_stack_sample/sample_2259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: scc-broker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: scc-broker\n  template:\n    metadata:\n      labels:\n        component: scc-broker\n    spec:\n      containers:\n      - name: scc-broker\n        image: socketcluster/scc-broker:v6.0.1\n        ports:\n        - containerPort: 8888\n        env:\n        - name: SCC_STATE_SERVER_HOST\n          value: scc-state\n        - name: SOCKETCLUSTER_WORKERS\n          value: '1'\n        - name: SOCKETCLUSTER_BROKERS\n          value: '1'\n        - name: SCC_INSTANCE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SCC_BROKER_SERVER_LOG_LEVEL\n          value: '2'\n        livenessProbe:\n          httpGet:\n            path: /health-check\n            port: 8888\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"scc-broker\" has cpu request 0"
  },
  {
    "id": "04975",
    "manifest_path": "data/manifests/the_stack_sample/sample_2259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: scc-broker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: scc-broker\n  template:\n    metadata:\n      labels:\n        component: scc-broker\n    spec:\n      containers:\n      - name: scc-broker\n        image: socketcluster/scc-broker:v6.0.1\n        ports:\n        - containerPort: 8888\n        env:\n        - name: SCC_STATE_SERVER_HOST\n          value: scc-state\n        - name: SOCKETCLUSTER_WORKERS\n          value: '1'\n        - name: SOCKETCLUSTER_BROKERS\n          value: '1'\n        - name: SCC_INSTANCE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SCC_BROKER_SERVER_LOG_LEVEL\n          value: '2'\n        livenessProbe:\n          httpGet:\n            path: /health-check\n            port: 8888\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"scc-broker\" has memory limit 0"
  },
  {
    "id": "04976",
    "manifest_path": "data/manifests/the_stack_sample/sample_2261.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210602-c9da972437\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "04977",
    "manifest_path": "data/manifests/the_stack_sample/sample_2261.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210602-c9da972437\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "04978",
    "manifest_path": "data/manifests/the_stack_sample/sample_2261.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210602-c9da972437\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "04979",
    "manifest_path": "data/manifests/the_stack_sample/sample_2261.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210602-c9da972437\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "04980",
    "manifest_path": "data/manifests/the_stack_sample/sample_2267.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: od-v1\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: od\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: od\n        version: v1\n        env: production\n    spec:\n      containers:\n      - name: od\n        image: gcr.io/instructor-partition/od-fasterrcnn\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"od\" is using an invalid container image, \"gcr.io/instructor-partition/od-fasterrcnn\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04981",
    "manifest_path": "data/manifests/the_stack_sample/sample_2267.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: od-v1\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: od\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: od\n        version: v1\n        env: production\n    spec:\n      containers:\n      - name: od\n        image: gcr.io/instructor-partition/od-fasterrcnn\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"od\" does not have a read-only root file system"
  },
  {
    "id": "04982",
    "manifest_path": "data/manifests/the_stack_sample/sample_2267.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: od-v1\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: od\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: od\n        version: v1\n        env: production\n    spec:\n      containers:\n      - name: od\n        image: gcr.io/instructor-partition/od-fasterrcnn\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"od\" is not set to runAsNonRoot"
  },
  {
    "id": "04983",
    "manifest_path": "data/manifests/the_stack_sample/sample_2267.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: od-v1\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: od\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: od\n        version: v1\n        env: production\n    spec:\n      containers:\n      - name: od\n        image: gcr.io/instructor-partition/od-fasterrcnn\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"od\" has cpu request 0"
  },
  {
    "id": "04984",
    "manifest_path": "data/manifests/the_stack_sample/sample_2267.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: od-v1\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: od\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: od\n        version: v1\n        env: production\n    spec:\n      containers:\n      - name: od\n        image: gcr.io/instructor-partition/od-fasterrcnn\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"od\" has memory limit 0"
  },
  {
    "id": "04985",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"service\" is using an invalid container image, \"ghcr.io/drogue-iot/user-auth-service:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04986",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wait-for-client-secret\" is using an invalid container image, \"registry.access.redhat.com/ubi8-minimal\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04987",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"service\" does not have a read-only root file system"
  },
  {
    "id": "04988",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-for-client-secret\" does not have a read-only root file system"
  },
  {
    "id": "04989",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"service\" is not set to runAsNonRoot"
  },
  {
    "id": "04990",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-for-client-secret\" is not set to runAsNonRoot"
  },
  {
    "id": "04991",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"service\" has cpu request 0"
  },
  {
    "id": "04992",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-for-client-secret\" has cpu request 0"
  },
  {
    "id": "04993",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-for-client-secret\" has memory limit 0"
  },
  {
    "id": "04994",
    "manifest_path": "data/manifests/the_stack_sample/sample_2274.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4427\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04995",
    "manifest_path": "data/manifests/the_stack_sample/sample_2274.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4427\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04996",
    "manifest_path": "data/manifests/the_stack_sample/sample_2274.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4427\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04997",
    "manifest_path": "data/manifests/the_stack_sample/sample_2274.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4427\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04998",
    "manifest_path": "data/manifests/the_stack_sample/sample_2274.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4427\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04999",
    "manifest_path": "data/manifests/the_stack_sample/sample_2275.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-keycloak-integration\n  namespace: sso-integration\nspec:\n  template:\n    spec:\n      containers:\n      - name: cluster-keycloak-integration\n        image: quay.io/leoliu2011/cluster-keycloak-integration:v1\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: argocd-configs\n        - secretRef:\n            name: sso-configs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cluster-keycloak-integration\" does not have a read-only root file system"
  },
  {
    "id": "05000",
    "manifest_path": "data/manifests/the_stack_sample/sample_2275.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-keycloak-integration\n  namespace: sso-integration\nspec:\n  template:\n    spec:\n      containers:\n      - name: cluster-keycloak-integration\n        image: quay.io/leoliu2011/cluster-keycloak-integration:v1\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: argocd-configs\n        - secretRef:\n            name: sso-configs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cluster-keycloak-integration\" is not set to runAsNonRoot"
  }
]